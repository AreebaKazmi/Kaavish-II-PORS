{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZhSsDYSf2L20",
    "colab_type": "code",
    "outputId": "25c0f9ec-d660-4c9f-ada3-5d5b23b82e82",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1589192274956,
     "user_tz": -300,
     "elapsed": 1110,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# os.chdir('/content/drive/My Drive/Deep Fashion Retrieval/deep-fashion-retrieval')\n",
    "# os.chdir(\"/home/ma02526/ResNet/deep-fashion-retrieval\")"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-JtZQgArAF9",
    "colab_type": "text"
   },
   "source": [
    "# Training with 26 most relevant categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tClassification Loss: 3.9848\r\n",
      "train.py:178: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9490\r\n",
      "Top 1 Accuracy: 1108/80000 (1%)\r\n",
      "Top 3 Accuracy: 3293/80000 (4%)\r\n",
      "Top 5 Accuracy: 6107/80000 (8%)\r\n",
      " \r\n",
      "Train Epoch: 1 [6400/209222 (3%)]\tClassification Loss: 2.2343\r\n",
      "Train Epoch: 1 [12800/209222 (6%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 1 [19200/209222 (9%)]\tClassification Loss: 1.9574\r\n",
      "Train Epoch: 1 [25600/209222 (12%)]\tClassification Loss: 1.7489\r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tClassification Loss: 1.8780\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [38400/209222 (18%)]\tClassification Loss: 2.0246\r\n",
      "Train Epoch: 1 [44800/209222 (21%)]\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 1 [51200/209222 (24%)]\tClassification Loss: 1.6724\r\n",
      "Train Epoch: 1 [57600/209222 (28%)]\tClassification Loss: 1.6888\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tClassification Loss: 1.5357\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [70400/209222 (34%)]\tClassification Loss: 1.3968\r\n",
      "Train Epoch: 1 [76800/209222 (37%)]\tClassification Loss: 1.6153\r\n",
      "Train Epoch: 1 [83200/209222 (40%)]\tClassification Loss: 1.7009\r\n",
      "Train Epoch: 1 [89600/209222 (43%)]\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tClassification Loss: 1.5827\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [102400/209222 (49%)]\tClassification Loss: 1.6751\r\n",
      "Train Epoch: 1 [108800/209222 (52%)]\tClassification Loss: 1.4357\r\n",
      "\r\n",
      "Test set: Average loss: 1.3943\r\n",
      "Top 1 Accuracy: 47677/80000 (60%)\r\n",
      "Top 3 Accuracy: 65045/80000 (81%)\r\n",
      "Top 5 Accuracy: 71398/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 1 [115200/209222 (55%)]\tClassification Loss: 1.7770\r\n",
      "Train Epoch: 1 [121600/209222 (58%)]\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tClassification Loss: 1.5969\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_2000.pth.tar\r\n",
      "Train Epoch: 1 [134400/209222 (64%)]\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 1 [140800/209222 (67%)]\tClassification Loss: 1.3437\r\n",
      "Train Epoch: 1 [147200/209222 (70%)]\tClassification Loss: 1.5423\r\n",
      "Train Epoch: 1 [153600/209222 (73%)]\tClassification Loss: 1.6283\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tClassification Loss: 1.7851\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_2500.pth.tar\r\n",
      "Train Epoch: 1 [166400/209222 (80%)]\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 1 [172800/209222 (83%)]\tClassification Loss: 1.3214\r\n",
      "Train Epoch: 1 [179200/209222 (86%)]\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 1 [185600/209222 (89%)]\tClassification Loss: 1.3464\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tClassification Loss: 1.4946\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uoTUVg6grWZ0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "outputId": "e68b2245-67c8-4f72-d1e2-4e9b1aa60bf4"
   },
   "source": [
    "# Freeze=True. LR=0.01. In-shop=False\n",
    "! python train.py"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/110534 (0%)]\tClassification Loss: 3.2551\r\n",
      "Test() called at step_no: 0\r\n",
      "train.py:163: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.4344, Accuracy: 20/1920 (1%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/110534 (1%)]\tClassification Loss: 2.9939\r\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tClassification Loss: 2.8167\r\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tClassification Loss: 2.7070\r\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tClassification Loss: 2.6303\r\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tClassification Loss: 2.5271\r\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tClassification Loss: 2.5832\r\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tClassification Loss: 2.6724\r\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tClassification Loss: 2.6078\r\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tClassification Loss: 2.5708\r\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tClassification Loss: 2.3925\r\n",
      "Test() called at step_no: 100\r\n",
      "\r\n",
      "Test set: Average loss: 2.5902, Accuracy: 511/1920 (27%)\r\n",
      "\r\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tClassification Loss: 2.5046\r\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tClassification Loss: 2.5380\r\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tClassification Loss: 2.3953\r\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tClassification Loss: 2.4406\r\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tClassification Loss: 2.3945\r\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tClassification Loss: 2.3660\r\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tClassification Loss: 2.4298\r\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tClassification Loss: 2.4177\r\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tClassification Loss: 2.4805\r\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tClassification Loss: 2.3700\r\n",
      "Test() called at step_no: 200\r\n",
      "\r\n",
      "Test set: Average loss: 2.4417, Accuracy: 547/1920 (28%)\r\n",
      "\r\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tClassification Loss: 2.2498\r\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tClassification Loss: 2.1636\r\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tClassification Loss: 2.2703\r\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tClassification Loss: 2.4436\r\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tClassification Loss: 2.3448\r\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tClassification Loss: 2.3144\r\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tClassification Loss: 2.3612\r\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tClassification Loss: 2.1631\r\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tClassification Loss: 2.2082\r\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tClassification Loss: 2.2089\r\n",
      "Test() called at step_no: 300\r\n",
      "\r\n",
      "Test set: Average loss: 2.3385, Accuracy: 601/1920 (31%)\r\n",
      "\r\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tClassification Loss: 2.2307\r\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tClassification Loss: 2.1508\r\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tClassification Loss: 2.1002\r\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tClassification Loss: 2.1807\r\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tClassification Loss: 2.2636\r\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tClassification Loss: 2.1108\r\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tClassification Loss: 2.1969\r\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tClassification Loss: 2.1099\r\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tClassification Loss: 1.8936\r\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tClassification Loss: 2.4549\r\n",
      "Test() called at step_no: 400\r\n",
      "\r\n",
      "Test set: Average loss: 2.2552, Accuracy: 733/1920 (38%)\r\n",
      "\r\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tClassification Loss: 2.1399\r\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tClassification Loss: 2.1464\r\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tClassification Loss: 1.8899\r\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tClassification Loss: 2.0277\r\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tClassification Loss: 2.2300\r\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tClassification Loss: 2.1036\r\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tClassification Loss: 2.0996\r\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tClassification Loss: 2.0948\r\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tClassification Loss: 2.0692\r\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tClassification Loss: 2.1721\r\n",
      "Test() called at step_no: 500\r\n",
      "\r\n",
      "Test set: Average loss: 2.1870, Accuracy: 828/1920 (43%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tClassification Loss: 2.0384\r\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tClassification Loss: 1.8231\r\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tClassification Loss: 2.1738\r\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tClassification Loss: 2.1805\r\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tClassification Loss: 2.0349\r\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tClassification Loss: 1.8516\r\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tClassification Loss: 1.9630\r\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tClassification Loss: 2.0086\r\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tClassification Loss: 1.9877\r\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tClassification Loss: 2.1610\r\n",
      "Test() called at step_no: 600\r\n",
      "\r\n",
      "Test set: Average loss: 2.1245, Accuracy: 855/1920 (45%)\r\n",
      "\r\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tClassification Loss: 1.8356\r\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tClassification Loss: 2.2976\r\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tClassification Loss: 2.0719\r\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tClassification Loss: 1.8911\r\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tClassification Loss: 2.1227\r\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tClassification Loss: 1.9395\r\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tClassification Loss: 1.9431\r\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tClassification Loss: 2.0866\r\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tClassification Loss: 1.9226\r\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tClassification Loss: 1.8732\r\n",
      "Test() called at step_no: 700\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "\r\n",
      "Test set: Average loss: 2.0742, Accuracy: 907/1920 (47%)\r\n",
      "\r\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tClassification Loss: 2.1108\r\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tClassification Loss: 1.8534\r\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tClassification Loss: 2.1422\r\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tClassification Loss: 2.1509\r\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tClassification Loss: 1.9987\r\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tClassification Loss: 1.9383\r\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tClassification Loss: 1.6299\r\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tClassification Loss: 1.8168\r\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tClassification Loss: 2.0715\r\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tClassification Loss: 1.9643\r\n",
      "Test() called at step_no: 800\r\n",
      "\r\n",
      "Test set: Average loss: 2.0302, Accuracy: 915/1920 (48%)\r\n",
      "\r\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tClassification Loss: 1.9027\r\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tClassification Loss: 2.0473\r\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tClassification Loss: 1.7538\r\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tClassification Loss: 2.1074\r\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tClassification Loss: 1.7861\r\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tClassification Loss: 2.1171\r\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tClassification Loss: 1.7808\r\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tClassification Loss: 1.8689\r\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tClassification Loss: 1.9704\r\n",
      "Test() called at step_no: 900\r\n",
      "\r\n",
      "Test set: Average loss: 1.9941, Accuracy: 941/1920 (49%)\r\n",
      "\r\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tClassification Loss: 1.7775\r\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tClassification Loss: 2.0835\r\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tClassification Loss: 1.9822\r\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tClassification Loss: 1.9752\r\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tClassification Loss: 2.3291\r\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tClassification Loss: 1.8478\r\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tClassification Loss: 2.2193\r\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tClassification Loss: 1.9576\r\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tClassification Loss: 2.0516\r\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tClassification Loss: 1.9364\r\n",
      "Test() called at step_no: 1000\r\n",
      "\r\n",
      "Test set: Average loss: 1.9617, Accuracy: 975/1920 (51%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tClassification Loss: 1.9434\r\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tClassification Loss: 1.7191\r\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tClassification Loss: 1.8294\r\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tClassification Loss: 2.0352\r\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tClassification Loss: 1.6578\r\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tClassification Loss: 1.7404\r\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tClassification Loss: 1.7488\r\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tClassification Loss: 1.8071\r\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tClassification Loss: 1.9007\r\n",
      "Test() called at step_no: 1100\r\n",
      "\r\n",
      "Test set: Average loss: 1.9393, Accuracy: 955/1920 (50%)\r\n",
      "\r\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tClassification Loss: 1.8689\r\n",
      "Train Epoch: 1 [71680/110534 (65%)]\tClassification Loss: 1.6614\r\n",
      "Train Epoch: 1 [72320/110534 (65%)]\tClassification Loss: 1.7960\r\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 1 [73600/110534 (67%)]\tClassification Loss: 1.8273\r\n",
      "Train Epoch: 1 [74240/110534 (67%)]\tClassification Loss: 1.9240\r\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tClassification Loss: 1.7521\r\n",
      "Train Epoch: 1 [75520/110534 (68%)]\tClassification Loss: 1.9369\r\n",
      "Train Epoch: 1 [76160/110534 (69%)]\tClassification Loss: 1.8471\r\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tClassification Loss: 1.7909\r\n",
      "Test() called at step_no: 1200\r\n",
      "\r\n",
      "Test set: Average loss: 1.9071, Accuracy: 971/1920 (51%)\r\n",
      "\r\n",
      "Train Epoch: 1 [77440/110534 (70%)]\tClassification Loss: 2.2319\r\n",
      "Train Epoch: 1 [78080/110534 (71%)]\tClassification Loss: 1.9769\r\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tClassification Loss: 1.9073\r\n",
      "Train Epoch: 1 [79360/110534 (72%)]\tClassification Loss: 2.0520\r\n",
      "Train Epoch: 1 [80000/110534 (72%)]\tClassification Loss: 2.0125\r\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tClassification Loss: 1.8970\r\n",
      "Train Epoch: 1 [81280/110534 (74%)]\tClassification Loss: 2.1608\r\n",
      "Train Epoch: 1 [81920/110534 (74%)]\tClassification Loss: 1.6204\r\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tClassification Loss: 2.0641\r\n",
      "Train Epoch: 1 [83200/110534 (75%)]\tClassification Loss: 1.5953\r\n",
      "Test() called at step_no: 1300\r\n",
      "\r\n",
      "Test set: Average loss: 1.8877, Accuracy: 967/1920 (50%)\r\n",
      "\r\n",
      "Train Epoch: 1 [83840/110534 (76%)]\tClassification Loss: 2.0409\r\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tClassification Loss: 1.8261\r\n",
      "Train Epoch: 1 [85120/110534 (77%)]\tClassification Loss: 1.8317\r\n",
      "Train Epoch: 1 [85760/110534 (78%)]\tClassification Loss: 2.0998\r\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tClassification Loss: 1.9269\r\n",
      "Train Epoch: 1 [87040/110534 (79%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 1 [87680/110534 (79%)]\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tClassification Loss: 1.8076\r\n",
      "Train Epoch: 1 [88960/110534 (80%)]\tClassification Loss: 1.5897\r\n",
      "Train Epoch: 1 [89600/110534 (81%)]\tClassification Loss: 1.7628\r\n",
      "Test() called at step_no: 1400\r\n",
      "\r\n",
      "Test set: Average loss: 1.8677, Accuracy: 977/1920 (51%)\r\n",
      "\r\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tClassification Loss: 1.8988\r\n",
      "Train Epoch: 1 [90880/110534 (82%)]\tClassification Loss: 2.2512\r\n",
      "Train Epoch: 1 [91520/110534 (83%)]\tClassification Loss: 1.5833\r\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tClassification Loss: 1.8878\r\n",
      "Train Epoch: 1 [92800/110534 (84%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 1 [93440/110534 (85%)]\tClassification Loss: 1.7095\r\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tClassification Loss: 1.9480\r\n",
      "Train Epoch: 1 [94720/110534 (86%)]\tClassification Loss: 1.7390\r\n",
      "Train Epoch: 1 [95360/110534 (86%)]\tClassification Loss: 1.7180\r\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tClassification Loss: 1.6105\r\n",
      "Test() called at step_no: 1500\r\n",
      "\r\n",
      "Test set: Average loss: 1.8500, Accuracy: 970/1920 (51%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [96640/110534 (87%)]\tClassification Loss: 1.9181\r\n",
      "Train Epoch: 1 [97280/110534 (88%)]\tClassification Loss: 1.6656\r\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tClassification Loss: 1.8339\r\n",
      "Train Epoch: 1 [98560/110534 (89%)]\tClassification Loss: 1.8624\r\n",
      "Train Epoch: 1 [99200/110534 (90%)]\tClassification Loss: 1.8666\r\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tClassification Loss: 1.8527\r\n",
      "Train Epoch: 1 [100480/110534 (91%)]\tClassification Loss: 1.7926\r\n",
      "Train Epoch: 1 [101120/110534 (91%)]\tClassification Loss: 1.8798\r\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 1 [102400/110534 (93%)]\tClassification Loss: 1.7350\r\n",
      "Test() called at step_no: 1600\r\n",
      "\r\n",
      "Test set: Average loss: 1.8343, Accuracy: 995/1920 (52%)\r\n",
      "\r\n",
      "Train Epoch: 1 [103040/110534 (93%)]\tClassification Loss: 1.9959\r\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tClassification Loss: 1.6528\r\n",
      "Train Epoch: 1 [104320/110534 (94%)]\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 1 [104960/110534 (95%)]\tClassification Loss: 1.7784\r\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tClassification Loss: 1.6264\r\n",
      "Train Epoch: 1 [106240/110534 (96%)]\tClassification Loss: 1.8381\r\n",
      "Train Epoch: 1 [106880/110534 (97%)]\tClassification Loss: 1.5564\r\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tClassification Loss: 1.7385\r\n",
      "Train Epoch: 1 [108160/110534 (98%)]\tClassification Loss: 1.7784\r\n",
      "Train Epoch: 1 [108800/110534 (98%)]\tClassification Loss: 1.7989\r\n",
      "Test() called at step_no: 1700\r\n",
      "\r\n",
      "Test set: Average loss: 1.8208, Accuracy: 996/1920 (52%)\r\n",
      "\r\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tClassification Loss: 1.9371\r\n",
      "Train Epoch: 1 [110080/110534 (100%)]\tClassification Loss: 1.7945\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/110534 (0%)]\tClassification Loss: 1.9682\r\n",
      "Test() called at step_no: 1727\r\n",
      "\r\n",
      "Test set: Average loss: 1.8192, Accuracy: 1002/1920 (52%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/110534 (1%)]\tClassification Loss: 1.7681\r\n",
      "Train Epoch: 2 [1280/110534 (1%)]\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tClassification Loss: 1.6789\r\n",
      "Train Epoch: 2 [2560/110534 (2%)]\tClassification Loss: 1.7281\r\n",
      "Train Epoch: 2 [3200/110534 (3%)]\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tClassification Loss: 1.7880\r\n",
      "Train Epoch: 2 [4480/110534 (4%)]\tClassification Loss: 2.0945\r\n",
      "Train Epoch: 2 [5120/110534 (5%)]\tClassification Loss: 1.8115\r\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tClassification Loss: 1.8072\r\n",
      "Train Epoch: 2 [6400/110534 (6%)]\tClassification Loss: 1.7863\r\n",
      "Test() called at step_no: 1827\r\n",
      "\r\n",
      "Test set: Average loss: 1.7992, Accuracy: 992/1920 (52%)\r\n",
      "\r\n",
      "Train Epoch: 2 [7040/110534 (6%)]\tClassification Loss: 1.9719\r\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tClassification Loss: 1.8087\r\n",
      "Train Epoch: 2 [8320/110534 (8%)]\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 2 [8960/110534 (8%)]\tClassification Loss: 1.8655\r\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tClassification Loss: 1.8182\r\n",
      "Train Epoch: 2 [10240/110534 (9%)]\tClassification Loss: 1.8352\r\n",
      "Train Epoch: 2 [10880/110534 (10%)]\tClassification Loss: 1.8640\r\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tClassification Loss: 1.8821\r\n",
      "Train Epoch: 2 [12160/110534 (11%)]\tClassification Loss: 1.8865\r\n",
      "Train Epoch: 2 [12800/110534 (12%)]\tClassification Loss: 2.0242\r\n",
      "Test() called at step_no: 1927\r\n",
      "\r\n",
      "Test set: Average loss: 1.7912, Accuracy: 983/1920 (51%)\r\n",
      "\r\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tClassification Loss: 1.9130\r\n",
      "Train Epoch: 2 [14080/110534 (13%)]\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 2 [14720/110534 (13%)]\tClassification Loss: 1.7410\r\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tClassification Loss: 1.8744\r\n",
      "Train Epoch: 2 [16000/110534 (14%)]\tClassification Loss: 2.1016\r\n",
      "Train Epoch: 2 [16640/110534 (15%)]\tClassification Loss: 1.8358\r\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tClassification Loss: 1.9986\r\n",
      "Train Epoch: 2 [17920/110534 (16%)]\tClassification Loss: 1.6860\r\n",
      "Train Epoch: 2 [18560/110534 (17%)]\tClassification Loss: 1.9345\r\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tClassification Loss: 1.8236\r\n",
      "Test() called at step_no: 2027\r\n",
      "\r\n",
      "Test set: Average loss: 1.7783, Accuracy: 1006/1920 (52%)\r\n",
      "\r\n",
      "Train Epoch: 2 [19840/110534 (18%)]\tClassification Loss: 1.8794\r\n",
      "Train Epoch: 2 [20480/110534 (19%)]\tClassification Loss: 1.7740\r\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tClassification Loss: 1.7314\r\n",
      "Train Epoch: 2 [21760/110534 (20%)]\tClassification Loss: 1.7789\r\n",
      "Train Epoch: 2 [22400/110534 (20%)]\tClassification Loss: 2.0558\r\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tClassification Loss: 1.8238\r\n",
      "Train Epoch: 2 [23680/110534 (21%)]\tClassification Loss: 1.7322\r\n",
      "Train Epoch: 2 [24320/110534 (22%)]\tClassification Loss: 1.5474\r\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tClassification Loss: 1.4767\r\n",
      "Train Epoch: 2 [25600/110534 (23%)]\tClassification Loss: 2.2381\r\n",
      "Test() called at step_no: 2127\r\n",
      "\r\n",
      "Test set: Average loss: 1.7713, Accuracy: 1012/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [26240/110534 (24%)]\tClassification Loss: 1.7124\r\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tClassification Loss: 1.8222\r\n",
      "Train Epoch: 2 [27520/110534 (25%)]\tClassification Loss: 1.5205\r\n",
      "Train Epoch: 2 [28160/110534 (25%)]\tClassification Loss: 1.7859\r\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tClassification Loss: 2.0189\r\n",
      "Train Epoch: 2 [29440/110534 (27%)]\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 2 [30080/110534 (27%)]\tClassification Loss: 1.9243\r\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tClassification Loss: 1.6605\r\n",
      "Train Epoch: 2 [31360/110534 (28%)]\tClassification Loss: 1.9420\r\n",
      "Train Epoch: 2 [32000/110534 (29%)]\tClassification Loss: 1.7582\r\n",
      "Test() called at step_no: 2227\r\n",
      "\r\n",
      "Test set: Average loss: 1.7626, Accuracy: 1021/1920 (53%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_500.pth.tar\r\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tClassification Loss: 1.7224\r\n",
      "Train Epoch: 2 [33280/110534 (30%)]\tClassification Loss: 1.5683\r\n",
      "Train Epoch: 2 [33920/110534 (31%)]\tClassification Loss: 1.8717\r\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tClassification Loss: 1.8545\r\n",
      "Train Epoch: 2 [35200/110534 (32%)]\tClassification Loss: 1.8261\r\n",
      "Train Epoch: 2 [35840/110534 (32%)]\tClassification Loss: 1.4637\r\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 2 [37120/110534 (34%)]\tClassification Loss: 1.8764\r\n",
      "Train Epoch: 2 [37760/110534 (34%)]\tClassification Loss: 1.6776\r\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tClassification Loss: 1.8068\r\n",
      "Test() called at step_no: 2327\r\n",
      "\r\n",
      "Test set: Average loss: 1.7520, Accuracy: 1016/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [39040/110534 (35%)]\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 2 [39680/110534 (36%)]\tClassification Loss: 1.9927\r\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tClassification Loss: 1.8082\r\n",
      "Train Epoch: 2 [40960/110534 (37%)]\tClassification Loss: 1.5326\r\n",
      "Train Epoch: 2 [41600/110534 (38%)]\tClassification Loss: 1.8624\r\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 2 [42880/110534 (39%)]\tClassification Loss: 1.6697\r\n",
      "Train Epoch: 2 [43520/110534 (39%)]\tClassification Loss: 1.9160\r\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 2 [44800/110534 (41%)]\tClassification Loss: 1.5424\r\n",
      "Test() called at step_no: 2427\r\n",
      "\r\n",
      "Test set: Average loss: 1.7446, Accuracy: 1020/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [45440/110534 (41%)]\tClassification Loss: 2.0003\r\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tClassification Loss: 1.7010\r\n",
      "Train Epoch: 2 [46720/110534 (42%)]\tClassification Loss: 1.8115\r\n",
      "Train Epoch: 2 [47360/110534 (43%)]\tClassification Loss: 1.8601\r\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tClassification Loss: 1.7956\r\n",
      "Train Epoch: 2 [48640/110534 (44%)]\tClassification Loss: 1.6321\r\n",
      "Train Epoch: 2 [49280/110534 (45%)]\tClassification Loss: 1.4114\r\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tClassification Loss: 1.5354\r\n",
      "Train Epoch: 2 [50560/110534 (46%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 2 [51200/110534 (46%)]\tClassification Loss: 1.7897\r\n",
      "Test() called at step_no: 2527\r\n",
      "\r\n",
      "Test set: Average loss: 1.7353, Accuracy: 1021/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tClassification Loss: 1.4529\r\n",
      "Train Epoch: 2 [52480/110534 (47%)]\tClassification Loss: 1.6111\r\n",
      "Train Epoch: 2 [53120/110534 (48%)]\tClassification Loss: 1.7754\r\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tClassification Loss: 1.6127\r\n",
      "Train Epoch: 2 [54400/110534 (49%)]\tClassification Loss: 1.8811\r\n",
      "Train Epoch: 2 [55040/110534 (50%)]\tClassification Loss: 1.6211\r\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tClassification Loss: 1.9302\r\n",
      "Train Epoch: 2 [56320/110534 (51%)]\tClassification Loss: 1.6360\r\n",
      "Train Epoch: 2 [56960/110534 (52%)]\tClassification Loss: 1.6531\r\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tClassification Loss: 1.8188\r\n",
      "Test() called at step_no: 2627\r\n",
      "\r\n",
      "Test set: Average loss: 1.7297, Accuracy: 1025/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [58240/110534 (53%)]\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 2 [58880/110534 (53%)]\tClassification Loss: 1.8650\r\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tClassification Loss: 1.7541\r\n",
      "Train Epoch: 2 [60160/110534 (54%)]\tClassification Loss: 1.9641\r\n",
      "Train Epoch: 2 [60800/110534 (55%)]\tClassification Loss: 2.1204\r\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tClassification Loss: 1.6895\r\n",
      "Train Epoch: 2 [62080/110534 (56%)]\tClassification Loss: 2.0898\r\n",
      "Train Epoch: 2 [62720/110534 (57%)]\tClassification Loss: 1.7411\r\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tClassification Loss: 1.9688\r\n",
      "Train Epoch: 2 [64000/110534 (58%)]\tClassification Loss: 1.7493\r\n",
      "Test() called at step_no: 2727\r\n",
      "\r\n",
      "Test set: Average loss: 1.7229, Accuracy: 1030/1920 (54%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1000.pth.tar\r\n",
      "Train Epoch: 2 [64640/110534 (58%)]\tClassification Loss: 1.7824\r\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tClassification Loss: 1.8142\r\n",
      "Train Epoch: 2 [65920/110534 (60%)]\tClassification Loss: 1.4356\r\n",
      "Train Epoch: 2 [66560/110534 (60%)]\tClassification Loss: 1.7349\r\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tClassification Loss: 1.9038\r\n",
      "Train Epoch: 2 [67840/110534 (61%)]\tClassification Loss: 1.4113\r\n",
      "Train Epoch: 2 [68480/110534 (62%)]\tClassification Loss: 1.5666\r\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tClassification Loss: 1.6539\r\n",
      "Train Epoch: 2 [69760/110534 (63%)]\tClassification Loss: 1.6511\r\n",
      "Train Epoch: 2 [70400/110534 (64%)]\tClassification Loss: 1.8177\r\n",
      "Test() called at step_no: 2827\r\n",
      "\r\n",
      "Test set: Average loss: 1.7237, Accuracy: 1009/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tClassification Loss: 1.7084\r\n",
      "Train Epoch: 2 [71680/110534 (65%)]\tClassification Loss: 1.5160\r\n",
      "Train Epoch: 2 [72320/110534 (65%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tClassification Loss: 1.7535\r\n",
      "Train Epoch: 2 [73600/110534 (67%)]\tClassification Loss: 1.6306\r\n",
      "Train Epoch: 2 [74240/110534 (67%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tClassification Loss: 1.5231\r\n",
      "Train Epoch: 2 [75520/110534 (68%)]\tClassification Loss: 1.7642\r\n",
      "Train Epoch: 2 [76160/110534 (69%)]\tClassification Loss: 1.6325\r\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tClassification Loss: 1.5579\r\n",
      "Test() called at step_no: 2927\r\n",
      "\r\n",
      "Test set: Average loss: 1.7118, Accuracy: 1029/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 2 [77440/110534 (70%)]\tClassification Loss: 1.8057\r\n",
      "Train Epoch: 2 [78080/110534 (71%)]\tClassification Loss: 1.7435\r\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tClassification Loss: 1.6538\r\n",
      "Train Epoch: 2 [79360/110534 (72%)]\tClassification Loss: 1.9719\r\n",
      "Train Epoch: 2 [80000/110534 (72%)]\tClassification Loss: 1.7325\r\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tClassification Loss: 1.7103\r\n",
      "Train Epoch: 2 [81280/110534 (74%)]\tClassification Loss: 2.1305\r\n",
      "Train Epoch: 2 [81920/110534 (74%)]\tClassification Loss: 1.4358\r\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tClassification Loss: 2.0793\r\n",
      "Train Epoch: 2 [83200/110534 (75%)]\tClassification Loss: 1.5107\r\n",
      "Test() called at step_no: 3027\r\n",
      "\r\n",
      "Test set: Average loss: 1.7075, Accuracy: 1021/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [83840/110534 (76%)]\tClassification Loss: 1.8968\r\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tClassification Loss: 1.5635\r\n",
      "Train Epoch: 2 [85120/110534 (77%)]\tClassification Loss: 1.7934\r\n",
      "Train Epoch: 2 [85760/110534 (78%)]\tClassification Loss: 1.9133\r\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tClassification Loss: 1.8902\r\n",
      "Train Epoch: 2 [87040/110534 (79%)]\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 2 [87680/110534 (79%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tClassification Loss: 1.7175\r\n",
      "Train Epoch: 2 [88960/110534 (80%)]\tClassification Loss: 1.4179\r\n",
      "Train Epoch: 2 [89600/110534 (81%)]\tClassification Loss: 1.7315\r\n",
      "Test() called at step_no: 3127\r\n",
      "\r\n",
      "Test set: Average loss: 1.7039, Accuracy: 1022/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tClassification Loss: 1.6913\r\n",
      "Train Epoch: 2 [90880/110534 (82%)]\tClassification Loss: 2.0900\r\n",
      "Train Epoch: 2 [91520/110534 (83%)]\tClassification Loss: 1.5950\r\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tClassification Loss: 1.6458\r\n",
      "Train Epoch: 2 [92800/110534 (84%)]\tClassification Loss: 1.4415\r\n",
      "Train Epoch: 2 [93440/110534 (85%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tClassification Loss: 1.9014\r\n",
      "Train Epoch: 2 [94720/110534 (86%)]\tClassification Loss: 1.5805\r\n",
      "Train Epoch: 2 [95360/110534 (86%)]\tClassification Loss: 1.6312\r\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tClassification Loss: 1.4984\r\n",
      "Test() called at step_no: 3227\r\n",
      "\r\n",
      "Test set: Average loss: 1.7000, Accuracy: 1017/1920 (53%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [96640/110534 (87%)]\tClassification Loss: 1.8832\r\n",
      "Train Epoch: 2 [97280/110534 (88%)]\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tClassification Loss: 1.7781\r\n",
      "Train Epoch: 2 [98560/110534 (89%)]\tClassification Loss: 1.8227\r\n",
      "Train Epoch: 2 [99200/110534 (90%)]\tClassification Loss: 1.6141\r\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tClassification Loss: 1.7003\r\n",
      "Train Epoch: 2 [100480/110534 (91%)]\tClassification Loss: 1.8377\r\n",
      "Train Epoch: 2 [101120/110534 (91%)]\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tClassification Loss: 1.5110\r\n",
      "Train Epoch: 2 [102400/110534 (93%)]\tClassification Loss: 1.6725\r\n",
      "Test() called at step_no: 3327\r\n",
      "\r\n",
      "Test set: Average loss: 1.6938, Accuracy: 1033/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 2 [103040/110534 (93%)]\tClassification Loss: 1.8459\r\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 2 [104320/110534 (94%)]\tClassification Loss: 1.5422\r\n",
      "Train Epoch: 2 [104960/110534 (95%)]\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tClassification Loss: 1.5056\r\n",
      "Train Epoch: 2 [106240/110534 (96%)]\tClassification Loss: 1.7199\r\n",
      "Train Epoch: 2 [106880/110534 (97%)]\tClassification Loss: 1.3134\r\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tClassification Loss: 1.6715\r\n",
      "Train Epoch: 2 [108160/110534 (98%)]\tClassification Loss: 1.5388\r\n",
      "Train Epoch: 2 [108800/110534 (98%)]\tClassification Loss: 1.7623\r\n",
      "Test() called at step_no: 3427\r\n",
      "\r\n",
      "Test set: Average loss: 1.6931, Accuracy: 1029/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tClassification Loss: 1.7171\r\n",
      "Train Epoch: 2 [110080/110534 (100%)]\tClassification Loss: 1.7325\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/110534 (0%)]\tClassification Loss: 1.9036\r\n",
      "Test() called at step_no: 3454\r\n",
      "\r\n",
      "Test set: Average loss: 1.6925, Accuracy: 1026/1920 (53%)\r\n",
      "\r\n",
      "Train Epoch: 3 [640/110534 (1%)]\tClassification Loss: 1.6025\r\n",
      "Train Epoch: 3 [1280/110534 (1%)]\tClassification Loss: 1.4339\r\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 3 [2560/110534 (2%)]\tClassification Loss: 1.6892\r\n",
      "Train Epoch: 3 [3200/110534 (3%)]\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tClassification Loss: 1.6788\r\n",
      "Train Epoch: 3 [4480/110534 (4%)]\tClassification Loss: 1.9142\r\n",
      "Train Epoch: 3 [5120/110534 (5%)]\tClassification Loss: 1.7867\r\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tClassification Loss: 1.8160\r\n",
      "Train Epoch: 3 [6400/110534 (6%)]\tClassification Loss: 1.7782\r\n",
      "Test() called at step_no: 3554\r\n",
      "\r\n",
      "Test set: Average loss: 1.6805, Accuracy: 1040/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [7040/110534 (6%)]\tClassification Loss: 1.9299\r\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tClassification Loss: 1.8060\r\n",
      "Train Epoch: 3 [8320/110534 (8%)]\tClassification Loss: 1.5936\r\n",
      "Train Epoch: 3 [8960/110534 (8%)]\tClassification Loss: 1.7031\r\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tClassification Loss: 1.6852\r\n",
      "Train Epoch: 3 [10240/110534 (9%)]\tClassification Loss: 1.7519\r\n",
      "Train Epoch: 3 [10880/110534 (10%)]\tClassification Loss: 1.6993\r\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tClassification Loss: 1.8268\r\n",
      "Train Epoch: 3 [12160/110534 (11%)]\tClassification Loss: 1.8948\r\n",
      "Train Epoch: 3 [12800/110534 (12%)]\tClassification Loss: 1.8661\r\n",
      "Test() called at step_no: 3654\r\n",
      "\r\n",
      "Test set: Average loss: 1.6809, Accuracy: 1037/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tClassification Loss: 1.7199\r\n",
      "Train Epoch: 3 [14080/110534 (13%)]\tClassification Loss: 1.5064\r\n",
      "Train Epoch: 3 [14720/110534 (13%)]\tClassification Loss: 1.5750\r\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tClassification Loss: 1.8421\r\n",
      "Train Epoch: 3 [16000/110534 (14%)]\tClassification Loss: 1.9899\r\n",
      "Train Epoch: 3 [16640/110534 (15%)]\tClassification Loss: 1.7851\r\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tClassification Loss: 1.7884\r\n",
      "Train Epoch: 3 [17920/110534 (16%)]\tClassification Loss: 1.6859\r\n",
      "Train Epoch: 3 [18560/110534 (17%)]\tClassification Loss: 1.8360\r\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tClassification Loss: 1.7194\r\n",
      "Test() called at step_no: 3754\r\n",
      "\r\n",
      "Test set: Average loss: 1.6742, Accuracy: 1044/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [19840/110534 (18%)]\tClassification Loss: 1.8359\r\n",
      "Train Epoch: 3 [20480/110534 (19%)]\tClassification Loss: 1.7447\r\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tClassification Loss: 1.5939\r\n",
      "Train Epoch: 3 [21760/110534 (20%)]\tClassification Loss: 1.7541\r\n",
      "Train Epoch: 3 [22400/110534 (20%)]\tClassification Loss: 1.9417\r\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tClassification Loss: 1.6776\r\n",
      "Train Epoch: 3 [23680/110534 (21%)]\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 3 [24320/110534 (22%)]\tClassification Loss: 1.5190\r\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tClassification Loss: 1.5939\r\n",
      "Train Epoch: 3 [25600/110534 (23%)]\tClassification Loss: 2.0663\r\n",
      "Test() called at step_no: 3854\r\n",
      "\r\n",
      "Test set: Average loss: 1.6733, Accuracy: 1035/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [26240/110534 (24%)]\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tClassification Loss: 1.7148\r\n",
      "Train Epoch: 3 [27520/110534 (25%)]\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 3 [28160/110534 (25%)]\tClassification Loss: 1.7343\r\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tClassification Loss: 1.8492\r\n",
      "Train Epoch: 3 [29440/110534 (27%)]\tClassification Loss: 1.5188\r\n",
      "Train Epoch: 3 [30080/110534 (27%)]\tClassification Loss: 1.7545\r\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tClassification Loss: 1.5403\r\n",
      "Train Epoch: 3 [31360/110534 (28%)]\tClassification Loss: 1.8269\r\n",
      "Train Epoch: 3 [32000/110534 (29%)]\tClassification Loss: 1.6451\r\n",
      "Test() called at step_no: 3954\r\n",
      "\r\n",
      "Test set: Average loss: 1.6709, Accuracy: 1042/1920 (54%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_500.pth.tar\r\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tClassification Loss: 1.6417\r\n",
      "Train Epoch: 3 [33280/110534 (30%)]\tClassification Loss: 1.4534\r\n",
      "Train Epoch: 3 [33920/110534 (31%)]\tClassification Loss: 1.7932\r\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tClassification Loss: 1.8444\r\n",
      "Train Epoch: 3 [35200/110534 (32%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 3 [35840/110534 (32%)]\tClassification Loss: 1.3746\r\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tClassification Loss: 1.5549\r\n",
      "Train Epoch: 3 [37120/110534 (34%)]\tClassification Loss: 1.7236\r\n",
      "Train Epoch: 3 [37760/110534 (34%)]\tClassification Loss: 1.6154\r\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tClassification Loss: 1.6997\r\n",
      "Test() called at step_no: 4054\r\n",
      "\r\n",
      "Test set: Average loss: 1.6663, Accuracy: 1038/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [39040/110534 (35%)]\tClassification Loss: 1.3345\r\n",
      "Train Epoch: 3 [39680/110534 (36%)]\tClassification Loss: 1.9093\r\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tClassification Loss: 1.8292\r\n",
      "Train Epoch: 3 [40960/110534 (37%)]\tClassification Loss: 1.4422\r\n",
      "Train Epoch: 3 [41600/110534 (38%)]\tClassification Loss: 1.7499\r\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 3 [42880/110534 (39%)]\tClassification Loss: 1.6878\r\n",
      "Train Epoch: 3 [43520/110534 (39%)]\tClassification Loss: 1.9191\r\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tClassification Loss: 1.6682\r\n",
      "Train Epoch: 3 [44800/110534 (41%)]\tClassification Loss: 1.5578\r\n",
      "Test() called at step_no: 4154\r\n",
      "\r\n",
      "Test set: Average loss: 1.6624, Accuracy: 1045/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [45440/110534 (41%)]\tClassification Loss: 1.9557\r\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tClassification Loss: 1.5999\r\n",
      "Train Epoch: 3 [46720/110534 (42%)]\tClassification Loss: 1.9053\r\n",
      "Train Epoch: 3 [47360/110534 (43%)]\tClassification Loss: 1.6763\r\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tClassification Loss: 1.7667\r\n",
      "Train Epoch: 3 [48640/110534 (44%)]\tClassification Loss: 1.5610\r\n",
      "Train Epoch: 3 [49280/110534 (45%)]\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tClassification Loss: 1.4449\r\n",
      "Train Epoch: 3 [50560/110534 (46%)]\tClassification Loss: 1.6828\r\n",
      "Train Epoch: 3 [51200/110534 (46%)]\tClassification Loss: 1.5829\r\n",
      "Test() called at step_no: 4254\r\n",
      "\r\n",
      "Test set: Average loss: 1.6585, Accuracy: 1043/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tClassification Loss: 1.4600\r\n",
      "Train Epoch: 3 [52480/110534 (47%)]\tClassification Loss: 1.6869\r\n",
      "Train Epoch: 3 [53120/110534 (48%)]\tClassification Loss: 1.8303\r\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 3 [54400/110534 (49%)]\tClassification Loss: 1.8446\r\n",
      "Train Epoch: 3 [55040/110534 (50%)]\tClassification Loss: 1.4401\r\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tClassification Loss: 2.0538\r\n",
      "Train Epoch: 3 [56320/110534 (51%)]\tClassification Loss: 1.5904\r\n",
      "Train Epoch: 3 [56960/110534 (52%)]\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tClassification Loss: 1.8485\r\n",
      "Test() called at step_no: 4354\r\n",
      "\r\n",
      "Test set: Average loss: 1.6562, Accuracy: 1055/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [58240/110534 (53%)]\tClassification Loss: 1.4732\r\n",
      "Train Epoch: 3 [58880/110534 (53%)]\tClassification Loss: 1.7872\r\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tClassification Loss: 1.7313\r\n",
      "Train Epoch: 3 [60160/110534 (54%)]\tClassification Loss: 1.8068\r\n",
      "Train Epoch: 3 [60800/110534 (55%)]\tClassification Loss: 2.0269\r\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tClassification Loss: 1.6226\r\n",
      "Train Epoch: 3 [62080/110534 (56%)]\tClassification Loss: 2.1739\r\n",
      "Train Epoch: 3 [62720/110534 (57%)]\tClassification Loss: 1.8908\r\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tClassification Loss: 1.9516\r\n",
      "Train Epoch: 3 [64000/110534 (58%)]\tClassification Loss: 1.6049\r\n",
      "Test() called at step_no: 4454\r\n",
      "\r\n",
      "Test set: Average loss: 1.6526, Accuracy: 1056/1920 (55%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1000.pth.tar\r\n",
      "Train Epoch: 3 [64640/110534 (58%)]\tClassification Loss: 1.7780\r\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tClassification Loss: 1.6522\r\n",
      "Train Epoch: 3 [65920/110534 (60%)]\tClassification Loss: 1.4165\r\n",
      "Train Epoch: 3 [66560/110534 (60%)]\tClassification Loss: 1.6222\r\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tClassification Loss: 1.8287\r\n",
      "Train Epoch: 3 [67840/110534 (61%)]\tClassification Loss: 1.3629\r\n",
      "Train Epoch: 3 [68480/110534 (62%)]\tClassification Loss: 1.7261\r\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 3 [69760/110534 (63%)]\tClassification Loss: 1.5624\r\n",
      "Train Epoch: 3 [70400/110534 (64%)]\tClassification Loss: 1.8214\r\n",
      "Test() called at step_no: 4554\r\n",
      "\r\n",
      "Test set: Average loss: 1.6570, Accuracy: 1046/1920 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tClassification Loss: 1.7204\r\n",
      "Train Epoch: 3 [71680/110534 (65%)]\tClassification Loss: 1.5889\r\n",
      "Train Epoch: 3 [72320/110534 (65%)]\tClassification Loss: 1.7090\r\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tClassification Loss: 1.5728\r\n",
      "Train Epoch: 3 [73600/110534 (67%)]\tClassification Loss: 1.6325\r\n",
      "Train Epoch: 3 [74240/110534 (67%)]\tClassification Loss: 1.4635\r\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 3 [75520/110534 (68%)]\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 3 [76160/110534 (69%)]\tClassification Loss: 1.7839\r\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tClassification Loss: 1.5598\r\n",
      "Test() called at step_no: 4654\r\n",
      "\r\n",
      "Test set: Average loss: 1.6463, Accuracy: 1053/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [77440/110534 (70%)]\tClassification Loss: 1.8827\r\n",
      "Train Epoch: 3 [78080/110534 (71%)]\tClassification Loss: 1.7215\r\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tClassification Loss: 1.7001\r\n",
      "Train Epoch: 3 [79360/110534 (72%)]\tClassification Loss: 2.0061\r\n",
      "Train Epoch: 3 [80000/110534 (72%)]\tClassification Loss: 1.7287\r\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tClassification Loss: 1.7930\r\n",
      "Train Epoch: 3 [81280/110534 (74%)]\tClassification Loss: 2.0565\r\n",
      "Train Epoch: 3 [81920/110534 (74%)]\tClassification Loss: 1.3913\r\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tClassification Loss: 1.8642\r\n",
      "Train Epoch: 3 [83200/110534 (75%)]\tClassification Loss: 1.4369\r\n",
      "Test() called at step_no: 4754\r\n",
      "\r\n",
      "Test set: Average loss: 1.6462, Accuracy: 1053/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [83840/110534 (76%)]\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tClassification Loss: 1.6631\r\n",
      "Train Epoch: 3 [85120/110534 (77%)]\tClassification Loss: 1.8060\r\n",
      "Train Epoch: 3 [85760/110534 (78%)]\tClassification Loss: 1.9194\r\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tClassification Loss: 1.7785\r\n",
      "Train Epoch: 3 [87040/110534 (79%)]\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 3 [87680/110534 (79%)]\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tClassification Loss: 1.6863\r\n",
      "Train Epoch: 3 [88960/110534 (80%)]\tClassification Loss: 1.4081\r\n",
      "Train Epoch: 3 [89600/110534 (81%)]\tClassification Loss: 1.6848\r\n",
      "Test() called at step_no: 4854\r\n",
      "\r\n",
      "Test set: Average loss: 1.6450, Accuracy: 1052/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tClassification Loss: 1.7194\r\n",
      "Train Epoch: 3 [90880/110534 (82%)]\tClassification Loss: 2.0552\r\n",
      "Train Epoch: 3 [91520/110534 (83%)]\tClassification Loss: 1.5130\r\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tClassification Loss: 1.7100\r\n",
      "Train Epoch: 3 [92800/110534 (84%)]\tClassification Loss: 1.3826\r\n",
      "Train Epoch: 3 [93440/110534 (85%)]\tClassification Loss: 1.4212\r\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tClassification Loss: 1.7455\r\n",
      "Train Epoch: 3 [94720/110534 (86%)]\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 3 [95360/110534 (86%)]\tClassification Loss: 1.5567\r\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tClassification Loss: 1.5006\r\n",
      "Test() called at step_no: 4954\r\n",
      "\r\n",
      "Test set: Average loss: 1.6422, Accuracy: 1047/1920 (55%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [96640/110534 (87%)]\tClassification Loss: 1.7401\r\n",
      "Train Epoch: 3 [97280/110534 (88%)]\tClassification Loss: 1.4675\r\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tClassification Loss: 1.6791\r\n",
      "Train Epoch: 3 [98560/110534 (89%)]\tClassification Loss: 1.9717\r\n",
      "Train Epoch: 3 [99200/110534 (90%)]\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 3 [100480/110534 (91%)]\tClassification Loss: 1.7800\r\n",
      "Train Epoch: 3 [101120/110534 (91%)]\tClassification Loss: 1.6780\r\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 3 [102400/110534 (93%)]\tClassification Loss: 1.6605\r\n",
      "Test() called at step_no: 5054\r\n",
      "\r\n",
      "Test set: Average loss: 1.6398, Accuracy: 1062/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [103040/110534 (93%)]\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 3 [104320/110534 (94%)]\tClassification Loss: 1.4507\r\n",
      "Train Epoch: 3 [104960/110534 (95%)]\tClassification Loss: 1.6273\r\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tClassification Loss: 1.3877\r\n",
      "Train Epoch: 3 [106240/110534 (96%)]\tClassification Loss: 1.5952\r\n",
      "Train Epoch: 3 [106880/110534 (97%)]\tClassification Loss: 1.3126\r\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tClassification Loss: 1.7348\r\n",
      "Train Epoch: 3 [108160/110534 (98%)]\tClassification Loss: 1.5485\r\n",
      "Train Epoch: 3 [108800/110534 (98%)]\tClassification Loss: 1.7201\r\n",
      "Test() called at step_no: 5154\r\n",
      "\r\n",
      "Test set: Average loss: 1.6417, Accuracy: 1060/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tClassification Loss: 1.7810\r\n",
      "Train Epoch: 3 [110080/110534 (100%)]\tClassification Loss: 1.6135\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/110534 (0%)]\tClassification Loss: 1.8481\r\n",
      "Test() called at step_no: 5181\r\n",
      "\r\n",
      "Test set: Average loss: 1.6433, Accuracy: 1057/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [640/110534 (1%)]\tClassification Loss: 1.6383\r\n",
      "Train Epoch: 4 [1280/110534 (1%)]\tClassification Loss: 1.4694\r\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tClassification Loss: 1.6091\r\n",
      "Train Epoch: 4 [2560/110534 (2%)]\tClassification Loss: 1.5501\r\n",
      "Train Epoch: 4 [3200/110534 (3%)]\tClassification Loss: 1.4760\r\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 4 [4480/110534 (4%)]\tClassification Loss: 2.0378\r\n",
      "Train Epoch: 4 [5120/110534 (5%)]\tClassification Loss: 1.7345\r\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tClassification Loss: 1.8048\r\n",
      "Train Epoch: 4 [6400/110534 (6%)]\tClassification Loss: 1.5762\r\n",
      "Test() called at step_no: 5281\r\n",
      "\r\n",
      "Test set: Average loss: 1.6314, Accuracy: 1059/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [7040/110534 (6%)]\tClassification Loss: 1.8659\r\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tClassification Loss: 1.6431\r\n",
      "Train Epoch: 4 [8320/110534 (8%)]\tClassification Loss: 1.5907\r\n",
      "Train Epoch: 4 [8960/110534 (8%)]\tClassification Loss: 1.7672\r\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tClassification Loss: 1.6664\r\n",
      "Train Epoch: 4 [10240/110534 (9%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 4 [10880/110534 (10%)]\tClassification Loss: 1.7295\r\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tClassification Loss: 1.8983\r\n",
      "Train Epoch: 4 [12160/110534 (11%)]\tClassification Loss: 1.8816\r\n",
      "Train Epoch: 4 [12800/110534 (12%)]\tClassification Loss: 1.9590\r\n",
      "Test() called at step_no: 5381\r\n",
      "\r\n",
      "Test set: Average loss: 1.6344, Accuracy: 1061/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tClassification Loss: 1.7757\r\n",
      "Train Epoch: 4 [14080/110534 (13%)]\tClassification Loss: 1.5026\r\n",
      "Train Epoch: 4 [14720/110534 (13%)]\tClassification Loss: 1.5678\r\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tClassification Loss: 1.6870\r\n",
      "Train Epoch: 4 [16000/110534 (14%)]\tClassification Loss: 1.9377\r\n",
      "Train Epoch: 4 [16640/110534 (15%)]\tClassification Loss: 1.6529\r\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tClassification Loss: 1.7969\r\n",
      "Train Epoch: 4 [17920/110534 (16%)]\tClassification Loss: 1.4822\r\n",
      "Train Epoch: 4 [18560/110534 (17%)]\tClassification Loss: 1.7831\r\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tClassification Loss: 1.7505\r\n",
      "Test() called at step_no: 5481\r\n",
      "\r\n",
      "Test set: Average loss: 1.6286, Accuracy: 1059/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [19840/110534 (18%)]\tClassification Loss: 1.6679\r\n",
      "Train Epoch: 4 [20480/110534 (19%)]\tClassification Loss: 1.8421\r\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 4 [21760/110534 (20%)]\tClassification Loss: 1.7297\r\n",
      "Train Epoch: 4 [22400/110534 (20%)]\tClassification Loss: 1.8521\r\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tClassification Loss: 1.6457\r\n",
      "Train Epoch: 4 [23680/110534 (21%)]\tClassification Loss: 1.6867\r\n",
      "Train Epoch: 4 [24320/110534 (22%)]\tClassification Loss: 1.4350\r\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tClassification Loss: 1.4140\r\n",
      "Train Epoch: 4 [25600/110534 (23%)]\tClassification Loss: 1.9514\r\n",
      "Test() called at step_no: 5581\r\n",
      "\r\n",
      "Test set: Average loss: 1.6295, Accuracy: 1067/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [26240/110534 (24%)]\tClassification Loss: 1.5230\r\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tClassification Loss: 1.7133\r\n",
      "Train Epoch: 4 [27520/110534 (25%)]\tClassification Loss: 1.3454\r\n",
      "Train Epoch: 4 [28160/110534 (25%)]\tClassification Loss: 1.5973\r\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tClassification Loss: 1.9573\r\n",
      "Train Epoch: 4 [29440/110534 (27%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 4 [30080/110534 (27%)]\tClassification Loss: 1.7346\r\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tClassification Loss: 1.5737\r\n",
      "Train Epoch: 4 [31360/110534 (28%)]\tClassification Loss: 1.7180\r\n",
      "Train Epoch: 4 [32000/110534 (29%)]\tClassification Loss: 1.5947\r\n",
      "Test() called at step_no: 5681\r\n",
      "\r\n",
      "Test set: Average loss: 1.6292, Accuracy: 1061/1920 (55%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_500.pth.tar\r\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tClassification Loss: 1.5682\r\n",
      "Train Epoch: 4 [33280/110534 (30%)]\tClassification Loss: 1.4637\r\n",
      "Train Epoch: 4 [33920/110534 (31%)]\tClassification Loss: 1.7077\r\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 4 [35200/110534 (32%)]\tClassification Loss: 1.4598\r\n",
      "Train Epoch: 4 [35840/110534 (32%)]\tClassification Loss: 1.4256\r\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tClassification Loss: 1.5276\r\n",
      "Train Epoch: 4 [37120/110534 (34%)]\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 4 [37760/110534 (34%)]\tClassification Loss: 1.6316\r\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tClassification Loss: 1.6186\r\n",
      "Test() called at step_no: 5781\r\n",
      "\r\n",
      "Test set: Average loss: 1.6242, Accuracy: 1067/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [39040/110534 (35%)]\tClassification Loss: 1.3350\r\n",
      "Train Epoch: 4 [39680/110534 (36%)]\tClassification Loss: 1.8029\r\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tClassification Loss: 1.7671\r\n",
      "Train Epoch: 4 [40960/110534 (37%)]\tClassification Loss: 1.4528\r\n",
      "Train Epoch: 4 [41600/110534 (38%)]\tClassification Loss: 1.6676\r\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 4 [42880/110534 (39%)]\tClassification Loss: 1.7219\r\n",
      "Train Epoch: 4 [43520/110534 (39%)]\tClassification Loss: 1.7439\r\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tClassification Loss: 1.4555\r\n",
      "Train Epoch: 4 [44800/110534 (41%)]\tClassification Loss: 1.3701\r\n",
      "Test() called at step_no: 5881\r\n",
      "\r\n",
      "Test set: Average loss: 1.6243, Accuracy: 1064/1920 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [45440/110534 (41%)]\tClassification Loss: 1.7627\r\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tClassification Loss: 1.5958\r\n",
      "Train Epoch: 4 [46720/110534 (42%)]\tClassification Loss: 1.8385\r\n",
      "Train Epoch: 4 [47360/110534 (43%)]\tClassification Loss: 1.7933\r\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 4 [48640/110534 (44%)]\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 4 [49280/110534 (45%)]\tClassification Loss: 1.3219\r\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 4 [50560/110534 (46%)]\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 4 [51200/110534 (46%)]\tClassification Loss: 1.5004\r\n",
      "Test() called at step_no: 5981\r\n",
      "\r\n",
      "Test set: Average loss: 1.6194, Accuracy: 1068/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tClassification Loss: 1.3419\r\n",
      "Train Epoch: 4 [52480/110534 (47%)]\tClassification Loss: 1.5943\r\n",
      "Train Epoch: 4 [53120/110534 (48%)]\tClassification Loss: 1.6508\r\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tClassification Loss: 1.5908\r\n",
      "Train Epoch: 4 [54400/110534 (49%)]\tClassification Loss: 1.8682\r\n",
      "Train Epoch: 4 [55040/110534 (50%)]\tClassification Loss: 1.3493\r\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tClassification Loss: 1.8650\r\n",
      "Train Epoch: 4 [56320/110534 (51%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 4 [56960/110534 (52%)]\tClassification Loss: 1.5662\r\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tClassification Loss: 1.8196\r\n",
      "Test() called at step_no: 6081\r\n",
      "\r\n",
      "Test set: Average loss: 1.6200, Accuracy: 1067/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [58240/110534 (53%)]\tClassification Loss: 1.4459\r\n",
      "Train Epoch: 4 [58880/110534 (53%)]\tClassification Loss: 1.8375\r\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tClassification Loss: 1.7093\r\n",
      "Train Epoch: 4 [60160/110534 (54%)]\tClassification Loss: 1.9390\r\n",
      "Train Epoch: 4 [60800/110534 (55%)]\tClassification Loss: 2.1361\r\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 4 [62080/110534 (56%)]\tClassification Loss: 2.0791\r\n",
      "Train Epoch: 4 [62720/110534 (57%)]\tClassification Loss: 1.6186\r\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tClassification Loss: 1.7356\r\n",
      "Train Epoch: 4 [64000/110534 (58%)]\tClassification Loss: 1.7540\r\n",
      "Test() called at step_no: 6181\r\n",
      "\r\n",
      "Test set: Average loss: 1.6183, Accuracy: 1063/1920 (55%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1000.pth.tar\r\n",
      "Train Epoch: 4 [64640/110534 (58%)]\tClassification Loss: 1.7350\r\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tClassification Loss: 1.7956\r\n",
      "Train Epoch: 4 [65920/110534 (60%)]\tClassification Loss: 1.5140\r\n",
      "Train Epoch: 4 [66560/110534 (60%)]\tClassification Loss: 1.6406\r\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tClassification Loss: 1.7399\r\n",
      "Train Epoch: 4 [67840/110534 (61%)]\tClassification Loss: 1.3289\r\n",
      "Train Epoch: 4 [68480/110534 (62%)]\tClassification Loss: 1.3917\r\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tClassification Loss: 1.5926\r\n",
      "Train Epoch: 4 [69760/110534 (63%)]\tClassification Loss: 1.6052\r\n",
      "Train Epoch: 4 [70400/110534 (64%)]\tClassification Loss: 1.6843\r\n",
      "Test() called at step_no: 6281\r\n",
      "\r\n",
      "Test set: Average loss: 1.6233, Accuracy: 1066/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tClassification Loss: 1.5364\r\n",
      "Train Epoch: 4 [71680/110534 (65%)]\tClassification Loss: 1.6648\r\n",
      "Train Epoch: 4 [72320/110534 (65%)]\tClassification Loss: 1.5661\r\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 4 [73600/110534 (67%)]\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 4 [74240/110534 (67%)]\tClassification Loss: 1.6448\r\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 4 [75520/110534 (68%)]\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 4 [76160/110534 (69%)]\tClassification Loss: 1.5875\r\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tClassification Loss: 1.5998\r\n",
      "Test() called at step_no: 6381\r\n",
      "\r\n",
      "Test set: Average loss: 1.6139, Accuracy: 1075/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [77440/110534 (70%)]\tClassification Loss: 1.7900\r\n",
      "Train Epoch: 4 [78080/110534 (71%)]\tClassification Loss: 1.6669\r\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 4 [79360/110534 (72%)]\tClassification Loss: 2.0295\r\n",
      "Train Epoch: 4 [80000/110534 (72%)]\tClassification Loss: 1.6516\r\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tClassification Loss: 1.7987\r\n",
      "Train Epoch: 4 [81280/110534 (74%)]\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 4 [81920/110534 (74%)]\tClassification Loss: 1.3839\r\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tClassification Loss: 1.8060\r\n",
      "Train Epoch: 4 [83200/110534 (75%)]\tClassification Loss: 1.5260\r\n",
      "Test() called at step_no: 6481\r\n",
      "\r\n",
      "Test set: Average loss: 1.6129, Accuracy: 1072/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [83840/110534 (76%)]\tClassification Loss: 1.8379\r\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 4 [85120/110534 (77%)]\tClassification Loss: 1.7239\r\n",
      "Train Epoch: 4 [85760/110534 (78%)]\tClassification Loss: 2.0712\r\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tClassification Loss: 1.7613\r\n",
      "Train Epoch: 4 [87040/110534 (79%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 4 [87680/110534 (79%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tClassification Loss: 1.5995\r\n",
      "Train Epoch: 4 [88960/110534 (80%)]\tClassification Loss: 1.3196\r\n",
      "Train Epoch: 4 [89600/110534 (81%)]\tClassification Loss: 1.4750\r\n",
      "Test() called at step_no: 6581\r\n",
      "\r\n",
      "Test set: Average loss: 1.6119, Accuracy: 1073/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tClassification Loss: 1.5699\r\n",
      "Train Epoch: 4 [90880/110534 (82%)]\tClassification Loss: 2.0506\r\n",
      "Train Epoch: 4 [91520/110534 (83%)]\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tClassification Loss: 1.5817\r\n",
      "Train Epoch: 4 [92800/110534 (84%)]\tClassification Loss: 1.2573\r\n",
      "Train Epoch: 4 [93440/110534 (85%)]\tClassification Loss: 1.4087\r\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tClassification Loss: 1.7184\r\n",
      "Train Epoch: 4 [94720/110534 (86%)]\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 4 [95360/110534 (86%)]\tClassification Loss: 1.5709\r\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tClassification Loss: 1.3221\r\n",
      "Test() called at step_no: 6681\r\n",
      "\r\n",
      "Test set: Average loss: 1.6115, Accuracy: 1067/1920 (56%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [96640/110534 (87%)]\tClassification Loss: 1.7218\r\n",
      "Train Epoch: 4 [97280/110534 (88%)]\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tClassification Loss: 1.6900\r\n",
      "Train Epoch: 4 [98560/110534 (89%)]\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 4 [99200/110534 (90%)]\tClassification Loss: 1.6489\r\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tClassification Loss: 1.5591\r\n",
      "Train Epoch: 4 [100480/110534 (91%)]\tClassification Loss: 1.8311\r\n",
      "Train Epoch: 4 [101120/110534 (91%)]\tClassification Loss: 1.5698\r\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tClassification Loss: 1.5972\r\n",
      "Train Epoch: 4 [102400/110534 (93%)]\tClassification Loss: 1.6804\r\n",
      "Test() called at step_no: 6781\r\n",
      "\r\n",
      "Test set: Average loss: 1.6098, Accuracy: 1077/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [103040/110534 (93%)]\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 4 [104320/110534 (94%)]\tClassification Loss: 1.3830\r\n",
      "Train Epoch: 4 [104960/110534 (95%)]\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tClassification Loss: 1.4200\r\n",
      "Train Epoch: 4 [106240/110534 (96%)]\tClassification Loss: 1.6593\r\n",
      "Train Epoch: 4 [106880/110534 (97%)]\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tClassification Loss: 1.6620\r\n",
      "Train Epoch: 4 [108160/110534 (98%)]\tClassification Loss: 1.6132\r\n",
      "Train Epoch: 4 [108800/110534 (98%)]\tClassification Loss: 1.7228\r\n",
      "Test() called at step_no: 6881\r\n",
      "\r\n",
      "Test set: Average loss: 1.6131, Accuracy: 1079/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tClassification Loss: 1.7770\r\n",
      "Train Epoch: 4 [110080/110534 (100%)]\tClassification Loss: 1.6604\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/110534 (0%)]\tClassification Loss: 1.6960\r\n",
      "Test() called at step_no: 6908\r\n",
      "\r\n",
      "Test set: Average loss: 1.6144, Accuracy: 1074/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [640/110534 (1%)]\tClassification Loss: 1.6568\r\n",
      "Train Epoch: 5 [1280/110534 (1%)]\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 5 [2560/110534 (2%)]\tClassification Loss: 1.6052\r\n",
      "Train Epoch: 5 [3200/110534 (3%)]\tClassification Loss: 1.5250\r\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tClassification Loss: 1.5635\r\n",
      "Train Epoch: 5 [4480/110534 (4%)]\tClassification Loss: 1.9280\r\n",
      "Train Epoch: 5 [5120/110534 (5%)]\tClassification Loss: 1.6920\r\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tClassification Loss: 1.6621\r\n",
      "Train Epoch: 5 [6400/110534 (6%)]\tClassification Loss: 1.7494\r\n",
      "Test() called at step_no: 7008\r\n",
      "\r\n",
      "Test set: Average loss: 1.6051, Accuracy: 1081/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [7040/110534 (6%)]\tClassification Loss: 1.9046\r\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tClassification Loss: 1.6359\r\n",
      "Train Epoch: 5 [8320/110534 (8%)]\tClassification Loss: 1.6753\r\n",
      "Train Epoch: 5 [8960/110534 (8%)]\tClassification Loss: 1.7832\r\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tClassification Loss: 1.7981\r\n",
      "Train Epoch: 5 [10240/110534 (9%)]\tClassification Loss: 1.8535\r\n",
      "Train Epoch: 5 [10880/110534 (10%)]\tClassification Loss: 1.8005\r\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tClassification Loss: 1.8134\r\n",
      "Train Epoch: 5 [12160/110534 (11%)]\tClassification Loss: 1.8219\r\n",
      "Train Epoch: 5 [12800/110534 (12%)]\tClassification Loss: 1.8529\r\n",
      "Test() called at step_no: 7108\r\n",
      "\r\n",
      "Test set: Average loss: 1.6082, Accuracy: 1069/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tClassification Loss: 1.7688\r\n",
      "Train Epoch: 5 [14080/110534 (13%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 5 [14720/110534 (13%)]\tClassification Loss: 1.6762\r\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tClassification Loss: 1.6917\r\n",
      "Train Epoch: 5 [16000/110534 (14%)]\tClassification Loss: 1.9409\r\n",
      "Train Epoch: 5 [16640/110534 (15%)]\tClassification Loss: 1.6902\r\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tClassification Loss: 1.7932\r\n",
      "Train Epoch: 5 [17920/110534 (16%)]\tClassification Loss: 1.6269\r\n",
      "Train Epoch: 5 [18560/110534 (17%)]\tClassification Loss: 1.8493\r\n",
      "Train Epoch: 5 [19200/110534 (17%)]\tClassification Loss: 1.7162\r\n",
      "Test() called at step_no: 7208\r\n",
      "\r\n",
      "Test set: Average loss: 1.6047, Accuracy: 1076/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [19840/110534 (18%)]\tClassification Loss: 1.7095\r\n",
      "Train Epoch: 5 [20480/110534 (19%)]\tClassification Loss: 1.6194\r\n",
      "Train Epoch: 5 [21120/110534 (19%)]\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 5 [21760/110534 (20%)]\tClassification Loss: 1.7746\r\n",
      "Train Epoch: 5 [22400/110534 (20%)]\tClassification Loss: 1.8941\r\n",
      "Train Epoch: 5 [23040/110534 (21%)]\tClassification Loss: 1.6096\r\n",
      "Train Epoch: 5 [23680/110534 (21%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 5 [24320/110534 (22%)]\tClassification Loss: 1.3811\r\n",
      "Train Epoch: 5 [24960/110534 (23%)]\tClassification Loss: 1.3352\r\n",
      "Train Epoch: 5 [25600/110534 (23%)]\tClassification Loss: 2.0708\r\n",
      "Test() called at step_no: 7308\r\n",
      "\r\n",
      "Test set: Average loss: 1.6033, Accuracy: 1073/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [26240/110534 (24%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 5 [26880/110534 (24%)]\tClassification Loss: 1.7372\r\n",
      "Train Epoch: 5 [27520/110534 (25%)]\tClassification Loss: 1.3534\r\n",
      "Train Epoch: 5 [28160/110534 (25%)]\tClassification Loss: 1.7155\r\n",
      "Train Epoch: 5 [28800/110534 (26%)]\tClassification Loss: 1.8584\r\n",
      "Train Epoch: 5 [29440/110534 (27%)]\tClassification Loss: 1.5115\r\n",
      "Train Epoch: 5 [30080/110534 (27%)]\tClassification Loss: 1.7333\r\n",
      "Train Epoch: 5 [30720/110534 (28%)]\tClassification Loss: 1.4017\r\n",
      "Train Epoch: 5 [31360/110534 (28%)]\tClassification Loss: 1.7816\r\n",
      "Train Epoch: 5 [32000/110534 (29%)]\tClassification Loss: 1.4606\r\n",
      "Test() called at step_no: 7408\r\n",
      "\r\n",
      "Test set: Average loss: 1.6032, Accuracy: 1072/1920 (56%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_500.pth.tar\r\n",
      "Train Epoch: 5 [32640/110534 (30%)]\tClassification Loss: 1.5873\r\n",
      "Train Epoch: 5 [33280/110534 (30%)]\tClassification Loss: 1.3801\r\n",
      "Train Epoch: 5 [33920/110534 (31%)]\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 5 [34560/110534 (31%)]\tClassification Loss: 1.6582\r\n",
      "Train Epoch: 5 [35200/110534 (32%)]\tClassification Loss: 1.5349\r\n",
      "Train Epoch: 5 [35840/110534 (32%)]\tClassification Loss: 1.3718\r\n",
      "Train Epoch: 5 [36480/110534 (33%)]\tClassification Loss: 1.4009\r\n",
      "Train Epoch: 5 [37120/110534 (34%)]\tClassification Loss: 1.6273\r\n",
      "Train Epoch: 5 [37760/110534 (34%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 5 [38400/110534 (35%)]\tClassification Loss: 1.7164\r\n",
      "Test() called at step_no: 7508\r\n",
      "\r\n",
      "Test set: Average loss: 1.6010, Accuracy: 1079/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [39040/110534 (35%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 5 [39680/110534 (36%)]\tClassification Loss: 1.7623\r\n",
      "Train Epoch: 5 [40320/110534 (36%)]\tClassification Loss: 1.8292\r\n",
      "Train Epoch: 5 [40960/110534 (37%)]\tClassification Loss: 1.4826\r\n",
      "Train Epoch: 5 [41600/110534 (38%)]\tClassification Loss: 1.7396\r\n",
      "Train Epoch: 5 [42240/110534 (38%)]\tClassification Loss: 1.3692\r\n",
      "Train Epoch: 5 [42880/110534 (39%)]\tClassification Loss: 1.6127\r\n",
      "Train Epoch: 5 [43520/110534 (39%)]\tClassification Loss: 1.8937\r\n",
      "Train Epoch: 5 [44160/110534 (40%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 5 [44800/110534 (41%)]\tClassification Loss: 1.4403\r\n",
      "Test() called at step_no: 7608\r\n",
      "\r\n",
      "Test set: Average loss: 1.5992, Accuracy: 1072/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [45440/110534 (41%)]\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 5 [46080/110534 (42%)]\tClassification Loss: 1.7361\r\n",
      "Train Epoch: 5 [46720/110534 (42%)]\tClassification Loss: 1.9010\r\n",
      "Train Epoch: 5 [47360/110534 (43%)]\tClassification Loss: 1.7122\r\n",
      "Train Epoch: 5 [48000/110534 (43%)]\tClassification Loss: 1.7038\r\n",
      "Train Epoch: 5 [48640/110534 (44%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 5 [49280/110534 (45%)]\tClassification Loss: 1.2394\r\n",
      "Train Epoch: 5 [49920/110534 (45%)]\tClassification Loss: 1.5096\r\n",
      "Train Epoch: 5 [50560/110534 (46%)]\tClassification Loss: 1.7095\r\n",
      "Train Epoch: 5 [51200/110534 (46%)]\tClassification Loss: 1.6175\r\n",
      "Test() called at step_no: 7708\r\n",
      "\r\n",
      "Test set: Average loss: 1.5979, Accuracy: 1080/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [51840/110534 (47%)]\tClassification Loss: 1.3430\r\n",
      "Train Epoch: 5 [52480/110534 (47%)]\tClassification Loss: 1.4753\r\n",
      "Train Epoch: 5 [53120/110534 (48%)]\tClassification Loss: 1.6831\r\n",
      "Train Epoch: 5 [53760/110534 (49%)]\tClassification Loss: 1.5889\r\n",
      "Train Epoch: 5 [54400/110534 (49%)]\tClassification Loss: 1.8544\r\n",
      "Train Epoch: 5 [55040/110534 (50%)]\tClassification Loss: 1.3222\r\n",
      "Train Epoch: 5 [55680/110534 (50%)]\tClassification Loss: 1.8309\r\n",
      "Train Epoch: 5 [56320/110534 (51%)]\tClassification Loss: 1.5095\r\n",
      "Train Epoch: 5 [56960/110534 (52%)]\tClassification Loss: 1.6903\r\n",
      "Train Epoch: 5 [57600/110534 (52%)]\tClassification Loss: 1.7509\r\n",
      "Test() called at step_no: 7808\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "\r\n",
      "Test set: Average loss: 1.5978, Accuracy: 1086/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 5 [58240/110534 (53%)]\tClassification Loss: 1.3361\r\n",
      "Train Epoch: 5 [58880/110534 (53%)]\tClassification Loss: 1.8610\r\n",
      "Train Epoch: 5 [59520/110534 (54%)]\tClassification Loss: 1.7155\r\n",
      "Train Epoch: 5 [60160/110534 (54%)]\tClassification Loss: 1.7815\r\n",
      "Train Epoch: 5 [60800/110534 (55%)]\tClassification Loss: 2.0222\r\n",
      "Train Epoch: 5 [61440/110534 (56%)]\tClassification Loss: 1.5212\r\n",
      "Train Epoch: 5 [62080/110534 (56%)]\tClassification Loss: 2.1518\r\n",
      "Train Epoch: 5 [62720/110534 (57%)]\tClassification Loss: 1.5965\r\n",
      "Train Epoch: 5 [63360/110534 (57%)]\tClassification Loss: 1.8503\r\n",
      "Train Epoch: 5 [64000/110534 (58%)]\tClassification Loss: 1.7405\r\n",
      "Test() called at step_no: 7908\r\n",
      "\r\n",
      "Test set: Average loss: 1.5944, Accuracy: 1078/1920 (56%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1000.pth.tar\r\n",
      "Train Epoch: 5 [64640/110534 (58%)]\tClassification Loss: 1.7824\r\n",
      "Train Epoch: 5 [65280/110534 (59%)]\tClassification Loss: 1.7522\r\n",
      "Train Epoch: 5 [65920/110534 (60%)]\tClassification Loss: 1.5073\r\n",
      "Train Epoch: 5 [66560/110534 (60%)]\tClassification Loss: 1.5566\r\n",
      "Train Epoch: 5 [67200/110534 (61%)]\tClassification Loss: 1.7844\r\n",
      "Train Epoch: 5 [67840/110534 (61%)]\tClassification Loss: 1.3759\r\n",
      "Train Epoch: 5 [68480/110534 (62%)]\tClassification Loss: 1.4105\r\n",
      "Train Epoch: 5 [69120/110534 (63%)]\tClassification Loss: 1.6053\r\n",
      "Train Epoch: 5 [69760/110534 (63%)]\tClassification Loss: 1.5433\r\n",
      "Train Epoch: 5 [70400/110534 (64%)]\tClassification Loss: 1.5444\r\n",
      "Test() called at step_no: 8008\r\n",
      "\r\n",
      "Test set: Average loss: 1.6003, Accuracy: 1078/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [71040/110534 (64%)]\tClassification Loss: 1.7401\r\n",
      "Train Epoch: 5 [71680/110534 (65%)]\tClassification Loss: 1.5969\r\n",
      "Train Epoch: 5 [72320/110534 (65%)]\tClassification Loss: 1.4329\r\n",
      "Train Epoch: 5 [72960/110534 (66%)]\tClassification Loss: 1.5916\r\n",
      "Train Epoch: 5 [73600/110534 (67%)]\tClassification Loss: 1.5728\r\n",
      "Train Epoch: 5 [74240/110534 (67%)]\tClassification Loss: 1.5026\r\n",
      "Train Epoch: 5 [74880/110534 (68%)]\tClassification Loss: 1.7609\r\n",
      "Train Epoch: 5 [75520/110534 (68%)]\tClassification Loss: 1.8338\r\n",
      "Train Epoch: 5 [76160/110534 (69%)]\tClassification Loss: 1.6127\r\n",
      "Train Epoch: 5 [76800/110534 (69%)]\tClassification Loss: 1.5830\r\n",
      "Test() called at step_no: 8108\r\n",
      "\r\n",
      "Test set: Average loss: 1.5932, Accuracy: 1079/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [77440/110534 (70%)]\tClassification Loss: 1.8918\r\n",
      "Train Epoch: 5 [78080/110534 (71%)]\tClassification Loss: 1.6241\r\n",
      "Train Epoch: 5 [78720/110534 (71%)]\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 5 [79360/110534 (72%)]\tClassification Loss: 2.0841\r\n",
      "Train Epoch: 5 [80000/110534 (72%)]\tClassification Loss: 1.6449\r\n",
      "Train Epoch: 5 [80640/110534 (73%)]\tClassification Loss: 1.8771\r\n",
      "Train Epoch: 5 [81280/110534 (74%)]\tClassification Loss: 1.9522\r\n",
      "Train Epoch: 5 [81920/110534 (74%)]\tClassification Loss: 1.4109\r\n",
      "Train Epoch: 5 [82560/110534 (75%)]\tClassification Loss: 1.8637\r\n",
      "Train Epoch: 5 [83200/110534 (75%)]\tClassification Loss: 1.4853\r\n",
      "Test() called at step_no: 8208\r\n",
      "\r\n",
      "Test set: Average loss: 1.5934, Accuracy: 1082/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [83840/110534 (76%)]\tClassification Loss: 1.8647\r\n",
      "Train Epoch: 5 [84480/110534 (76%)]\tClassification Loss: 1.6186\r\n",
      "Train Epoch: 5 [85120/110534 (77%)]\tClassification Loss: 1.7915\r\n",
      "Train Epoch: 5 [85760/110534 (78%)]\tClassification Loss: 1.8857\r\n",
      "Train Epoch: 5 [86400/110534 (78%)]\tClassification Loss: 1.8741\r\n",
      "Train Epoch: 5 [87040/110534 (79%)]\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 5 [87680/110534 (79%)]\tClassification Loss: 1.4085\r\n",
      "Train Epoch: 5 [88320/110534 (80%)]\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 5 [88960/110534 (80%)]\tClassification Loss: 1.3960\r\n",
      "Train Epoch: 5 [89600/110534 (81%)]\tClassification Loss: 1.6278\r\n",
      "Test() called at step_no: 8308\r\n",
      "\r\n",
      "Test set: Average loss: 1.5932, Accuracy: 1084/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [90240/110534 (82%)]\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 5 [90880/110534 (82%)]\tClassification Loss: 2.0443\r\n",
      "Train Epoch: 5 [91520/110534 (83%)]\tClassification Loss: 1.3788\r\n",
      "Train Epoch: 5 [92160/110534 (83%)]\tClassification Loss: 1.6886\r\n",
      "Train Epoch: 5 [92800/110534 (84%)]\tClassification Loss: 1.1899\r\n",
      "Train Epoch: 5 [93440/110534 (85%)]\tClassification Loss: 1.4547\r\n",
      "Train Epoch: 5 [94080/110534 (85%)]\tClassification Loss: 1.8158\r\n",
      "Train Epoch: 5 [94720/110534 (86%)]\tClassification Loss: 1.4594\r\n",
      "Train Epoch: 5 [95360/110534 (86%)]\tClassification Loss: 1.5154\r\n",
      "Train Epoch: 5 [96000/110534 (87%)]\tClassification Loss: 1.4021\r\n",
      "Test() called at step_no: 8408\r\n",
      "\r\n",
      "Test set: Average loss: 1.5915, Accuracy: 1080/1920 (56%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [96640/110534 (87%)]\tClassification Loss: 1.7007\r\n",
      "Train Epoch: 5 [97280/110534 (88%)]\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 5 [97920/110534 (89%)]\tClassification Loss: 1.6726\r\n",
      "Train Epoch: 5 [98560/110534 (89%)]\tClassification Loss: 1.7520\r\n",
      "Train Epoch: 5 [99200/110534 (90%)]\tClassification Loss: 1.5625\r\n",
      "Train Epoch: 5 [99840/110534 (90%)]\tClassification Loss: 1.6850\r\n",
      "Train Epoch: 5 [100480/110534 (91%)]\tClassification Loss: 1.7011\r\n",
      "Train Epoch: 5 [101120/110534 (91%)]\tClassification Loss: 1.6285\r\n",
      "Train Epoch: 5 [101760/110534 (92%)]\tClassification Loss: 1.5348\r\n",
      "Train Epoch: 5 [102400/110534 (93%)]\tClassification Loss: 1.6337\r\n",
      "Test() called at step_no: 8508\r\n",
      "\r\n",
      "Test set: Average loss: 1.5933, Accuracy: 1085/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 5 [103040/110534 (93%)]\tClassification Loss: 1.7683\r\n",
      "Train Epoch: 5 [103680/110534 (94%)]\tClassification Loss: 1.5693\r\n",
      "Train Epoch: 5 [104320/110534 (94%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 5 [104960/110534 (95%)]\tClassification Loss: 1.5753\r\n",
      "Train Epoch: 5 [105600/110534 (96%)]\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 5 [106240/110534 (96%)]\tClassification Loss: 1.5032\r\n",
      "Train Epoch: 5 [106880/110534 (97%)]\tClassification Loss: 1.2764\r\n",
      "Train Epoch: 5 [107520/110534 (97%)]\tClassification Loss: 1.4654\r\n",
      "Train Epoch: 5 [108160/110534 (98%)]\tClassification Loss: 1.5597\r\n",
      "Train Epoch: 5 [108800/110534 (98%)]\tClassification Loss: 1.7139\r\n",
      "Test() called at step_no: 8608\r\n",
      "\r\n",
      "Test set: Average loss: 1.5942, Accuracy: 1082/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 5 [109440/110534 (99%)]\tClassification Loss: 1.7402\r\n",
      "Train Epoch: 5 [110080/110534 (100%)]\tClassification Loss: 1.6670\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/110534 (0%)]\tClassification Loss: 1.7949\r\n",
      "Test() called at step_no: 8635\r\n",
      "\r\n",
      "Test set: Average loss: 1.5946, Accuracy: 1078/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 6 [640/110534 (1%)]\tClassification Loss: 1.5155\r\n",
      "Train Epoch: 6 [1280/110534 (1%)]\tClassification Loss: 1.4270\r\n",
      "Train Epoch: 6 [1920/110534 (2%)]\tClassification Loss: 1.3646\r\n",
      "Train Epoch: 6 [2560/110534 (2%)]\tClassification Loss: 1.5203\r\n",
      "Train Epoch: 6 [3200/110534 (3%)]\tClassification Loss: 1.5904\r\n",
      "Train Epoch: 6 [3840/110534 (3%)]\tClassification Loss: 1.6545\r\n",
      "Train Epoch: 6 [4480/110534 (4%)]\tClassification Loss: 1.8810\r\n",
      "Train Epoch: 6 [5120/110534 (5%)]\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 6 [5760/110534 (5%)]\tClassification Loss: 1.6947\r\n",
      "Train Epoch: 6 [6400/110534 (6%)]\tClassification Loss: 1.6488\r\n",
      "Test() called at step_no: 8735\r\n",
      "\r\n",
      "Test set: Average loss: 1.5860, Accuracy: 1087/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [7040/110534 (6%)]\tClassification Loss: 1.8656\r\n",
      "Train Epoch: 6 [7680/110534 (7%)]\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 6 [8320/110534 (8%)]\tClassification Loss: 1.7653\r\n",
      "Train Epoch: 6 [8960/110534 (8%)]\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 6 [9600/110534 (9%)]\tClassification Loss: 1.7492\r\n",
      "Train Epoch: 6 [10240/110534 (9%)]\tClassification Loss: 1.7198\r\n",
      "Train Epoch: 6 [10880/110534 (10%)]\tClassification Loss: 1.7912\r\n",
      "Train Epoch: 6 [11520/110534 (10%)]\tClassification Loss: 1.9215\r\n",
      "Train Epoch: 6 [12160/110534 (11%)]\tClassification Loss: 1.9102\r\n",
      "Train Epoch: 6 [12800/110534 (12%)]\tClassification Loss: 1.8989\r\n",
      "Test() called at step_no: 8835\r\n",
      "\r\n",
      "Test set: Average loss: 1.5901, Accuracy: 1088/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [13440/110534 (12%)]\tClassification Loss: 1.8225\r\n",
      "Train Epoch: 6 [14080/110534 (13%)]\tClassification Loss: 1.3461\r\n",
      "Train Epoch: 6 [14720/110534 (13%)]\tClassification Loss: 1.4693\r\n",
      "Train Epoch: 6 [15360/110534 (14%)]\tClassification Loss: 1.7347\r\n",
      "Train Epoch: 6 [16000/110534 (14%)]\tClassification Loss: 1.9359\r\n",
      "Train Epoch: 6 [16640/110534 (15%)]\tClassification Loss: 1.6581\r\n",
      "Train Epoch: 6 [17280/110534 (16%)]\tClassification Loss: 1.8054\r\n",
      "Train Epoch: 6 [17920/110534 (16%)]\tClassification Loss: 1.5293\r\n",
      "Train Epoch: 6 [18560/110534 (17%)]\tClassification Loss: 1.7957\r\n",
      "Train Epoch: 6 [19200/110534 (17%)]\tClassification Loss: 1.6809\r\n",
      "Test() called at step_no: 8935\r\n",
      "\r\n",
      "Test set: Average loss: 1.5863, Accuracy: 1087/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [19840/110534 (18%)]\tClassification Loss: 1.6743\r\n",
      "Train Epoch: 6 [20480/110534 (19%)]\tClassification Loss: 1.6910\r\n",
      "Train Epoch: 6 [21120/110534 (19%)]\tClassification Loss: 1.4996\r\n",
      "Train Epoch: 6 [21760/110534 (20%)]\tClassification Loss: 1.7471\r\n",
      "Train Epoch: 6 [22400/110534 (20%)]\tClassification Loss: 1.9270\r\n",
      "Train Epoch: 6 [23040/110534 (21%)]\tClassification Loss: 1.5321\r\n",
      "Train Epoch: 6 [23680/110534 (21%)]\tClassification Loss: 1.6896\r\n",
      "Train Epoch: 6 [24320/110534 (22%)]\tClassification Loss: 1.4743\r\n",
      "Train Epoch: 6 [24960/110534 (23%)]\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 6 [25600/110534 (23%)]\tClassification Loss: 1.8955\r\n",
      "Test() called at step_no: 9035\r\n",
      "\r\n",
      "Test set: Average loss: 1.5869, Accuracy: 1081/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 6 [26240/110534 (24%)]\tClassification Loss: 1.7293\r\n",
      "Train Epoch: 6 [26880/110534 (24%)]\tClassification Loss: 1.6882\r\n",
      "Train Epoch: 6 [27520/110534 (25%)]\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 6 [28160/110534 (25%)]\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 6 [28800/110534 (26%)]\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 6 [29440/110534 (27%)]\tClassification Loss: 1.5440\r\n",
      "Train Epoch: 6 [30080/110534 (27%)]\tClassification Loss: 1.5198\r\n",
      "Train Epoch: 6 [30720/110534 (28%)]\tClassification Loss: 1.7552\r\n",
      "Train Epoch: 6 [31360/110534 (28%)]\tClassification Loss: 1.7234\r\n",
      "Train Epoch: 6 [32000/110534 (29%)]\tClassification Loss: 1.4748\r\n",
      "Test() called at step_no: 9135\r\n",
      "\r\n",
      "Test set: Average loss: 1.5883, Accuracy: 1077/1920 (56%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_500.pth.tar\r\n",
      "Train Epoch: 6 [32640/110534 (30%)]\tClassification Loss: 1.4658\r\n",
      "Train Epoch: 6 [33280/110534 (30%)]\tClassification Loss: 1.4849\r\n",
      "Train Epoch: 6 [33920/110534 (31%)]\tClassification Loss: 1.7174\r\n",
      "Train Epoch: 6 [34560/110534 (31%)]\tClassification Loss: 1.7231\r\n",
      "Train Epoch: 6 [35200/110534 (32%)]\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 6 [35840/110534 (32%)]\tClassification Loss: 1.3469\r\n",
      "Train Epoch: 6 [36480/110534 (33%)]\tClassification Loss: 1.5990\r\n",
      "Train Epoch: 6 [37120/110534 (34%)]\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 6 [37760/110534 (34%)]\tClassification Loss: 1.6234\r\n",
      "Train Epoch: 6 [38400/110534 (35%)]\tClassification Loss: 1.5137\r\n",
      "Test() called at step_no: 9235\r\n",
      "\r\n",
      "Test set: Average loss: 1.5850, Accuracy: 1082/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 6 [39040/110534 (35%)]\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 6 [39680/110534 (36%)]\tClassification Loss: 1.6550\r\n",
      "Train Epoch: 6 [40320/110534 (36%)]\tClassification Loss: 1.7109\r\n",
      "Train Epoch: 6 [40960/110534 (37%)]\tClassification Loss: 1.5260\r\n",
      "Train Epoch: 6 [41600/110534 (38%)]\tClassification Loss: 1.6725\r\n",
      "Train Epoch: 6 [42240/110534 (38%)]\tClassification Loss: 1.4454\r\n",
      "Train Epoch: 6 [42880/110534 (39%)]\tClassification Loss: 1.5857\r\n",
      "Train Epoch: 6 [43520/110534 (39%)]\tClassification Loss: 1.6688\r\n",
      "Train Epoch: 6 [44160/110534 (40%)]\tClassification Loss: 1.4804\r\n",
      "Train Epoch: 6 [44800/110534 (41%)]\tClassification Loss: 1.3424\r\n",
      "Test() called at step_no: 9335\r\n",
      "\r\n",
      "Test set: Average loss: 1.5849, Accuracy: 1085/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [45440/110534 (41%)]\tClassification Loss: 1.7749\r\n",
      "Train Epoch: 6 [46080/110534 (42%)]\tClassification Loss: 1.6620\r\n",
      "Train Epoch: 6 [46720/110534 (42%)]\tClassification Loss: 1.9218\r\n",
      "Train Epoch: 6 [47360/110534 (43%)]\tClassification Loss: 1.6951\r\n",
      "Train Epoch: 6 [48000/110534 (43%)]\tClassification Loss: 1.6991\r\n",
      "Train Epoch: 6 [48640/110534 (44%)]\tClassification Loss: 1.4770\r\n",
      "Train Epoch: 6 [49280/110534 (45%)]\tClassification Loss: 1.3373\r\n",
      "Train Epoch: 6 [49920/110534 (45%)]\tClassification Loss: 1.3090\r\n",
      "Train Epoch: 6 [50560/110534 (46%)]\tClassification Loss: 1.6606\r\n",
      "Train Epoch: 6 [51200/110534 (46%)]\tClassification Loss: 1.6053\r\n",
      "Test() called at step_no: 9435\r\n",
      "\r\n",
      "Test set: Average loss: 1.5817, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [51840/110534 (47%)]\tClassification Loss: 1.3580\r\n",
      "Train Epoch: 6 [52480/110534 (47%)]\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 6 [53120/110534 (48%)]\tClassification Loss: 1.6522\r\n",
      "Train Epoch: 6 [53760/110534 (49%)]\tClassification Loss: 1.3819\r\n",
      "Train Epoch: 6 [54400/110534 (49%)]\tClassification Loss: 1.8634\r\n",
      "Train Epoch: 6 [55040/110534 (50%)]\tClassification Loss: 1.4388\r\n",
      "Train Epoch: 6 [55680/110534 (50%)]\tClassification Loss: 1.8892\r\n",
      "Train Epoch: 6 [56320/110534 (51%)]\tClassification Loss: 1.5970\r\n",
      "Train Epoch: 6 [56960/110534 (52%)]\tClassification Loss: 1.5206\r\n",
      "Train Epoch: 6 [57600/110534 (52%)]\tClassification Loss: 1.7359\r\n",
      "Test() called at step_no: 9535\r\n",
      "\r\n",
      "Test set: Average loss: 1.5852, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [58240/110534 (53%)]\tClassification Loss: 1.4582\r\n",
      "Train Epoch: 6 [58880/110534 (53%)]\tClassification Loss: 1.8987\r\n",
      "Train Epoch: 6 [59520/110534 (54%)]\tClassification Loss: 1.6618\r\n",
      "Train Epoch: 6 [60160/110534 (54%)]\tClassification Loss: 1.8503\r\n",
      "Train Epoch: 6 [60800/110534 (55%)]\tClassification Loss: 1.9322\r\n",
      "Train Epoch: 6 [61440/110534 (56%)]\tClassification Loss: 1.5406\r\n",
      "Train Epoch: 6 [62080/110534 (56%)]\tClassification Loss: 2.0338\r\n",
      "Train Epoch: 6 [62720/110534 (57%)]\tClassification Loss: 1.7261\r\n",
      "Train Epoch: 6 [63360/110534 (57%)]\tClassification Loss: 1.7779\r\n",
      "Train Epoch: 6 [64000/110534 (58%)]\tClassification Loss: 1.6990\r\n",
      "Test() called at step_no: 9635\r\n",
      "\r\n",
      "Test set: Average loss: 1.5797, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1000.pth.tar\r\n",
      "Train Epoch: 6 [64640/110534 (58%)]\tClassification Loss: 1.6643\r\n",
      "Train Epoch: 6 [65280/110534 (59%)]\tClassification Loss: 1.6112\r\n",
      "Train Epoch: 6 [65920/110534 (60%)]\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 6 [66560/110534 (60%)]\tClassification Loss: 1.6611\r\n",
      "Train Epoch: 6 [67200/110534 (61%)]\tClassification Loss: 1.7299\r\n",
      "Train Epoch: 6 [67840/110534 (61%)]\tClassification Loss: 1.3405\r\n",
      "Train Epoch: 6 [68480/110534 (62%)]\tClassification Loss: 1.4912\r\n",
      "Train Epoch: 6 [69120/110534 (63%)]\tClassification Loss: 1.4500\r\n",
      "Train Epoch: 6 [69760/110534 (63%)]\tClassification Loss: 1.6555\r\n",
      "Train Epoch: 6 [70400/110534 (64%)]\tClassification Loss: 1.6941\r\n",
      "Test() called at step_no: 9735\r\n",
      "\r\n",
      "Test set: Average loss: 1.5855, Accuracy: 1083/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 6 [71040/110534 (64%)]\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 6 [71680/110534 (65%)]\tClassification Loss: 1.5733\r\n",
      "Train Epoch: 6 [72320/110534 (65%)]\tClassification Loss: 1.5170\r\n",
      "Train Epoch: 6 [72960/110534 (66%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 6 [73600/110534 (67%)]\tClassification Loss: 1.5795\r\n",
      "Train Epoch: 6 [74240/110534 (67%)]\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 6 [74880/110534 (68%)]\tClassification Loss: 1.4790\r\n",
      "Train Epoch: 6 [75520/110534 (68%)]\tClassification Loss: 1.6267\r\n",
      "Train Epoch: 6 [76160/110534 (69%)]\tClassification Loss: 1.5583\r\n",
      "Train Epoch: 6 [76800/110534 (69%)]\tClassification Loss: 1.4730\r\n",
      "Test() called at step_no: 9835\r\n",
      "\r\n",
      "Test set: Average loss: 1.5790, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [77440/110534 (70%)]\tClassification Loss: 1.8423\r\n",
      "Train Epoch: 6 [78080/110534 (71%)]\tClassification Loss: 1.5995\r\n",
      "Train Epoch: 6 [78720/110534 (71%)]\tClassification Loss: 1.5152\r\n",
      "Train Epoch: 6 [79360/110534 (72%)]\tClassification Loss: 2.0021\r\n",
      "Train Epoch: 6 [80000/110534 (72%)]\tClassification Loss: 1.5663\r\n",
      "Train Epoch: 6 [80640/110534 (73%)]\tClassification Loss: 1.7837\r\n",
      "Train Epoch: 6 [81280/110534 (74%)]\tClassification Loss: 1.9587\r\n",
      "Train Epoch: 6 [81920/110534 (74%)]\tClassification Loss: 1.2947\r\n",
      "Train Epoch: 6 [82560/110534 (75%)]\tClassification Loss: 1.9018\r\n",
      "Train Epoch: 6 [83200/110534 (75%)]\tClassification Loss: 1.3776\r\n",
      "Test() called at step_no: 9935\r\n",
      "\r\n",
      "Test set: Average loss: 1.5790, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [83840/110534 (76%)]\tClassification Loss: 1.9239\r\n",
      "Train Epoch: 6 [84480/110534 (76%)]\tClassification Loss: 1.6224\r\n",
      "Train Epoch: 6 [85120/110534 (77%)]\tClassification Loss: 1.7603\r\n",
      "Train Epoch: 6 [85760/110534 (78%)]\tClassification Loss: 2.0067\r\n",
      "Train Epoch: 6 [86400/110534 (78%)]\tClassification Loss: 1.8261\r\n",
      "Train Epoch: 6 [87040/110534 (79%)]\tClassification Loss: 1.6476\r\n",
      "Train Epoch: 6 [87680/110534 (79%)]\tClassification Loss: 1.6018\r\n",
      "Train Epoch: 6 [88320/110534 (80%)]\tClassification Loss: 1.5502\r\n",
      "Train Epoch: 6 [88960/110534 (80%)]\tClassification Loss: 1.3755\r\n",
      "Train Epoch: 6 [89600/110534 (81%)]\tClassification Loss: 1.6379\r\n",
      "Test() called at step_no: 10035\r\n",
      "\r\n",
      "Test set: Average loss: 1.5807, Accuracy: 1087/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [90240/110534 (82%)]\tClassification Loss: 1.6920\r\n",
      "Train Epoch: 6 [90880/110534 (82%)]\tClassification Loss: 2.0048\r\n",
      "Train Epoch: 6 [91520/110534 (83%)]\tClassification Loss: 1.4369\r\n",
      "Train Epoch: 6 [92160/110534 (83%)]\tClassification Loss: 1.5559\r\n",
      "Train Epoch: 6 [92800/110534 (84%)]\tClassification Loss: 1.2736\r\n",
      "Train Epoch: 6 [93440/110534 (85%)]\tClassification Loss: 1.3062\r\n",
      "Train Epoch: 6 [94080/110534 (85%)]\tClassification Loss: 1.8332\r\n",
      "Train Epoch: 6 [94720/110534 (86%)]\tClassification Loss: 1.4193\r\n",
      "Train Epoch: 6 [95360/110534 (86%)]\tClassification Loss: 1.5166\r\n",
      "Train Epoch: 6 [96000/110534 (87%)]\tClassification Loss: 1.3853\r\n",
      "Test() called at step_no: 10135\r\n",
      "\r\n",
      "Test set: Average loss: 1.5782, Accuracy: 1085/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [96640/110534 (87%)]\tClassification Loss: 1.6832\r\n",
      "Train Epoch: 6 [97280/110534 (88%)]\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 6 [97920/110534 (89%)]\tClassification Loss: 1.7501\r\n",
      "Train Epoch: 6 [98560/110534 (89%)]\tClassification Loss: 1.8174\r\n",
      "Train Epoch: 6 [99200/110534 (90%)]\tClassification Loss: 1.5267\r\n",
      "Train Epoch: 6 [99840/110534 (90%)]\tClassification Loss: 1.5524\r\n",
      "Train Epoch: 6 [100480/110534 (91%)]\tClassification Loss: 1.7670\r\n",
      "Train Epoch: 6 [101120/110534 (91%)]\tClassification Loss: 1.6086\r\n",
      "Train Epoch: 6 [101760/110534 (92%)]\tClassification Loss: 1.5686\r\n",
      "Train Epoch: 6 [102400/110534 (93%)]\tClassification Loss: 1.6725\r\n",
      "Test() called at step_no: 10235\r\n",
      "\r\n",
      "Test set: Average loss: 1.5785, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [103040/110534 (93%)]\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 6 [103680/110534 (94%)]\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 6 [104320/110534 (94%)]\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 6 [104960/110534 (95%)]\tClassification Loss: 1.6516\r\n",
      "Train Epoch: 6 [105600/110534 (96%)]\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 6 [106240/110534 (96%)]\tClassification Loss: 1.4975\r\n",
      "Train Epoch: 6 [106880/110534 (97%)]\tClassification Loss: 1.2319\r\n",
      "Train Epoch: 6 [107520/110534 (97%)]\tClassification Loss: 1.8033\r\n",
      "Train Epoch: 6 [108160/110534 (98%)]\tClassification Loss: 1.5589\r\n",
      "Train Epoch: 6 [108800/110534 (98%)]\tClassification Loss: 1.7616\r\n",
      "Test() called at step_no: 10335\r\n",
      "\r\n",
      "Test set: Average loss: 1.5810, Accuracy: 1088/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 6 [109440/110534 (99%)]\tClassification Loss: 1.5311\r\n",
      "Train Epoch: 6 [110080/110534 (100%)]\tClassification Loss: 1.5664\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_final.pth.tar\r\n",
      "Train Epoch: 7 [0/110534 (0%)]\tClassification Loss: 1.7246\r\n",
      "Test() called at step_no: 10362\r\n",
      "\r\n",
      "Test set: Average loss: 1.5808, Accuracy: 1085/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [640/110534 (1%)]\tClassification Loss: 1.6124\r\n",
      "Train Epoch: 7 [1280/110534 (1%)]\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 7 [1920/110534 (2%)]\tClassification Loss: 1.4798\r\n",
      "Train Epoch: 7 [2560/110534 (2%)]\tClassification Loss: 1.4690\r\n",
      "Train Epoch: 7 [3200/110534 (3%)]\tClassification Loss: 1.5633\r\n",
      "Train Epoch: 7 [3840/110534 (3%)]\tClassification Loss: 1.6195\r\n",
      "Train Epoch: 7 [4480/110534 (4%)]\tClassification Loss: 1.8240\r\n",
      "Train Epoch: 7 [5120/110534 (5%)]\tClassification Loss: 1.6746\r\n",
      "Train Epoch: 7 [5760/110534 (5%)]\tClassification Loss: 1.5834\r\n",
      "Train Epoch: 7 [6400/110534 (6%)]\tClassification Loss: 1.6532\r\n",
      "Test() called at step_no: 10462\r\n",
      "\r\n",
      "Test set: Average loss: 1.5745, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [7040/110534 (6%)]\tClassification Loss: 1.8090\r\n",
      "Train Epoch: 7 [7680/110534 (7%)]\tClassification Loss: 1.4927\r\n",
      "Train Epoch: 7 [8320/110534 (8%)]\tClassification Loss: 1.6569\r\n",
      "Train Epoch: 7 [8960/110534 (8%)]\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 7 [9600/110534 (9%)]\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 7 [10240/110534 (9%)]\tClassification Loss: 1.7929\r\n",
      "Train Epoch: 7 [10880/110534 (10%)]\tClassification Loss: 1.8036\r\n",
      "Train Epoch: 7 [11520/110534 (10%)]\tClassification Loss: 1.9652\r\n",
      "Train Epoch: 7 [12160/110534 (11%)]\tClassification Loss: 1.9137\r\n",
      "Train Epoch: 7 [12800/110534 (12%)]\tClassification Loss: 1.9612\r\n",
      "Test() called at step_no: 10562\r\n",
      "\r\n",
      "Test set: Average loss: 1.5779, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [13440/110534 (12%)]\tClassification Loss: 1.6632\r\n",
      "Train Epoch: 7 [14080/110534 (13%)]\tClassification Loss: 1.4816\r\n",
      "Train Epoch: 7 [14720/110534 (13%)]\tClassification Loss: 1.6423\r\n",
      "Train Epoch: 7 [15360/110534 (14%)]\tClassification Loss: 1.6229\r\n",
      "Train Epoch: 7 [16000/110534 (14%)]\tClassification Loss: 2.0606\r\n",
      "Train Epoch: 7 [16640/110534 (15%)]\tClassification Loss: 1.6324\r\n",
      "Train Epoch: 7 [17280/110534 (16%)]\tClassification Loss: 1.6380\r\n",
      "Train Epoch: 7 [17920/110534 (16%)]\tClassification Loss: 1.6692\r\n",
      "Train Epoch: 7 [18560/110534 (17%)]\tClassification Loss: 1.7647\r\n",
      "Train Epoch: 7 [19200/110534 (17%)]\tClassification Loss: 1.6927\r\n",
      "Test() called at step_no: 10662\r\n",
      "\r\n",
      "Test set: Average loss: 1.5747, Accuracy: 1098/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [19840/110534 (18%)]\tClassification Loss: 1.4791\r\n",
      "Train Epoch: 7 [20480/110534 (19%)]\tClassification Loss: 1.6427\r\n",
      "Train Epoch: 7 [21120/110534 (19%)]\tClassification Loss: 1.5858\r\n",
      "Train Epoch: 7 [21760/110534 (20%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 7 [22400/110534 (20%)]\tClassification Loss: 1.8672\r\n",
      "Train Epoch: 7 [23040/110534 (21%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 7 [23680/110534 (21%)]\tClassification Loss: 1.5965\r\n",
      "Train Epoch: 7 [24320/110534 (22%)]\tClassification Loss: 1.4278\r\n",
      "Train Epoch: 7 [24960/110534 (23%)]\tClassification Loss: 1.4345\r\n",
      "Train Epoch: 7 [25600/110534 (23%)]\tClassification Loss: 1.9732\r\n",
      "Test() called at step_no: 10762\r\n",
      "\r\n",
      "Test set: Average loss: 1.5750, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [26240/110534 (24%)]\tClassification Loss: 1.6073\r\n",
      "Train Epoch: 7 [26880/110534 (24%)]\tClassification Loss: 1.5999\r\n",
      "Train Epoch: 7 [27520/110534 (25%)]\tClassification Loss: 1.2750\r\n",
      "Train Epoch: 7 [28160/110534 (25%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 7 [28800/110534 (26%)]\tClassification Loss: 1.8283\r\n",
      "Train Epoch: 7 [29440/110534 (27%)]\tClassification Loss: 1.5025\r\n",
      "Train Epoch: 7 [30080/110534 (27%)]\tClassification Loss: 1.8279\r\n",
      "Train Epoch: 7 [30720/110534 (28%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 7 [31360/110534 (28%)]\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 7 [32000/110534 (29%)]\tClassification Loss: 1.4549\r\n",
      "Test() called at step_no: 10862\r\n",
      "\r\n",
      "Test set: Average loss: 1.5747, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_500.pth.tar\r\n",
      "Train Epoch: 7 [32640/110534 (30%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 7 [33280/110534 (30%)]\tClassification Loss: 1.3199\r\n",
      "Train Epoch: 7 [33920/110534 (31%)]\tClassification Loss: 1.7655\r\n",
      "Train Epoch: 7 [34560/110534 (31%)]\tClassification Loss: 1.7229\r\n",
      "Train Epoch: 7 [35200/110534 (32%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 7 [35840/110534 (32%)]\tClassification Loss: 1.3366\r\n",
      "Train Epoch: 7 [36480/110534 (33%)]\tClassification Loss: 1.5730\r\n",
      "Train Epoch: 7 [37120/110534 (34%)]\tClassification Loss: 1.5242\r\n",
      "Train Epoch: 7 [37760/110534 (34%)]\tClassification Loss: 1.7750\r\n",
      "Train Epoch: 7 [38400/110534 (35%)]\tClassification Loss: 1.6359\r\n",
      "Test() called at step_no: 10962\r\n",
      "\r\n",
      "Test set: Average loss: 1.5734, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [39040/110534 (35%)]\tClassification Loss: 1.2936\r\n",
      "Train Epoch: 7 [39680/110534 (36%)]\tClassification Loss: 1.7802\r\n",
      "Train Epoch: 7 [40320/110534 (36%)]\tClassification Loss: 1.7241\r\n",
      "Train Epoch: 7 [40960/110534 (37%)]\tClassification Loss: 1.4221\r\n",
      "Train Epoch: 7 [41600/110534 (38%)]\tClassification Loss: 1.6078\r\n",
      "Train Epoch: 7 [42240/110534 (38%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 7 [42880/110534 (39%)]\tClassification Loss: 1.6075\r\n",
      "Train Epoch: 7 [43520/110534 (39%)]\tClassification Loss: 1.5883\r\n",
      "Train Epoch: 7 [44160/110534 (40%)]\tClassification Loss: 1.5664\r\n",
      "Train Epoch: 7 [44800/110534 (41%)]\tClassification Loss: 1.3666\r\n",
      "Test() called at step_no: 11062\r\n",
      "\r\n",
      "Test set: Average loss: 1.5739, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [45440/110534 (41%)]\tClassification Loss: 1.7552\r\n",
      "Train Epoch: 7 [46080/110534 (42%)]\tClassification Loss: 1.6913\r\n",
      "Train Epoch: 7 [46720/110534 (42%)]\tClassification Loss: 1.9497\r\n",
      "Train Epoch: 7 [47360/110534 (43%)]\tClassification Loss: 1.7052\r\n",
      "Train Epoch: 7 [48000/110534 (43%)]\tClassification Loss: 1.7174\r\n",
      "Train Epoch: 7 [48640/110534 (44%)]\tClassification Loss: 1.6311\r\n",
      "Train Epoch: 7 [49280/110534 (45%)]\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 7 [49920/110534 (45%)]\tClassification Loss: 1.2196\r\n",
      "Train Epoch: 7 [50560/110534 (46%)]\tClassification Loss: 1.7362\r\n",
      "Train Epoch: 7 [51200/110534 (46%)]\tClassification Loss: 1.5739\r\n",
      "Test() called at step_no: 11162\r\n",
      "\r\n",
      "Test set: Average loss: 1.5731, Accuracy: 1093/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [51840/110534 (47%)]\tClassification Loss: 1.3527\r\n",
      "Train Epoch: 7 [52480/110534 (47%)]\tClassification Loss: 1.5070\r\n",
      "Train Epoch: 7 [53120/110534 (48%)]\tClassification Loss: 1.6376\r\n",
      "Train Epoch: 7 [53760/110534 (49%)]\tClassification Loss: 1.6026\r\n",
      "Train Epoch: 7 [54400/110534 (49%)]\tClassification Loss: 1.8512\r\n",
      "Train Epoch: 7 [55040/110534 (50%)]\tClassification Loss: 1.4349\r\n",
      "Train Epoch: 7 [55680/110534 (50%)]\tClassification Loss: 1.7370\r\n",
      "Train Epoch: 7 [56320/110534 (51%)]\tClassification Loss: 1.4420\r\n",
      "Train Epoch: 7 [56960/110534 (52%)]\tClassification Loss: 1.6031\r\n",
      "Train Epoch: 7 [57600/110534 (52%)]\tClassification Loss: 1.7231\r\n",
      "Test() called at step_no: 11262\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\r\n",
      "    n = write(self._handle, buf)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "\r\n",
      "Test set: Average loss: 1.5730, Accuracy: 1092/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [58240/110534 (53%)]\tClassification Loss: 1.4523\r\n",
      "Train Epoch: 7 [58880/110534 (53%)]\tClassification Loss: 1.8338\r\n",
      "Train Epoch: 7 [59520/110534 (54%)]\tClassification Loss: 1.7392\r\n",
      "Train Epoch: 7 [60160/110534 (54%)]\tClassification Loss: 1.8247\r\n",
      "Train Epoch: 7 [60800/110534 (55%)]\tClassification Loss: 1.9024\r\n",
      "Train Epoch: 7 [61440/110534 (56%)]\tClassification Loss: 1.6193\r\n",
      "Train Epoch: 7 [62080/110534 (56%)]\tClassification Loss: 2.0708\r\n",
      "Train Epoch: 7 [62720/110534 (57%)]\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 7 [63360/110534 (57%)]\tClassification Loss: 1.7674\r\n",
      "Train Epoch: 7 [64000/110534 (58%)]\tClassification Loss: 1.5958\r\n",
      "Test() called at step_no: 11362\r\n",
      "\r\n",
      "Test set: Average loss: 1.5701, Accuracy: 1090/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1000.pth.tar\r\n",
      "Train Epoch: 7 [64640/110534 (58%)]\tClassification Loss: 1.6101\r\n",
      "Train Epoch: 7 [65280/110534 (59%)]\tClassification Loss: 1.6324\r\n",
      "Train Epoch: 7 [65920/110534 (60%)]\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 7 [66560/110534 (60%)]\tClassification Loss: 1.6265\r\n",
      "Train Epoch: 7 [67200/110534 (61%)]\tClassification Loss: 1.6909\r\n",
      "Train Epoch: 7 [67840/110534 (61%)]\tClassification Loss: 1.3502\r\n",
      "Train Epoch: 7 [68480/110534 (62%)]\tClassification Loss: 1.3618\r\n",
      "Train Epoch: 7 [69120/110534 (63%)]\tClassification Loss: 1.6537\r\n",
      "Train Epoch: 7 [69760/110534 (63%)]\tClassification Loss: 1.5166\r\n",
      "Train Epoch: 7 [70400/110534 (64%)]\tClassification Loss: 1.6849\r\n",
      "Test() called at step_no: 11462\r\n",
      "\r\n",
      "Test set: Average loss: 1.5740, Accuracy: 1083/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 7 [71040/110534 (64%)]\tClassification Loss: 1.7559\r\n",
      "Train Epoch: 7 [71680/110534 (65%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 7 [72320/110534 (65%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 7 [72960/110534 (66%)]\tClassification Loss: 1.5233\r\n",
      "Train Epoch: 7 [73600/110534 (67%)]\tClassification Loss: 1.6223\r\n",
      "Train Epoch: 7 [74240/110534 (67%)]\tClassification Loss: 1.4545\r\n",
      "Train Epoch: 7 [74880/110534 (68%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 7 [75520/110534 (68%)]\tClassification Loss: 1.5609\r\n",
      "Train Epoch: 7 [76160/110534 (69%)]\tClassification Loss: 1.5132\r\n",
      "Train Epoch: 7 [76800/110534 (69%)]\tClassification Loss: 1.4411\r\n",
      "Test() called at step_no: 11562\r\n",
      "\r\n",
      "Test set: Average loss: 1.5690, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [77440/110534 (70%)]\tClassification Loss: 1.8095\r\n",
      "Train Epoch: 7 [78080/110534 (71%)]\tClassification Loss: 1.7892\r\n",
      "Train Epoch: 7 [78720/110534 (71%)]\tClassification Loss: 1.6844\r\n",
      "Train Epoch: 7 [79360/110534 (72%)]\tClassification Loss: 1.9834\r\n",
      "Train Epoch: 7 [80000/110534 (72%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 7 [80640/110534 (73%)]\tClassification Loss: 1.6954\r\n",
      "Train Epoch: 7 [81280/110534 (74%)]\tClassification Loss: 1.9714\r\n",
      "Train Epoch: 7 [81920/110534 (74%)]\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 7 [82560/110534 (75%)]\tClassification Loss: 1.8244\r\n",
      "Train Epoch: 7 [83200/110534 (75%)]\tClassification Loss: 1.5845\r\n",
      "Test() called at step_no: 11662\r\n",
      "\r\n",
      "Test set: Average loss: 1.5692, Accuracy: 1099/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [83840/110534 (76%)]\tClassification Loss: 1.7906\r\n",
      "Train Epoch: 7 [84480/110534 (76%)]\tClassification Loss: 1.6285\r\n",
      "Train Epoch: 7 [85120/110534 (77%)]\tClassification Loss: 1.7892\r\n",
      "Train Epoch: 7 [85760/110534 (78%)]\tClassification Loss: 1.9608\r\n",
      "Train Epoch: 7 [86400/110534 (78%)]\tClassification Loss: 1.7096\r\n",
      "Train Epoch: 7 [87040/110534 (79%)]\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 7 [87680/110534 (79%)]\tClassification Loss: 1.3170\r\n",
      "Train Epoch: 7 [88320/110534 (80%)]\tClassification Loss: 1.5954\r\n",
      "Train Epoch: 7 [88960/110534 (80%)]\tClassification Loss: 1.3639\r\n",
      "Train Epoch: 7 [89600/110534 (81%)]\tClassification Loss: 1.4152\r\n",
      "Test() called at step_no: 11762\r\n",
      "\r\n",
      "Test set: Average loss: 1.5702, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [90240/110534 (82%)]\tClassification Loss: 1.5602\r\n",
      "Train Epoch: 7 [90880/110534 (82%)]\tClassification Loss: 1.9534\r\n",
      "Train Epoch: 7 [91520/110534 (83%)]\tClassification Loss: 1.4019\r\n",
      "Train Epoch: 7 [92160/110534 (83%)]\tClassification Loss: 1.6315\r\n",
      "Train Epoch: 7 [92800/110534 (84%)]\tClassification Loss: 1.3493\r\n",
      "Train Epoch: 7 [93440/110534 (85%)]\tClassification Loss: 1.3304\r\n",
      "Train Epoch: 7 [94080/110534 (85%)]\tClassification Loss: 1.6247\r\n",
      "Train Epoch: 7 [94720/110534 (86%)]\tClassification Loss: 1.3312\r\n",
      "Train Epoch: 7 [95360/110534 (86%)]\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 7 [96000/110534 (87%)]\tClassification Loss: 1.1812\r\n",
      "Test() called at step_no: 11862\r\n",
      "\r\n",
      "Test set: Average loss: 1.5680, Accuracy: 1087/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1500.pth.tar\r\n",
      "Train Epoch: 7 [96640/110534 (87%)]\tClassification Loss: 1.6748\r\n",
      "Train Epoch: 7 [97280/110534 (88%)]\tClassification Loss: 1.4148\r\n",
      "Train Epoch: 7 [97920/110534 (89%)]\tClassification Loss: 1.7038\r\n",
      "Train Epoch: 7 [98560/110534 (89%)]\tClassification Loss: 1.7349\r\n",
      "Train Epoch: 7 [99200/110534 (90%)]\tClassification Loss: 1.6237\r\n",
      "Train Epoch: 7 [99840/110534 (90%)]\tClassification Loss: 1.6425\r\n",
      "Train Epoch: 7 [100480/110534 (91%)]\tClassification Loss: 1.7035\r\n",
      "Train Epoch: 7 [101120/110534 (91%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 7 [101760/110534 (92%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 7 [102400/110534 (93%)]\tClassification Loss: 1.6700\r\n",
      "Test() called at step_no: 11962\r\n",
      "\r\n",
      "Test set: Average loss: 1.5741, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [103040/110534 (93%)]\tClassification Loss: 1.8258\r\n",
      "Train Epoch: 7 [103680/110534 (94%)]\tClassification Loss: 1.3578\r\n",
      "Train Epoch: 7 [104320/110534 (94%)]\tClassification Loss: 1.3920\r\n",
      "Train Epoch: 7 [104960/110534 (95%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 7 [105600/110534 (96%)]\tClassification Loss: 1.4413\r\n",
      "Train Epoch: 7 [106240/110534 (96%)]\tClassification Loss: 1.5702\r\n",
      "Train Epoch: 7 [106880/110534 (97%)]\tClassification Loss: 1.2811\r\n",
      "Train Epoch: 7 [107520/110534 (97%)]\tClassification Loss: 1.5726\r\n",
      "Train Epoch: 7 [108160/110534 (98%)]\tClassification Loss: 1.4562\r\n",
      "Train Epoch: 7 [108800/110534 (98%)]\tClassification Loss: 1.7200\r\n",
      "Test() called at step_no: 12062\r\n",
      "\r\n",
      "Test set: Average loss: 1.5722, Accuracy: 1090/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 7 [109440/110534 (99%)]\tClassification Loss: 1.6064\r\n",
      "Train Epoch: 7 [110080/110534 (100%)]\tClassification Loss: 1.6654\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_final.pth.tar\r\n",
      "Train Epoch: 8 [0/110534 (0%)]\tClassification Loss: 1.6108\r\n",
      "Test() called at step_no: 12089\r\n",
      "\r\n",
      "Test set: Average loss: 1.5765, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [640/110534 (1%)]\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 8 [1280/110534 (1%)]\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 8 [1920/110534 (2%)]\tClassification Loss: 1.5674\r\n",
      "Train Epoch: 8 [2560/110534 (2%)]\tClassification Loss: 1.4505\r\n",
      "Train Epoch: 8 [3200/110534 (3%)]\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 8 [3840/110534 (3%)]\tClassification Loss: 1.6334\r\n",
      "Train Epoch: 8 [4480/110534 (4%)]\tClassification Loss: 1.9187\r\n",
      "Train Epoch: 8 [5120/110534 (5%)]\tClassification Loss: 1.5396\r\n",
      "Train Epoch: 8 [5760/110534 (5%)]\tClassification Loss: 1.5564\r\n",
      "Train Epoch: 8 [6400/110534 (6%)]\tClassification Loss: 1.6778\r\n",
      "Test() called at step_no: 12189\r\n",
      "\r\n",
      "Test set: Average loss: 1.5673, Accuracy: 1098/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [7040/110534 (6%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 8 [7680/110534 (7%)]\tClassification Loss: 1.6047\r\n",
      "Train Epoch: 8 [8320/110534 (8%)]\tClassification Loss: 1.6467\r\n",
      "Train Epoch: 8 [8960/110534 (8%)]\tClassification Loss: 1.8276\r\n",
      "Train Epoch: 8 [9600/110534 (9%)]\tClassification Loss: 1.5163\r\n",
      "Train Epoch: 8 [10240/110534 (9%)]\tClassification Loss: 1.6500\r\n",
      "Train Epoch: 8 [10880/110534 (10%)]\tClassification Loss: 1.6466\r\n",
      "Train Epoch: 8 [11520/110534 (10%)]\tClassification Loss: 1.8740\r\n",
      "Train Epoch: 8 [12160/110534 (11%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 8 [12800/110534 (12%)]\tClassification Loss: 1.8515\r\n",
      "Test() called at step_no: 12289\r\n",
      "\r\n",
      "Test set: Average loss: 1.5733, Accuracy: 1084/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 8 [13440/110534 (12%)]\tClassification Loss: 1.8338\r\n",
      "Train Epoch: 8 [14080/110534 (13%)]\tClassification Loss: 1.5334\r\n",
      "Train Epoch: 8 [14720/110534 (13%)]\tClassification Loss: 1.4930\r\n",
      "Train Epoch: 8 [15360/110534 (14%)]\tClassification Loss: 1.5989\r\n",
      "Train Epoch: 8 [16000/110534 (14%)]\tClassification Loss: 1.9052\r\n",
      "Train Epoch: 8 [16640/110534 (15%)]\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 8 [17280/110534 (16%)]\tClassification Loss: 1.6758\r\n",
      "Train Epoch: 8 [17920/110534 (16%)]\tClassification Loss: 1.4898\r\n",
      "Train Epoch: 8 [18560/110534 (17%)]\tClassification Loss: 1.8914\r\n",
      "Train Epoch: 8 [19200/110534 (17%)]\tClassification Loss: 1.6372\r\n",
      "Test() called at step_no: 12389\r\n",
      "\r\n",
      "Test set: Average loss: 1.5676, Accuracy: 1090/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [19840/110534 (18%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 8 [20480/110534 (19%)]\tClassification Loss: 1.6058\r\n",
      "Train Epoch: 8 [21120/110534 (19%)]\tClassification Loss: 1.4996\r\n",
      "Train Epoch: 8 [21760/110534 (20%)]\tClassification Loss: 1.7465\r\n",
      "Train Epoch: 8 [22400/110534 (20%)]\tClassification Loss: 1.9017\r\n",
      "Train Epoch: 8 [23040/110534 (21%)]\tClassification Loss: 1.5722\r\n",
      "Train Epoch: 8 [23680/110534 (21%)]\tClassification Loss: 1.7593\r\n",
      "Train Epoch: 8 [24320/110534 (22%)]\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 8 [24960/110534 (23%)]\tClassification Loss: 1.3808\r\n",
      "Train Epoch: 8 [25600/110534 (23%)]\tClassification Loss: 1.9397\r\n",
      "Test() called at step_no: 12489\r\n",
      "\r\n",
      "Test set: Average loss: 1.5692, Accuracy: 1088/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [26240/110534 (24%)]\tClassification Loss: 1.5688\r\n",
      "Train Epoch: 8 [26880/110534 (24%)]\tClassification Loss: 1.6303\r\n",
      "Train Epoch: 8 [27520/110534 (25%)]\tClassification Loss: 1.3863\r\n",
      "Train Epoch: 8 [28160/110534 (25%)]\tClassification Loss: 1.5602\r\n",
      "Train Epoch: 8 [28800/110534 (26%)]\tClassification Loss: 1.6765\r\n",
      "Train Epoch: 8 [29440/110534 (27%)]\tClassification Loss: 1.4750\r\n",
      "Train Epoch: 8 [30080/110534 (27%)]\tClassification Loss: 1.8450\r\n",
      "Train Epoch: 8 [30720/110534 (28%)]\tClassification Loss: 1.6065\r\n",
      "Train Epoch: 8 [31360/110534 (28%)]\tClassification Loss: 1.7072\r\n",
      "Train Epoch: 8 [32000/110534 (29%)]\tClassification Loss: 1.6423\r\n",
      "Test() called at step_no: 12589\r\n",
      "\r\n",
      "Test set: Average loss: 1.5684, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_500.pth.tar\r\n",
      "Train Epoch: 8 [32640/110534 (30%)]\tClassification Loss: 1.5165\r\n",
      "Train Epoch: 8 [33280/110534 (30%)]\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 8 [33920/110534 (31%)]\tClassification Loss: 1.7530\r\n",
      "Train Epoch: 8 [34560/110534 (31%)]\tClassification Loss: 1.5718\r\n",
      "Train Epoch: 8 [35200/110534 (32%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 8 [35840/110534 (32%)]\tClassification Loss: 1.3157\r\n",
      "Train Epoch: 8 [36480/110534 (33%)]\tClassification Loss: 1.6460\r\n",
      "Train Epoch: 8 [37120/110534 (34%)]\tClassification Loss: 1.5757\r\n",
      "Train Epoch: 8 [37760/110534 (34%)]\tClassification Loss: 1.7511\r\n",
      "Train Epoch: 8 [38400/110534 (35%)]\tClassification Loss: 1.6238\r\n",
      "Test() called at step_no: 12689\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "\r\n",
      "Test set: Average loss: 1.5667, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [39040/110534 (35%)]\tClassification Loss: 1.3045\r\n",
      "Train Epoch: 8 [39680/110534 (36%)]\tClassification Loss: 1.7859\r\n",
      "Train Epoch: 8 [40320/110534 (36%)]\tClassification Loss: 1.7310\r\n",
      "Train Epoch: 8 [40960/110534 (37%)]\tClassification Loss: 1.3919\r\n",
      "Train Epoch: 8 [41600/110534 (38%)]\tClassification Loss: 1.7569\r\n",
      "Train Epoch: 8 [42240/110534 (38%)]\tClassification Loss: 1.3819\r\n",
      "Train Epoch: 8 [42880/110534 (39%)]\tClassification Loss: 1.7531\r\n",
      "Train Epoch: 8 [43520/110534 (39%)]\tClassification Loss: 1.6784\r\n",
      "Train Epoch: 8 [44160/110534 (40%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 8 [44800/110534 (41%)]\tClassification Loss: 1.4380\r\n",
      "Test() called at step_no: 12789\r\n",
      "\r\n",
      "Test set: Average loss: 1.5666, Accuracy: 1092/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [45440/110534 (41%)]\tClassification Loss: 1.8085\r\n",
      "Train Epoch: 8 [46080/110534 (42%)]\tClassification Loss: 1.5253\r\n",
      "Train Epoch: 8 [46720/110534 (42%)]\tClassification Loss: 1.8716\r\n",
      "Train Epoch: 8 [47360/110534 (43%)]\tClassification Loss: 1.7364\r\n",
      "Train Epoch: 8 [48000/110534 (43%)]\tClassification Loss: 1.8078\r\n",
      "Train Epoch: 8 [48640/110534 (44%)]\tClassification Loss: 1.6247\r\n",
      "Train Epoch: 8 [49280/110534 (45%)]\tClassification Loss: 1.3501\r\n",
      "Train Epoch: 8 [49920/110534 (45%)]\tClassification Loss: 1.3409\r\n",
      "Train Epoch: 8 [50560/110534 (46%)]\tClassification Loss: 1.8473\r\n",
      "Train Epoch: 8 [51200/110534 (46%)]\tClassification Loss: 1.6289\r\n",
      "Test() called at step_no: 12889\r\n",
      "\r\n",
      "Test set: Average loss: 1.5650, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [51840/110534 (47%)]\tClassification Loss: 1.3647\r\n",
      "Train Epoch: 8 [52480/110534 (47%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 8 [53120/110534 (48%)]\tClassification Loss: 1.6811\r\n",
      "Train Epoch: 8 [53760/110534 (49%)]\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 8 [54400/110534 (49%)]\tClassification Loss: 1.9186\r\n",
      "Train Epoch: 8 [55040/110534 (50%)]\tClassification Loss: 1.3409\r\n",
      "Train Epoch: 8 [55680/110534 (50%)]\tClassification Loss: 1.8691\r\n",
      "Train Epoch: 8 [56320/110534 (51%)]\tClassification Loss: 1.5648\r\n",
      "Train Epoch: 8 [56960/110534 (52%)]\tClassification Loss: 1.4994\r\n",
      "Train Epoch: 8 [57600/110534 (52%)]\tClassification Loss: 1.5712\r\n",
      "Test() called at step_no: 12989\r\n",
      "\r\n",
      "Test set: Average loss: 1.5666, Accuracy: 1086/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [58240/110534 (53%)]\tClassification Loss: 1.5469\r\n",
      "Train Epoch: 8 [58880/110534 (53%)]\tClassification Loss: 1.7473\r\n",
      "Train Epoch: 8 [59520/110534 (54%)]\tClassification Loss: 1.7567\r\n",
      "Train Epoch: 8 [60160/110534 (54%)]\tClassification Loss: 1.8237\r\n",
      "Train Epoch: 8 [60800/110534 (55%)]\tClassification Loss: 1.8451\r\n",
      "Train Epoch: 8 [61440/110534 (56%)]\tClassification Loss: 1.5270\r\n",
      "Train Epoch: 8 [62080/110534 (56%)]\tClassification Loss: 2.0238\r\n",
      "Train Epoch: 8 [62720/110534 (57%)]\tClassification Loss: 1.6613\r\n",
      "Train Epoch: 8 [63360/110534 (57%)]\tClassification Loss: 1.7268\r\n",
      "Train Epoch: 8 [64000/110534 (58%)]\tClassification Loss: 1.6211\r\n",
      "Test() called at step_no: 13089\r\n",
      "\r\n",
      "Test set: Average loss: 1.5621, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1000.pth.tar\r\n",
      "Train Epoch: 8 [64640/110534 (58%)]\tClassification Loss: 1.6626\r\n",
      "Train Epoch: 8 [65280/110534 (59%)]\tClassification Loss: 1.6639\r\n",
      "Train Epoch: 8 [65920/110534 (60%)]\tClassification Loss: 1.3967\r\n",
      "Train Epoch: 8 [66560/110534 (60%)]\tClassification Loss: 1.6681\r\n",
      "Train Epoch: 8 [67200/110534 (61%)]\tClassification Loss: 1.8868\r\n",
      "Train Epoch: 8 [67840/110534 (61%)]\tClassification Loss: 1.2204\r\n",
      "Train Epoch: 8 [68480/110534 (62%)]\tClassification Loss: 1.4808\r\n",
      "Train Epoch: 8 [69120/110534 (63%)]\tClassification Loss: 1.4283\r\n",
      "Train Epoch: 8 [69760/110534 (63%)]\tClassification Loss: 1.5826\r\n",
      "Train Epoch: 8 [70400/110534 (64%)]\tClassification Loss: 1.5946\r\n",
      "Test() called at step_no: 13189\r\n",
      "\r\n",
      "Test set: Average loss: 1.5702, Accuracy: 1082/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 8 [71040/110534 (64%)]\tClassification Loss: 1.5720\r\n",
      "Train Epoch: 8 [71680/110534 (65%)]\tClassification Loss: 1.4844\r\n",
      "Train Epoch: 8 [72320/110534 (65%)]\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 8 [72960/110534 (66%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 8 [73600/110534 (67%)]\tClassification Loss: 1.4783\r\n",
      "Train Epoch: 8 [74240/110534 (67%)]\tClassification Loss: 1.4069\r\n",
      "Train Epoch: 8 [74880/110534 (68%)]\tClassification Loss: 1.5985\r\n",
      "Train Epoch: 8 [75520/110534 (68%)]\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 8 [76160/110534 (69%)]\tClassification Loss: 1.4382\r\n",
      "Train Epoch: 8 [76800/110534 (69%)]\tClassification Loss: 1.5159\r\n",
      "Test() called at step_no: 13289\r\n",
      "\r\n",
      "Test set: Average loss: 1.5639, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [77440/110534 (70%)]\tClassification Loss: 1.8374\r\n",
      "Train Epoch: 8 [78080/110534 (71%)]\tClassification Loss: 1.7210\r\n",
      "Train Epoch: 8 [78720/110534 (71%)]\tClassification Loss: 1.6704\r\n",
      "Train Epoch: 8 [79360/110534 (72%)]\tClassification Loss: 2.0100\r\n",
      "Train Epoch: 8 [80000/110534 (72%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 8 [80640/110534 (73%)]\tClassification Loss: 1.6875\r\n",
      "Train Epoch: 8 [81280/110534 (74%)]\tClassification Loss: 1.9302\r\n",
      "Train Epoch: 8 [81920/110534 (74%)]\tClassification Loss: 1.3126\r\n",
      "Train Epoch: 8 [82560/110534 (75%)]\tClassification Loss: 1.8142\r\n",
      "Train Epoch: 8 [83200/110534 (75%)]\tClassification Loss: 1.4578\r\n",
      "Test() called at step_no: 13389\r\n",
      "\r\n",
      "Test set: Average loss: 1.5610, Accuracy: 1100/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [83840/110534 (76%)]\tClassification Loss: 1.8458\r\n",
      "Train Epoch: 8 [84480/110534 (76%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 8 [85120/110534 (77%)]\tClassification Loss: 1.7692\r\n",
      "Train Epoch: 8 [85760/110534 (78%)]\tClassification Loss: 1.9106\r\n",
      "Train Epoch: 8 [86400/110534 (78%)]\tClassification Loss: 1.6446\r\n",
      "Train Epoch: 8 [87040/110534 (79%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 8 [87680/110534 (79%)]\tClassification Loss: 1.5509\r\n",
      "Train Epoch: 8 [88320/110534 (80%)]\tClassification Loss: 1.4952\r\n",
      "Train Epoch: 8 [88960/110534 (80%)]\tClassification Loss: 1.4512\r\n",
      "Train Epoch: 8 [89600/110534 (81%)]\tClassification Loss: 1.4573\r\n",
      "Test() called at step_no: 13489\r\n",
      "\r\n",
      "Test set: Average loss: 1.5653, Accuracy: 1101/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [90240/110534 (82%)]\tClassification Loss: 1.6537\r\n",
      "Train Epoch: 8 [90880/110534 (82%)]\tClassification Loss: 1.9670\r\n",
      "Train Epoch: 8 [91520/110534 (83%)]\tClassification Loss: 1.4972\r\n",
      "Train Epoch: 8 [92160/110534 (83%)]\tClassification Loss: 1.5550\r\n",
      "Train Epoch: 8 [92800/110534 (84%)]\tClassification Loss: 1.1893\r\n",
      "Train Epoch: 8 [93440/110534 (85%)]\tClassification Loss: 1.4895\r\n",
      "Train Epoch: 8 [94080/110534 (85%)]\tClassification Loss: 1.6235\r\n",
      "Train Epoch: 8 [94720/110534 (86%)]\tClassification Loss: 1.5568\r\n",
      "Train Epoch: 8 [95360/110534 (86%)]\tClassification Loss: 1.5256\r\n",
      "Train Epoch: 8 [96000/110534 (87%)]\tClassification Loss: 1.4313\r\n",
      "Test() called at step_no: 13589\r\n",
      "\r\n",
      "Test set: Average loss: 1.5625, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1500.pth.tar\r\n",
      "Train Epoch: 8 [96640/110534 (87%)]\tClassification Loss: 1.7566\r\n",
      "Train Epoch: 8 [97280/110534 (88%)]\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 8 [97920/110534 (89%)]\tClassification Loss: 1.6583\r\n",
      "Train Epoch: 8 [98560/110534 (89%)]\tClassification Loss: 1.6138\r\n",
      "Train Epoch: 8 [99200/110534 (90%)]\tClassification Loss: 1.6302\r\n",
      "Train Epoch: 8 [99840/110534 (90%)]\tClassification Loss: 1.6101\r\n",
      "Train Epoch: 8 [100480/110534 (91%)]\tClassification Loss: 1.8001\r\n",
      "Train Epoch: 8 [101120/110534 (91%)]\tClassification Loss: 1.5670\r\n",
      "Train Epoch: 8 [101760/110534 (92%)]\tClassification Loss: 1.4950\r\n",
      "Train Epoch: 8 [102400/110534 (93%)]\tClassification Loss: 1.6498\r\n",
      "Test() called at step_no: 13689\r\n",
      "\r\n",
      "Test set: Average loss: 1.5669, Accuracy: 1106/1920 (58%)\r\n",
      "\r\n",
      "Train Epoch: 8 [103040/110534 (93%)]\tClassification Loss: 1.5736\r\n",
      "Train Epoch: 8 [103680/110534 (94%)]\tClassification Loss: 1.5217\r\n",
      "Train Epoch: 8 [104320/110534 (94%)]\tClassification Loss: 1.4442\r\n",
      "Train Epoch: 8 [104960/110534 (95%)]\tClassification Loss: 1.5942\r\n",
      "Train Epoch: 8 [105600/110534 (96%)]\tClassification Loss: 1.3595\r\n",
      "Train Epoch: 8 [106240/110534 (96%)]\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 8 [106880/110534 (97%)]\tClassification Loss: 1.1847\r\n",
      "Train Epoch: 8 [107520/110534 (97%)]\tClassification Loss: 1.4967\r\n",
      "Train Epoch: 8 [108160/110534 (98%)]\tClassification Loss: 1.6282\r\n",
      "Train Epoch: 8 [108800/110534 (98%)]\tClassification Loss: 1.8236\r\n",
      "Test() called at step_no: 13789\r\n",
      "\r\n",
      "Test set: Average loss: 1.5660, Accuracy: 1097/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 8 [109440/110534 (99%)]\tClassification Loss: 1.5207\r\n",
      "Train Epoch: 8 [110080/110534 (100%)]\tClassification Loss: 1.5939\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_final.pth.tar\r\n",
      "Train Epoch: 9 [0/110534 (0%)]\tClassification Loss: 1.8242\r\n",
      "Test() called at step_no: 13816\r\n",
      "\r\n",
      "Test set: Average loss: 1.5700, Accuracy: 1100/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [640/110534 (1%)]\tClassification Loss: 1.6804\r\n",
      "Train Epoch: 9 [1280/110534 (1%)]\tClassification Loss: 1.4426\r\n",
      "Train Epoch: 9 [1920/110534 (2%)]\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 9 [2560/110534 (2%)]\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 9 [3200/110534 (3%)]\tClassification Loss: 1.5370\r\n",
      "Train Epoch: 9 [3840/110534 (3%)]\tClassification Loss: 1.6712\r\n",
      "Train Epoch: 9 [4480/110534 (4%)]\tClassification Loss: 1.8238\r\n",
      "Train Epoch: 9 [5120/110534 (5%)]\tClassification Loss: 1.7110\r\n",
      "Train Epoch: 9 [5760/110534 (5%)]\tClassification Loss: 1.6438\r\n",
      "Train Epoch: 9 [6400/110534 (6%)]\tClassification Loss: 1.6654\r\n",
      "Test() called at step_no: 13916\r\n",
      "\r\n",
      "Test set: Average loss: 1.5621, Accuracy: 1100/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [7040/110534 (6%)]\tClassification Loss: 1.9765\r\n",
      "Train Epoch: 9 [7680/110534 (7%)]\tClassification Loss: 1.5553\r\n",
      "Train Epoch: 9 [8320/110534 (8%)]\tClassification Loss: 1.6039\r\n",
      "Train Epoch: 9 [8960/110534 (8%)]\tClassification Loss: 1.7178\r\n",
      "Train Epoch: 9 [9600/110534 (9%)]\tClassification Loss: 1.6375\r\n",
      "Train Epoch: 9 [10240/110534 (9%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 9 [10880/110534 (10%)]\tClassification Loss: 1.6151\r\n",
      "Train Epoch: 9 [11520/110534 (10%)]\tClassification Loss: 1.8319\r\n",
      "Train Epoch: 9 [12160/110534 (11%)]\tClassification Loss: 1.8417\r\n",
      "Train Epoch: 9 [12800/110534 (12%)]\tClassification Loss: 1.8650\r\n",
      "Test() called at step_no: 14016\r\n",
      "\r\n",
      "Test set: Average loss: 1.5670, Accuracy: 1082/1920 (56%)\r\n",
      "\r\n",
      "Train Epoch: 9 [13440/110534 (12%)]\tClassification Loss: 1.8098\r\n",
      "Train Epoch: 9 [14080/110534 (13%)]\tClassification Loss: 1.4816\r\n",
      "Train Epoch: 9 [14720/110534 (13%)]\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 9 [15360/110534 (14%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 9 [16000/110534 (14%)]\tClassification Loss: 1.8998\r\n",
      "Train Epoch: 9 [16640/110534 (15%)]\tClassification Loss: 1.8188\r\n",
      "Train Epoch: 9 [17280/110534 (16%)]\tClassification Loss: 1.6998\r\n",
      "Train Epoch: 9 [17920/110534 (16%)]\tClassification Loss: 1.5206\r\n",
      "Train Epoch: 9 [18560/110534 (17%)]\tClassification Loss: 1.7528\r\n",
      "Train Epoch: 9 [19200/110534 (17%)]\tClassification Loss: 1.5993\r\n",
      "Test() called at step_no: 14116\r\n",
      "\r\n",
      "Test set: Average loss: 1.5626, Accuracy: 1086/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [19840/110534 (18%)]\tClassification Loss: 1.6653\r\n",
      "Train Epoch: 9 [20480/110534 (19%)]\tClassification Loss: 1.7227\r\n",
      "Train Epoch: 9 [21120/110534 (19%)]\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 9 [21760/110534 (20%)]\tClassification Loss: 1.6854\r\n",
      "Train Epoch: 9 [22400/110534 (20%)]\tClassification Loss: 1.9119\r\n",
      "Train Epoch: 9 [23040/110534 (21%)]\tClassification Loss: 1.4416\r\n",
      "Train Epoch: 9 [23680/110534 (21%)]\tClassification Loss: 1.6260\r\n",
      "Train Epoch: 9 [24320/110534 (22%)]\tClassification Loss: 1.4729\r\n",
      "Train Epoch: 9 [24960/110534 (23%)]\tClassification Loss: 1.4268\r\n",
      "Train Epoch: 9 [25600/110534 (23%)]\tClassification Loss: 1.8227\r\n",
      "Test() called at step_no: 14216\r\n",
      "\r\n",
      "Test set: Average loss: 1.5631, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [26240/110534 (24%)]\tClassification Loss: 1.5375\r\n",
      "Train Epoch: 9 [26880/110534 (24%)]\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 9 [27520/110534 (25%)]\tClassification Loss: 1.3010\r\n",
      "Train Epoch: 9 [28160/110534 (25%)]\tClassification Loss: 1.6898\r\n",
      "Train Epoch: 9 [28800/110534 (26%)]\tClassification Loss: 1.5927\r\n",
      "Train Epoch: 9 [29440/110534 (27%)]\tClassification Loss: 1.5072\r\n",
      "Train Epoch: 9 [30080/110534 (27%)]\tClassification Loss: 1.9032\r\n",
      "Train Epoch: 9 [30720/110534 (28%)]\tClassification Loss: 1.6463\r\n",
      "Train Epoch: 9 [31360/110534 (28%)]\tClassification Loss: 1.6749\r\n",
      "Train Epoch: 9 [32000/110534 (29%)]\tClassification Loss: 1.5194\r\n",
      "Test() called at step_no: 14316\r\n",
      "\r\n",
      "Test set: Average loss: 1.5634, Accuracy: 1099/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_500.pth.tar\r\n",
      "Train Epoch: 9 [32640/110534 (30%)]\tClassification Loss: 1.5252\r\n",
      "Train Epoch: 9 [33280/110534 (30%)]\tClassification Loss: 1.3534\r\n",
      "Train Epoch: 9 [33920/110534 (31%)]\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 9 [34560/110534 (31%)]\tClassification Loss: 1.6617\r\n",
      "Train Epoch: 9 [35200/110534 (32%)]\tClassification Loss: 1.5136\r\n",
      "Train Epoch: 9 [35840/110534 (32%)]\tClassification Loss: 1.2817\r\n",
      "Train Epoch: 9 [36480/110534 (33%)]\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 9 [37120/110534 (34%)]\tClassification Loss: 1.6589\r\n",
      "Train Epoch: 9 [37760/110534 (34%)]\tClassification Loss: 1.6477\r\n",
      "Train Epoch: 9 [38400/110534 (35%)]\tClassification Loss: 1.5206\r\n",
      "Test() called at step_no: 14416\r\n",
      "\r\n",
      "Test set: Average loss: 1.5622, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [39040/110534 (35%)]\tClassification Loss: 1.4487\r\n",
      "Train Epoch: 9 [39680/110534 (36%)]\tClassification Loss: 1.7077\r\n",
      "Train Epoch: 9 [40320/110534 (36%)]\tClassification Loss: 1.6399\r\n",
      "Train Epoch: 9 [40960/110534 (37%)]\tClassification Loss: 1.3092\r\n",
      "Train Epoch: 9 [41600/110534 (38%)]\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 9 [42240/110534 (38%)]\tClassification Loss: 1.4535\r\n",
      "Train Epoch: 9 [42880/110534 (39%)]\tClassification Loss: 1.4559\r\n",
      "Train Epoch: 9 [43520/110534 (39%)]\tClassification Loss: 1.8271\r\n",
      "Train Epoch: 9 [44160/110534 (40%)]\tClassification Loss: 1.5656\r\n",
      "Train Epoch: 9 [44800/110534 (41%)]\tClassification Loss: 1.2734\r\n",
      "Test() called at step_no: 14516\r\n",
      "\r\n",
      "Test set: Average loss: 1.5615, Accuracy: 1089/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [45440/110534 (41%)]\tClassification Loss: 1.7024\r\n",
      "Train Epoch: 9 [46080/110534 (42%)]\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 9 [46720/110534 (42%)]\tClassification Loss: 1.8798\r\n",
      "Train Epoch: 9 [47360/110534 (43%)]\tClassification Loss: 1.6229\r\n",
      "Train Epoch: 9 [48000/110534 (43%)]\tClassification Loss: 1.7115\r\n",
      "Train Epoch: 9 [48640/110534 (44%)]\tClassification Loss: 1.4696\r\n",
      "Train Epoch: 9 [49280/110534 (45%)]\tClassification Loss: 1.3351\r\n",
      "Train Epoch: 9 [49920/110534 (45%)]\tClassification Loss: 1.2646\r\n",
      "Train Epoch: 9 [50560/110534 (46%)]\tClassification Loss: 1.7044\r\n",
      "Train Epoch: 9 [51200/110534 (46%)]\tClassification Loss: 1.5195\r\n",
      "Test() called at step_no: 14616\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\r\n",
      "    send_bytes(obj)\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n",
      "    self._send_bytes(m[offset:offset + size])\r\n",
      "  File \"/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n",
      "    self._send(header + buf)\r\n",
      "\r\n",
      "Test set: Average loss: 1.5609, Accuracy: 1104/1920 (58%)\r\n",
      "\r\n",
      "Train Epoch: 9 [51840/110534 (47%)]\tClassification Loss: 1.4025\r\n",
      "Train Epoch: 9 [52480/110534 (47%)]\tClassification Loss: 1.4288\r\n",
      "Train Epoch: 9 [53120/110534 (48%)]\tClassification Loss: 1.6655\r\n",
      "Train Epoch: 9 [53760/110534 (49%)]\tClassification Loss: 1.6954\r\n",
      "Train Epoch: 9 [54400/110534 (49%)]\tClassification Loss: 1.9645\r\n",
      "Train Epoch: 9 [55040/110534 (50%)]\tClassification Loss: 1.3121\r\n",
      "Train Epoch: 9 [55680/110534 (50%)]\tClassification Loss: 1.8211\r\n",
      "Train Epoch: 9 [56320/110534 (51%)]\tClassification Loss: 1.5695\r\n",
      "Train Epoch: 9 [56960/110534 (52%)]\tClassification Loss: 1.4107\r\n",
      "Train Epoch: 9 [57600/110534 (52%)]\tClassification Loss: 1.6338\r\n",
      "Test() called at step_no: 14716\r\n",
      "\r\n",
      "Test set: Average loss: 1.5613, Accuracy: 1100/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [58240/110534 (53%)]\tClassification Loss: 1.2129\r\n",
      "Train Epoch: 9 [58880/110534 (53%)]\tClassification Loss: 1.7888\r\n",
      "Train Epoch: 9 [59520/110534 (54%)]\tClassification Loss: 1.7089\r\n",
      "Train Epoch: 9 [60160/110534 (54%)]\tClassification Loss: 1.9251\r\n",
      "Train Epoch: 9 [60800/110534 (55%)]\tClassification Loss: 1.9135\r\n",
      "Train Epoch: 9 [61440/110534 (56%)]\tClassification Loss: 1.4867\r\n",
      "Train Epoch: 9 [62080/110534 (56%)]\tClassification Loss: 2.0857\r\n",
      "Train Epoch: 9 [62720/110534 (57%)]\tClassification Loss: 1.6237\r\n",
      "Train Epoch: 9 [63360/110534 (57%)]\tClassification Loss: 1.7600\r\n",
      "Train Epoch: 9 [64000/110534 (58%)]\tClassification Loss: 1.7087\r\n",
      "Test() called at step_no: 14816\r\n",
      "\r\n",
      "Test set: Average loss: 1.5582, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1000.pth.tar\r\n",
      "Train Epoch: 9 [64640/110534 (58%)]\tClassification Loss: 1.7138\r\n",
      "Train Epoch: 9 [65280/110534 (59%)]\tClassification Loss: 1.6703\r\n",
      "Train Epoch: 9 [65920/110534 (60%)]\tClassification Loss: 1.4292\r\n",
      "Train Epoch: 9 [66560/110534 (60%)]\tClassification Loss: 1.5866\r\n",
      "Train Epoch: 9 [67200/110534 (61%)]\tClassification Loss: 1.7922\r\n",
      "Train Epoch: 9 [67840/110534 (61%)]\tClassification Loss: 1.2834\r\n",
      "Train Epoch: 9 [68480/110534 (62%)]\tClassification Loss: 1.5204\r\n",
      "Train Epoch: 9 [69120/110534 (63%)]\tClassification Loss: 1.5396\r\n",
      "Train Epoch: 9 [69760/110534 (63%)]\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 9 [70400/110534 (64%)]\tClassification Loss: 1.7181\r\n",
      "Test() called at step_no: 14916\r\n",
      "\r\n",
      "Test set: Average loss: 1.5631, Accuracy: 1093/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [71040/110534 (64%)]\tClassification Loss: 1.6661\r\n",
      "Train Epoch: 9 [71680/110534 (65%)]\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 9 [72320/110534 (65%)]\tClassification Loss: 1.5205\r\n",
      "Train Epoch: 9 [72960/110534 (66%)]\tClassification Loss: 1.6419\r\n",
      "Train Epoch: 9 [73600/110534 (67%)]\tClassification Loss: 1.6996\r\n",
      "Train Epoch: 9 [74240/110534 (67%)]\tClassification Loss: 1.5748\r\n",
      "Train Epoch: 9 [74880/110534 (68%)]\tClassification Loss: 1.4875\r\n",
      "Train Epoch: 9 [75520/110534 (68%)]\tClassification Loss: 1.6974\r\n",
      "Train Epoch: 9 [76160/110534 (69%)]\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 9 [76800/110534 (69%)]\tClassification Loss: 1.6222\r\n",
      "Test() called at step_no: 15016\r\n",
      "\r\n",
      "Test set: Average loss: 1.5593, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [77440/110534 (70%)]\tClassification Loss: 1.6606\r\n",
      "Train Epoch: 9 [78080/110534 (71%)]\tClassification Loss: 1.7379\r\n",
      "Train Epoch: 9 [78720/110534 (71%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 9 [79360/110534 (72%)]\tClassification Loss: 2.0425\r\n",
      "Train Epoch: 9 [80000/110534 (72%)]\tClassification Loss: 1.6335\r\n",
      "Train Epoch: 9 [80640/110534 (73%)]\tClassification Loss: 1.6026\r\n",
      "Train Epoch: 9 [81280/110534 (74%)]\tClassification Loss: 1.8210\r\n",
      "Train Epoch: 9 [81920/110534 (74%)]\tClassification Loss: 1.5217\r\n",
      "Train Epoch: 9 [82560/110534 (75%)]\tClassification Loss: 1.7878\r\n",
      "Train Epoch: 9 [83200/110534 (75%)]\tClassification Loss: 1.3615\r\n",
      "Test() called at step_no: 15116\r\n",
      "\r\n",
      "Test set: Average loss: 1.5580, Accuracy: 1103/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [83840/110534 (76%)]\tClassification Loss: 1.9458\r\n",
      "Train Epoch: 9 [84480/110534 (76%)]\tClassification Loss: 1.7970\r\n",
      "Train Epoch: 9 [85120/110534 (77%)]\tClassification Loss: 1.5827\r\n",
      "Train Epoch: 9 [85760/110534 (78%)]\tClassification Loss: 2.0008\r\n",
      "Train Epoch: 9 [86400/110534 (78%)]\tClassification Loss: 1.7751\r\n",
      "Train Epoch: 9 [87040/110534 (79%)]\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 9 [87680/110534 (79%)]\tClassification Loss: 1.3929\r\n",
      "Train Epoch: 9 [88320/110534 (80%)]\tClassification Loss: 1.5542\r\n",
      "Train Epoch: 9 [88960/110534 (80%)]\tClassification Loss: 1.4258\r\n",
      "Train Epoch: 9 [89600/110534 (81%)]\tClassification Loss: 1.4500\r\n",
      "Test() called at step_no: 15216\r\n",
      "\r\n",
      "Test set: Average loss: 1.5614, Accuracy: 1104/1920 (58%)\r\n",
      "\r\n",
      "Train Epoch: 9 [90240/110534 (82%)]\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 9 [90880/110534 (82%)]\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 9 [91520/110534 (83%)]\tClassification Loss: 1.5758\r\n",
      "Train Epoch: 9 [92160/110534 (83%)]\tClassification Loss: 1.5473\r\n",
      "Train Epoch: 9 [92800/110534 (84%)]\tClassification Loss: 1.1718\r\n",
      "Train Epoch: 9 [93440/110534 (85%)]\tClassification Loss: 1.3264\r\n",
      "Train Epoch: 9 [94080/110534 (85%)]\tClassification Loss: 1.8022\r\n",
      "Train Epoch: 9 [94720/110534 (86%)]\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 9 [95360/110534 (86%)]\tClassification Loss: 1.5665\r\n",
      "Train Epoch: 9 [96000/110534 (87%)]\tClassification Loss: 1.4154\r\n",
      "Test() called at step_no: 15316\r\n",
      "\r\n",
      "Test set: Average loss: 1.5583, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1500.pth.tar\r\n",
      "Train Epoch: 9 [96640/110534 (87%)]\tClassification Loss: 1.7507\r\n",
      "Train Epoch: 9 [97280/110534 (88%)]\tClassification Loss: 1.4218\r\n",
      "Train Epoch: 9 [97920/110534 (89%)]\tClassification Loss: 1.5243\r\n",
      "Train Epoch: 9 [98560/110534 (89%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 9 [99200/110534 (90%)]\tClassification Loss: 1.4399\r\n",
      "Train Epoch: 9 [99840/110534 (90%)]\tClassification Loss: 1.6119\r\n",
      "Train Epoch: 9 [100480/110534 (91%)]\tClassification Loss: 1.7111\r\n",
      "Train Epoch: 9 [101120/110534 (91%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 9 [101760/110534 (92%)]\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 9 [102400/110534 (93%)]\tClassification Loss: 1.5767\r\n",
      "Test() called at step_no: 15416\r\n",
      "\r\n",
      "Test set: Average loss: 1.5609, Accuracy: 1099/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [103040/110534 (93%)]\tClassification Loss: 1.6593\r\n",
      "Train Epoch: 9 [103680/110534 (94%)]\tClassification Loss: 1.3866\r\n",
      "Train Epoch: 9 [104320/110534 (94%)]\tClassification Loss: 1.3943\r\n",
      "Train Epoch: 9 [104960/110534 (95%)]\tClassification Loss: 1.5933\r\n",
      "Train Epoch: 9 [105600/110534 (96%)]\tClassification Loss: 1.3697\r\n",
      "Train Epoch: 9 [106240/110534 (96%)]\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 9 [106880/110534 (97%)]\tClassification Loss: 1.3174\r\n",
      "Train Epoch: 9 [107520/110534 (97%)]\tClassification Loss: 1.6371\r\n",
      "Train Epoch: 9 [108160/110534 (98%)]\tClassification Loss: 1.3342\r\n",
      "Train Epoch: 9 [108800/110534 (98%)]\tClassification Loss: 1.8222\r\n",
      "Test() called at step_no: 15516\r\n",
      "\r\n",
      "Test set: Average loss: 1.5613, Accuracy: 1098/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 9 [109440/110534 (99%)]\tClassification Loss: 1.6232\r\n",
      "Train Epoch: 9 [110080/110534 (100%)]\tClassification Loss: 1.5634\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_final.pth.tar\r\n",
      "Train Epoch: 10 [0/110534 (0%)]\tClassification Loss: 1.6439\r\n",
      "Test() called at step_no: 15543\r\n",
      "\r\n",
      "Test set: Average loss: 1.5648, Accuracy: 1093/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [640/110534 (1%)]\tClassification Loss: 1.6585\r\n",
      "Train Epoch: 10 [1280/110534 (1%)]\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 10 [1920/110534 (2%)]\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 10 [2560/110534 (2%)]\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 10 [3200/110534 (3%)]\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 10 [3840/110534 (3%)]\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 10 [4480/110534 (4%)]\tClassification Loss: 1.8515\r\n",
      "Train Epoch: 10 [5120/110534 (5%)]\tClassification Loss: 1.7320\r\n",
      "Train Epoch: 10 [5760/110534 (5%)]\tClassification Loss: 1.6197\r\n",
      "Train Epoch: 10 [6400/110534 (6%)]\tClassification Loss: 1.5797\r\n",
      "Test() called at step_no: 15643\r\n",
      "\r\n",
      "Test set: Average loss: 1.5579, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [7040/110534 (6%)]\tClassification Loss: 1.9121\r\n",
      "Train Epoch: 10 [7680/110534 (7%)]\tClassification Loss: 1.7218\r\n",
      "Train Epoch: 10 [8320/110534 (8%)]\tClassification Loss: 1.5529\r\n",
      "Train Epoch: 10 [8960/110534 (8%)]\tClassification Loss: 1.6676\r\n",
      "Train Epoch: 10 [9600/110534 (9%)]\tClassification Loss: 1.6354\r\n",
      "Train Epoch: 10 [10240/110534 (9%)]\tClassification Loss: 1.6064\r\n",
      "Train Epoch: 10 [10880/110534 (10%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 10 [11520/110534 (10%)]\tClassification Loss: 1.8895\r\n",
      "Train Epoch: 10 [12160/110534 (11%)]\tClassification Loss: 1.9691\r\n",
      "Train Epoch: 10 [12800/110534 (12%)]\tClassification Loss: 1.7019\r\n",
      "Test() called at step_no: 15743\r\n",
      "\r\n",
      "Test set: Average loss: 1.5630, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [13440/110534 (12%)]\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 10 [14080/110534 (13%)]\tClassification Loss: 1.5309\r\n",
      "Train Epoch: 10 [14720/110534 (13%)]\tClassification Loss: 1.6534\r\n",
      "Train Epoch: 10 [15360/110534 (14%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 10 [16000/110534 (14%)]\tClassification Loss: 1.9002\r\n",
      "Train Epoch: 10 [16640/110534 (15%)]\tClassification Loss: 1.7665\r\n",
      "Train Epoch: 10 [17280/110534 (16%)]\tClassification Loss: 1.7542\r\n",
      "Train Epoch: 10 [17920/110534 (16%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 10 [18560/110534 (17%)]\tClassification Loss: 1.7164\r\n",
      "Train Epoch: 10 [19200/110534 (17%)]\tClassification Loss: 1.5949\r\n",
      "Test() called at step_no: 15843\r\n",
      "\r\n",
      "Test set: Average loss: 1.5584, Accuracy: 1085/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [19840/110534 (18%)]\tClassification Loss: 1.6225\r\n",
      "Train Epoch: 10 [20480/110534 (19%)]\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 10 [21120/110534 (19%)]\tClassification Loss: 1.5413\r\n",
      "Train Epoch: 10 [21760/110534 (20%)]\tClassification Loss: 1.5765\r\n",
      "Train Epoch: 10 [22400/110534 (20%)]\tClassification Loss: 1.8548\r\n",
      "Train Epoch: 10 [23040/110534 (21%)]\tClassification Loss: 1.5553\r\n",
      "Train Epoch: 10 [23680/110534 (21%)]\tClassification Loss: 1.5065\r\n",
      "Train Epoch: 10 [24320/110534 (22%)]\tClassification Loss: 1.3928\r\n",
      "Train Epoch: 10 [24960/110534 (23%)]\tClassification Loss: 1.5030\r\n",
      "Train Epoch: 10 [25600/110534 (23%)]\tClassification Loss: 2.0062\r\n",
      "Test() called at step_no: 15943\r\n",
      "\r\n",
      "Test set: Average loss: 1.5588, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [26240/110534 (24%)]\tClassification Loss: 1.5909\r\n",
      "Train Epoch: 10 [26880/110534 (24%)]\tClassification Loss: 1.6420\r\n",
      "Train Epoch: 10 [27520/110534 (25%)]\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 10 [28160/110534 (25%)]\tClassification Loss: 1.5755\r\n",
      "Train Epoch: 10 [28800/110534 (26%)]\tClassification Loss: 1.6669\r\n",
      "Train Epoch: 10 [29440/110534 (27%)]\tClassification Loss: 1.4563\r\n",
      "Train Epoch: 10 [30080/110534 (27%)]\tClassification Loss: 1.6971\r\n",
      "Train Epoch: 10 [30720/110534 (28%)]\tClassification Loss: 1.7127\r\n",
      "Train Epoch: 10 [31360/110534 (28%)]\tClassification Loss: 1.7479\r\n",
      "Train Epoch: 10 [32000/110534 (29%)]\tClassification Loss: 1.6388\r\n",
      "Test() called at step_no: 16043\r\n",
      "\r\n",
      "Test set: Average loss: 1.5618, Accuracy: 1094/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_500.pth.tar\r\n",
      "Train Epoch: 10 [32640/110534 (30%)]\tClassification Loss: 1.5121\r\n",
      "Train Epoch: 10 [33280/110534 (30%)]\tClassification Loss: 1.3642\r\n",
      "Train Epoch: 10 [33920/110534 (31%)]\tClassification Loss: 1.7284\r\n",
      "Train Epoch: 10 [34560/110534 (31%)]\tClassification Loss: 1.7910\r\n",
      "Train Epoch: 10 [35200/110534 (32%)]\tClassification Loss: 1.4589\r\n",
      "Train Epoch: 10 [35840/110534 (32%)]\tClassification Loss: 1.2404\r\n",
      "Train Epoch: 10 [36480/110534 (33%)]\tClassification Loss: 1.5199\r\n",
      "Train Epoch: 10 [37120/110534 (34%)]\tClassification Loss: 1.7482\r\n",
      "Train Epoch: 10 [37760/110534 (34%)]\tClassification Loss: 1.6631\r\n",
      "Train Epoch: 10 [38400/110534 (35%)]\tClassification Loss: 1.5659\r\n",
      "Test() called at step_no: 16143\r\n",
      "\r\n",
      "Test set: Average loss: 1.5579, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [39040/110534 (35%)]\tClassification Loss: 1.2634\r\n",
      "Train Epoch: 10 [39680/110534 (36%)]\tClassification Loss: 1.8153\r\n",
      "Train Epoch: 10 [40320/110534 (36%)]\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 10 [40960/110534 (37%)]\tClassification Loss: 1.2362\r\n",
      "Train Epoch: 10 [41600/110534 (38%)]\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 10 [42240/110534 (38%)]\tClassification Loss: 1.4205\r\n",
      "Train Epoch: 10 [42880/110534 (39%)]\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 10 [43520/110534 (39%)]\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 10 [44160/110534 (40%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 10 [44800/110534 (41%)]\tClassification Loss: 1.3459\r\n",
      "Test() called at step_no: 16243\r\n",
      "\r\n",
      "Test set: Average loss: 1.5588, Accuracy: 1090/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [45440/110534 (41%)]\tClassification Loss: 1.6998\r\n",
      "Train Epoch: 10 [46080/110534 (42%)]\tClassification Loss: 1.5870\r\n",
      "Train Epoch: 10 [46720/110534 (42%)]\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 10 [47360/110534 (43%)]\tClassification Loss: 1.8178\r\n",
      "Train Epoch: 10 [48000/110534 (43%)]\tClassification Loss: 1.7850\r\n",
      "Train Epoch: 10 [48640/110534 (44%)]\tClassification Loss: 1.4788\r\n",
      "Train Epoch: 10 [49280/110534 (45%)]\tClassification Loss: 1.2906\r\n",
      "Train Epoch: 10 [49920/110534 (45%)]\tClassification Loss: 1.3514\r\n",
      "Train Epoch: 10 [50560/110534 (46%)]\tClassification Loss: 1.7415\r\n",
      "Train Epoch: 10 [51200/110534 (46%)]\tClassification Loss: 1.5177\r\n",
      "Test() called at step_no: 16343\r\n",
      "\r\n",
      "Test set: Average loss: 1.5577, Accuracy: 1107/1920 (58%)\r\n",
      "\r\n",
      "Train Epoch: 10 [51840/110534 (47%)]\tClassification Loss: 1.1641\r\n",
      "Train Epoch: 10 [52480/110534 (47%)]\tClassification Loss: 1.6603\r\n",
      "Train Epoch: 10 [53120/110534 (48%)]\tClassification Loss: 1.5450\r\n",
      "Train Epoch: 10 [53760/110534 (49%)]\tClassification Loss: 1.5658\r\n",
      "Train Epoch: 10 [54400/110534 (49%)]\tClassification Loss: 1.9675\r\n",
      "Train Epoch: 10 [55040/110534 (50%)]\tClassification Loss: 1.2859\r\n",
      "Train Epoch: 10 [55680/110534 (50%)]\tClassification Loss: 1.8664\r\n",
      "Train Epoch: 10 [56320/110534 (51%)]\tClassification Loss: 1.4239\r\n",
      "Train Epoch: 10 [56960/110534 (52%)]\tClassification Loss: 1.5382\r\n",
      "Train Epoch: 10 [57600/110534 (52%)]\tClassification Loss: 1.5773\r\n",
      "Test() called at step_no: 16443\r\n",
      "\r\n",
      "Test set: Average loss: 1.5583, Accuracy: 1096/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [58240/110534 (53%)]\tClassification Loss: 1.3846\r\n",
      "Train Epoch: 10 [58880/110534 (53%)]\tClassification Loss: 1.7542\r\n",
      "Train Epoch: 10 [59520/110534 (54%)]\tClassification Loss: 1.7253\r\n",
      "Train Epoch: 10 [60160/110534 (54%)]\tClassification Loss: 1.7647\r\n",
      "Train Epoch: 10 [60800/110534 (55%)]\tClassification Loss: 2.0027\r\n",
      "Train Epoch: 10 [61440/110534 (56%)]\tClassification Loss: 1.5601\r\n",
      "Train Epoch: 10 [62080/110534 (56%)]\tClassification Loss: 2.1453\r\n",
      "Train Epoch: 10 [62720/110534 (57%)]\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 10 [63360/110534 (57%)]\tClassification Loss: 1.7136\r\n",
      "Train Epoch: 10 [64000/110534 (58%)]\tClassification Loss: 1.6896\r\n",
      "Test() called at step_no: 16543\r\n",
      "\r\n",
      "Test set: Average loss: 1.5542, Accuracy: 1098/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1000.pth.tar\r\n",
      "Train Epoch: 10 [64640/110534 (58%)]\tClassification Loss: 1.7820\r\n",
      "Train Epoch: 10 [65280/110534 (59%)]\tClassification Loss: 1.6938\r\n",
      "Train Epoch: 10 [65920/110534 (60%)]\tClassification Loss: 1.4535\r\n",
      "Train Epoch: 10 [66560/110534 (60%)]\tClassification Loss: 1.5959\r\n",
      "Train Epoch: 10 [67200/110534 (61%)]\tClassification Loss: 1.7915\r\n",
      "Train Epoch: 10 [67840/110534 (61%)]\tClassification Loss: 1.3073\r\n",
      "Train Epoch: 10 [68480/110534 (62%)]\tClassification Loss: 1.4977\r\n",
      "Train Epoch: 10 [69120/110534 (63%)]\tClassification Loss: 1.3852\r\n",
      "Train Epoch: 10 [69760/110534 (63%)]\tClassification Loss: 1.5898\r\n",
      "Train Epoch: 10 [70400/110534 (64%)]\tClassification Loss: 1.6082\r\n",
      "Test() called at step_no: 16643\r\n",
      "\r\n",
      "Test set: Average loss: 1.5598, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [71040/110534 (64%)]\tClassification Loss: 1.5493\r\n",
      "Train Epoch: 10 [71680/110534 (65%)]\tClassification Loss: 1.4647\r\n",
      "Train Epoch: 10 [72320/110534 (65%)]\tClassification Loss: 1.7086\r\n",
      "Train Epoch: 10 [72960/110534 (66%)]\tClassification Loss: 1.6677\r\n",
      "Train Epoch: 10 [73600/110534 (67%)]\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 10 [74240/110534 (67%)]\tClassification Loss: 1.4970\r\n",
      "Train Epoch: 10 [74880/110534 (68%)]\tClassification Loss: 1.5209\r\n",
      "Train Epoch: 10 [75520/110534 (68%)]\tClassification Loss: 1.6572\r\n",
      "Train Epoch: 10 [76160/110534 (69%)]\tClassification Loss: 1.4107\r\n",
      "Train Epoch: 10 [76800/110534 (69%)]\tClassification Loss: 1.4922\r\n",
      "Test() called at step_no: 16743\r\n",
      "\r\n",
      "Test set: Average loss: 1.5557, Accuracy: 1100/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [77440/110534 (70%)]\tClassification Loss: 1.6655\r\n",
      "Train Epoch: 10 [78080/110534 (71%)]\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 10 [78720/110534 (71%)]\tClassification Loss: 1.8056\r\n",
      "Train Epoch: 10 [79360/110534 (72%)]\tClassification Loss: 1.9211\r\n",
      "Train Epoch: 10 [80000/110534 (72%)]\tClassification Loss: 1.5455\r\n",
      "Train Epoch: 10 [80640/110534 (73%)]\tClassification Loss: 1.7310\r\n",
      "Train Epoch: 10 [81280/110534 (74%)]\tClassification Loss: 1.9274\r\n",
      "Train Epoch: 10 [81920/110534 (74%)]\tClassification Loss: 1.2654\r\n",
      "Train Epoch: 10 [82560/110534 (75%)]\tClassification Loss: 1.8985\r\n",
      "Train Epoch: 10 [83200/110534 (75%)]\tClassification Loss: 1.4504\r\n",
      "Test() called at step_no: 16843\r\n",
      "\r\n",
      "Test set: Average loss: 1.5551, Accuracy: 1108/1920 (58%)\r\n",
      "\r\n",
      "Train Epoch: 10 [83840/110534 (76%)]\tClassification Loss: 1.8593\r\n",
      "Train Epoch: 10 [84480/110534 (76%)]\tClassification Loss: 1.6317\r\n",
      "Train Epoch: 10 [85120/110534 (77%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 10 [85760/110534 (78%)]\tClassification Loss: 1.9230\r\n",
      "Train Epoch: 10 [86400/110534 (78%)]\tClassification Loss: 1.8047\r\n",
      "Train Epoch: 10 [87040/110534 (79%)]\tClassification Loss: 1.3679\r\n",
      "Train Epoch: 10 [87680/110534 (79%)]\tClassification Loss: 1.4350\r\n",
      "Train Epoch: 10 [88320/110534 (80%)]\tClassification Loss: 1.5501\r\n",
      "Train Epoch: 10 [88960/110534 (80%)]\tClassification Loss: 1.3619\r\n",
      "Train Epoch: 10 [89600/110534 (81%)]\tClassification Loss: 1.5445\r\n",
      "Test() called at step_no: 16943\r\n",
      "\r\n",
      "Test set: Average loss: 1.5555, Accuracy: 1098/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [90240/110534 (82%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 10 [90880/110534 (82%)]\tClassification Loss: 2.0018\r\n",
      "Train Epoch: 10 [91520/110534 (83%)]\tClassification Loss: 1.5211\r\n",
      "Train Epoch: 10 [92160/110534 (83%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 10 [92800/110534 (84%)]\tClassification Loss: 1.3321\r\n",
      "Train Epoch: 10 [93440/110534 (85%)]\tClassification Loss: 1.1238\r\n",
      "Train Epoch: 10 [94080/110534 (85%)]\tClassification Loss: 1.7529\r\n",
      "Train Epoch: 10 [94720/110534 (86%)]\tClassification Loss: 1.5348\r\n",
      "Train Epoch: 10 [95360/110534 (86%)]\tClassification Loss: 1.4874\r\n",
      "Train Epoch: 10 [96000/110534 (87%)]\tClassification Loss: 1.1469\r\n",
      "Test() called at step_no: 17043\r\n",
      "\r\n",
      "Test set: Average loss: 1.5559, Accuracy: 1091/1920 (57%)\r\n",
      "\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1500.pth.tar\r\n",
      "Train Epoch: 10 [96640/110534 (87%)]\tClassification Loss: 1.8166\r\n",
      "Train Epoch: 10 [97280/110534 (88%)]\tClassification Loss: 1.4730\r\n",
      "Train Epoch: 10 [97920/110534 (89%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 10 [98560/110534 (89%)]\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 10 [99200/110534 (90%)]\tClassification Loss: 1.6311\r\n",
      "Train Epoch: 10 [99840/110534 (90%)]\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 10 [100480/110534 (91%)]\tClassification Loss: 1.5967\r\n",
      "Train Epoch: 10 [101120/110534 (91%)]\tClassification Loss: 1.6719\r\n",
      "Train Epoch: 10 [101760/110534 (92%)]\tClassification Loss: 1.5786\r\n",
      "Train Epoch: 10 [102400/110534 (93%)]\tClassification Loss: 1.6361\r\n",
      "Test() called at step_no: 17143\r\n",
      "\r\n",
      "Test set: Average loss: 1.5586, Accuracy: 1101/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [103040/110534 (93%)]\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 10 [103680/110534 (94%)]\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 10 [104320/110534 (94%)]\tClassification Loss: 1.4696\r\n",
      "Train Epoch: 10 [104960/110534 (95%)]\tClassification Loss: 1.4826\r\n",
      "Train Epoch: 10 [105600/110534 (96%)]\tClassification Loss: 1.5218\r\n",
      "Train Epoch: 10 [106240/110534 (96%)]\tClassification Loss: 1.5147\r\n",
      "Train Epoch: 10 [106880/110534 (97%)]\tClassification Loss: 1.2417\r\n",
      "Train Epoch: 10 [107520/110534 (97%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 10 [108160/110534 (98%)]\tClassification Loss: 1.6548\r\n",
      "Train Epoch: 10 [108800/110534 (98%)]\tClassification Loss: 1.5614\r\n",
      "Test() called at step_no: 17243\r\n",
      "\r\n",
      "Test set: Average loss: 1.5604, Accuracy: 1095/1920 (57%)\r\n",
      "\r\n",
      "Train Epoch: 10 [109440/110534 (99%)]\tClassification Loss: 1.5382\r\n",
      "Train Epoch: 10 [110080/110534 (100%)]\tClassification Loss: 1.6347\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_final.pth.tar\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jydikJ4fdXdf",
    "colab_type": "code",
    "outputId": "253d2624-ba17-4fef-8bea-4eda61707948",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1584862857505,
     "user_tz": -300,
     "elapsed": 669079,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Freeze=True. LR=0.02. transfer/inshop=False/lr=0.002/May30_13-11-32\n",
    "! python train.py"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/110534 (0%)]\tClassification Loss: 3.2679\r\n",
      "train.py:166: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.2928, Accuracy: 894/12800 (7%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/110534 (1%)]\tClassification Loss: 2.9009\r\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tClassification Loss: 2.7474\r\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tClassification Loss: 2.6479\r\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tClassification Loss: 2.5800\r\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tClassification Loss: 2.2845\r\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tClassification Loss: 2.4172\r\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tClassification Loss: 2.5752\r\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tClassification Loss: 2.3015\r\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tClassification Loss: 2.4090\r\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tClassification Loss: 2.4540\r\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tClassification Loss: 2.3917\r\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tClassification Loss: 2.1372\r\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tClassification Loss: 2.4351\r\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tClassification Loss: 2.1608\r\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tClassification Loss: 2.1623\r\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tClassification Loss: 2.0228\r\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tClassification Loss: 2.2473\r\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tClassification Loss: 2.2258\r\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tClassification Loss: 2.1558\r\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tClassification Loss: 2.1298\r\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tClassification Loss: 2.1383\r\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tClassification Loss: 2.2301\r\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tClassification Loss: 2.3022\r\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tClassification Loss: 2.0419\r\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tClassification Loss: 2.2972\r\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tClassification Loss: 2.2647\r\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tClassification Loss: 2.2355\r\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tClassification Loss: 2.0930\r\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tClassification Loss: 2.0965\r\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tClassification Loss: 1.9741\r\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tClassification Loss: 2.0466\r\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tClassification Loss: 2.1861\r\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tClassification Loss: 1.9595\r\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tClassification Loss: 1.9802\r\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tClassification Loss: 2.1562\r\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tClassification Loss: 1.9745\r\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tClassification Loss: 1.8629\r\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tClassification Loss: 2.0361\r\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tClassification Loss: 2.0497\r\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tClassification Loss: 1.7165\r\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tClassification Loss: 1.9424\r\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tClassification Loss: 2.0970\r\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tClassification Loss: 1.9966\r\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tClassification Loss: 2.0391\r\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tClassification Loss: 1.9477\r\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tClassification Loss: 1.6874\r\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tClassification Loss: 1.9972\r\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tClassification Loss: 1.8603\r\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tClassification Loss: 1.6031\r\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tClassification Loss: 1.9814\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tClassification Loss: 1.8368\r\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tClassification Loss: 1.8970\r\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tClassification Loss: 1.9030\r\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tClassification Loss: 1.9892\r\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tClassification Loss: 1.8536\r\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tClassification Loss: 1.8469\r\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tClassification Loss: 1.7337\r\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tClassification Loss: 2.1609\r\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tClassification Loss: 1.5346\r\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tClassification Loss: 2.0680\r\n",
      "\r\n",
      "Test set: Average loss: 1.8067, Accuracy: 6147/12800 (48%)\r\n",
      "\r\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tClassification Loss: 1.7486\r\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tClassification Loss: 1.9776\r\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tClassification Loss: 2.0124\r\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tClassification Loss: 1.6338\r\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tClassification Loss: 2.0317\r\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tClassification Loss: 2.0157\r\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tClassification Loss: 1.7802\r\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tClassification Loss: 1.9276\r\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tClassification Loss: 1.9493\r\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tClassification Loss: 1.7392\r\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tClassification Loss: 1.6225\r\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tClassification Loss: 1.8829\r\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tClassification Loss: 1.7566\r\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tClassification Loss: 1.6614\r\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tClassification Loss: 2.0309\r\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tClassification Loss: 1.8619\r\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tClassification Loss: 1.7733\r\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tClassification Loss: 1.7521\r\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tClassification Loss: 2.0084\r\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tClassification Loss: 1.7530\r\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tClassification Loss: 1.6871\r\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tClassification Loss: 1.8449\r\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tClassification Loss: 1.8938\r\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tClassification Loss: 1.7222\r\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tClassification Loss: 1.7425\r\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tClassification Loss: 1.8037\r\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tClassification Loss: 1.8706\r\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tClassification Loss: 1.8010\r\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tClassification Loss: 1.7474\r\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tClassification Loss: 1.7486\r\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tClassification Loss: 1.7496\r\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tClassification Loss: 1.7276\r\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tClassification Loss: 1.7427\r\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tClassification Loss: 1.8570\r\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tClassification Loss: 1.6863\r\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tClassification Loss: 1.8463\r\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tClassification Loss: 1.8705\r\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tClassification Loss: 1.7824\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tClassification Loss: 1.8631\r\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tClassification Loss: 1.8076\r\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tClassification Loss: 1.6302\r\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tClassification Loss: 1.6812\r\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tClassification Loss: 1.9014\r\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tClassification Loss: 1.7857\r\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tClassification Loss: 1.7948\r\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tClassification Loss: 1.8055\r\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tClassification Loss: 1.6733\r\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tClassification Loss: 1.8280\r\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tClassification Loss: 1.8012\r\n",
      "Train Epoch: 1 [71680/110534 (65%)]\tClassification Loss: 1.6780\r\n",
      "Train Epoch: 1 [72320/110534 (65%)]\tClassification Loss: 1.7167\r\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tClassification Loss: 1.5884\r\n",
      "Train Epoch: 1 [73600/110534 (67%)]\tClassification Loss: 1.7533\r\n",
      "Train Epoch: 1 [74240/110534 (67%)]\tClassification Loss: 1.6751\r\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tClassification Loss: 1.6556\r\n",
      "Train Epoch: 1 [75520/110534 (68%)]\tClassification Loss: 1.7860\r\n",
      "Train Epoch: 1 [76160/110534 (69%)]\tClassification Loss: 1.7704\r\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tClassification Loss: 2.0020\r\n",
      "\r\n",
      "Test set: Average loss: 1.6636, Accuracy: 6420/12800 (50%)\r\n",
      "\r\n",
      "Train Epoch: 1 [77440/110534 (70%)]\tClassification Loss: 1.9900\r\n",
      "Train Epoch: 1 [78080/110534 (71%)]\tClassification Loss: 1.5463\r\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tClassification Loss: 1.8302\r\n",
      "Train Epoch: 1 [79360/110534 (72%)]\tClassification Loss: 1.8223\r\n",
      "Train Epoch: 1 [80000/110534 (72%)]\tClassification Loss: 1.6503\r\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tClassification Loss: 1.8623\r\n",
      "Train Epoch: 1 [81280/110534 (74%)]\tClassification Loss: 1.5479\r\n",
      "Train Epoch: 1 [81920/110534 (74%)]\tClassification Loss: 1.7199\r\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tClassification Loss: 1.9109\r\n",
      "Train Epoch: 1 [83200/110534 (75%)]\tClassification Loss: 1.5877\r\n",
      "Train Epoch: 1 [83840/110534 (76%)]\tClassification Loss: 1.9510\r\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 1 [85120/110534 (77%)]\tClassification Loss: 1.9546\r\n",
      "Train Epoch: 1 [85760/110534 (78%)]\tClassification Loss: 1.5823\r\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tClassification Loss: 1.6613\r\n",
      "Train Epoch: 1 [87040/110534 (79%)]\tClassification Loss: 1.7361\r\n",
      "Train Epoch: 1 [87680/110534 (79%)]\tClassification Loss: 1.8200\r\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tClassification Loss: 1.5666\r\n",
      "Train Epoch: 1 [88960/110534 (80%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 1 [89600/110534 (81%)]\tClassification Loss: 1.7261\r\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tClassification Loss: 1.8982\r\n",
      "Train Epoch: 1 [90880/110534 (82%)]\tClassification Loss: 1.6394\r\n",
      "Train Epoch: 1 [91520/110534 (83%)]\tClassification Loss: 1.7602\r\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 1 [92800/110534 (84%)]\tClassification Loss: 1.7609\r\n",
      "Train Epoch: 1 [93440/110534 (85%)]\tClassification Loss: 1.8983\r\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tClassification Loss: 1.5569\r\n",
      "Train Epoch: 1 [94720/110534 (86%)]\tClassification Loss: 1.8234\r\n",
      "Train Epoch: 1 [95360/110534 (86%)]\tClassification Loss: 1.7003\r\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tClassification Loss: 2.0311\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [96640/110534 (87%)]\tClassification Loss: 1.7165\r\n",
      "Train Epoch: 1 [97280/110534 (88%)]\tClassification Loss: 1.7868\r\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tClassification Loss: 1.7128\r\n",
      "Train Epoch: 1 [98560/110534 (89%)]\tClassification Loss: 1.6310\r\n",
      "Train Epoch: 1 [99200/110534 (90%)]\tClassification Loss: 2.1099\r\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tClassification Loss: 1.5209\r\n",
      "Train Epoch: 1 [100480/110534 (91%)]\tClassification Loss: 1.6938\r\n",
      "Train Epoch: 1 [101120/110534 (91%)]\tClassification Loss: 1.4934\r\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tClassification Loss: 1.8635\r\n",
      "Train Epoch: 1 [102400/110534 (93%)]\tClassification Loss: 1.7441\r\n",
      "Train Epoch: 1 [103040/110534 (93%)]\tClassification Loss: 1.7902\r\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tClassification Loss: 1.5305\r\n",
      "Train Epoch: 1 [104320/110534 (94%)]\tClassification Loss: 1.7179\r\n",
      "Train Epoch: 1 [104960/110534 (95%)]\tClassification Loss: 1.7825\r\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tClassification Loss: 1.9764\r\n",
      "Train Epoch: 1 [106240/110534 (96%)]\tClassification Loss: 1.5947\r\n",
      "Train Epoch: 1 [106880/110534 (97%)]\tClassification Loss: 1.6483\r\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tClassification Loss: 1.5427\r\n",
      "Train Epoch: 1 [108160/110534 (98%)]\tClassification Loss: 1.4528\r\n",
      "Train Epoch: 1 [108800/110534 (98%)]\tClassification Loss: 1.6255\r\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tClassification Loss: 1.6735\r\n",
      "Train Epoch: 1 [110080/110534 (100%)]\tClassification Loss: 1.8213\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/110534 (0%)]\tClassification Loss: 1.8696\r\n",
      "\r\n",
      "Test set: Average loss: 1.6082, Accuracy: 6568/12800 (51%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/110534 (1%)]\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 2 [1280/110534 (1%)]\tClassification Loss: 1.6336\r\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 2 [2560/110534 (2%)]\tClassification Loss: 1.7431\r\n",
      "Train Epoch: 2 [3200/110534 (3%)]\tClassification Loss: 1.4232\r\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 2 [4480/110534 (4%)]\tClassification Loss: 1.7525\r\n",
      "Train Epoch: 2 [5120/110534 (5%)]\tClassification Loss: 1.6729\r\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tClassification Loss: 1.7878\r\n",
      "Train Epoch: 2 [6400/110534 (6%)]\tClassification Loss: 1.9934\r\n",
      "Train Epoch: 2 [7040/110534 (6%)]\tClassification Loss: 1.8064\r\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 2 [8320/110534 (8%)]\tClassification Loss: 1.8679\r\n",
      "Train Epoch: 2 [8960/110534 (8%)]\tClassification Loss: 1.6601\r\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tClassification Loss: 1.5964\r\n",
      "Train Epoch: 2 [10240/110534 (9%)]\tClassification Loss: 1.4847\r\n",
      "Train Epoch: 2 [10880/110534 (10%)]\tClassification Loss: 1.7499\r\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tClassification Loss: 1.6758\r\n",
      "Train Epoch: 2 [12160/110534 (11%)]\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 2 [12800/110534 (12%)]\tClassification Loss: 1.7281\r\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tClassification Loss: 1.5687\r\n",
      "Train Epoch: 2 [14080/110534 (13%)]\tClassification Loss: 1.6920\r\n",
      "Train Epoch: 2 [14720/110534 (13%)]\tClassification Loss: 1.7066\r\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tClassification Loss: 1.7933\r\n",
      "Train Epoch: 2 [16000/110534 (14%)]\tClassification Loss: 1.7959\r\n",
      "Train Epoch: 2 [16640/110534 (15%)]\tClassification Loss: 1.6829\r\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tClassification Loss: 1.9056\r\n",
      "Train Epoch: 2 [17920/110534 (16%)]\tClassification Loss: 1.7673\r\n",
      "Train Epoch: 2 [18560/110534 (17%)]\tClassification Loss: 1.6741\r\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tClassification Loss: 1.7321\r\n",
      "Train Epoch: 2 [19840/110534 (18%)]\tClassification Loss: 1.6456\r\n",
      "Train Epoch: 2 [20480/110534 (19%)]\tClassification Loss: 1.8896\r\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 2 [21760/110534 (20%)]\tClassification Loss: 1.7015\r\n",
      "Train Epoch: 2 [22400/110534 (20%)]\tClassification Loss: 1.7091\r\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tClassification Loss: 1.6586\r\n",
      "Train Epoch: 2 [23680/110534 (21%)]\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 2 [24320/110534 (22%)]\tClassification Loss: 1.7137\r\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tClassification Loss: 1.7685\r\n",
      "Train Epoch: 2 [25600/110534 (23%)]\tClassification Loss: 1.5098\r\n",
      "Train Epoch: 2 [26240/110534 (24%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tClassification Loss: 1.8445\r\n",
      "Train Epoch: 2 [27520/110534 (25%)]\tClassification Loss: 1.7598\r\n",
      "Train Epoch: 2 [28160/110534 (25%)]\tClassification Loss: 1.6757\r\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tClassification Loss: 1.7359\r\n",
      "Train Epoch: 2 [29440/110534 (27%)]\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 2 [30080/110534 (27%)]\tClassification Loss: 1.7149\r\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tClassification Loss: 1.5072\r\n",
      "Train Epoch: 2 [31360/110534 (28%)]\tClassification Loss: 1.3453\r\n",
      "Train Epoch: 2 [32000/110534 (29%)]\tClassification Loss: 1.7023\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_500.pth.tar\r\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tClassification Loss: 1.6145\r\n",
      "Train Epoch: 2 [33280/110534 (30%)]\tClassification Loss: 1.7126\r\n",
      "Train Epoch: 2 [33920/110534 (31%)]\tClassification Loss: 1.7997\r\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 2 [35200/110534 (32%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 2 [35840/110534 (32%)]\tClassification Loss: 1.5322\r\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tClassification Loss: 1.6403\r\n",
      "Train Epoch: 2 [37120/110534 (34%)]\tClassification Loss: 1.7844\r\n",
      "Train Epoch: 2 [37760/110534 (34%)]\tClassification Loss: 1.3418\r\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tClassification Loss: 1.8942\r\n",
      "\r\n",
      "Test set: Average loss: 1.5781, Accuracy: 6669/12800 (52%)\r\n",
      "\r\n",
      "Train Epoch: 2 [39040/110534 (35%)]\tClassification Loss: 1.4095\r\n",
      "Train Epoch: 2 [39680/110534 (36%)]\tClassification Loss: 1.7040\r\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tClassification Loss: 1.8798\r\n",
      "Train Epoch: 2 [40960/110534 (37%)]\tClassification Loss: 1.4187\r\n",
      "Train Epoch: 2 [41600/110534 (38%)]\tClassification Loss: 1.8288\r\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tClassification Loss: 1.9589\r\n",
      "Train Epoch: 2 [42880/110534 (39%)]\tClassification Loss: 1.7017\r\n",
      "Train Epoch: 2 [43520/110534 (39%)]\tClassification Loss: 1.4984\r\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tClassification Loss: 1.7672\r\n",
      "Train Epoch: 2 [44800/110534 (41%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 2 [45440/110534 (41%)]\tClassification Loss: 1.4317\r\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tClassification Loss: 1.6747\r\n",
      "Train Epoch: 2 [46720/110534 (42%)]\tClassification Loss: 1.5728\r\n",
      "Train Epoch: 2 [47360/110534 (43%)]\tClassification Loss: 1.6065\r\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tClassification Loss: 1.7330\r\n",
      "Train Epoch: 2 [48640/110534 (44%)]\tClassification Loss: 1.6325\r\n",
      "Train Epoch: 2 [49280/110534 (45%)]\tClassification Loss: 1.5092\r\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 2 [50560/110534 (46%)]\tClassification Loss: 2.0419\r\n",
      "Train Epoch: 2 [51200/110534 (46%)]\tClassification Loss: 1.7274\r\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tClassification Loss: 1.6747\r\n",
      "Train Epoch: 2 [52480/110534 (47%)]\tClassification Loss: 1.5470\r\n",
      "Train Epoch: 2 [53120/110534 (48%)]\tClassification Loss: 1.6702\r\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tClassification Loss: 1.7623\r\n",
      "Train Epoch: 2 [54400/110534 (49%)]\tClassification Loss: 1.6119\r\n",
      "Train Epoch: 2 [55040/110534 (50%)]\tClassification Loss: 1.5240\r\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tClassification Loss: 1.6537\r\n",
      "Train Epoch: 2 [56320/110534 (51%)]\tClassification Loss: 1.7097\r\n",
      "Train Epoch: 2 [56960/110534 (52%)]\tClassification Loss: 1.9190\r\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 2 [58240/110534 (53%)]\tClassification Loss: 1.5785\r\n",
      "Train Epoch: 2 [58880/110534 (53%)]\tClassification Loss: 1.4981\r\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tClassification Loss: 1.5343\r\n",
      "Train Epoch: 2 [60160/110534 (54%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 2 [60800/110534 (55%)]\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tClassification Loss: 1.8984\r\n",
      "Train Epoch: 2 [62080/110534 (56%)]\tClassification Loss: 1.6121\r\n",
      "Train Epoch: 2 [62720/110534 (57%)]\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tClassification Loss: 1.7367\r\n",
      "Train Epoch: 2 [64000/110534 (58%)]\tClassification Loss: 1.6994\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1000.pth.tar\r\n",
      "Train Epoch: 2 [64640/110534 (58%)]\tClassification Loss: 1.7402\r\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tClassification Loss: 1.8163\r\n",
      "Train Epoch: 2 [65920/110534 (60%)]\tClassification Loss: 1.4465\r\n",
      "Train Epoch: 2 [66560/110534 (60%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tClassification Loss: 1.6958\r\n",
      "Train Epoch: 2 [67840/110534 (61%)]\tClassification Loss: 1.6440\r\n",
      "Train Epoch: 2 [68480/110534 (62%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tClassification Loss: 1.3994\r\n",
      "Train Epoch: 2 [69760/110534 (63%)]\tClassification Loss: 1.5170\r\n",
      "Train Epoch: 2 [70400/110534 (64%)]\tClassification Loss: 2.0093\r\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tClassification Loss: 1.6815\r\n",
      "Train Epoch: 2 [71680/110534 (65%)]\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 2 [72320/110534 (65%)]\tClassification Loss: 1.5837\r\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tClassification Loss: 1.3978\r\n",
      "Train Epoch: 2 [73600/110534 (67%)]\tClassification Loss: 1.6808\r\n",
      "Train Epoch: 2 [74240/110534 (67%)]\tClassification Loss: 1.4630\r\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tClassification Loss: 1.5412\r\n",
      "Train Epoch: 2 [75520/110534 (68%)]\tClassification Loss: 1.7690\r\n",
      "Train Epoch: 2 [76160/110534 (69%)]\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tClassification Loss: 1.9199\r\n",
      "\r\n",
      "Test set: Average loss: 1.5511, Accuracy: 6723/12800 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [77440/110534 (70%)]\tClassification Loss: 1.7983\r\n",
      "Train Epoch: 2 [78080/110534 (71%)]\tClassification Loss: 1.5601\r\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tClassification Loss: 1.8029\r\n",
      "Train Epoch: 2 [79360/110534 (72%)]\tClassification Loss: 1.5934\r\n",
      "Train Epoch: 2 [80000/110534 (72%)]\tClassification Loss: 1.5390\r\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tClassification Loss: 1.6967\r\n",
      "Train Epoch: 2 [81280/110534 (74%)]\tClassification Loss: 1.4074\r\n",
      "Train Epoch: 2 [81920/110534 (74%)]\tClassification Loss: 1.6033\r\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tClassification Loss: 1.9315\r\n",
      "Train Epoch: 2 [83200/110534 (75%)]\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 2 [83840/110534 (76%)]\tClassification Loss: 1.8703\r\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tClassification Loss: 1.5683\r\n",
      "Train Epoch: 2 [85120/110534 (77%)]\tClassification Loss: 1.8474\r\n",
      "Train Epoch: 2 [85760/110534 (78%)]\tClassification Loss: 1.5733\r\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tClassification Loss: 1.5858\r\n",
      "Train Epoch: 2 [87040/110534 (79%)]\tClassification Loss: 1.5975\r\n",
      "Train Epoch: 2 [87680/110534 (79%)]\tClassification Loss: 1.7630\r\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 2 [88960/110534 (80%)]\tClassification Loss: 1.3157\r\n",
      "Train Epoch: 2 [89600/110534 (81%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tClassification Loss: 1.8362\r\n",
      "Train Epoch: 2 [90880/110534 (82%)]\tClassification Loss: 1.4717\r\n",
      "Train Epoch: 2 [91520/110534 (83%)]\tClassification Loss: 1.6802\r\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tClassification Loss: 1.6064\r\n",
      "Train Epoch: 2 [92800/110534 (84%)]\tClassification Loss: 1.6350\r\n",
      "Train Epoch: 2 [93440/110534 (85%)]\tClassification Loss: 1.5665\r\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tClassification Loss: 1.5381\r\n",
      "Train Epoch: 2 [94720/110534 (86%)]\tClassification Loss: 1.8732\r\n",
      "Train Epoch: 2 [95360/110534 (86%)]\tClassification Loss: 1.5993\r\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tClassification Loss: 1.8951\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [96640/110534 (87%)]\tClassification Loss: 1.8386\r\n",
      "Train Epoch: 2 [97280/110534 (88%)]\tClassification Loss: 1.6479\r\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tClassification Loss: 1.4895\r\n",
      "Train Epoch: 2 [98560/110534 (89%)]\tClassification Loss: 1.4055\r\n",
      "Train Epoch: 2 [99200/110534 (90%)]\tClassification Loss: 2.1273\r\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tClassification Loss: 1.4750\r\n",
      "Train Epoch: 2 [100480/110534 (91%)]\tClassification Loss: 1.5409\r\n",
      "Train Epoch: 2 [101120/110534 (91%)]\tClassification Loss: 1.2810\r\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tClassification Loss: 1.8029\r\n",
      "Train Epoch: 2 [102400/110534 (93%)]\tClassification Loss: 1.6882\r\n",
      "Train Epoch: 2 [103040/110534 (93%)]\tClassification Loss: 1.5735\r\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tClassification Loss: 1.4465\r\n",
      "Train Epoch: 2 [104320/110534 (94%)]\tClassification Loss: 1.5304\r\n",
      "Train Epoch: 2 [104960/110534 (95%)]\tClassification Loss: 1.6027\r\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tClassification Loss: 1.8718\r\n",
      "Train Epoch: 2 [106240/110534 (96%)]\tClassification Loss: 1.4760\r\n",
      "Train Epoch: 2 [106880/110534 (97%)]\tClassification Loss: 1.5893\r\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tClassification Loss: 1.5508\r\n",
      "Train Epoch: 2 [108160/110534 (98%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 2 [108800/110534 (98%)]\tClassification Loss: 1.4721\r\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 2 [110080/110534 (100%)]\tClassification Loss: 1.7367\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/110534 (0%)]\tClassification Loss: 1.8121\r\n",
      "\r\n",
      "Test set: Average loss: 1.5347, Accuracy: 6817/12800 (53%)\r\n",
      "\r\n",
      "Train Epoch: 3 [640/110534 (1%)]\tClassification Loss: 1.6663\r\n",
      "Train Epoch: 3 [1280/110534 (1%)]\tClassification Loss: 1.6187\r\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tClassification Loss: 1.5529\r\n",
      "Train Epoch: 3 [2560/110534 (2%)]\tClassification Loss: 1.7089\r\n",
      "Train Epoch: 3 [3200/110534 (3%)]\tClassification Loss: 1.4335\r\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tClassification Loss: 1.7929\r\n",
      "Train Epoch: 3 [4480/110534 (4%)]\tClassification Loss: 1.5573\r\n",
      "Train Epoch: 3 [5120/110534 (5%)]\tClassification Loss: 1.5439\r\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tClassification Loss: 1.8023\r\n",
      "Train Epoch: 3 [6400/110534 (6%)]\tClassification Loss: 1.9647\r\n",
      "Train Epoch: 3 [7040/110534 (6%)]\tClassification Loss: 1.7807\r\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tClassification Loss: 1.4606\r\n",
      "Train Epoch: 3 [8320/110534 (8%)]\tClassification Loss: 1.8194\r\n",
      "Train Epoch: 3 [8960/110534 (8%)]\tClassification Loss: 1.5578\r\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tClassification Loss: 1.5697\r\n",
      "Train Epoch: 3 [10240/110534 (9%)]\tClassification Loss: 1.3667\r\n",
      "Train Epoch: 3 [10880/110534 (10%)]\tClassification Loss: 1.5864\r\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 3 [12160/110534 (11%)]\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 3 [12800/110534 (12%)]\tClassification Loss: 1.8075\r\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 3 [14080/110534 (13%)]\tClassification Loss: 1.6833\r\n",
      "Train Epoch: 3 [14720/110534 (13%)]\tClassification Loss: 1.6819\r\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tClassification Loss: 1.6858\r\n",
      "Train Epoch: 3 [16000/110534 (14%)]\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 3 [16640/110534 (15%)]\tClassification Loss: 1.6243\r\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tClassification Loss: 1.9996\r\n",
      "Train Epoch: 3 [17920/110534 (16%)]\tClassification Loss: 1.7990\r\n",
      "Train Epoch: 3 [18560/110534 (17%)]\tClassification Loss: 1.6761\r\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tClassification Loss: 1.6026\r\n",
      "Train Epoch: 3 [19840/110534 (18%)]\tClassification Loss: 1.6224\r\n",
      "Train Epoch: 3 [20480/110534 (19%)]\tClassification Loss: 1.6898\r\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tClassification Loss: 1.5476\r\n",
      "Train Epoch: 3 [21760/110534 (20%)]\tClassification Loss: 1.5858\r\n",
      "Train Epoch: 3 [22400/110534 (20%)]\tClassification Loss: 1.5819\r\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tClassification Loss: 1.6029\r\n",
      "Train Epoch: 3 [23680/110534 (21%)]\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 3 [24320/110534 (22%)]\tClassification Loss: 1.7519\r\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 3 [25600/110534 (23%)]\tClassification Loss: 1.2702\r\n",
      "Train Epoch: 3 [26240/110534 (24%)]\tClassification Loss: 1.6893\r\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tClassification Loss: 1.6770\r\n",
      "Train Epoch: 3 [27520/110534 (25%)]\tClassification Loss: 1.6887\r\n",
      "Train Epoch: 3 [28160/110534 (25%)]\tClassification Loss: 1.7520\r\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tClassification Loss: 1.6118\r\n",
      "Train Epoch: 3 [29440/110534 (27%)]\tClassification Loss: 1.3867\r\n",
      "Train Epoch: 3 [30080/110534 (27%)]\tClassification Loss: 1.6032\r\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tClassification Loss: 1.3384\r\n",
      "Train Epoch: 3 [31360/110534 (28%)]\tClassification Loss: 1.2673\r\n",
      "Train Epoch: 3 [32000/110534 (29%)]\tClassification Loss: 1.8090\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_500.pth.tar\r\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tClassification Loss: 1.6125\r\n",
      "Train Epoch: 3 [33280/110534 (30%)]\tClassification Loss: 1.6048\r\n",
      "Train Epoch: 3 [33920/110534 (31%)]\tClassification Loss: 1.8699\r\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tClassification Loss: 1.7520\r\n",
      "Train Epoch: 3 [35200/110534 (32%)]\tClassification Loss: 1.5988\r\n",
      "Train Epoch: 3 [35840/110534 (32%)]\tClassification Loss: 1.5702\r\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tClassification Loss: 1.5104\r\n",
      "Train Epoch: 3 [37120/110534 (34%)]\tClassification Loss: 1.9001\r\n",
      "Train Epoch: 3 [37760/110534 (34%)]\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tClassification Loss: 1.6737\r\n",
      "\r\n",
      "Test set: Average loss: 1.5281, Accuracy: 6871/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [39040/110534 (35%)]\tClassification Loss: 1.4275\r\n",
      "Train Epoch: 3 [39680/110534 (36%)]\tClassification Loss: 1.6666\r\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tClassification Loss: 1.8151\r\n",
      "Train Epoch: 3 [40960/110534 (37%)]\tClassification Loss: 1.5256\r\n",
      "Train Epoch: 3 [41600/110534 (38%)]\tClassification Loss: 1.9676\r\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tClassification Loss: 1.7756\r\n",
      "Train Epoch: 3 [42880/110534 (39%)]\tClassification Loss: 1.5135\r\n",
      "Train Epoch: 3 [43520/110534 (39%)]\tClassification Loss: 1.5481\r\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tClassification Loss: 1.7500\r\n",
      "Train Epoch: 3 [44800/110534 (41%)]\tClassification Loss: 1.6053\r\n",
      "Train Epoch: 3 [45440/110534 (41%)]\tClassification Loss: 1.3851\r\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tClassification Loss: 1.7134\r\n",
      "Train Epoch: 3 [46720/110534 (42%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 3 [47360/110534 (43%)]\tClassification Loss: 1.5811\r\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tClassification Loss: 1.7339\r\n",
      "Train Epoch: 3 [48640/110534 (44%)]\tClassification Loss: 1.6576\r\n",
      "Train Epoch: 3 [49280/110534 (45%)]\tClassification Loss: 1.4814\r\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tClassification Loss: 1.4766\r\n",
      "Train Epoch: 3 [50560/110534 (46%)]\tClassification Loss: 1.8728\r\n",
      "Train Epoch: 3 [51200/110534 (46%)]\tClassification Loss: 1.5878\r\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tClassification Loss: 1.5968\r\n",
      "Train Epoch: 3 [52480/110534 (47%)]\tClassification Loss: 1.4484\r\n",
      "Train Epoch: 3 [53120/110534 (48%)]\tClassification Loss: 1.6000\r\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tClassification Loss: 1.6944\r\n",
      "Train Epoch: 3 [54400/110534 (49%)]\tClassification Loss: 1.4922\r\n",
      "Train Epoch: 3 [55040/110534 (50%)]\tClassification Loss: 1.3386\r\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tClassification Loss: 1.6611\r\n",
      "Train Epoch: 3 [56320/110534 (51%)]\tClassification Loss: 1.6464\r\n",
      "Train Epoch: 3 [56960/110534 (52%)]\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tClassification Loss: 1.5832\r\n",
      "Train Epoch: 3 [58240/110534 (53%)]\tClassification Loss: 1.5771\r\n",
      "Train Epoch: 3 [58880/110534 (53%)]\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 3 [60160/110534 (54%)]\tClassification Loss: 1.4225\r\n",
      "Train Epoch: 3 [60800/110534 (55%)]\tClassification Loss: 1.5953\r\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 3 [62080/110534 (56%)]\tClassification Loss: 1.5956\r\n",
      "Train Epoch: 3 [62720/110534 (57%)]\tClassification Loss: 1.6407\r\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tClassification Loss: 1.8428\r\n",
      "Train Epoch: 3 [64000/110534 (58%)]\tClassification Loss: 1.7141\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1000.pth.tar\r\n",
      "Train Epoch: 3 [64640/110534 (58%)]\tClassification Loss: 1.7488\r\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 3 [65920/110534 (60%)]\tClassification Loss: 1.5159\r\n",
      "Train Epoch: 3 [66560/110534 (60%)]\tClassification Loss: 1.4837\r\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tClassification Loss: 1.7620\r\n",
      "Train Epoch: 3 [67840/110534 (61%)]\tClassification Loss: 1.7134\r\n",
      "Train Epoch: 3 [68480/110534 (62%)]\tClassification Loss: 1.6399\r\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 3 [69760/110534 (63%)]\tClassification Loss: 1.5561\r\n",
      "Train Epoch: 3 [70400/110534 (64%)]\tClassification Loss: 1.9280\r\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tClassification Loss: 1.6590\r\n",
      "Train Epoch: 3 [71680/110534 (65%)]\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 3 [72320/110534 (65%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tClassification Loss: 1.4187\r\n",
      "Train Epoch: 3 [73600/110534 (67%)]\tClassification Loss: 1.7105\r\n",
      "Train Epoch: 3 [74240/110534 (67%)]\tClassification Loss: 1.5043\r\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 3 [75520/110534 (68%)]\tClassification Loss: 1.6982\r\n",
      "Train Epoch: 3 [76160/110534 (69%)]\tClassification Loss: 1.5845\r\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tClassification Loss: 1.8446\r\n",
      "\r\n",
      "Test set: Average loss: 1.5112, Accuracy: 6905/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [77440/110534 (70%)]\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 3 [78080/110534 (71%)]\tClassification Loss: 1.5634\r\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tClassification Loss: 1.7891\r\n",
      "Train Epoch: 3 [79360/110534 (72%)]\tClassification Loss: 1.6160\r\n",
      "Train Epoch: 3 [80000/110534 (72%)]\tClassification Loss: 1.4600\r\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tClassification Loss: 1.5171\r\n",
      "Train Epoch: 3 [81280/110534 (74%)]\tClassification Loss: 1.3467\r\n",
      "Train Epoch: 3 [81920/110534 (74%)]\tClassification Loss: 1.6303\r\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tClassification Loss: 1.8360\r\n",
      "Train Epoch: 3 [83200/110534 (75%)]\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 3 [83840/110534 (76%)]\tClassification Loss: 1.8899\r\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tClassification Loss: 1.5666\r\n",
      "Train Epoch: 3 [85120/110534 (77%)]\tClassification Loss: 1.8356\r\n",
      "Train Epoch: 3 [85760/110534 (78%)]\tClassification Loss: 1.4830\r\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tClassification Loss: 1.5488\r\n",
      "Train Epoch: 3 [87040/110534 (79%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 3 [87680/110534 (79%)]\tClassification Loss: 1.5506\r\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tClassification Loss: 1.4298\r\n",
      "Train Epoch: 3 [88960/110534 (80%)]\tClassification Loss: 1.3079\r\n",
      "Train Epoch: 3 [89600/110534 (81%)]\tClassification Loss: 1.5259\r\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 3 [90880/110534 (82%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 3 [91520/110534 (83%)]\tClassification Loss: 1.5609\r\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tClassification Loss: 1.5795\r\n",
      "Train Epoch: 3 [92800/110534 (84%)]\tClassification Loss: 1.6081\r\n",
      "Train Epoch: 3 [93440/110534 (85%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tClassification Loss: 1.4451\r\n",
      "Train Epoch: 3 [94720/110534 (86%)]\tClassification Loss: 1.8947\r\n",
      "Train Epoch: 3 [95360/110534 (86%)]\tClassification Loss: 1.4726\r\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tClassification Loss: 2.0023\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [96640/110534 (87%)]\tClassification Loss: 1.6881\r\n",
      "Train Epoch: 3 [97280/110534 (88%)]\tClassification Loss: 1.7487\r\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tClassification Loss: 1.5220\r\n",
      "Train Epoch: 3 [98560/110534 (89%)]\tClassification Loss: 1.4847\r\n",
      "Train Epoch: 3 [99200/110534 (90%)]\tClassification Loss: 1.9700\r\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 3 [100480/110534 (91%)]\tClassification Loss: 1.5882\r\n",
      "Train Epoch: 3 [101120/110534 (91%)]\tClassification Loss: 1.5276\r\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tClassification Loss: 1.8355\r\n",
      "Train Epoch: 3 [102400/110534 (93%)]\tClassification Loss: 1.7532\r\n",
      "Train Epoch: 3 [103040/110534 (93%)]\tClassification Loss: 1.6453\r\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tClassification Loss: 1.3789\r\n",
      "Train Epoch: 3 [104320/110534 (94%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 3 [104960/110534 (95%)]\tClassification Loss: 1.7747\r\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tClassification Loss: 1.9508\r\n",
      "Train Epoch: 3 [106240/110534 (96%)]\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 3 [106880/110534 (97%)]\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 3 [108160/110534 (98%)]\tClassification Loss: 1.4483\r\n",
      "Train Epoch: 3 [108800/110534 (98%)]\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tClassification Loss: 1.4507\r\n",
      "Train Epoch: 3 [110080/110534 (100%)]\tClassification Loss: 1.7606\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/110534 (0%)]\tClassification Loss: 1.8712\r\n",
      "\r\n",
      "Test set: Average loss: 1.5038, Accuracy: 6919/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 4 [640/110534 (1%)]\tClassification Loss: 1.6690\r\n",
      "Train Epoch: 4 [1280/110534 (1%)]\tClassification Loss: 1.6371\r\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 4 [2560/110534 (2%)]\tClassification Loss: 1.7443\r\n",
      "Train Epoch: 4 [3200/110534 (3%)]\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 4 [4480/110534 (4%)]\tClassification Loss: 1.4483\r\n",
      "Train Epoch: 4 [5120/110534 (5%)]\tClassification Loss: 1.6530\r\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 4 [6400/110534 (6%)]\tClassification Loss: 1.8228\r\n",
      "Train Epoch: 4 [7040/110534 (6%)]\tClassification Loss: 1.8083\r\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tClassification Loss: 1.3829\r\n",
      "Train Epoch: 4 [8320/110534 (8%)]\tClassification Loss: 1.7513\r\n",
      "Train Epoch: 4 [8960/110534 (8%)]\tClassification Loss: 1.5160\r\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 4 [10240/110534 (9%)]\tClassification Loss: 1.2773\r\n",
      "Train Epoch: 4 [10880/110534 (10%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tClassification Loss: 1.5930\r\n",
      "Train Epoch: 4 [12160/110534 (11%)]\tClassification Loss: 1.6681\r\n",
      "Train Epoch: 4 [12800/110534 (12%)]\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 4 [14080/110534 (13%)]\tClassification Loss: 1.6045\r\n",
      "Train Epoch: 4 [14720/110534 (13%)]\tClassification Loss: 1.5475\r\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tClassification Loss: 1.6439\r\n",
      "Train Epoch: 4 [16000/110534 (14%)]\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 4 [16640/110534 (15%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tClassification Loss: 1.8090\r\n",
      "Train Epoch: 4 [17920/110534 (16%)]\tClassification Loss: 1.7138\r\n",
      "Train Epoch: 4 [18560/110534 (17%)]\tClassification Loss: 1.6184\r\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 4 [19840/110534 (18%)]\tClassification Loss: 1.5637\r\n",
      "Train Epoch: 4 [20480/110534 (19%)]\tClassification Loss: 1.8096\r\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tClassification Loss: 1.6160\r\n",
      "Train Epoch: 4 [21760/110534 (20%)]\tClassification Loss: 1.5649\r\n",
      "Train Epoch: 4 [22400/110534 (20%)]\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tClassification Loss: 1.4290\r\n",
      "Train Epoch: 4 [23680/110534 (21%)]\tClassification Loss: 1.6765\r\n",
      "Train Epoch: 4 [24320/110534 (22%)]\tClassification Loss: 1.5694\r\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tClassification Loss: 1.6575\r\n",
      "Train Epoch: 4 [25600/110534 (23%)]\tClassification Loss: 1.4505\r\n",
      "Train Epoch: 4 [26240/110534 (24%)]\tClassification Loss: 1.6735\r\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 4 [27520/110534 (25%)]\tClassification Loss: 1.5682\r\n",
      "Train Epoch: 4 [28160/110534 (25%)]\tClassification Loss: 1.5398\r\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tClassification Loss: 1.7008\r\n",
      "Train Epoch: 4 [29440/110534 (27%)]\tClassification Loss: 1.5424\r\n",
      "Train Epoch: 4 [30080/110534 (27%)]\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tClassification Loss: 1.4209\r\n",
      "Train Epoch: 4 [31360/110534 (28%)]\tClassification Loss: 1.1948\r\n",
      "Train Epoch: 4 [32000/110534 (29%)]\tClassification Loss: 1.6864\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_500.pth.tar\r\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tClassification Loss: 1.6283\r\n",
      "Train Epoch: 4 [33280/110534 (30%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 4 [33920/110534 (31%)]\tClassification Loss: 1.6435\r\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tClassification Loss: 1.7287\r\n",
      "Train Epoch: 4 [35200/110534 (32%)]\tClassification Loss: 1.5312\r\n",
      "Train Epoch: 4 [35840/110534 (32%)]\tClassification Loss: 1.4644\r\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tClassification Loss: 1.6031\r\n",
      "Train Epoch: 4 [37120/110534 (34%)]\tClassification Loss: 1.7895\r\n",
      "Train Epoch: 4 [37760/110534 (34%)]\tClassification Loss: 1.3327\r\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tClassification Loss: 1.7945\r\n",
      "\r\n",
      "Test set: Average loss: 1.5013, Accuracy: 6916/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 4 [39040/110534 (35%)]\tClassification Loss: 1.5041\r\n",
      "Train Epoch: 4 [39680/110534 (36%)]\tClassification Loss: 1.6146\r\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tClassification Loss: 1.7640\r\n",
      "Train Epoch: 4 [40960/110534 (37%)]\tClassification Loss: 1.4953\r\n",
      "Train Epoch: 4 [41600/110534 (38%)]\tClassification Loss: 1.9657\r\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tClassification Loss: 1.6379\r\n",
      "Train Epoch: 4 [42880/110534 (39%)]\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 4 [43520/110534 (39%)]\tClassification Loss: 1.4768\r\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tClassification Loss: 1.7077\r\n",
      "Train Epoch: 4 [44800/110534 (41%)]\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 4 [45440/110534 (41%)]\tClassification Loss: 1.3880\r\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tClassification Loss: 1.7867\r\n",
      "Train Epoch: 4 [46720/110534 (42%)]\tClassification Loss: 1.6979\r\n",
      "Train Epoch: 4 [47360/110534 (43%)]\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tClassification Loss: 1.7220\r\n",
      "Train Epoch: 4 [48640/110534 (44%)]\tClassification Loss: 1.6582\r\n",
      "Train Epoch: 4 [49280/110534 (45%)]\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tClassification Loss: 1.4188\r\n",
      "Train Epoch: 4 [50560/110534 (46%)]\tClassification Loss: 1.8486\r\n",
      "Train Epoch: 4 [51200/110534 (46%)]\tClassification Loss: 1.5920\r\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tClassification Loss: 1.6555\r\n",
      "Train Epoch: 4 [52480/110534 (47%)]\tClassification Loss: 1.4804\r\n",
      "Train Epoch: 4 [53120/110534 (48%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 4 [54400/110534 (49%)]\tClassification Loss: 1.3558\r\n",
      "Train Epoch: 4 [55040/110534 (50%)]\tClassification Loss: 1.3250\r\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tClassification Loss: 1.6900\r\n",
      "Train Epoch: 4 [56320/110534 (51%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 4 [56960/110534 (52%)]\tClassification Loss: 1.9247\r\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tClassification Loss: 1.4979\r\n",
      "Train Epoch: 4 [58240/110534 (53%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 4 [58880/110534 (53%)]\tClassification Loss: 1.4789\r\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tClassification Loss: 1.6339\r\n",
      "Train Epoch: 4 [60160/110534 (54%)]\tClassification Loss: 1.3992\r\n",
      "Train Epoch: 4 [60800/110534 (55%)]\tClassification Loss: 1.6112\r\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tClassification Loss: 1.7713\r\n",
      "Train Epoch: 4 [62080/110534 (56%)]\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 4 [62720/110534 (57%)]\tClassification Loss: 1.4638\r\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tClassification Loss: 1.8725\r\n",
      "Train Epoch: 4 [64000/110534 (58%)]\tClassification Loss: 1.6414\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1000.pth.tar\r\n",
      "Train Epoch: 4 [64640/110534 (58%)]\tClassification Loss: 1.8405\r\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tClassification Loss: 1.7262\r\n",
      "Train Epoch: 4 [65920/110534 (60%)]\tClassification Loss: 1.3784\r\n",
      "Train Epoch: 4 [66560/110534 (60%)]\tClassification Loss: 1.5564\r\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tClassification Loss: 1.6894\r\n",
      "Train Epoch: 4 [67840/110534 (61%)]\tClassification Loss: 1.5692\r\n",
      "Train Epoch: 4 [68480/110534 (62%)]\tClassification Loss: 1.6833\r\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tClassification Loss: 1.3543\r\n",
      "Train Epoch: 4 [69760/110534 (63%)]\tClassification Loss: 1.3418\r\n",
      "Train Epoch: 4 [70400/110534 (64%)]\tClassification Loss: 1.7348\r\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tClassification Loss: 1.7913\r\n",
      "Train Epoch: 4 [71680/110534 (65%)]\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 4 [72320/110534 (65%)]\tClassification Loss: 1.5299\r\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tClassification Loss: 1.4923\r\n",
      "Train Epoch: 4 [73600/110534 (67%)]\tClassification Loss: 1.7303\r\n",
      "Train Epoch: 4 [74240/110534 (67%)]\tClassification Loss: 1.3927\r\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tClassification Loss: 1.4597\r\n",
      "Train Epoch: 4 [75520/110534 (68%)]\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 4 [76160/110534 (69%)]\tClassification Loss: 1.5457\r\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tClassification Loss: 1.7814\r\n",
      "\r\n",
      "Test set: Average loss: 1.4869, Accuracy: 6951/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 4 [77440/110534 (70%)]\tClassification Loss: 1.7929\r\n",
      "Train Epoch: 4 [78080/110534 (71%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tClassification Loss: 1.6598\r\n",
      "Train Epoch: 4 [79360/110534 (72%)]\tClassification Loss: 1.5925\r\n",
      "Train Epoch: 4 [80000/110534 (72%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 4 [81280/110534 (74%)]\tClassification Loss: 1.2236\r\n",
      "Train Epoch: 4 [81920/110534 (74%)]\tClassification Loss: 1.4620\r\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 4 [83200/110534 (75%)]\tClassification Loss: 1.5679\r\n",
      "Train Epoch: 4 [83840/110534 (76%)]\tClassification Loss: 1.8288\r\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tClassification Loss: 1.4304\r\n",
      "Train Epoch: 4 [85120/110534 (77%)]\tClassification Loss: 1.8323\r\n",
      "Train Epoch: 4 [85760/110534 (78%)]\tClassification Loss: 1.6266\r\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 4 [87040/110534 (79%)]\tClassification Loss: 1.4712\r\n",
      "Train Epoch: 4 [87680/110534 (79%)]\tClassification Loss: 1.5945\r\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tClassification Loss: 1.4022\r\n",
      "Train Epoch: 4 [88960/110534 (80%)]\tClassification Loss: 1.3909\r\n",
      "Train Epoch: 4 [89600/110534 (81%)]\tClassification Loss: 1.5974\r\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tClassification Loss: 1.7137\r\n",
      "Train Epoch: 4 [90880/110534 (82%)]\tClassification Loss: 1.5408\r\n",
      "Train Epoch: 4 [91520/110534 (83%)]\tClassification Loss: 1.6097\r\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tClassification Loss: 1.5321\r\n",
      "Train Epoch: 4 [92800/110534 (84%)]\tClassification Loss: 1.4742\r\n",
      "Train Epoch: 4 [93440/110534 (85%)]\tClassification Loss: 1.4779\r\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tClassification Loss: 1.4898\r\n",
      "Train Epoch: 4 [94720/110534 (86%)]\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 4 [95360/110534 (86%)]\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tClassification Loss: 1.9488\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [96640/110534 (87%)]\tClassification Loss: 1.5873\r\n",
      "Train Epoch: 4 [97280/110534 (88%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tClassification Loss: 1.5035\r\n",
      "Train Epoch: 4 [98560/110534 (89%)]\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 4 [99200/110534 (90%)]\tClassification Loss: 2.0697\r\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tClassification Loss: 1.4832\r\n",
      "Train Epoch: 4 [100480/110534 (91%)]\tClassification Loss: 1.5010\r\n",
      "Train Epoch: 4 [101120/110534 (91%)]\tClassification Loss: 1.3656\r\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tClassification Loss: 1.7536\r\n",
      "Train Epoch: 4 [102400/110534 (93%)]\tClassification Loss: 1.6937\r\n",
      "Train Epoch: 4 [103040/110534 (93%)]\tClassification Loss: 1.6053\r\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tClassification Loss: 1.4654\r\n",
      "Train Epoch: 4 [104320/110534 (94%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 4 [104960/110534 (95%)]\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tClassification Loss: 1.8623\r\n",
      "Train Epoch: 4 [106240/110534 (96%)]\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 4 [106880/110534 (97%)]\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tClassification Loss: 1.4582\r\n",
      "Train Epoch: 4 [108160/110534 (98%)]\tClassification Loss: 1.3222\r\n",
      "Train Epoch: 4 [108800/110534 (98%)]\tClassification Loss: 1.4881\r\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 4 [110080/110534 (100%)]\tClassification Loss: 1.6868\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/110534 (0%)]\tClassification Loss: 1.8114\r\n",
      "\r\n",
      "Test set: Average loss: 1.4842, Accuracy: 6949/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 5 [640/110534 (1%)]\tClassification Loss: 1.6458\r\n",
      "Train Epoch: 5 [1280/110534 (1%)]\tClassification Loss: 1.5799\r\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tClassification Loss: 1.5884\r\n",
      "Train Epoch: 5 [2560/110534 (2%)]\tClassification Loss: 1.6837\r\n",
      "Train Epoch: 5 [3200/110534 (3%)]\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tClassification Loss: 1.6885\r\n",
      "Train Epoch: 5 [4480/110534 (4%)]\tClassification Loss: 1.5184\r\n",
      "Train Epoch: 5 [5120/110534 (5%)]\tClassification Loss: 1.6721\r\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tClassification Loss: 1.7874\r\n",
      "Train Epoch: 5 [6400/110534 (6%)]\tClassification Loss: 1.9466\r\n",
      "Train Epoch: 5 [7040/110534 (6%)]\tClassification Loss: 1.6834\r\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tClassification Loss: 1.5006\r\n",
      "Train Epoch: 5 [8320/110534 (8%)]\tClassification Loss: 1.7935\r\n",
      "Train Epoch: 5 [8960/110534 (8%)]\tClassification Loss: 1.5589\r\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tClassification Loss: 1.4459\r\n",
      "Train Epoch: 5 [10240/110534 (9%)]\tClassification Loss: 1.3968\r\n",
      "Train Epoch: 5 [10880/110534 (10%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tClassification Loss: 1.6825\r\n",
      "Train Epoch: 5 [12160/110534 (11%)]\tClassification Loss: 1.6153\r\n",
      "Train Epoch: 5 [12800/110534 (12%)]\tClassification Loss: 1.5334\r\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tClassification Loss: 1.6082\r\n",
      "Train Epoch: 5 [14080/110534 (13%)]\tClassification Loss: 1.5804\r\n",
      "Train Epoch: 5 [14720/110534 (13%)]\tClassification Loss: 1.5098\r\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 5 [16000/110534 (14%)]\tClassification Loss: 1.8597\r\n",
      "Train Epoch: 5 [16640/110534 (15%)]\tClassification Loss: 1.5610\r\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tClassification Loss: 1.8374\r\n",
      "Train Epoch: 5 [17920/110534 (16%)]\tClassification Loss: 1.7810\r\n",
      "Train Epoch: 5 [18560/110534 (17%)]\tClassification Loss: 1.6242\r\n",
      "Train Epoch: 5 [19200/110534 (17%)]\tClassification Loss: 1.3922\r\n",
      "Train Epoch: 5 [19840/110534 (18%)]\tClassification Loss: 1.4885\r\n",
      "Train Epoch: 5 [20480/110534 (19%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 5 [21120/110534 (19%)]\tClassification Loss: 1.4710\r\n",
      "Train Epoch: 5 [21760/110534 (20%)]\tClassification Loss: 1.5116\r\n",
      "Train Epoch: 5 [22400/110534 (20%)]\tClassification Loss: 1.6047\r\n",
      "Train Epoch: 5 [23040/110534 (21%)]\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 5 [23680/110534 (21%)]\tClassification Loss: 1.4822\r\n",
      "Train Epoch: 5 [24320/110534 (22%)]\tClassification Loss: 1.5581\r\n",
      "Train Epoch: 5 [24960/110534 (23%)]\tClassification Loss: 1.5999\r\n",
      "Train Epoch: 5 [25600/110534 (23%)]\tClassification Loss: 1.2585\r\n",
      "Train Epoch: 5 [26240/110534 (24%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 5 [26880/110534 (24%)]\tClassification Loss: 1.7050\r\n",
      "Train Epoch: 5 [27520/110534 (25%)]\tClassification Loss: 1.6484\r\n",
      "Train Epoch: 5 [28160/110534 (25%)]\tClassification Loss: 1.7070\r\n",
      "Train Epoch: 5 [28800/110534 (26%)]\tClassification Loss: 1.6300\r\n",
      "Train Epoch: 5 [29440/110534 (27%)]\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 5 [30080/110534 (27%)]\tClassification Loss: 1.5947\r\n",
      "Train Epoch: 5 [30720/110534 (28%)]\tClassification Loss: 1.3920\r\n",
      "Train Epoch: 5 [31360/110534 (28%)]\tClassification Loss: 1.3371\r\n",
      "Train Epoch: 5 [32000/110534 (29%)]\tClassification Loss: 1.6820\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_500.pth.tar\r\n",
      "Train Epoch: 5 [32640/110534 (30%)]\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 5 [33280/110534 (30%)]\tClassification Loss: 1.4869\r\n",
      "Train Epoch: 5 [33920/110534 (31%)]\tClassification Loss: 1.8050\r\n",
      "Train Epoch: 5 [34560/110534 (31%)]\tClassification Loss: 1.6730\r\n",
      "Train Epoch: 5 [35200/110534 (32%)]\tClassification Loss: 1.6121\r\n",
      "Train Epoch: 5 [35840/110534 (32%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 5 [36480/110534 (33%)]\tClassification Loss: 1.5524\r\n",
      "Train Epoch: 5 [37120/110534 (34%)]\tClassification Loss: 1.7696\r\n",
      "Train Epoch: 5 [37760/110534 (34%)]\tClassification Loss: 1.4122\r\n",
      "Train Epoch: 5 [38400/110534 (35%)]\tClassification Loss: 1.7665\r\n",
      "\r\n",
      "Test set: Average loss: 1.4860, Accuracy: 6949/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 5 [39040/110534 (35%)]\tClassification Loss: 1.3854\r\n",
      "Train Epoch: 5 [39680/110534 (36%)]\tClassification Loss: 1.7998\r\n",
      "Train Epoch: 5 [40320/110534 (36%)]\tClassification Loss: 1.7741\r\n",
      "Train Epoch: 5 [40960/110534 (37%)]\tClassification Loss: 1.3165\r\n",
      "Train Epoch: 5 [41600/110534 (38%)]\tClassification Loss: 1.8483\r\n",
      "Train Epoch: 5 [42240/110534 (38%)]\tClassification Loss: 1.7749\r\n",
      "Train Epoch: 5 [42880/110534 (39%)]\tClassification Loss: 1.5881\r\n",
      "Train Epoch: 5 [43520/110534 (39%)]\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 5 [44160/110534 (40%)]\tClassification Loss: 1.5963\r\n",
      "Train Epoch: 5 [44800/110534 (41%)]\tClassification Loss: 1.5080\r\n",
      "Train Epoch: 5 [45440/110534 (41%)]\tClassification Loss: 1.4344\r\n",
      "Train Epoch: 5 [46080/110534 (42%)]\tClassification Loss: 1.7065\r\n",
      "Train Epoch: 5 [46720/110534 (42%)]\tClassification Loss: 1.4982\r\n",
      "Train Epoch: 5 [47360/110534 (43%)]\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 5 [48000/110534 (43%)]\tClassification Loss: 1.7363\r\n",
      "Train Epoch: 5 [48640/110534 (44%)]\tClassification Loss: 1.6950\r\n",
      "Train Epoch: 5 [49280/110534 (45%)]\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 5 [49920/110534 (45%)]\tClassification Loss: 1.4482\r\n",
      "Train Epoch: 5 [50560/110534 (46%)]\tClassification Loss: 1.8368\r\n",
      "Train Epoch: 5 [51200/110534 (46%)]\tClassification Loss: 1.6001\r\n",
      "Train Epoch: 5 [51840/110534 (47%)]\tClassification Loss: 1.6085\r\n",
      "Train Epoch: 5 [52480/110534 (47%)]\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 5 [53120/110534 (48%)]\tClassification Loss: 1.5747\r\n",
      "Train Epoch: 5 [53760/110534 (49%)]\tClassification Loss: 1.7890\r\n",
      "Train Epoch: 5 [54400/110534 (49%)]\tClassification Loss: 1.5903\r\n",
      "Train Epoch: 5 [55040/110534 (50%)]\tClassification Loss: 1.3484\r\n",
      "Train Epoch: 5 [55680/110534 (50%)]\tClassification Loss: 1.6406\r\n",
      "Train Epoch: 5 [56320/110534 (51%)]\tClassification Loss: 1.5599\r\n",
      "Train Epoch: 5 [56960/110534 (52%)]\tClassification Loss: 1.9094\r\n",
      "Train Epoch: 5 [57600/110534 (52%)]\tClassification Loss: 1.5674\r\n",
      "Train Epoch: 5 [58240/110534 (53%)]\tClassification Loss: 1.4716\r\n",
      "Train Epoch: 5 [58880/110534 (53%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 5 [59520/110534 (54%)]\tClassification Loss: 1.6959\r\n",
      "Train Epoch: 5 [60160/110534 (54%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 5 [60800/110534 (55%)]\tClassification Loss: 1.5349\r\n",
      "Train Epoch: 5 [61440/110534 (56%)]\tClassification Loss: 1.7207\r\n",
      "Train Epoch: 5 [62080/110534 (56%)]\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 5 [62720/110534 (57%)]\tClassification Loss: 1.6306\r\n",
      "Train Epoch: 5 [63360/110534 (57%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 5 [64000/110534 (58%)]\tClassification Loss: 1.6900\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1000.pth.tar\r\n",
      "Train Epoch: 5 [64640/110534 (58%)]\tClassification Loss: 1.7559\r\n",
      "Train Epoch: 5 [65280/110534 (59%)]\tClassification Loss: 1.7250\r\n",
      "Train Epoch: 5 [65920/110534 (60%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 5 [66560/110534 (60%)]\tClassification Loss: 1.3833\r\n",
      "Train Epoch: 5 [67200/110534 (61%)]\tClassification Loss: 1.6018\r\n",
      "Train Epoch: 5 [67840/110534 (61%)]\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 5 [68480/110534 (62%)]\tClassification Loss: 1.6279\r\n",
      "Train Epoch: 5 [69120/110534 (63%)]\tClassification Loss: 1.4471\r\n",
      "Train Epoch: 5 [69760/110534 (63%)]\tClassification Loss: 1.3081\r\n",
      "Train Epoch: 5 [70400/110534 (64%)]\tClassification Loss: 1.9168\r\n",
      "Train Epoch: 5 [71040/110534 (64%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 5 [71680/110534 (65%)]\tClassification Loss: 1.2990\r\n",
      "Train Epoch: 5 [72320/110534 (65%)]\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 5 [72960/110534 (66%)]\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 5 [73600/110534 (67%)]\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 5 [74240/110534 (67%)]\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 5 [74880/110534 (68%)]\tClassification Loss: 1.4501\r\n",
      "Train Epoch: 5 [75520/110534 (68%)]\tClassification Loss: 1.7305\r\n",
      "Train Epoch: 5 [76160/110534 (69%)]\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 5 [76800/110534 (69%)]\tClassification Loss: 1.7036\r\n",
      "\r\n",
      "Test set: Average loss: 1.4717, Accuracy: 6980/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 5 [77440/110534 (70%)]\tClassification Loss: 1.6213\r\n",
      "Train Epoch: 5 [78080/110534 (71%)]\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 5 [78720/110534 (71%)]\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 5 [79360/110534 (72%)]\tClassification Loss: 1.8060\r\n",
      "Train Epoch: 5 [80000/110534 (72%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 5 [80640/110534 (73%)]\tClassification Loss: 1.3956\r\n",
      "Train Epoch: 5 [81280/110534 (74%)]\tClassification Loss: 1.2955\r\n",
      "Train Epoch: 5 [81920/110534 (74%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 5 [82560/110534 (75%)]\tClassification Loss: 1.9351\r\n",
      "Train Epoch: 5 [83200/110534 (75%)]\tClassification Loss: 1.3551\r\n",
      "Train Epoch: 5 [83840/110534 (76%)]\tClassification Loss: 1.8648\r\n",
      "Train Epoch: 5 [84480/110534 (76%)]\tClassification Loss: 1.3908\r\n",
      "Train Epoch: 5 [85120/110534 (77%)]\tClassification Loss: 1.7484\r\n",
      "Train Epoch: 5 [85760/110534 (78%)]\tClassification Loss: 1.6012\r\n",
      "Train Epoch: 5 [86400/110534 (78%)]\tClassification Loss: 1.6749\r\n",
      "Train Epoch: 5 [87040/110534 (79%)]\tClassification Loss: 1.4427\r\n",
      "Train Epoch: 5 [87680/110534 (79%)]\tClassification Loss: 1.6211\r\n",
      "Train Epoch: 5 [88320/110534 (80%)]\tClassification Loss: 1.4122\r\n",
      "Train Epoch: 5 [88960/110534 (80%)]\tClassification Loss: 1.2981\r\n",
      "Train Epoch: 5 [89600/110534 (81%)]\tClassification Loss: 1.4527\r\n",
      "Train Epoch: 5 [90240/110534 (82%)]\tClassification Loss: 1.7909\r\n",
      "Train Epoch: 5 [90880/110534 (82%)]\tClassification Loss: 1.5184\r\n",
      "Train Epoch: 5 [91520/110534 (83%)]\tClassification Loss: 1.4561\r\n",
      "Train Epoch: 5 [92160/110534 (83%)]\tClassification Loss: 1.4503\r\n",
      "Train Epoch: 5 [92800/110534 (84%)]\tClassification Loss: 1.4563\r\n",
      "Train Epoch: 5 [93440/110534 (85%)]\tClassification Loss: 1.5390\r\n",
      "Train Epoch: 5 [94080/110534 (85%)]\tClassification Loss: 1.3170\r\n",
      "Train Epoch: 5 [94720/110534 (86%)]\tClassification Loss: 1.7746\r\n",
      "Train Epoch: 5 [95360/110534 (86%)]\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 5 [96000/110534 (87%)]\tClassification Loss: 1.8519\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [96640/110534 (87%)]\tClassification Loss: 1.6642\r\n",
      "Train Epoch: 5 [97280/110534 (88%)]\tClassification Loss: 1.5074\r\n",
      "Train Epoch: 5 [97920/110534 (89%)]\tClassification Loss: 1.5561\r\n",
      "Train Epoch: 5 [98560/110534 (89%)]\tClassification Loss: 1.5241\r\n",
      "Train Epoch: 5 [99200/110534 (90%)]\tClassification Loss: 2.0007\r\n",
      "Train Epoch: 5 [99840/110534 (90%)]\tClassification Loss: 1.5811\r\n",
      "Train Epoch: 5 [100480/110534 (91%)]\tClassification Loss: 1.5638\r\n",
      "Train Epoch: 5 [101120/110534 (91%)]\tClassification Loss: 1.3257\r\n",
      "Train Epoch: 5 [101760/110534 (92%)]\tClassification Loss: 1.7551\r\n",
      "Train Epoch: 5 [102400/110534 (93%)]\tClassification Loss: 1.7804\r\n",
      "Train Epoch: 5 [103040/110534 (93%)]\tClassification Loss: 1.6864\r\n",
      "Train Epoch: 5 [103680/110534 (94%)]\tClassification Loss: 1.3796\r\n",
      "Train Epoch: 5 [104320/110534 (94%)]\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 5 [104960/110534 (95%)]\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 5 [105600/110534 (96%)]\tClassification Loss: 1.7431\r\n",
      "Train Epoch: 5 [106240/110534 (96%)]\tClassification Loss: 1.4128\r\n",
      "Train Epoch: 5 [106880/110534 (97%)]\tClassification Loss: 1.6600\r\n",
      "Train Epoch: 5 [107520/110534 (97%)]\tClassification Loss: 1.5873\r\n",
      "Train Epoch: 5 [108160/110534 (98%)]\tClassification Loss: 1.3598\r\n",
      "Train Epoch: 5 [108800/110534 (98%)]\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 5 [109440/110534 (99%)]\tClassification Loss: 1.5342\r\n",
      "Train Epoch: 5 [110080/110534 (100%)]\tClassification Loss: 1.7033\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/110534 (0%)]\tClassification Loss: 1.6843\r\n",
      "\r\n",
      "Test set: Average loss: 1.4719, Accuracy: 6999/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 6 [640/110534 (1%)]\tClassification Loss: 1.4710\r\n",
      "Train Epoch: 6 [1280/110534 (1%)]\tClassification Loss: 1.5371\r\n",
      "Train Epoch: 6 [1920/110534 (2%)]\tClassification Loss: 1.4618\r\n",
      "Train Epoch: 6 [2560/110534 (2%)]\tClassification Loss: 1.6009\r\n",
      "Train Epoch: 6 [3200/110534 (3%)]\tClassification Loss: 1.5540\r\n",
      "Train Epoch: 6 [3840/110534 (3%)]\tClassification Loss: 1.6807\r\n",
      "Train Epoch: 6 [4480/110534 (4%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 6 [5120/110534 (5%)]\tClassification Loss: 1.5078\r\n",
      "Train Epoch: 6 [5760/110534 (5%)]\tClassification Loss: 1.6671\r\n",
      "Train Epoch: 6 [6400/110534 (6%)]\tClassification Loss: 1.8100\r\n",
      "Train Epoch: 6 [7040/110534 (6%)]\tClassification Loss: 1.5860\r\n",
      "Train Epoch: 6 [7680/110534 (7%)]\tClassification Loss: 1.4342\r\n",
      "Train Epoch: 6 [8320/110534 (8%)]\tClassification Loss: 1.8101\r\n",
      "Train Epoch: 6 [8960/110534 (8%)]\tClassification Loss: 1.7884\r\n",
      "Train Epoch: 6 [9600/110534 (9%)]\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 6 [10240/110534 (9%)]\tClassification Loss: 1.2832\r\n",
      "Train Epoch: 6 [10880/110534 (10%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 6 [11520/110534 (10%)]\tClassification Loss: 1.6084\r\n",
      "Train Epoch: 6 [12160/110534 (11%)]\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 6 [12800/110534 (12%)]\tClassification Loss: 1.8295\r\n",
      "Train Epoch: 6 [13440/110534 (12%)]\tClassification Loss: 1.5038\r\n",
      "Train Epoch: 6 [14080/110534 (13%)]\tClassification Loss: 1.6289\r\n",
      "Train Epoch: 6 [14720/110534 (13%)]\tClassification Loss: 1.7322\r\n",
      "Train Epoch: 6 [15360/110534 (14%)]\tClassification Loss: 1.5348\r\n",
      "Train Epoch: 6 [16000/110534 (14%)]\tClassification Loss: 1.7773\r\n",
      "Train Epoch: 6 [16640/110534 (15%)]\tClassification Loss: 1.5264\r\n",
      "Train Epoch: 6 [17280/110534 (16%)]\tClassification Loss: 1.8894\r\n",
      "Train Epoch: 6 [17920/110534 (16%)]\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 6 [18560/110534 (17%)]\tClassification Loss: 1.5688\r\n",
      "Train Epoch: 6 [19200/110534 (17%)]\tClassification Loss: 1.5238\r\n",
      "Train Epoch: 6 [19840/110534 (18%)]\tClassification Loss: 1.5180\r\n",
      "Train Epoch: 6 [20480/110534 (19%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 6 [21120/110534 (19%)]\tClassification Loss: 1.4053\r\n",
      "Train Epoch: 6 [21760/110534 (20%)]\tClassification Loss: 1.4326\r\n",
      "Train Epoch: 6 [22400/110534 (20%)]\tClassification Loss: 1.7540\r\n",
      "Train Epoch: 6 [23040/110534 (21%)]\tClassification Loss: 1.5478\r\n",
      "Train Epoch: 6 [23680/110534 (21%)]\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 6 [24320/110534 (22%)]\tClassification Loss: 1.5781\r\n",
      "Train Epoch: 6 [24960/110534 (23%)]\tClassification Loss: 1.4492\r\n",
      "Train Epoch: 6 [25600/110534 (23%)]\tClassification Loss: 1.3418\r\n",
      "Train Epoch: 6 [26240/110534 (24%)]\tClassification Loss: 1.4903\r\n",
      "Train Epoch: 6 [26880/110534 (24%)]\tClassification Loss: 1.6512\r\n",
      "Train Epoch: 6 [27520/110534 (25%)]\tClassification Loss: 1.7008\r\n",
      "Train Epoch: 6 [28160/110534 (25%)]\tClassification Loss: 1.6954\r\n",
      "Train Epoch: 6 [28800/110534 (26%)]\tClassification Loss: 1.6607\r\n",
      "Train Epoch: 6 [29440/110534 (27%)]\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 6 [30080/110534 (27%)]\tClassification Loss: 1.5481\r\n",
      "Train Epoch: 6 [30720/110534 (28%)]\tClassification Loss: 1.3945\r\n",
      "Train Epoch: 6 [31360/110534 (28%)]\tClassification Loss: 1.2637\r\n",
      "Train Epoch: 6 [32000/110534 (29%)]\tClassification Loss: 1.6962\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_500.pth.tar\r\n",
      "Train Epoch: 6 [32640/110534 (30%)]\tClassification Loss: 1.5040\r\n",
      "Train Epoch: 6 [33280/110534 (30%)]\tClassification Loss: 1.4171\r\n",
      "Train Epoch: 6 [33920/110534 (31%)]\tClassification Loss: 1.6197\r\n",
      "Train Epoch: 6 [34560/110534 (31%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 6 [35200/110534 (32%)]\tClassification Loss: 1.4533\r\n",
      "Train Epoch: 6 [35840/110534 (32%)]\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 6 [36480/110534 (33%)]\tClassification Loss: 1.5489\r\n",
      "Train Epoch: 6 [37120/110534 (34%)]\tClassification Loss: 1.6637\r\n",
      "Train Epoch: 6 [37760/110534 (34%)]\tClassification Loss: 1.3356\r\n",
      "Train Epoch: 6 [38400/110534 (35%)]\tClassification Loss: 1.6989\r\n",
      "\r\n",
      "Test set: Average loss: 1.4791, Accuracy: 6973/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 6 [39040/110534 (35%)]\tClassification Loss: 1.3854\r\n",
      "Train Epoch: 6 [39680/110534 (36%)]\tClassification Loss: 1.5694\r\n",
      "Train Epoch: 6 [40320/110534 (36%)]\tClassification Loss: 1.8227\r\n",
      "Train Epoch: 6 [40960/110534 (37%)]\tClassification Loss: 1.3631\r\n",
      "Train Epoch: 6 [41600/110534 (38%)]\tClassification Loss: 1.7622\r\n",
      "Train Epoch: 6 [42240/110534 (38%)]\tClassification Loss: 1.8333\r\n",
      "Train Epoch: 6 [42880/110534 (39%)]\tClassification Loss: 1.5830\r\n",
      "Train Epoch: 6 [43520/110534 (39%)]\tClassification Loss: 1.6168\r\n",
      "Train Epoch: 6 [44160/110534 (40%)]\tClassification Loss: 1.6518\r\n",
      "Train Epoch: 6 [44800/110534 (41%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 6 [45440/110534 (41%)]\tClassification Loss: 1.3699\r\n",
      "Train Epoch: 6 [46080/110534 (42%)]\tClassification Loss: 1.6968\r\n",
      "Train Epoch: 6 [46720/110534 (42%)]\tClassification Loss: 1.4030\r\n",
      "Train Epoch: 6 [47360/110534 (43%)]\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 6 [48000/110534 (43%)]\tClassification Loss: 1.6459\r\n",
      "Train Epoch: 6 [48640/110534 (44%)]\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 6 [49280/110534 (45%)]\tClassification Loss: 1.5010\r\n",
      "Train Epoch: 6 [49920/110534 (45%)]\tClassification Loss: 1.5307\r\n",
      "Train Epoch: 6 [50560/110534 (46%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 6 [51200/110534 (46%)]\tClassification Loss: 1.6797\r\n",
      "Train Epoch: 6 [51840/110534 (47%)]\tClassification Loss: 1.5356\r\n",
      "Train Epoch: 6 [52480/110534 (47%)]\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 6 [53120/110534 (48%)]\tClassification Loss: 1.6128\r\n",
      "Train Epoch: 6 [53760/110534 (49%)]\tClassification Loss: 1.5774\r\n",
      "Train Epoch: 6 [54400/110534 (49%)]\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 6 [55040/110534 (50%)]\tClassification Loss: 1.3064\r\n",
      "Train Epoch: 6 [55680/110534 (50%)]\tClassification Loss: 1.6337\r\n",
      "Train Epoch: 6 [56320/110534 (51%)]\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 6 [56960/110534 (52%)]\tClassification Loss: 2.2147\r\n",
      "Train Epoch: 6 [57600/110534 (52%)]\tClassification Loss: 1.5383\r\n",
      "Train Epoch: 6 [58240/110534 (53%)]\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 6 [58880/110534 (53%)]\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 6 [59520/110534 (54%)]\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 6 [60160/110534 (54%)]\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 6 [60800/110534 (55%)]\tClassification Loss: 1.4703\r\n",
      "Train Epoch: 6 [61440/110534 (56%)]\tClassification Loss: 1.8180\r\n",
      "Train Epoch: 6 [62080/110534 (56%)]\tClassification Loss: 1.5880\r\n",
      "Train Epoch: 6 [62720/110534 (57%)]\tClassification Loss: 1.6792\r\n",
      "Train Epoch: 6 [63360/110534 (57%)]\tClassification Loss: 1.6120\r\n",
      "Train Epoch: 6 [64000/110534 (58%)]\tClassification Loss: 1.6592\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1000.pth.tar\r\n",
      "Train Epoch: 6 [64640/110534 (58%)]\tClassification Loss: 1.7959\r\n",
      "Train Epoch: 6 [65280/110534 (59%)]\tClassification Loss: 1.7933\r\n",
      "Train Epoch: 6 [65920/110534 (60%)]\tClassification Loss: 1.5471\r\n",
      "Train Epoch: 6 [66560/110534 (60%)]\tClassification Loss: 1.3037\r\n",
      "Train Epoch: 6 [67200/110534 (61%)]\tClassification Loss: 1.7130\r\n",
      "Train Epoch: 6 [67840/110534 (61%)]\tClassification Loss: 1.7074\r\n",
      "Train Epoch: 6 [68480/110534 (62%)]\tClassification Loss: 1.5879\r\n",
      "Train Epoch: 6 [69120/110534 (63%)]\tClassification Loss: 1.3365\r\n",
      "Train Epoch: 6 [69760/110534 (63%)]\tClassification Loss: 1.4187\r\n",
      "Train Epoch: 6 [70400/110534 (64%)]\tClassification Loss: 1.6591\r\n",
      "Train Epoch: 6 [71040/110534 (64%)]\tClassification Loss: 1.7642\r\n",
      "Train Epoch: 6 [71680/110534 (65%)]\tClassification Loss: 1.3967\r\n",
      "Train Epoch: 6 [72320/110534 (65%)]\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 6 [72960/110534 (66%)]\tClassification Loss: 1.4534\r\n",
      "Train Epoch: 6 [73600/110534 (67%)]\tClassification Loss: 1.5582\r\n",
      "Train Epoch: 6 [74240/110534 (67%)]\tClassification Loss: 1.6014\r\n",
      "Train Epoch: 6 [74880/110534 (68%)]\tClassification Loss: 1.4583\r\n",
      "Train Epoch: 6 [75520/110534 (68%)]\tClassification Loss: 1.6514\r\n",
      "Train Epoch: 6 [76160/110534 (69%)]\tClassification Loss: 1.5543\r\n",
      "Train Epoch: 6 [76800/110534 (69%)]\tClassification Loss: 1.8218\r\n",
      "\r\n",
      "Test set: Average loss: 1.4620, Accuracy: 7011/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 6 [77440/110534 (70%)]\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 6 [78080/110534 (71%)]\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 6 [78720/110534 (71%)]\tClassification Loss: 1.5510\r\n",
      "Train Epoch: 6 [79360/110534 (72%)]\tClassification Loss: 1.6546\r\n",
      "Train Epoch: 6 [80000/110534 (72%)]\tClassification Loss: 1.4670\r\n",
      "Train Epoch: 6 [80640/110534 (73%)]\tClassification Loss: 1.3492\r\n",
      "Train Epoch: 6 [81280/110534 (74%)]\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 6 [81920/110534 (74%)]\tClassification Loss: 1.6046\r\n",
      "Train Epoch: 6 [82560/110534 (75%)]\tClassification Loss: 1.8690\r\n",
      "Train Epoch: 6 [83200/110534 (75%)]\tClassification Loss: 1.3085\r\n",
      "Train Epoch: 6 [83840/110534 (76%)]\tClassification Loss: 1.9198\r\n",
      "Train Epoch: 6 [84480/110534 (76%)]\tClassification Loss: 1.4981\r\n",
      "Train Epoch: 6 [85120/110534 (77%)]\tClassification Loss: 1.7192\r\n",
      "Train Epoch: 6 [85760/110534 (78%)]\tClassification Loss: 1.5218\r\n",
      "Train Epoch: 6 [86400/110534 (78%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 6 [87040/110534 (79%)]\tClassification Loss: 1.5246\r\n",
      "Train Epoch: 6 [87680/110534 (79%)]\tClassification Loss: 1.6082\r\n",
      "Train Epoch: 6 [88320/110534 (80%)]\tClassification Loss: 1.3623\r\n",
      "Train Epoch: 6 [88960/110534 (80%)]\tClassification Loss: 1.4255\r\n",
      "Train Epoch: 6 [89600/110534 (81%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 6 [90240/110534 (82%)]\tClassification Loss: 1.7857\r\n",
      "Train Epoch: 6 [90880/110534 (82%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 6 [91520/110534 (83%)]\tClassification Loss: 1.5635\r\n",
      "Train Epoch: 6 [92160/110534 (83%)]\tClassification Loss: 1.4029\r\n",
      "Train Epoch: 6 [92800/110534 (84%)]\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 6 [93440/110534 (85%)]\tClassification Loss: 1.5239\r\n",
      "Train Epoch: 6 [94080/110534 (85%)]\tClassification Loss: 1.4024\r\n",
      "Train Epoch: 6 [94720/110534 (86%)]\tClassification Loss: 1.6939\r\n",
      "Train Epoch: 6 [95360/110534 (86%)]\tClassification Loss: 1.6165\r\n",
      "Train Epoch: 6 [96000/110534 (87%)]\tClassification Loss: 1.8159\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [96640/110534 (87%)]\tClassification Loss: 1.7094\r\n",
      "Train Epoch: 6 [97280/110534 (88%)]\tClassification Loss: 1.5954\r\n",
      "Train Epoch: 6 [97920/110534 (89%)]\tClassification Loss: 1.4320\r\n",
      "Train Epoch: 6 [98560/110534 (89%)]\tClassification Loss: 1.4329\r\n",
      "Train Epoch: 6 [99200/110534 (90%)]\tClassification Loss: 1.9000\r\n",
      "Train Epoch: 6 [99840/110534 (90%)]\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 6 [100480/110534 (91%)]\tClassification Loss: 1.4623\r\n",
      "Train Epoch: 6 [101120/110534 (91%)]\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 6 [101760/110534 (92%)]\tClassification Loss: 1.6044\r\n",
      "Train Epoch: 6 [102400/110534 (93%)]\tClassification Loss: 1.7196\r\n",
      "Train Epoch: 6 [103040/110534 (93%)]\tClassification Loss: 1.5798\r\n",
      "Train Epoch: 6 [103680/110534 (94%)]\tClassification Loss: 1.3640\r\n",
      "Train Epoch: 6 [104320/110534 (94%)]\tClassification Loss: 1.6407\r\n",
      "Train Epoch: 6 [104960/110534 (95%)]\tClassification Loss: 1.5626\r\n",
      "Train Epoch: 6 [105600/110534 (96%)]\tClassification Loss: 1.9055\r\n",
      "Train Epoch: 6 [106240/110534 (96%)]\tClassification Loss: 1.3643\r\n",
      "Train Epoch: 6 [106880/110534 (97%)]\tClassification Loss: 1.6431\r\n",
      "Train Epoch: 6 [107520/110534 (97%)]\tClassification Loss: 1.4159\r\n",
      "Train Epoch: 6 [108160/110534 (98%)]\tClassification Loss: 1.5322\r\n",
      "Train Epoch: 6 [108800/110534 (98%)]\tClassification Loss: 1.3650\r\n",
      "Train Epoch: 6 [109440/110534 (99%)]\tClassification Loss: 1.6030\r\n",
      "Train Epoch: 6 [110080/110534 (100%)]\tClassification Loss: 1.7033\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_final.pth.tar\r\n",
      "Train Epoch: 7 [0/110534 (0%)]\tClassification Loss: 1.6748\r\n",
      "\r\n",
      "Test set: Average loss: 1.4673, Accuracy: 6991/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 7 [640/110534 (1%)]\tClassification Loss: 1.6461\r\n",
      "Train Epoch: 7 [1280/110534 (1%)]\tClassification Loss: 1.5118\r\n",
      "Train Epoch: 7 [1920/110534 (2%)]\tClassification Loss: 1.4461\r\n",
      "Train Epoch: 7 [2560/110534 (2%)]\tClassification Loss: 1.7618\r\n",
      "Train Epoch: 7 [3200/110534 (3%)]\tClassification Loss: 1.4568\r\n",
      "Train Epoch: 7 [3840/110534 (3%)]\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 7 [4480/110534 (4%)]\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 7 [5120/110534 (5%)]\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 7 [5760/110534 (5%)]\tClassification Loss: 1.6239\r\n",
      "Train Epoch: 7 [6400/110534 (6%)]\tClassification Loss: 1.9003\r\n",
      "Train Epoch: 7 [7040/110534 (6%)]\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 7 [7680/110534 (7%)]\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 7 [8320/110534 (8%)]\tClassification Loss: 1.7336\r\n",
      "Train Epoch: 7 [8960/110534 (8%)]\tClassification Loss: 1.5129\r\n",
      "Train Epoch: 7 [9600/110534 (9%)]\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 7 [10240/110534 (9%)]\tClassification Loss: 1.3589\r\n",
      "Train Epoch: 7 [10880/110534 (10%)]\tClassification Loss: 1.6818\r\n",
      "Train Epoch: 7 [11520/110534 (10%)]\tClassification Loss: 1.5336\r\n",
      "Train Epoch: 7 [12160/110534 (11%)]\tClassification Loss: 1.6397\r\n",
      "Train Epoch: 7 [12800/110534 (12%)]\tClassification Loss: 1.6662\r\n",
      "Train Epoch: 7 [13440/110534 (12%)]\tClassification Loss: 1.6064\r\n",
      "Train Epoch: 7 [14080/110534 (13%)]\tClassification Loss: 1.4831\r\n",
      "Train Epoch: 7 [14720/110534 (13%)]\tClassification Loss: 1.6523\r\n",
      "Train Epoch: 7 [15360/110534 (14%)]\tClassification Loss: 1.4534\r\n",
      "Train Epoch: 7 [16000/110534 (14%)]\tClassification Loss: 1.7635\r\n",
      "Train Epoch: 7 [16640/110534 (15%)]\tClassification Loss: 1.5809\r\n",
      "Train Epoch: 7 [17280/110534 (16%)]\tClassification Loss: 1.7837\r\n",
      "Train Epoch: 7 [17920/110534 (16%)]\tClassification Loss: 1.8090\r\n",
      "Train Epoch: 7 [18560/110534 (17%)]\tClassification Loss: 1.6559\r\n",
      "Train Epoch: 7 [19200/110534 (17%)]\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 7 [19840/110534 (18%)]\tClassification Loss: 1.5460\r\n",
      "Train Epoch: 7 [20480/110534 (19%)]\tClassification Loss: 1.6214\r\n",
      "Train Epoch: 7 [21120/110534 (19%)]\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 7 [21760/110534 (20%)]\tClassification Loss: 1.5739\r\n",
      "Train Epoch: 7 [22400/110534 (20%)]\tClassification Loss: 1.7033\r\n",
      "Train Epoch: 7 [23040/110534 (21%)]\tClassification Loss: 1.4828\r\n",
      "Train Epoch: 7 [23680/110534 (21%)]\tClassification Loss: 1.5185\r\n",
      "Train Epoch: 7 [24320/110534 (22%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 7 [24960/110534 (23%)]\tClassification Loss: 1.5880\r\n",
      "Train Epoch: 7 [25600/110534 (23%)]\tClassification Loss: 1.3746\r\n",
      "Train Epoch: 7 [26240/110534 (24%)]\tClassification Loss: 1.6237\r\n",
      "Train Epoch: 7 [26880/110534 (24%)]\tClassification Loss: 1.7049\r\n",
      "Train Epoch: 7 [27520/110534 (25%)]\tClassification Loss: 1.6037\r\n",
      "Train Epoch: 7 [28160/110534 (25%)]\tClassification Loss: 1.6045\r\n",
      "Train Epoch: 7 [28800/110534 (26%)]\tClassification Loss: 1.5182\r\n",
      "Train Epoch: 7 [29440/110534 (27%)]\tClassification Loss: 1.4250\r\n",
      "Train Epoch: 7 [30080/110534 (27%)]\tClassification Loss: 1.6090\r\n",
      "Train Epoch: 7 [30720/110534 (28%)]\tClassification Loss: 1.2842\r\n",
      "Train Epoch: 7 [31360/110534 (28%)]\tClassification Loss: 1.2868\r\n",
      "Train Epoch: 7 [32000/110534 (29%)]\tClassification Loss: 1.6887\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_500.pth.tar\r\n",
      "Train Epoch: 7 [32640/110534 (30%)]\tClassification Loss: 1.5307\r\n",
      "Train Epoch: 7 [33280/110534 (30%)]\tClassification Loss: 1.3444\r\n",
      "Train Epoch: 7 [33920/110534 (31%)]\tClassification Loss: 1.6779\r\n",
      "Train Epoch: 7 [34560/110534 (31%)]\tClassification Loss: 1.5695\r\n",
      "Train Epoch: 7 [35200/110534 (32%)]\tClassification Loss: 1.5678\r\n",
      "Train Epoch: 7 [35840/110534 (32%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 7 [36480/110534 (33%)]\tClassification Loss: 1.4564\r\n",
      "Train Epoch: 7 [37120/110534 (34%)]\tClassification Loss: 1.6683\r\n",
      "Train Epoch: 7 [37760/110534 (34%)]\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 7 [38400/110534 (35%)]\tClassification Loss: 1.7720\r\n",
      "\r\n",
      "Test set: Average loss: 1.4690, Accuracy: 6997/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 7 [39040/110534 (35%)]\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 7 [39680/110534 (36%)]\tClassification Loss: 1.6586\r\n",
      "Train Epoch: 7 [40320/110534 (36%)]\tClassification Loss: 1.7634\r\n",
      "Train Epoch: 7 [40960/110534 (37%)]\tClassification Loss: 1.4705\r\n",
      "Train Epoch: 7 [41600/110534 (38%)]\tClassification Loss: 1.7130\r\n",
      "Train Epoch: 7 [42240/110534 (38%)]\tClassification Loss: 1.6433\r\n",
      "Train Epoch: 7 [42880/110534 (39%)]\tClassification Loss: 1.4067\r\n",
      "Train Epoch: 7 [43520/110534 (39%)]\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 7 [44160/110534 (40%)]\tClassification Loss: 1.6495\r\n",
      "Train Epoch: 7 [44800/110534 (41%)]\tClassification Loss: 1.5704\r\n",
      "Train Epoch: 7 [45440/110534 (41%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 7 [46080/110534 (42%)]\tClassification Loss: 1.6531\r\n",
      "Train Epoch: 7 [46720/110534 (42%)]\tClassification Loss: 1.4017\r\n",
      "Train Epoch: 7 [47360/110534 (43%)]\tClassification Loss: 1.4353\r\n",
      "Train Epoch: 7 [48000/110534 (43%)]\tClassification Loss: 1.6372\r\n",
      "Train Epoch: 7 [48640/110534 (44%)]\tClassification Loss: 1.6945\r\n",
      "Train Epoch: 7 [49280/110534 (45%)]\tClassification Loss: 1.5239\r\n",
      "Train Epoch: 7 [49920/110534 (45%)]\tClassification Loss: 1.3616\r\n",
      "Train Epoch: 7 [50560/110534 (46%)]\tClassification Loss: 1.9200\r\n",
      "Train Epoch: 7 [51200/110534 (46%)]\tClassification Loss: 1.6332\r\n",
      "Train Epoch: 7 [51840/110534 (47%)]\tClassification Loss: 1.6388\r\n",
      "Train Epoch: 7 [52480/110534 (47%)]\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 7 [53120/110534 (48%)]\tClassification Loss: 1.5807\r\n",
      "Train Epoch: 7 [53760/110534 (49%)]\tClassification Loss: 1.6973\r\n",
      "Train Epoch: 7 [54400/110534 (49%)]\tClassification Loss: 1.2776\r\n",
      "Train Epoch: 7 [55040/110534 (50%)]\tClassification Loss: 1.2367\r\n",
      "Train Epoch: 7 [55680/110534 (50%)]\tClassification Loss: 1.6729\r\n",
      "Train Epoch: 7 [56320/110534 (51%)]\tClassification Loss: 1.7257\r\n",
      "Train Epoch: 7 [56960/110534 (52%)]\tClassification Loss: 1.8682\r\n",
      "Train Epoch: 7 [57600/110534 (52%)]\tClassification Loss: 1.4175\r\n",
      "Train Epoch: 7 [58240/110534 (53%)]\tClassification Loss: 1.4520\r\n",
      "Train Epoch: 7 [58880/110534 (53%)]\tClassification Loss: 1.4807\r\n",
      "Train Epoch: 7 [59520/110534 (54%)]\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 7 [60160/110534 (54%)]\tClassification Loss: 1.5035\r\n",
      "Train Epoch: 7 [60800/110534 (55%)]\tClassification Loss: 1.6491\r\n",
      "Train Epoch: 7 [61440/110534 (56%)]\tClassification Loss: 1.7976\r\n",
      "Train Epoch: 7 [62080/110534 (56%)]\tClassification Loss: 1.5857\r\n",
      "Train Epoch: 7 [62720/110534 (57%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 7 [63360/110534 (57%)]\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 7 [64000/110534 (58%)]\tClassification Loss: 1.7207\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1000.pth.tar\r\n",
      "Train Epoch: 7 [64640/110534 (58%)]\tClassification Loss: 1.7158\r\n",
      "Train Epoch: 7 [65280/110534 (59%)]\tClassification Loss: 1.7965\r\n",
      "Train Epoch: 7 [65920/110534 (60%)]\tClassification Loss: 1.3495\r\n",
      "Train Epoch: 7 [66560/110534 (60%)]\tClassification Loss: 1.2983\r\n",
      "Train Epoch: 7 [67200/110534 (61%)]\tClassification Loss: 1.5057\r\n",
      "Train Epoch: 7 [67840/110534 (61%)]\tClassification Loss: 1.6179\r\n",
      "Train Epoch: 7 [68480/110534 (62%)]\tClassification Loss: 1.6762\r\n",
      "Train Epoch: 7 [69120/110534 (63%)]\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 7 [69760/110534 (63%)]\tClassification Loss: 1.3647\r\n",
      "Train Epoch: 7 [70400/110534 (64%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 7 [71040/110534 (64%)]\tClassification Loss: 1.7236\r\n",
      "Train Epoch: 7 [71680/110534 (65%)]\tClassification Loss: 1.3123\r\n",
      "Train Epoch: 7 [72320/110534 (65%)]\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 7 [72960/110534 (66%)]\tClassification Loss: 1.5025\r\n",
      "Train Epoch: 7 [73600/110534 (67%)]\tClassification Loss: 1.6032\r\n",
      "Train Epoch: 7 [74240/110534 (67%)]\tClassification Loss: 1.4350\r\n",
      "Train Epoch: 7 [74880/110534 (68%)]\tClassification Loss: 1.2780\r\n",
      "Train Epoch: 7 [75520/110534 (68%)]\tClassification Loss: 1.8385\r\n",
      "Train Epoch: 7 [76160/110534 (69%)]\tClassification Loss: 1.6491\r\n",
      "Train Epoch: 7 [76800/110534 (69%)]\tClassification Loss: 1.9159\r\n",
      "\r\n",
      "Test set: Average loss: 1.4547, Accuracy: 7015/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 7 [77440/110534 (70%)]\tClassification Loss: 1.5074\r\n",
      "Train Epoch: 7 [78080/110534 (71%)]\tClassification Loss: 1.4986\r\n",
      "Train Epoch: 7 [78720/110534 (71%)]\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 7 [79360/110534 (72%)]\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 7 [80000/110534 (72%)]\tClassification Loss: 1.4446\r\n",
      "Train Epoch: 7 [80640/110534 (73%)]\tClassification Loss: 1.5711\r\n",
      "Train Epoch: 7 [81280/110534 (74%)]\tClassification Loss: 1.3122\r\n",
      "Train Epoch: 7 [81920/110534 (74%)]\tClassification Loss: 1.5197\r\n",
      "Train Epoch: 7 [82560/110534 (75%)]\tClassification Loss: 1.7271\r\n",
      "Train Epoch: 7 [83200/110534 (75%)]\tClassification Loss: 1.3858\r\n",
      "Train Epoch: 7 [83840/110534 (76%)]\tClassification Loss: 1.5994\r\n",
      "Train Epoch: 7 [84480/110534 (76%)]\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 7 [85120/110534 (77%)]\tClassification Loss: 1.7128\r\n",
      "Train Epoch: 7 [85760/110534 (78%)]\tClassification Loss: 1.4913\r\n",
      "Train Epoch: 7 [86400/110534 (78%)]\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 7 [87040/110534 (79%)]\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 7 [87680/110534 (79%)]\tClassification Loss: 1.6712\r\n",
      "Train Epoch: 7 [88320/110534 (80%)]\tClassification Loss: 1.4381\r\n",
      "Train Epoch: 7 [88960/110534 (80%)]\tClassification Loss: 1.3988\r\n",
      "Train Epoch: 7 [89600/110534 (81%)]\tClassification Loss: 1.4181\r\n",
      "Train Epoch: 7 [90240/110534 (82%)]\tClassification Loss: 1.7629\r\n",
      "Train Epoch: 7 [90880/110534 (82%)]\tClassification Loss: 1.4805\r\n",
      "Train Epoch: 7 [91520/110534 (83%)]\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 7 [92160/110534 (83%)]\tClassification Loss: 1.4236\r\n",
      "Train Epoch: 7 [92800/110534 (84%)]\tClassification Loss: 1.5111\r\n",
      "Train Epoch: 7 [93440/110534 (85%)]\tClassification Loss: 1.4292\r\n",
      "Train Epoch: 7 [94080/110534 (85%)]\tClassification Loss: 1.3894\r\n",
      "Train Epoch: 7 [94720/110534 (86%)]\tClassification Loss: 1.6638\r\n",
      "Train Epoch: 7 [95360/110534 (86%)]\tClassification Loss: 1.4551\r\n",
      "Train Epoch: 7 [96000/110534 (87%)]\tClassification Loss: 1.8263\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1500.pth.tar\r\n",
      "Train Epoch: 7 [96640/110534 (87%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 7 [97280/110534 (88%)]\tClassification Loss: 1.6096\r\n",
      "Train Epoch: 7 [97920/110534 (89%)]\tClassification Loss: 1.4781\r\n",
      "Train Epoch: 7 [98560/110534 (89%)]\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 7 [99200/110534 (90%)]\tClassification Loss: 2.1465\r\n",
      "Train Epoch: 7 [99840/110534 (90%)]\tClassification Loss: 1.6105\r\n",
      "Train Epoch: 7 [100480/110534 (91%)]\tClassification Loss: 1.5315\r\n",
      "Train Epoch: 7 [101120/110534 (91%)]\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 7 [101760/110534 (92%)]\tClassification Loss: 1.7134\r\n",
      "Train Epoch: 7 [102400/110534 (93%)]\tClassification Loss: 1.6708\r\n",
      "Train Epoch: 7 [103040/110534 (93%)]\tClassification Loss: 1.6189\r\n",
      "Train Epoch: 7 [103680/110534 (94%)]\tClassification Loss: 1.5079\r\n",
      "Train Epoch: 7 [104320/110534 (94%)]\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 7 [104960/110534 (95%)]\tClassification Loss: 1.5986\r\n",
      "Train Epoch: 7 [105600/110534 (96%)]\tClassification Loss: 1.8200\r\n",
      "Train Epoch: 7 [106240/110534 (96%)]\tClassification Loss: 1.3021\r\n",
      "Train Epoch: 7 [106880/110534 (97%)]\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 7 [107520/110534 (97%)]\tClassification Loss: 1.4668\r\n",
      "Train Epoch: 7 [108160/110534 (98%)]\tClassification Loss: 1.3624\r\n",
      "Train Epoch: 7 [108800/110534 (98%)]\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 7 [109440/110534 (99%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 7 [110080/110534 (100%)]\tClassification Loss: 1.7083\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_final.pth.tar\r\n",
      "Train Epoch: 8 [0/110534 (0%)]\tClassification Loss: 1.8230\r\n",
      "\r\n",
      "Test set: Average loss: 1.4579, Accuracy: 7019/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 8 [640/110534 (1%)]\tClassification Loss: 1.5144\r\n",
      "Train Epoch: 8 [1280/110534 (1%)]\tClassification Loss: 1.4324\r\n",
      "Train Epoch: 8 [1920/110534 (2%)]\tClassification Loss: 1.5313\r\n",
      "Train Epoch: 8 [2560/110534 (2%)]\tClassification Loss: 1.7101\r\n",
      "Train Epoch: 8 [3200/110534 (3%)]\tClassification Loss: 1.4841\r\n",
      "Train Epoch: 8 [3840/110534 (3%)]\tClassification Loss: 1.5110\r\n",
      "Train Epoch: 8 [4480/110534 (4%)]\tClassification Loss: 1.4099\r\n",
      "Train Epoch: 8 [5120/110534 (5%)]\tClassification Loss: 1.6157\r\n",
      "Train Epoch: 8 [5760/110534 (5%)]\tClassification Loss: 1.6311\r\n",
      "Train Epoch: 8 [6400/110534 (6%)]\tClassification Loss: 1.9560\r\n",
      "Train Epoch: 8 [7040/110534 (6%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 8 [7680/110534 (7%)]\tClassification Loss: 1.4288\r\n",
      "Train Epoch: 8 [8320/110534 (8%)]\tClassification Loss: 1.7732\r\n",
      "Train Epoch: 8 [8960/110534 (8%)]\tClassification Loss: 1.5443\r\n",
      "Train Epoch: 8 [9600/110534 (9%)]\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 8 [10240/110534 (9%)]\tClassification Loss: 1.4038\r\n",
      "Train Epoch: 8 [10880/110534 (10%)]\tClassification Loss: 1.6187\r\n",
      "Train Epoch: 8 [11520/110534 (10%)]\tClassification Loss: 1.5786\r\n",
      "Train Epoch: 8 [12160/110534 (11%)]\tClassification Loss: 1.5673\r\n",
      "Train Epoch: 8 [12800/110534 (12%)]\tClassification Loss: 1.7059\r\n",
      "Train Epoch: 8 [13440/110534 (12%)]\tClassification Loss: 1.5752\r\n",
      "Train Epoch: 8 [14080/110534 (13%)]\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 8 [14720/110534 (13%)]\tClassification Loss: 1.6804\r\n",
      "Train Epoch: 8 [15360/110534 (14%)]\tClassification Loss: 1.6648\r\n",
      "Train Epoch: 8 [16000/110534 (14%)]\tClassification Loss: 1.7602\r\n",
      "Train Epoch: 8 [16640/110534 (15%)]\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 8 [17280/110534 (16%)]\tClassification Loss: 1.7684\r\n",
      "Train Epoch: 8 [17920/110534 (16%)]\tClassification Loss: 1.6957\r\n",
      "Train Epoch: 8 [18560/110534 (17%)]\tClassification Loss: 1.6061\r\n",
      "Train Epoch: 8 [19200/110534 (17%)]\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 8 [19840/110534 (18%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 8 [20480/110534 (19%)]\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 8 [21120/110534 (19%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 8 [21760/110534 (20%)]\tClassification Loss: 1.4165\r\n",
      "Train Epoch: 8 [22400/110534 (20%)]\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 8 [23040/110534 (21%)]\tClassification Loss: 1.5469\r\n",
      "Train Epoch: 8 [23680/110534 (21%)]\tClassification Loss: 1.5000\r\n",
      "Train Epoch: 8 [24320/110534 (22%)]\tClassification Loss: 1.3759\r\n",
      "Train Epoch: 8 [24960/110534 (23%)]\tClassification Loss: 1.3818\r\n",
      "Train Epoch: 8 [25600/110534 (23%)]\tClassification Loss: 1.3070\r\n",
      "Train Epoch: 8 [26240/110534 (24%)]\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 8 [26880/110534 (24%)]\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 8 [27520/110534 (25%)]\tClassification Loss: 1.6461\r\n",
      "Train Epoch: 8 [28160/110534 (25%)]\tClassification Loss: 1.6253\r\n",
      "Train Epoch: 8 [28800/110534 (26%)]\tClassification Loss: 1.6522\r\n",
      "Train Epoch: 8 [29440/110534 (27%)]\tClassification Loss: 1.5410\r\n",
      "Train Epoch: 8 [30080/110534 (27%)]\tClassification Loss: 1.8457\r\n",
      "Train Epoch: 8 [30720/110534 (28%)]\tClassification Loss: 1.5112\r\n",
      "Train Epoch: 8 [31360/110534 (28%)]\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 8 [32000/110534 (29%)]\tClassification Loss: 1.5590\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_500.pth.tar\r\n",
      "Train Epoch: 8 [32640/110534 (30%)]\tClassification Loss: 1.3962\r\n",
      "Train Epoch: 8 [33280/110534 (30%)]\tClassification Loss: 1.3199\r\n",
      "Train Epoch: 8 [33920/110534 (31%)]\tClassification Loss: 1.4568\r\n",
      "Train Epoch: 8 [34560/110534 (31%)]\tClassification Loss: 1.7216\r\n",
      "Train Epoch: 8 [35200/110534 (32%)]\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 8 [35840/110534 (32%)]\tClassification Loss: 1.5338\r\n",
      "Train Epoch: 8 [36480/110534 (33%)]\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 8 [37120/110534 (34%)]\tClassification Loss: 1.7208\r\n",
      "Train Epoch: 8 [37760/110534 (34%)]\tClassification Loss: 1.4489\r\n",
      "Train Epoch: 8 [38400/110534 (35%)]\tClassification Loss: 1.7572\r\n",
      "\r\n",
      "Test set: Average loss: 1.4646, Accuracy: 6984/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 8 [39040/110534 (35%)]\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 8 [39680/110534 (36%)]\tClassification Loss: 1.6158\r\n",
      "Train Epoch: 8 [40320/110534 (36%)]\tClassification Loss: 1.8435\r\n",
      "Train Epoch: 8 [40960/110534 (37%)]\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 8 [41600/110534 (38%)]\tClassification Loss: 1.6588\r\n",
      "Train Epoch: 8 [42240/110534 (38%)]\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 8 [42880/110534 (39%)]\tClassification Loss: 1.5171\r\n",
      "Train Epoch: 8 [43520/110534 (39%)]\tClassification Loss: 1.6952\r\n",
      "Train Epoch: 8 [44160/110534 (40%)]\tClassification Loss: 1.6584\r\n",
      "Train Epoch: 8 [44800/110534 (41%)]\tClassification Loss: 1.6942\r\n",
      "Train Epoch: 8 [45440/110534 (41%)]\tClassification Loss: 1.3521\r\n",
      "Train Epoch: 8 [46080/110534 (42%)]\tClassification Loss: 1.5165\r\n",
      "Train Epoch: 8 [46720/110534 (42%)]\tClassification Loss: 1.3740\r\n",
      "Train Epoch: 8 [47360/110534 (43%)]\tClassification Loss: 1.3272\r\n",
      "Train Epoch: 8 [48000/110534 (43%)]\tClassification Loss: 1.8531\r\n",
      "Train Epoch: 8 [48640/110534 (44%)]\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 8 [49280/110534 (45%)]\tClassification Loss: 1.3975\r\n",
      "Train Epoch: 8 [49920/110534 (45%)]\tClassification Loss: 1.4118\r\n",
      "Train Epoch: 8 [50560/110534 (46%)]\tClassification Loss: 1.8047\r\n",
      "Train Epoch: 8 [51200/110534 (46%)]\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 8 [51840/110534 (47%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 8 [52480/110534 (47%)]\tClassification Loss: 1.6017\r\n",
      "Train Epoch: 8 [53120/110534 (48%)]\tClassification Loss: 1.5661\r\n",
      "Train Epoch: 8 [53760/110534 (49%)]\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 8 [54400/110534 (49%)]\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 8 [55040/110534 (50%)]\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 8 [55680/110534 (50%)]\tClassification Loss: 1.9180\r\n",
      "Train Epoch: 8 [56320/110534 (51%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 8 [56960/110534 (52%)]\tClassification Loss: 2.0038\r\n",
      "Train Epoch: 8 [57600/110534 (52%)]\tClassification Loss: 1.5072\r\n",
      "Train Epoch: 8 [58240/110534 (53%)]\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 8 [58880/110534 (53%)]\tClassification Loss: 1.5056\r\n",
      "Train Epoch: 8 [59520/110534 (54%)]\tClassification Loss: 1.4718\r\n",
      "Train Epoch: 8 [60160/110534 (54%)]\tClassification Loss: 1.3991\r\n",
      "Train Epoch: 8 [60800/110534 (55%)]\tClassification Loss: 1.5494\r\n",
      "Train Epoch: 8 [61440/110534 (56%)]\tClassification Loss: 1.8312\r\n",
      "Train Epoch: 8 [62080/110534 (56%)]\tClassification Loss: 1.7401\r\n",
      "Train Epoch: 8 [62720/110534 (57%)]\tClassification Loss: 1.7054\r\n",
      "Train Epoch: 8 [63360/110534 (57%)]\tClassification Loss: 1.6217\r\n",
      "Train Epoch: 8 [64000/110534 (58%)]\tClassification Loss: 1.7110\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1000.pth.tar\r\n",
      "Train Epoch: 8 [64640/110534 (58%)]\tClassification Loss: 1.8269\r\n",
      "Train Epoch: 8 [65280/110534 (59%)]\tClassification Loss: 1.6911\r\n",
      "Train Epoch: 8 [65920/110534 (60%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 8 [66560/110534 (60%)]\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 8 [67200/110534 (61%)]\tClassification Loss: 1.6316\r\n",
      "Train Epoch: 8 [67840/110534 (61%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 8 [68480/110534 (62%)]\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 8 [69120/110534 (63%)]\tClassification Loss: 1.5240\r\n",
      "Train Epoch: 8 [69760/110534 (63%)]\tClassification Loss: 1.3741\r\n",
      "Train Epoch: 8 [70400/110534 (64%)]\tClassification Loss: 1.9562\r\n",
      "Train Epoch: 8 [71040/110534 (64%)]\tClassification Loss: 1.5452\r\n",
      "Train Epoch: 8 [71680/110534 (65%)]\tClassification Loss: 1.3190\r\n",
      "Train Epoch: 8 [72320/110534 (65%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 8 [72960/110534 (66%)]\tClassification Loss: 1.3251\r\n",
      "Train Epoch: 8 [73600/110534 (67%)]\tClassification Loss: 1.6520\r\n",
      "Train Epoch: 8 [74240/110534 (67%)]\tClassification Loss: 1.5540\r\n",
      "Train Epoch: 8 [74880/110534 (68%)]\tClassification Loss: 1.4389\r\n",
      "Train Epoch: 8 [75520/110534 (68%)]\tClassification Loss: 1.7091\r\n",
      "Train Epoch: 8 [76160/110534 (69%)]\tClassification Loss: 1.6280\r\n",
      "Train Epoch: 8 [76800/110534 (69%)]\tClassification Loss: 1.7427\r\n",
      "\r\n",
      "Test set: Average loss: 1.4520, Accuracy: 7008/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 8 [77440/110534 (70%)]\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 8 [78080/110534 (71%)]\tClassification Loss: 1.4150\r\n",
      "Train Epoch: 8 [78720/110534 (71%)]\tClassification Loss: 1.6228\r\n",
      "Train Epoch: 8 [79360/110534 (72%)]\tClassification Loss: 1.7690\r\n",
      "Train Epoch: 8 [80000/110534 (72%)]\tClassification Loss: 1.4207\r\n",
      "Train Epoch: 8 [80640/110534 (73%)]\tClassification Loss: 1.4807\r\n",
      "Train Epoch: 8 [81280/110534 (74%)]\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 8 [81920/110534 (74%)]\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 8 [82560/110534 (75%)]\tClassification Loss: 1.8729\r\n",
      "Train Epoch: 8 [83200/110534 (75%)]\tClassification Loss: 1.3282\r\n",
      "Train Epoch: 8 [83840/110534 (76%)]\tClassification Loss: 1.7006\r\n",
      "Train Epoch: 8 [84480/110534 (76%)]\tClassification Loss: 1.3970\r\n",
      "Train Epoch: 8 [85120/110534 (77%)]\tClassification Loss: 1.8213\r\n",
      "Train Epoch: 8 [85760/110534 (78%)]\tClassification Loss: 1.6278\r\n",
      "Train Epoch: 8 [86400/110534 (78%)]\tClassification Loss: 1.4571\r\n",
      "Train Epoch: 8 [87040/110534 (79%)]\tClassification Loss: 1.3320\r\n",
      "Train Epoch: 8 [87680/110534 (79%)]\tClassification Loss: 1.7290\r\n",
      "Train Epoch: 8 [88320/110534 (80%)]\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 8 [88960/110534 (80%)]\tClassification Loss: 1.3517\r\n",
      "Train Epoch: 8 [89600/110534 (81%)]\tClassification Loss: 1.5612\r\n",
      "Train Epoch: 8 [90240/110534 (82%)]\tClassification Loss: 1.7838\r\n",
      "Train Epoch: 8 [90880/110534 (82%)]\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 8 [91520/110534 (83%)]\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 8 [92160/110534 (83%)]\tClassification Loss: 1.4642\r\n",
      "Train Epoch: 8 [92800/110534 (84%)]\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 8 [93440/110534 (85%)]\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 8 [94080/110534 (85%)]\tClassification Loss: 1.3537\r\n",
      "Train Epoch: 8 [94720/110534 (86%)]\tClassification Loss: 1.6875\r\n",
      "Train Epoch: 8 [95360/110534 (86%)]\tClassification Loss: 1.3775\r\n",
      "Train Epoch: 8 [96000/110534 (87%)]\tClassification Loss: 1.7956\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1500.pth.tar\r\n",
      "Train Epoch: 8 [96640/110534 (87%)]\tClassification Loss: 1.5365\r\n",
      "Train Epoch: 8 [97280/110534 (88%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 8 [97920/110534 (89%)]\tClassification Loss: 1.4269\r\n",
      "Train Epoch: 8 [98560/110534 (89%)]\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 8 [99200/110534 (90%)]\tClassification Loss: 1.8420\r\n",
      "Train Epoch: 8 [99840/110534 (90%)]\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 8 [100480/110534 (91%)]\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 8 [101120/110534 (91%)]\tClassification Loss: 1.4830\r\n",
      "Train Epoch: 8 [101760/110534 (92%)]\tClassification Loss: 1.7547\r\n",
      "Train Epoch: 8 [102400/110534 (93%)]\tClassification Loss: 1.7420\r\n",
      "Train Epoch: 8 [103040/110534 (93%)]\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 8 [103680/110534 (94%)]\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 8 [104320/110534 (94%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 8 [104960/110534 (95%)]\tClassification Loss: 1.6054\r\n",
      "Train Epoch: 8 [105600/110534 (96%)]\tClassification Loss: 1.8082\r\n",
      "Train Epoch: 8 [106240/110534 (96%)]\tClassification Loss: 1.3315\r\n",
      "Train Epoch: 8 [106880/110534 (97%)]\tClassification Loss: 1.6067\r\n",
      "Train Epoch: 8 [107520/110534 (97%)]\tClassification Loss: 1.5660\r\n",
      "Train Epoch: 8 [108160/110534 (98%)]\tClassification Loss: 1.2918\r\n",
      "Train Epoch: 8 [108800/110534 (98%)]\tClassification Loss: 1.4409\r\n",
      "Train Epoch: 8 [109440/110534 (99%)]\tClassification Loss: 1.6022\r\n",
      "Train Epoch: 8 [110080/110534 (100%)]\tClassification Loss: 1.6211\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_final.pth.tar\r\n",
      "Train Epoch: 9 [0/110534 (0%)]\tClassification Loss: 1.8176\r\n",
      "\r\n",
      "Test set: Average loss: 1.4579, Accuracy: 6999/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 9 [640/110534 (1%)]\tClassification Loss: 1.6426\r\n",
      "Train Epoch: 9 [1280/110534 (1%)]\tClassification Loss: 1.4364\r\n",
      "Train Epoch: 9 [1920/110534 (2%)]\tClassification Loss: 1.4120\r\n",
      "Train Epoch: 9 [2560/110534 (2%)]\tClassification Loss: 1.7332\r\n",
      "Train Epoch: 9 [3200/110534 (3%)]\tClassification Loss: 1.4926\r\n",
      "Train Epoch: 9 [3840/110534 (3%)]\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 9 [4480/110534 (4%)]\tClassification Loss: 1.3855\r\n",
      "Train Epoch: 9 [5120/110534 (5%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 9 [5760/110534 (5%)]\tClassification Loss: 1.6157\r\n",
      "Train Epoch: 9 [6400/110534 (6%)]\tClassification Loss: 1.7778\r\n",
      "Train Epoch: 9 [7040/110534 (6%)]\tClassification Loss: 1.8447\r\n",
      "Train Epoch: 9 [7680/110534 (7%)]\tClassification Loss: 1.3151\r\n",
      "Train Epoch: 9 [8320/110534 (8%)]\tClassification Loss: 1.7440\r\n",
      "Train Epoch: 9 [8960/110534 (8%)]\tClassification Loss: 1.4514\r\n",
      "Train Epoch: 9 [9600/110534 (9%)]\tClassification Loss: 1.4763\r\n",
      "Train Epoch: 9 [10240/110534 (9%)]\tClassification Loss: 1.3171\r\n",
      "Train Epoch: 9 [10880/110534 (10%)]\tClassification Loss: 1.6294\r\n",
      "Train Epoch: 9 [11520/110534 (10%)]\tClassification Loss: 1.4179\r\n",
      "Train Epoch: 9 [12160/110534 (11%)]\tClassification Loss: 1.4411\r\n",
      "Train Epoch: 9 [12800/110534 (12%)]\tClassification Loss: 1.7229\r\n",
      "Train Epoch: 9 [13440/110534 (12%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 9 [14080/110534 (13%)]\tClassification Loss: 1.5619\r\n",
      "Train Epoch: 9 [14720/110534 (13%)]\tClassification Loss: 1.5755\r\n",
      "Train Epoch: 9 [15360/110534 (14%)]\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 9 [16000/110534 (14%)]\tClassification Loss: 1.6326\r\n",
      "Train Epoch: 9 [16640/110534 (15%)]\tClassification Loss: 1.6378\r\n",
      "Train Epoch: 9 [17280/110534 (16%)]\tClassification Loss: 1.6761\r\n",
      "Train Epoch: 9 [17920/110534 (16%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 9 [18560/110534 (17%)]\tClassification Loss: 1.6040\r\n",
      "Train Epoch: 9 [19200/110534 (17%)]\tClassification Loss: 1.4844\r\n",
      "Train Epoch: 9 [19840/110534 (18%)]\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 9 [20480/110534 (19%)]\tClassification Loss: 1.6679\r\n",
      "Train Epoch: 9 [21120/110534 (19%)]\tClassification Loss: 1.5435\r\n",
      "Train Epoch: 9 [21760/110534 (20%)]\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 9 [22400/110534 (20%)]\tClassification Loss: 1.6081\r\n",
      "Train Epoch: 9 [23040/110534 (21%)]\tClassification Loss: 1.4467\r\n",
      "Train Epoch: 9 [23680/110534 (21%)]\tClassification Loss: 1.5269\r\n",
      "Train Epoch: 9 [24320/110534 (22%)]\tClassification Loss: 1.5922\r\n",
      "Train Epoch: 9 [24960/110534 (23%)]\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 9 [25600/110534 (23%)]\tClassification Loss: 1.1917\r\n",
      "Train Epoch: 9 [26240/110534 (24%)]\tClassification Loss: 1.6620\r\n",
      "Train Epoch: 9 [26880/110534 (24%)]\tClassification Loss: 1.5774\r\n",
      "Train Epoch: 9 [27520/110534 (25%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 9 [28160/110534 (25%)]\tClassification Loss: 1.6596\r\n",
      "Train Epoch: 9 [28800/110534 (26%)]\tClassification Loss: 1.7004\r\n",
      "Train Epoch: 9 [29440/110534 (27%)]\tClassification Loss: 1.5957\r\n",
      "Train Epoch: 9 [30080/110534 (27%)]\tClassification Loss: 1.5791\r\n",
      "Train Epoch: 9 [30720/110534 (28%)]\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 9 [31360/110534 (28%)]\tClassification Loss: 1.2096\r\n",
      "Train Epoch: 9 [32000/110534 (29%)]\tClassification Loss: 1.5203\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_500.pth.tar\r\n",
      "Train Epoch: 9 [32640/110534 (30%)]\tClassification Loss: 1.4761\r\n",
      "Train Epoch: 9 [33280/110534 (30%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 9 [33920/110534 (31%)]\tClassification Loss: 1.6964\r\n",
      "Train Epoch: 9 [34560/110534 (31%)]\tClassification Loss: 1.7061\r\n",
      "Train Epoch: 9 [35200/110534 (32%)]\tClassification Loss: 1.5502\r\n",
      "Train Epoch: 9 [35840/110534 (32%)]\tClassification Loss: 1.4766\r\n",
      "Train Epoch: 9 [36480/110534 (33%)]\tClassification Loss: 1.4140\r\n",
      "Train Epoch: 9 [37120/110534 (34%)]\tClassification Loss: 1.7458\r\n",
      "Train Epoch: 9 [37760/110534 (34%)]\tClassification Loss: 1.3217\r\n",
      "Train Epoch: 9 [38400/110534 (35%)]\tClassification Loss: 1.7873\r\n",
      "\r\n",
      "Test set: Average loss: 1.4639, Accuracy: 6970/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 9 [39040/110534 (35%)]\tClassification Loss: 1.3831\r\n",
      "Train Epoch: 9 [39680/110534 (36%)]\tClassification Loss: 1.7683\r\n",
      "Train Epoch: 9 [40320/110534 (36%)]\tClassification Loss: 1.8625\r\n",
      "Train Epoch: 9 [40960/110534 (37%)]\tClassification Loss: 1.3461\r\n",
      "Train Epoch: 9 [41600/110534 (38%)]\tClassification Loss: 1.8129\r\n",
      "Train Epoch: 9 [42240/110534 (38%)]\tClassification Loss: 1.6612\r\n",
      "Train Epoch: 9 [42880/110534 (39%)]\tClassification Loss: 1.5515\r\n",
      "Train Epoch: 9 [43520/110534 (39%)]\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 9 [44160/110534 (40%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 9 [44800/110534 (41%)]\tClassification Loss: 1.8187\r\n",
      "Train Epoch: 9 [45440/110534 (41%)]\tClassification Loss: 1.2124\r\n",
      "Train Epoch: 9 [46080/110534 (42%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 9 [46720/110534 (42%)]\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 9 [47360/110534 (43%)]\tClassification Loss: 1.3945\r\n",
      "Train Epoch: 9 [48000/110534 (43%)]\tClassification Loss: 1.7152\r\n",
      "Train Epoch: 9 [48640/110534 (44%)]\tClassification Loss: 1.7390\r\n",
      "Train Epoch: 9 [49280/110534 (45%)]\tClassification Loss: 1.4310\r\n",
      "Train Epoch: 9 [49920/110534 (45%)]\tClassification Loss: 1.4244\r\n",
      "Train Epoch: 9 [50560/110534 (46%)]\tClassification Loss: 1.6604\r\n",
      "Train Epoch: 9 [51200/110534 (46%)]\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 9 [51840/110534 (47%)]\tClassification Loss: 1.5151\r\n",
      "Train Epoch: 9 [52480/110534 (47%)]\tClassification Loss: 1.4539\r\n",
      "Train Epoch: 9 [53120/110534 (48%)]\tClassification Loss: 1.5802\r\n",
      "Train Epoch: 9 [53760/110534 (49%)]\tClassification Loss: 1.4901\r\n",
      "Train Epoch: 9 [54400/110534 (49%)]\tClassification Loss: 1.3666\r\n",
      "Train Epoch: 9 [55040/110534 (50%)]\tClassification Loss: 1.4149\r\n",
      "Train Epoch: 9 [55680/110534 (50%)]\tClassification Loss: 1.6583\r\n",
      "Train Epoch: 9 [56320/110534 (51%)]\tClassification Loss: 1.7129\r\n",
      "Train Epoch: 9 [56960/110534 (52%)]\tClassification Loss: 1.8450\r\n",
      "Train Epoch: 9 [57600/110534 (52%)]\tClassification Loss: 1.4951\r\n",
      "Train Epoch: 9 [58240/110534 (53%)]\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 9 [58880/110534 (53%)]\tClassification Loss: 1.4322\r\n",
      "Train Epoch: 9 [59520/110534 (54%)]\tClassification Loss: 1.5967\r\n",
      "Train Epoch: 9 [60160/110534 (54%)]\tClassification Loss: 1.5644\r\n",
      "Train Epoch: 9 [60800/110534 (55%)]\tClassification Loss: 1.3908\r\n",
      "Train Epoch: 9 [61440/110534 (56%)]\tClassification Loss: 1.6799\r\n",
      "Train Epoch: 9 [62080/110534 (56%)]\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 9 [62720/110534 (57%)]\tClassification Loss: 1.5711\r\n",
      "Train Epoch: 9 [63360/110534 (57%)]\tClassification Loss: 1.5580\r\n",
      "Train Epoch: 9 [64000/110534 (58%)]\tClassification Loss: 1.6762\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1000.pth.tar\r\n",
      "Train Epoch: 9 [64640/110534 (58%)]\tClassification Loss: 1.7605\r\n",
      "Train Epoch: 9 [65280/110534 (59%)]\tClassification Loss: 1.8020\r\n",
      "Train Epoch: 9 [65920/110534 (60%)]\tClassification Loss: 1.3324\r\n",
      "Train Epoch: 9 [66560/110534 (60%)]\tClassification Loss: 1.4117\r\n",
      "Train Epoch: 9 [67200/110534 (61%)]\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 9 [67840/110534 (61%)]\tClassification Loss: 1.6085\r\n",
      "Train Epoch: 9 [68480/110534 (62%)]\tClassification Loss: 1.5381\r\n",
      "Train Epoch: 9 [69120/110534 (63%)]\tClassification Loss: 1.6494\r\n",
      "Train Epoch: 9 [69760/110534 (63%)]\tClassification Loss: 1.3859\r\n",
      "Train Epoch: 9 [70400/110534 (64%)]\tClassification Loss: 1.6929\r\n",
      "Train Epoch: 9 [71040/110534 (64%)]\tClassification Loss: 1.7566\r\n",
      "Train Epoch: 9 [71680/110534 (65%)]\tClassification Loss: 1.3039\r\n",
      "Train Epoch: 9 [72320/110534 (65%)]\tClassification Loss: 1.5422\r\n",
      "Train Epoch: 9 [72960/110534 (66%)]\tClassification Loss: 1.4325\r\n",
      "Train Epoch: 9 [73600/110534 (67%)]\tClassification Loss: 1.7109\r\n",
      "Train Epoch: 9 [74240/110534 (67%)]\tClassification Loss: 1.4509\r\n",
      "Train Epoch: 9 [74880/110534 (68%)]\tClassification Loss: 1.5008\r\n",
      "Train Epoch: 9 [75520/110534 (68%)]\tClassification Loss: 1.6734\r\n",
      "Train Epoch: 9 [76160/110534 (69%)]\tClassification Loss: 1.6051\r\n",
      "Train Epoch: 9 [76800/110534 (69%)]\tClassification Loss: 1.9993\r\n",
      "\r\n",
      "Test set: Average loss: 1.4499, Accuracy: 6993/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 9 [77440/110534 (70%)]\tClassification Loss: 1.6266\r\n",
      "Train Epoch: 9 [78080/110534 (71%)]\tClassification Loss: 1.4655\r\n",
      "Train Epoch: 9 [78720/110534 (71%)]\tClassification Loss: 1.6120\r\n",
      "Train Epoch: 9 [79360/110534 (72%)]\tClassification Loss: 1.8995\r\n",
      "Train Epoch: 9 [80000/110534 (72%)]\tClassification Loss: 1.3499\r\n",
      "Train Epoch: 9 [80640/110534 (73%)]\tClassification Loss: 1.5283\r\n",
      "Train Epoch: 9 [81280/110534 (74%)]\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 9 [81920/110534 (74%)]\tClassification Loss: 1.5578\r\n",
      "Train Epoch: 9 [82560/110534 (75%)]\tClassification Loss: 1.7048\r\n",
      "Train Epoch: 9 [83200/110534 (75%)]\tClassification Loss: 1.4211\r\n",
      "Train Epoch: 9 [83840/110534 (76%)]\tClassification Loss: 1.7633\r\n",
      "Train Epoch: 9 [84480/110534 (76%)]\tClassification Loss: 1.3705\r\n",
      "Train Epoch: 9 [85120/110534 (77%)]\tClassification Loss: 1.7161\r\n",
      "Train Epoch: 9 [85760/110534 (78%)]\tClassification Loss: 1.4718\r\n",
      "Train Epoch: 9 [86400/110534 (78%)]\tClassification Loss: 1.5421\r\n",
      "Train Epoch: 9 [87040/110534 (79%)]\tClassification Loss: 1.4237\r\n",
      "Train Epoch: 9 [87680/110534 (79%)]\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 9 [88320/110534 (80%)]\tClassification Loss: 1.3552\r\n",
      "Train Epoch: 9 [88960/110534 (80%)]\tClassification Loss: 1.3595\r\n",
      "Train Epoch: 9 [89600/110534 (81%)]\tClassification Loss: 1.3771\r\n",
      "Train Epoch: 9 [90240/110534 (82%)]\tClassification Loss: 1.7280\r\n",
      "Train Epoch: 9 [90880/110534 (82%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 9 [91520/110534 (83%)]\tClassification Loss: 1.5743\r\n",
      "Train Epoch: 9 [92160/110534 (83%)]\tClassification Loss: 1.3896\r\n",
      "Train Epoch: 9 [92800/110534 (84%)]\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 9 [93440/110534 (85%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 9 [94080/110534 (85%)]\tClassification Loss: 1.5581\r\n",
      "Train Epoch: 9 [94720/110534 (86%)]\tClassification Loss: 1.7527\r\n",
      "Train Epoch: 9 [95360/110534 (86%)]\tClassification Loss: 1.5176\r\n",
      "Train Epoch: 9 [96000/110534 (87%)]\tClassification Loss: 1.8226\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1500.pth.tar\r\n",
      "Train Epoch: 9 [96640/110534 (87%)]\tClassification Loss: 1.5335\r\n",
      "Train Epoch: 9 [97280/110534 (88%)]\tClassification Loss: 1.6355\r\n",
      "Train Epoch: 9 [97920/110534 (89%)]\tClassification Loss: 1.4470\r\n",
      "Train Epoch: 9 [98560/110534 (89%)]\tClassification Loss: 1.4308\r\n",
      "Train Epoch: 9 [99200/110534 (90%)]\tClassification Loss: 1.8958\r\n",
      "Train Epoch: 9 [99840/110534 (90%)]\tClassification Loss: 1.4947\r\n",
      "Train Epoch: 9 [100480/110534 (91%)]\tClassification Loss: 1.4946\r\n",
      "Train Epoch: 9 [101120/110534 (91%)]\tClassification Loss: 1.4241\r\n",
      "Train Epoch: 9 [101760/110534 (92%)]\tClassification Loss: 1.7226\r\n",
      "Train Epoch: 9 [102400/110534 (93%)]\tClassification Loss: 1.7232\r\n",
      "Train Epoch: 9 [103040/110534 (93%)]\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 9 [103680/110534 (94%)]\tClassification Loss: 1.4459\r\n",
      "Train Epoch: 9 [104320/110534 (94%)]\tClassification Loss: 1.4316\r\n",
      "Train Epoch: 9 [104960/110534 (95%)]\tClassification Loss: 1.5773\r\n",
      "Train Epoch: 9 [105600/110534 (96%)]\tClassification Loss: 1.6858\r\n",
      "Train Epoch: 9 [106240/110534 (96%)]\tClassification Loss: 1.3412\r\n",
      "Train Epoch: 9 [106880/110534 (97%)]\tClassification Loss: 1.3984\r\n",
      "Train Epoch: 9 [107520/110534 (97%)]\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 9 [108160/110534 (98%)]\tClassification Loss: 1.4885\r\n",
      "Train Epoch: 9 [108800/110534 (98%)]\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 9 [109440/110534 (99%)]\tClassification Loss: 1.5720\r\n",
      "Train Epoch: 9 [110080/110534 (100%)]\tClassification Loss: 1.7546\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_final.pth.tar\r\n",
      "Train Epoch: 10 [0/110534 (0%)]\tClassification Loss: 1.7021\r\n",
      "\r\n",
      "Test set: Average loss: 1.4532, Accuracy: 6996/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 10 [640/110534 (1%)]\tClassification Loss: 1.5539\r\n",
      "Train Epoch: 10 [1280/110534 (1%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 10 [1920/110534 (2%)]\tClassification Loss: 1.4151\r\n",
      "Train Epoch: 10 [2560/110534 (2%)]\tClassification Loss: 1.6979\r\n",
      "Train Epoch: 10 [3200/110534 (3%)]\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 10 [3840/110534 (3%)]\tClassification Loss: 1.5937\r\n",
      "Train Epoch: 10 [4480/110534 (4%)]\tClassification Loss: 1.4691\r\n",
      "Train Epoch: 10 [5120/110534 (5%)]\tClassification Loss: 1.5988\r\n",
      "Train Epoch: 10 [5760/110534 (5%)]\tClassification Loss: 1.6338\r\n",
      "Train Epoch: 10 [6400/110534 (6%)]\tClassification Loss: 1.8471\r\n",
      "Train Epoch: 10 [7040/110534 (6%)]\tClassification Loss: 1.5691\r\n",
      "Train Epoch: 10 [7680/110534 (7%)]\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 10 [8320/110534 (8%)]\tClassification Loss: 1.7093\r\n",
      "Train Epoch: 10 [8960/110534 (8%)]\tClassification Loss: 1.5583\r\n",
      "Train Epoch: 10 [9600/110534 (9%)]\tClassification Loss: 1.7220\r\n",
      "Train Epoch: 10 [10240/110534 (9%)]\tClassification Loss: 1.3299\r\n",
      "Train Epoch: 10 [10880/110534 (10%)]\tClassification Loss: 1.6322\r\n",
      "Train Epoch: 10 [11520/110534 (10%)]\tClassification Loss: 1.5173\r\n",
      "Train Epoch: 10 [12160/110534 (11%)]\tClassification Loss: 1.5135\r\n",
      "Train Epoch: 10 [12800/110534 (12%)]\tClassification Loss: 1.7048\r\n",
      "Train Epoch: 10 [13440/110534 (12%)]\tClassification Loss: 1.5407\r\n",
      "Train Epoch: 10 [14080/110534 (13%)]\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 10 [14720/110534 (13%)]\tClassification Loss: 1.6860\r\n",
      "Train Epoch: 10 [15360/110534 (14%)]\tClassification Loss: 1.5284\r\n",
      "Train Epoch: 10 [16000/110534 (14%)]\tClassification Loss: 1.7639\r\n",
      "Train Epoch: 10 [16640/110534 (15%)]\tClassification Loss: 1.5572\r\n",
      "Train Epoch: 10 [17280/110534 (16%)]\tClassification Loss: 1.8139\r\n",
      "Train Epoch: 10 [17920/110534 (16%)]\tClassification Loss: 1.7756\r\n",
      "Train Epoch: 10 [18560/110534 (17%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 10 [19200/110534 (17%)]\tClassification Loss: 1.7394\r\n",
      "Train Epoch: 10 [19840/110534 (18%)]\tClassification Loss: 1.3590\r\n",
      "Train Epoch: 10 [20480/110534 (19%)]\tClassification Loss: 1.6981\r\n",
      "Train Epoch: 10 [21120/110534 (19%)]\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 10 [21760/110534 (20%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 10 [22400/110534 (20%)]\tClassification Loss: 1.7120\r\n",
      "Train Epoch: 10 [23040/110534 (21%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 10 [23680/110534 (21%)]\tClassification Loss: 1.6646\r\n",
      "Train Epoch: 10 [24320/110534 (22%)]\tClassification Loss: 1.4140\r\n",
      "Train Epoch: 10 [24960/110534 (23%)]\tClassification Loss: 1.6201\r\n",
      "Train Epoch: 10 [25600/110534 (23%)]\tClassification Loss: 1.2302\r\n",
      "Train Epoch: 10 [26240/110534 (24%)]\tClassification Loss: 1.5753\r\n",
      "Train Epoch: 10 [26880/110534 (24%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 10 [27520/110534 (25%)]\tClassification Loss: 1.6483\r\n",
      "Train Epoch: 10 [28160/110534 (25%)]\tClassification Loss: 1.7133\r\n",
      "Train Epoch: 10 [28800/110534 (26%)]\tClassification Loss: 1.6878\r\n",
      "Train Epoch: 10 [29440/110534 (27%)]\tClassification Loss: 1.4878\r\n",
      "Train Epoch: 10 [30080/110534 (27%)]\tClassification Loss: 1.5993\r\n",
      "Train Epoch: 10 [30720/110534 (28%)]\tClassification Loss: 1.2348\r\n",
      "Train Epoch: 10 [31360/110534 (28%)]\tClassification Loss: 1.2257\r\n",
      "Train Epoch: 10 [32000/110534 (29%)]\tClassification Loss: 1.5703\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_500.pth.tar\r\n",
      "Train Epoch: 10 [32640/110534 (30%)]\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 10 [33280/110534 (30%)]\tClassification Loss: 1.4449\r\n",
      "Train Epoch: 10 [33920/110534 (31%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 10 [34560/110534 (31%)]\tClassification Loss: 1.5231\r\n",
      "Train Epoch: 10 [35200/110534 (32%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 10 [35840/110534 (32%)]\tClassification Loss: 1.5021\r\n",
      "Train Epoch: 10 [36480/110534 (33%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 10 [37120/110534 (34%)]\tClassification Loss: 1.6916\r\n",
      "Train Epoch: 10 [37760/110534 (34%)]\tClassification Loss: 1.5227\r\n",
      "Train Epoch: 10 [38400/110534 (35%)]\tClassification Loss: 1.5778\r\n",
      "\r\n",
      "Test set: Average loss: 1.4599, Accuracy: 6952/12800 (54%)\r\n",
      "\r\n",
      "Train Epoch: 10 [39040/110534 (35%)]\tClassification Loss: 1.2795\r\n",
      "Train Epoch: 10 [39680/110534 (36%)]\tClassification Loss: 1.6940\r\n",
      "Train Epoch: 10 [40320/110534 (36%)]\tClassification Loss: 1.8536\r\n",
      "Train Epoch: 10 [40960/110534 (37%)]\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 10 [41600/110534 (38%)]\tClassification Loss: 1.8386\r\n",
      "Train Epoch: 10 [42240/110534 (38%)]\tClassification Loss: 1.7778\r\n",
      "Train Epoch: 10 [42880/110534 (39%)]\tClassification Loss: 1.5178\r\n",
      "Train Epoch: 10 [43520/110534 (39%)]\tClassification Loss: 1.5481\r\n",
      "Train Epoch: 10 [44160/110534 (40%)]\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 10 [44800/110534 (41%)]\tClassification Loss: 1.6533\r\n",
      "Train Epoch: 10 [45440/110534 (41%)]\tClassification Loss: 1.2852\r\n",
      "Train Epoch: 10 [46080/110534 (42%)]\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 10 [46720/110534 (42%)]\tClassification Loss: 1.3922\r\n",
      "Train Epoch: 10 [47360/110534 (43%)]\tClassification Loss: 1.3756\r\n",
      "Train Epoch: 10 [48000/110534 (43%)]\tClassification Loss: 1.7205\r\n",
      "Train Epoch: 10 [48640/110534 (44%)]\tClassification Loss: 1.7498\r\n",
      "Train Epoch: 10 [49280/110534 (45%)]\tClassification Loss: 1.3623\r\n",
      "Train Epoch: 10 [49920/110534 (45%)]\tClassification Loss: 1.4057\r\n",
      "Train Epoch: 10 [50560/110534 (46%)]\tClassification Loss: 1.6677\r\n",
      "Train Epoch: 10 [51200/110534 (46%)]\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 10 [51840/110534 (47%)]\tClassification Loss: 1.4365\r\n",
      "Train Epoch: 10 [52480/110534 (47%)]\tClassification Loss: 1.4535\r\n",
      "Train Epoch: 10 [53120/110534 (48%)]\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 10 [53760/110534 (49%)]\tClassification Loss: 1.5968\r\n",
      "Train Epoch: 10 [54400/110534 (49%)]\tClassification Loss: 1.2944\r\n",
      "Train Epoch: 10 [55040/110534 (50%)]\tClassification Loss: 1.3351\r\n",
      "Train Epoch: 10 [55680/110534 (50%)]\tClassification Loss: 1.7324\r\n",
      "Train Epoch: 10 [56320/110534 (51%)]\tClassification Loss: 1.6608\r\n",
      "Train Epoch: 10 [56960/110534 (52%)]\tClassification Loss: 1.9055\r\n",
      "Train Epoch: 10 [57600/110534 (52%)]\tClassification Loss: 1.5021\r\n",
      "Train Epoch: 10 [58240/110534 (53%)]\tClassification Loss: 1.4775\r\n",
      "Train Epoch: 10 [58880/110534 (53%)]\tClassification Loss: 1.3938\r\n",
      "Train Epoch: 10 [59520/110534 (54%)]\tClassification Loss: 1.5228\r\n",
      "Train Epoch: 10 [60160/110534 (54%)]\tClassification Loss: 1.4223\r\n",
      "Train Epoch: 10 [60800/110534 (55%)]\tClassification Loss: 1.4072\r\n",
      "Train Epoch: 10 [61440/110534 (56%)]\tClassification Loss: 1.7944\r\n",
      "Train Epoch: 10 [62080/110534 (56%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 10 [62720/110534 (57%)]\tClassification Loss: 1.6030\r\n",
      "Train Epoch: 10 [63360/110534 (57%)]\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 10 [64000/110534 (58%)]\tClassification Loss: 1.7436\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1000.pth.tar\r\n",
      "Train Epoch: 10 [64640/110534 (58%)]\tClassification Loss: 1.7213\r\n",
      "Train Epoch: 10 [65280/110534 (59%)]\tClassification Loss: 1.8383\r\n",
      "Train Epoch: 10 [65920/110534 (60%)]\tClassification Loss: 1.4144\r\n",
      "Train Epoch: 10 [66560/110534 (60%)]\tClassification Loss: 1.3986\r\n",
      "Train Epoch: 10 [67200/110534 (61%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 10 [67840/110534 (61%)]\tClassification Loss: 1.5989\r\n",
      "Train Epoch: 10 [68480/110534 (62%)]\tClassification Loss: 1.6303\r\n",
      "Train Epoch: 10 [69120/110534 (63%)]\tClassification Loss: 1.3783\r\n",
      "Train Epoch: 10 [69760/110534 (63%)]\tClassification Loss: 1.3157\r\n",
      "Train Epoch: 10 [70400/110534 (64%)]\tClassification Loss: 1.6368\r\n",
      "Train Epoch: 10 [71040/110534 (64%)]\tClassification Loss: 1.8372\r\n",
      "Train Epoch: 10 [71680/110534 (65%)]\tClassification Loss: 1.5296\r\n",
      "Train Epoch: 10 [72320/110534 (65%)]\tClassification Loss: 1.5008\r\n",
      "Train Epoch: 10 [72960/110534 (66%)]\tClassification Loss: 1.4420\r\n",
      "Train Epoch: 10 [73600/110534 (67%)]\tClassification Loss: 1.7176\r\n",
      "Train Epoch: 10 [74240/110534 (67%)]\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 10 [74880/110534 (68%)]\tClassification Loss: 1.5382\r\n",
      "Train Epoch: 10 [75520/110534 (68%)]\tClassification Loss: 1.5661\r\n",
      "Train Epoch: 10 [76160/110534 (69%)]\tClassification Loss: 1.6988\r\n",
      "Train Epoch: 10 [76800/110534 (69%)]\tClassification Loss: 1.7465\r\n",
      "\r\n",
      "Test set: Average loss: 1.4475, Accuracy: 6983/12800 (55%)\r\n",
      "\r\n",
      "Train Epoch: 10 [77440/110534 (70%)]\tClassification Loss: 1.4984\r\n",
      "Train Epoch: 10 [78080/110534 (71%)]\tClassification Loss: 1.6191\r\n",
      "Train Epoch: 10 [78720/110534 (71%)]\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 10 [79360/110534 (72%)]\tClassification Loss: 1.6297\r\n",
      "Train Epoch: 10 [80000/110534 (72%)]\tClassification Loss: 1.4260\r\n",
      "Train Epoch: 10 [80640/110534 (73%)]\tClassification Loss: 1.4285\r\n",
      "Train Epoch: 10 [81280/110534 (74%)]\tClassification Loss: 1.2413\r\n",
      "Train Epoch: 10 [81920/110534 (74%)]\tClassification Loss: 1.6143\r\n",
      "Train Epoch: 10 [82560/110534 (75%)]\tClassification Loss: 1.6686\r\n",
      "Train Epoch: 10 [83200/110534 (75%)]\tClassification Loss: 1.3266\r\n",
      "Train Epoch: 10 [83840/110534 (76%)]\tClassification Loss: 1.7665\r\n",
      "Train Epoch: 10 [84480/110534 (76%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 10 [85120/110534 (77%)]\tClassification Loss: 1.7706\r\n",
      "Train Epoch: 10 [85760/110534 (78%)]\tClassification Loss: 1.6164\r\n",
      "Train Epoch: 10 [86400/110534 (78%)]\tClassification Loss: 1.6369\r\n",
      "Train Epoch: 10 [87040/110534 (79%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 10 [87680/110534 (79%)]\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 10 [88320/110534 (80%)]\tClassification Loss: 1.3324\r\n",
      "Train Epoch: 10 [88960/110534 (80%)]\tClassification Loss: 1.2698\r\n",
      "Train Epoch: 10 [89600/110534 (81%)]\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 10 [90240/110534 (82%)]\tClassification Loss: 1.8582\r\n",
      "Train Epoch: 10 [90880/110534 (82%)]\tClassification Loss: 1.3813\r\n",
      "Train Epoch: 10 [91520/110534 (83%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 10 [92160/110534 (83%)]\tClassification Loss: 1.5174\r\n",
      "Train Epoch: 10 [92800/110534 (84%)]\tClassification Loss: 1.7175\r\n",
      "Train Epoch: 10 [93440/110534 (85%)]\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 10 [94080/110534 (85%)]\tClassification Loss: 1.3784\r\n",
      "Train Epoch: 10 [94720/110534 (86%)]\tClassification Loss: 1.6815\r\n",
      "Train Epoch: 10 [95360/110534 (86%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 10 [96000/110534 (87%)]\tClassification Loss: 1.8923\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1500.pth.tar\r\n",
      "Train Epoch: 10 [96640/110534 (87%)]\tClassification Loss: 1.5387\r\n",
      "Train Epoch: 10 [97280/110534 (88%)]\tClassification Loss: 1.7101\r\n",
      "Train Epoch: 10 [97920/110534 (89%)]\tClassification Loss: 1.3628\r\n",
      "Train Epoch: 10 [98560/110534 (89%)]\tClassification Loss: 1.4915\r\n",
      "Train Epoch: 10 [99200/110534 (90%)]\tClassification Loss: 1.9198\r\n",
      "Train Epoch: 10 [99840/110534 (90%)]\tClassification Loss: 1.5843\r\n",
      "Train Epoch: 10 [100480/110534 (91%)]\tClassification Loss: 1.5543\r\n",
      "Train Epoch: 10 [101120/110534 (91%)]\tClassification Loss: 1.4130\r\n",
      "Train Epoch: 10 [101760/110534 (92%)]\tClassification Loss: 1.6391\r\n",
      "Train Epoch: 10 [102400/110534 (93%)]\tClassification Loss: 1.7455\r\n",
      "Train Epoch: 10 [103040/110534 (93%)]\tClassification Loss: 1.5771\r\n",
      "Train Epoch: 10 [103680/110534 (94%)]\tClassification Loss: 1.4421\r\n",
      "Train Epoch: 10 [104320/110534 (94%)]\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 10 [104960/110534 (95%)]\tClassification Loss: 1.6280\r\n",
      "Train Epoch: 10 [105600/110534 (96%)]\tClassification Loss: 1.8501\r\n",
      "Train Epoch: 10 [106240/110534 (96%)]\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 10 [106880/110534 (97%)]\tClassification Loss: 1.3487\r\n",
      "Train Epoch: 10 [107520/110534 (97%)]\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 10 [108160/110534 (98%)]\tClassification Loss: 1.3892\r\n",
      "Train Epoch: 10 [108800/110534 (98%)]\tClassification Loss: 1.4274\r\n",
      "Train Epoch: 10 [109440/110534 (99%)]\tClassification Loss: 1.5980\r\n",
      "Train Epoch: 10 [110080/110534 (100%)]\tClassification Loss: 1.6989\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_final.pth.tar\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oFYsssLDaSDJ",
    "colab_type": "code",
    "outputId": "c8b0c1e4-9039-4b20-cef8-c971efa437aa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1584954545220,
     "user_tz": -300,
     "elapsed": 3922822,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Freeze=True. LR=0.05\n",
    "! python train.py"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/110534 (0%)]\tClassification Loss: 3.2919\r\n",
      "train.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.2468, Accuracy: 257/42368 (1%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/110534 (1%)]\tClassification Loss: 2.5549\r\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tClassification Loss: 2.5333\r\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tClassification Loss: 2.2296\r\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tClassification Loss: 2.1743\r\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tClassification Loss: 2.2104\r\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tClassification Loss: 2.5181\r\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tClassification Loss: 2.0961\r\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tClassification Loss: 2.2449\r\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tClassification Loss: 2.1797\r\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tClassification Loss: 2.1965\r\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tClassification Loss: 1.9058\r\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tClassification Loss: 1.9071\r\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tClassification Loss: 2.2524\r\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tClassification Loss: 1.8857\r\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tClassification Loss: 2.0349\r\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tClassification Loss: 1.7545\r\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tClassification Loss: 1.8268\r\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tClassification Loss: 1.8706\r\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tClassification Loss: 1.8624\r\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tClassification Loss: 2.1293\r\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tClassification Loss: 2.0139\r\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tClassification Loss: 1.9390\r\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tClassification Loss: 1.8053\r\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tClassification Loss: 1.7255\r\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tClassification Loss: 1.9685\r\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tClassification Loss: 1.7780\r\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tClassification Loss: 2.1488\r\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tClassification Loss: 1.9485\r\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tClassification Loss: 1.6449\r\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tClassification Loss: 1.5922\r\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tClassification Loss: 1.5111\r\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tClassification Loss: 1.6778\r\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tClassification Loss: 1.9361\r\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tClassification Loss: 1.8062\r\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tClassification Loss: 1.7167\r\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tClassification Loss: 1.8613\r\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tClassification Loss: 1.6740\r\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tClassification Loss: 1.7613\r\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tClassification Loss: 2.0382\r\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tClassification Loss: 1.7524\r\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tClassification Loss: 1.4889\r\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tClassification Loss: 1.8321\r\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tClassification Loss: 1.6915\r\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tClassification Loss: 2.0027\r\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tClassification Loss: 1.5776\r\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tClassification Loss: 1.5812\r\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tClassification Loss: 1.5039\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tClassification Loss: 2.0684\r\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tClassification Loss: 1.8140\r\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tClassification Loss: 1.6480\r\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tClassification Loss: 1.6720\r\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tClassification Loss: 1.6313\r\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tClassification Loss: 1.8542\r\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tClassification Loss: 1.9204\r\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tClassification Loss: 1.5204\r\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tClassification Loss: 1.6498\r\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tClassification Loss: 1.6190\r\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tClassification Loss: 1.4325\r\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tClassification Loss: 1.7197\r\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tClassification Loss: 1.7941\r\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tClassification Loss: 1.7045\r\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tClassification Loss: 1.8569\r\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tClassification Loss: 1.8712\r\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tClassification Loss: 1.6740\r\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tClassification Loss: 1.7576\r\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tClassification Loss: 1.5435\r\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tClassification Loss: 1.6849\r\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tClassification Loss: 1.7341\r\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tClassification Loss: 1.5365\r\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tClassification Loss: 1.5877\r\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tClassification Loss: 1.8829\r\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tClassification Loss: 1.7710\r\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tClassification Loss: 1.7253\r\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tClassification Loss: 1.4724\r\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tClassification Loss: 1.7451\r\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tClassification Loss: 1.3784\r\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tClassification Loss: 1.6506\r\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tClassification Loss: 2.0123\r\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tClassification Loss: 1.6931\r\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tClassification Loss: 1.9763\r\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tClassification Loss: 1.7403\r\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tClassification Loss: 1.9480\r\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tClassification Loss: 1.6358\r\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tClassification Loss: 1.7282\r\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tClassification Loss: 1.8514\r\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tClassification Loss: 1.7132\r\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tClassification Loss: 1.2058\r\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tClassification Loss: 1.6595\r\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tClassification Loss: 1.4150\r\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tClassification Loss: 1.6916\r\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tClassification Loss: 1.5317\r\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tClassification Loss: 1.6764\r\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tClassification Loss: 1.7181\r\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tClassification Loss: 1.4181\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tClassification Loss: 1.8301\r\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tClassification Loss: 1.6778\r\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tClassification Loss: 1.6541\r\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tClassification Loss: 1.4270\r\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tClassification Loss: 1.3523\r\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tClassification Loss: 1.7937\r\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tClassification Loss: 1.7544\r\n",
      "Train Epoch: 1 [71680/110534 (65%)]\tClassification Loss: 1.9806\r\n",
      "Train Epoch: 1 [72320/110534 (65%)]\tClassification Loss: 1.6804\r\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tClassification Loss: 1.5804\r\n",
      "Train Epoch: 1 [73600/110534 (67%)]\tClassification Loss: 1.6155\r\n",
      "Train Epoch: 1 [74240/110534 (67%)]\tClassification Loss: 1.5719\r\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tClassification Loss: 1.8530\r\n",
      "Train Epoch: 1 [75520/110534 (68%)]\tClassification Loss: 1.6570\r\n",
      "Train Epoch: 1 [76160/110534 (69%)]\tClassification Loss: 1.4317\r\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tClassification Loss: 1.7361\r\n",
      "Train Epoch: 1 [77440/110534 (70%)]\tClassification Loss: 1.9212\r\n",
      "Train Epoch: 1 [78080/110534 (71%)]\tClassification Loss: 1.7236\r\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tClassification Loss: 1.5433\r\n",
      "Train Epoch: 1 [79360/110534 (72%)]\tClassification Loss: 1.6706\r\n",
      "Train Epoch: 1 [80000/110534 (72%)]\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tClassification Loss: 1.6746\r\n",
      "Train Epoch: 1 [81280/110534 (74%)]\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 1 [81920/110534 (74%)]\tClassification Loss: 1.8215\r\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tClassification Loss: 1.9507\r\n",
      "Train Epoch: 1 [83200/110534 (75%)]\tClassification Loss: 1.6823\r\n",
      "Train Epoch: 1 [83840/110534 (76%)]\tClassification Loss: 1.6075\r\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tClassification Loss: 1.4725\r\n",
      "Train Epoch: 1 [85120/110534 (77%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 1 [85760/110534 (78%)]\tClassification Loss: 1.4881\r\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tClassification Loss: 1.5174\r\n",
      "Train Epoch: 1 [87040/110534 (79%)]\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 1 [87680/110534 (79%)]\tClassification Loss: 1.9415\r\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tClassification Loss: 1.7286\r\n",
      "Train Epoch: 1 [88960/110534 (80%)]\tClassification Loss: 1.5089\r\n",
      "Train Epoch: 1 [89600/110534 (81%)]\tClassification Loss: 1.6522\r\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tClassification Loss: 1.7605\r\n",
      "Train Epoch: 1 [90880/110534 (82%)]\tClassification Loss: 1.7405\r\n",
      "Train Epoch: 1 [91520/110534 (83%)]\tClassification Loss: 1.7746\r\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tClassification Loss: 1.4678\r\n",
      "Train Epoch: 1 [92800/110534 (84%)]\tClassification Loss: 1.5643\r\n",
      "Train Epoch: 1 [93440/110534 (85%)]\tClassification Loss: 1.7555\r\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tClassification Loss: 1.5996\r\n",
      "Train Epoch: 1 [94720/110534 (86%)]\tClassification Loss: 1.5492\r\n",
      "Train Epoch: 1 [95360/110534 (86%)]\tClassification Loss: 1.6483\r\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tClassification Loss: 1.6061\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [96640/110534 (87%)]\tClassification Loss: 1.5505\r\n",
      "Train Epoch: 1 [97280/110534 (88%)]\tClassification Loss: 1.4972\r\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tClassification Loss: 1.8130\r\n",
      "Train Epoch: 1 [98560/110534 (89%)]\tClassification Loss: 1.8747\r\n",
      "Train Epoch: 1 [99200/110534 (90%)]\tClassification Loss: 1.3778\r\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tClassification Loss: 1.6948\r\n",
      "Train Epoch: 1 [100480/110534 (91%)]\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 1 [101120/110534 (91%)]\tClassification Loss: 1.6895\r\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tClassification Loss: 1.3409\r\n",
      "Train Epoch: 1 [102400/110534 (93%)]\tClassification Loss: 1.5617\r\n",
      "Train Epoch: 1 [103040/110534 (93%)]\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tClassification Loss: 1.7110\r\n",
      "Train Epoch: 1 [104320/110534 (94%)]\tClassification Loss: 1.6659\r\n",
      "Train Epoch: 1 [104960/110534 (95%)]\tClassification Loss: 1.7941\r\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tClassification Loss: 1.5797\r\n",
      "Train Epoch: 1 [106240/110534 (96%)]\tClassification Loss: 1.6936\r\n",
      "Train Epoch: 1 [106880/110534 (97%)]\tClassification Loss: 1.6162\r\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 1 [108160/110534 (98%)]\tClassification Loss: 1.5208\r\n",
      "Train Epoch: 1 [108800/110534 (98%)]\tClassification Loss: 1.3829\r\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tClassification Loss: 1.7243\r\n",
      "Train Epoch: 1 [110080/110534 (100%)]\tClassification Loss: 1.9230\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/110534 (0%)]\tClassification Loss: 1.9899\r\n",
      "\r\n",
      "Test set: Average loss: 1.5006, Accuracy: 22516/42368 (53%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/110534 (1%)]\tClassification Loss: 1.6259\r\n",
      "Train Epoch: 2 [1280/110534 (1%)]\tClassification Loss: 1.5611\r\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tClassification Loss: 1.6652\r\n",
      "Train Epoch: 2 [2560/110534 (2%)]\tClassification Loss: 1.7263\r\n",
      "Train Epoch: 2 [3200/110534 (3%)]\tClassification Loss: 1.5236\r\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tClassification Loss: 1.9249\r\n",
      "Train Epoch: 2 [4480/110534 (4%)]\tClassification Loss: 1.4490\r\n",
      "Train Epoch: 2 [5120/110534 (5%)]\tClassification Loss: 1.6268\r\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 2 [6400/110534 (6%)]\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 2 [7040/110534 (6%)]\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tClassification Loss: 1.7105\r\n",
      "Train Epoch: 2 [8320/110534 (8%)]\tClassification Loss: 1.9273\r\n",
      "Train Epoch: 2 [8960/110534 (8%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tClassification Loss: 1.5575\r\n",
      "Train Epoch: 2 [10240/110534 (9%)]\tClassification Loss: 1.3827\r\n",
      "Train Epoch: 2 [10880/110534 (10%)]\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tClassification Loss: 1.6115\r\n",
      "Train Epoch: 2 [12160/110534 (11%)]\tClassification Loss: 1.4829\r\n",
      "Train Epoch: 2 [12800/110534 (12%)]\tClassification Loss: 1.8319\r\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tClassification Loss: 1.8631\r\n",
      "Train Epoch: 2 [14080/110534 (13%)]\tClassification Loss: 1.5398\r\n",
      "Train Epoch: 2 [14720/110534 (13%)]\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tClassification Loss: 1.4092\r\n",
      "Train Epoch: 2 [16000/110534 (14%)]\tClassification Loss: 1.7763\r\n",
      "Train Epoch: 2 [16640/110534 (15%)]\tClassification Loss: 1.5266\r\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tClassification Loss: 1.7909\r\n",
      "Train Epoch: 2 [17920/110534 (16%)]\tClassification Loss: 1.8333\r\n",
      "Train Epoch: 2 [18560/110534 (17%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tClassification Loss: 1.4522\r\n",
      "Train Epoch: 2 [19840/110534 (18%)]\tClassification Loss: 1.4448\r\n",
      "Train Epoch: 2 [20480/110534 (19%)]\tClassification Loss: 1.2179\r\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tClassification Loss: 1.3024\r\n",
      "Train Epoch: 2 [21760/110534 (20%)]\tClassification Loss: 1.5869\r\n",
      "Train Epoch: 2 [22400/110534 (20%)]\tClassification Loss: 1.4825\r\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 2 [23680/110534 (21%)]\tClassification Loss: 1.4522\r\n",
      "Train Epoch: 2 [24320/110534 (22%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tClassification Loss: 1.4324\r\n",
      "Train Epoch: 2 [25600/110534 (23%)]\tClassification Loss: 1.6003\r\n",
      "Train Epoch: 2 [26240/110534 (24%)]\tClassification Loss: 1.9410\r\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tClassification Loss: 1.4656\r\n",
      "Train Epoch: 2 [27520/110534 (25%)]\tClassification Loss: 1.7748\r\n",
      "Train Epoch: 2 [28160/110534 (25%)]\tClassification Loss: 1.4190\r\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 2 [29440/110534 (27%)]\tClassification Loss: 1.5856\r\n",
      "Train Epoch: 2 [30080/110534 (27%)]\tClassification Loss: 1.7640\r\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tClassification Loss: 1.3707\r\n",
      "Train Epoch: 2 [31360/110534 (28%)]\tClassification Loss: 1.3983\r\n",
      "Train Epoch: 2 [32000/110534 (29%)]\tClassification Loss: 1.4890\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_500.pth.tar\r\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tClassification Loss: 2.0050\r\n",
      "Train Epoch: 2 [33280/110534 (30%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 2 [33920/110534 (31%)]\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tClassification Loss: 1.6099\r\n",
      "Train Epoch: 2 [35200/110534 (32%)]\tClassification Loss: 1.3712\r\n",
      "Train Epoch: 2 [35840/110534 (32%)]\tClassification Loss: 1.6339\r\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tClassification Loss: 1.7933\r\n",
      "Train Epoch: 2 [37120/110534 (34%)]\tClassification Loss: 1.2633\r\n",
      "Train Epoch: 2 [37760/110534 (34%)]\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 2 [39040/110534 (35%)]\tClassification Loss: 1.2168\r\n",
      "Train Epoch: 2 [39680/110534 (36%)]\tClassification Loss: 1.4006\r\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tClassification Loss: 1.7200\r\n",
      "Train Epoch: 2 [40960/110534 (37%)]\tClassification Loss: 1.6844\r\n",
      "Train Epoch: 2 [41600/110534 (38%)]\tClassification Loss: 1.7237\r\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tClassification Loss: 1.9079\r\n",
      "Train Epoch: 2 [42880/110534 (39%)]\tClassification Loss: 1.4802\r\n",
      "Train Epoch: 2 [43520/110534 (39%)]\tClassification Loss: 1.6699\r\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tClassification Loss: 1.7399\r\n",
      "Train Epoch: 2 [44800/110534 (41%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 2 [45440/110534 (41%)]\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tClassification Loss: 1.7240\r\n",
      "Train Epoch: 2 [46720/110534 (42%)]\tClassification Loss: 1.4020\r\n",
      "Train Epoch: 2 [47360/110534 (43%)]\tClassification Loss: 1.5635\r\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tClassification Loss: 1.5780\r\n",
      "Train Epoch: 2 [48640/110534 (44%)]\tClassification Loss: 1.7766\r\n",
      "Train Epoch: 2 [49280/110534 (45%)]\tClassification Loss: 1.5800\r\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 2 [50560/110534 (46%)]\tClassification Loss: 1.5842\r\n",
      "Train Epoch: 2 [51200/110534 (46%)]\tClassification Loss: 1.3023\r\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tClassification Loss: 1.4618\r\n",
      "Train Epoch: 2 [52480/110534 (47%)]\tClassification Loss: 1.5597\r\n",
      "Train Epoch: 2 [53120/110534 (48%)]\tClassification Loss: 1.8514\r\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tClassification Loss: 1.6173\r\n",
      "Train Epoch: 2 [54400/110534 (49%)]\tClassification Loss: 1.7833\r\n",
      "Train Epoch: 2 [55040/110534 (50%)]\tClassification Loss: 1.7835\r\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tClassification Loss: 1.9632\r\n",
      "Train Epoch: 2 [56320/110534 (51%)]\tClassification Loss: 1.5279\r\n",
      "Train Epoch: 2 [56960/110534 (52%)]\tClassification Loss: 1.6526\r\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tClassification Loss: 1.6854\r\n",
      "Train Epoch: 2 [58240/110534 (53%)]\tClassification Loss: 1.6065\r\n",
      "Train Epoch: 2 [58880/110534 (53%)]\tClassification Loss: 1.1194\r\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tClassification Loss: 1.5263\r\n",
      "Train Epoch: 2 [60160/110534 (54%)]\tClassification Loss: 1.3675\r\n",
      "Train Epoch: 2 [60800/110534 (55%)]\tClassification Loss: 1.4686\r\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 2 [62080/110534 (56%)]\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 2 [62720/110534 (57%)]\tClassification Loss: 1.7533\r\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tClassification Loss: 1.6102\r\n",
      "Train Epoch: 2 [64000/110534 (58%)]\tClassification Loss: 1.3671\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1000.pth.tar\r\n",
      "Train Epoch: 2 [64640/110534 (58%)]\tClassification Loss: 1.6878\r\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tClassification Loss: 1.4547\r\n",
      "Train Epoch: 2 [65920/110534 (60%)]\tClassification Loss: 1.8149\r\n",
      "Train Epoch: 2 [66560/110534 (60%)]\tClassification Loss: 1.4878\r\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tClassification Loss: 1.3222\r\n",
      "Train Epoch: 2 [67840/110534 (61%)]\tClassification Loss: 1.4776\r\n",
      "Train Epoch: 2 [68480/110534 (62%)]\tClassification Loss: 1.7160\r\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tClassification Loss: 1.2371\r\n",
      "Train Epoch: 2 [69760/110534 (63%)]\tClassification Loss: 1.7277\r\n",
      "Train Epoch: 2 [70400/110534 (64%)]\tClassification Loss: 1.4900\r\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tClassification Loss: 1.7072\r\n",
      "Train Epoch: 2 [71680/110534 (65%)]\tClassification Loss: 1.8571\r\n",
      "Train Epoch: 2 [72320/110534 (65%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tClassification Loss: 1.4810\r\n",
      "Train Epoch: 2 [73600/110534 (67%)]\tClassification Loss: 1.5260\r\n",
      "Train Epoch: 2 [74240/110534 (67%)]\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tClassification Loss: 1.7588\r\n",
      "Train Epoch: 2 [75520/110534 (68%)]\tClassification Loss: 1.5925\r\n",
      "Train Epoch: 2 [76160/110534 (69%)]\tClassification Loss: 1.3484\r\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tClassification Loss: 1.6605\r\n",
      "Train Epoch: 2 [77440/110534 (70%)]\tClassification Loss: 1.6926\r\n",
      "Train Epoch: 2 [78080/110534 (71%)]\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tClassification Loss: 1.5632\r\n",
      "Train Epoch: 2 [79360/110534 (72%)]\tClassification Loss: 1.6129\r\n",
      "Train Epoch: 2 [80000/110534 (72%)]\tClassification Loss: 1.7043\r\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tClassification Loss: 1.6280\r\n",
      "Train Epoch: 2 [81280/110534 (74%)]\tClassification Loss: 1.4800\r\n",
      "Train Epoch: 2 [81920/110534 (74%)]\tClassification Loss: 1.7580\r\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tClassification Loss: 1.8131\r\n",
      "Train Epoch: 2 [83200/110534 (75%)]\tClassification Loss: 1.5709\r\n",
      "Train Epoch: 2 [83840/110534 (76%)]\tClassification Loss: 1.6248\r\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 2 [85120/110534 (77%)]\tClassification Loss: 1.4315\r\n",
      "Train Epoch: 2 [85760/110534 (78%)]\tClassification Loss: 1.3803\r\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 2 [87040/110534 (79%)]\tClassification Loss: 1.6242\r\n",
      "Train Epoch: 2 [87680/110534 (79%)]\tClassification Loss: 1.9267\r\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tClassification Loss: 1.5484\r\n",
      "Train Epoch: 2 [88960/110534 (80%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 2 [89600/110534 (81%)]\tClassification Loss: 1.5590\r\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 2 [90880/110534 (82%)]\tClassification Loss: 1.7860\r\n",
      "Train Epoch: 2 [91520/110534 (83%)]\tClassification Loss: 1.5696\r\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 2 [92800/110534 (84%)]\tClassification Loss: 1.4030\r\n",
      "Train Epoch: 2 [93440/110534 (85%)]\tClassification Loss: 1.5135\r\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 2 [94720/110534 (86%)]\tClassification Loss: 1.5004\r\n",
      "Train Epoch: 2 [95360/110534 (86%)]\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tClassification Loss: 1.5437\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [96640/110534 (87%)]\tClassification Loss: 1.5249\r\n",
      "Train Epoch: 2 [97280/110534 (88%)]\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tClassification Loss: 1.8482\r\n",
      "Train Epoch: 2 [98560/110534 (89%)]\tClassification Loss: 1.8246\r\n",
      "Train Epoch: 2 [99200/110534 (90%)]\tClassification Loss: 1.2165\r\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tClassification Loss: 1.5059\r\n",
      "Train Epoch: 2 [100480/110534 (91%)]\tClassification Loss: 1.4394\r\n",
      "Train Epoch: 2 [101120/110534 (91%)]\tClassification Loss: 1.7090\r\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tClassification Loss: 1.3572\r\n",
      "Train Epoch: 2 [102400/110534 (93%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 2 [103040/110534 (93%)]\tClassification Loss: 1.3983\r\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tClassification Loss: 1.5874\r\n",
      "Train Epoch: 2 [104320/110534 (94%)]\tClassification Loss: 1.4596\r\n",
      "Train Epoch: 2 [104960/110534 (95%)]\tClassification Loss: 1.6782\r\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tClassification Loss: 1.4807\r\n",
      "Train Epoch: 2 [106240/110534 (96%)]\tClassification Loss: 1.5118\r\n",
      "Train Epoch: 2 [106880/110534 (97%)]\tClassification Loss: 1.6932\r\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tClassification Loss: 1.4059\r\n",
      "Train Epoch: 2 [108160/110534 (98%)]\tClassification Loss: 1.5068\r\n",
      "Train Epoch: 2 [108800/110534 (98%)]\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tClassification Loss: 1.7533\r\n",
      "Train Epoch: 2 [110080/110534 (100%)]\tClassification Loss: 1.9953\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/110534 (0%)]\tClassification Loss: 2.0231\r\n",
      "\r\n",
      "Test set: Average loss: 1.4543, Accuracy: 22956/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [640/110534 (1%)]\tClassification Loss: 1.6202\r\n",
      "Train Epoch: 3 [1280/110534 (1%)]\tClassification Loss: 1.5397\r\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tClassification Loss: 1.8038\r\n",
      "Train Epoch: 3 [2560/110534 (2%)]\tClassification Loss: 1.5506\r\n",
      "Train Epoch: 3 [3200/110534 (3%)]\tClassification Loss: 1.4167\r\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tClassification Loss: 1.7025\r\n",
      "Train Epoch: 3 [4480/110534 (4%)]\tClassification Loss: 1.4078\r\n",
      "Train Epoch: 3 [5120/110534 (5%)]\tClassification Loss: 2.0267\r\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 3 [6400/110534 (6%)]\tClassification Loss: 1.6453\r\n",
      "Train Epoch: 3 [7040/110534 (6%)]\tClassification Loss: 1.5139\r\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tClassification Loss: 1.6514\r\n",
      "Train Epoch: 3 [8320/110534 (8%)]\tClassification Loss: 1.8653\r\n",
      "Train Epoch: 3 [8960/110534 (8%)]\tClassification Loss: 1.4449\r\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tClassification Loss: 1.5254\r\n",
      "Train Epoch: 3 [10240/110534 (9%)]\tClassification Loss: 1.4136\r\n",
      "Train Epoch: 3 [10880/110534 (10%)]\tClassification Loss: 1.4197\r\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tClassification Loss: 1.6087\r\n",
      "Train Epoch: 3 [12160/110534 (11%)]\tClassification Loss: 1.3785\r\n",
      "Train Epoch: 3 [12800/110534 (12%)]\tClassification Loss: 1.7212\r\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tClassification Loss: 1.8253\r\n",
      "Train Epoch: 3 [14080/110534 (13%)]\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 3 [14720/110534 (13%)]\tClassification Loss: 1.3377\r\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 3 [16000/110534 (14%)]\tClassification Loss: 1.6619\r\n",
      "Train Epoch: 3 [16640/110534 (15%)]\tClassification Loss: 1.5091\r\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tClassification Loss: 1.6886\r\n",
      "Train Epoch: 3 [17920/110534 (16%)]\tClassification Loss: 1.6851\r\n",
      "Train Epoch: 3 [18560/110534 (17%)]\tClassification Loss: 1.5269\r\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tClassification Loss: 1.3558\r\n",
      "Train Epoch: 3 [19840/110534 (18%)]\tClassification Loss: 1.3780\r\n",
      "Train Epoch: 3 [20480/110534 (19%)]\tClassification Loss: 1.2887\r\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tClassification Loss: 1.4809\r\n",
      "Train Epoch: 3 [21760/110534 (20%)]\tClassification Loss: 1.5830\r\n",
      "Train Epoch: 3 [22400/110534 (20%)]\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 3 [23680/110534 (21%)]\tClassification Loss: 1.3489\r\n",
      "Train Epoch: 3 [24320/110534 (22%)]\tClassification Loss: 1.5838\r\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 3 [25600/110534 (23%)]\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 3 [26240/110534 (24%)]\tClassification Loss: 1.7759\r\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tClassification Loss: 1.4927\r\n",
      "Train Epoch: 3 [27520/110534 (25%)]\tClassification Loss: 1.6453\r\n",
      "Train Epoch: 3 [28160/110534 (25%)]\tClassification Loss: 1.4490\r\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tClassification Loss: 1.7659\r\n",
      "Train Epoch: 3 [29440/110534 (27%)]\tClassification Loss: 1.6991\r\n",
      "Train Epoch: 3 [30080/110534 (27%)]\tClassification Loss: 1.6666\r\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tClassification Loss: 1.4881\r\n",
      "Train Epoch: 3 [31360/110534 (28%)]\tClassification Loss: 1.4086\r\n",
      "Train Epoch: 3 [32000/110534 (29%)]\tClassification Loss: 1.4601\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_500.pth.tar\r\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tClassification Loss: 2.0329\r\n",
      "Train Epoch: 3 [33280/110534 (30%)]\tClassification Loss: 1.5564\r\n",
      "Train Epoch: 3 [33920/110534 (31%)]\tClassification Loss: 1.6142\r\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tClassification Loss: 1.5476\r\n",
      "Train Epoch: 3 [35200/110534 (32%)]\tClassification Loss: 1.6579\r\n",
      "Train Epoch: 3 [35840/110534 (32%)]\tClassification Loss: 1.6337\r\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tClassification Loss: 1.5764\r\n",
      "Train Epoch: 3 [37120/110534 (34%)]\tClassification Loss: 1.2717\r\n",
      "Train Epoch: 3 [37760/110534 (34%)]\tClassification Loss: 1.3850\r\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tClassification Loss: 1.5098\r\n",
      "Train Epoch: 3 [39040/110534 (35%)]\tClassification Loss: 1.1699\r\n",
      "Train Epoch: 3 [39680/110534 (36%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tClassification Loss: 1.5976\r\n",
      "Train Epoch: 3 [40960/110534 (37%)]\tClassification Loss: 1.6597\r\n",
      "Train Epoch: 3 [41600/110534 (38%)]\tClassification Loss: 1.5943\r\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tClassification Loss: 1.8717\r\n",
      "Train Epoch: 3 [42880/110534 (39%)]\tClassification Loss: 1.5286\r\n",
      "Train Epoch: 3 [43520/110534 (39%)]\tClassification Loss: 1.5055\r\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tClassification Loss: 1.4680\r\n",
      "Train Epoch: 3 [44800/110534 (41%)]\tClassification Loss: 1.4017\r\n",
      "Train Epoch: 3 [45440/110534 (41%)]\tClassification Loss: 1.4248\r\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tClassification Loss: 1.7016\r\n",
      "Train Epoch: 3 [46720/110534 (42%)]\tClassification Loss: 1.3449\r\n",
      "Train Epoch: 3 [47360/110534 (43%)]\tClassification Loss: 1.4871\r\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 3 [48640/110534 (44%)]\tClassification Loss: 1.6700\r\n",
      "Train Epoch: 3 [49280/110534 (45%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 3 [50560/110534 (46%)]\tClassification Loss: 1.7661\r\n",
      "Train Epoch: 3 [51200/110534 (46%)]\tClassification Loss: 1.3666\r\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tClassification Loss: 1.4484\r\n",
      "Train Epoch: 3 [52480/110534 (47%)]\tClassification Loss: 1.4349\r\n",
      "Train Epoch: 3 [53120/110534 (48%)]\tClassification Loss: 1.7108\r\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tClassification Loss: 1.5634\r\n",
      "Train Epoch: 3 [54400/110534 (49%)]\tClassification Loss: 1.7782\r\n",
      "Train Epoch: 3 [55040/110534 (50%)]\tClassification Loss: 1.7336\r\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tClassification Loss: 1.8373\r\n",
      "Train Epoch: 3 [56320/110534 (51%)]\tClassification Loss: 1.4552\r\n",
      "Train Epoch: 3 [56960/110534 (52%)]\tClassification Loss: 1.5962\r\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tClassification Loss: 1.6553\r\n",
      "Train Epoch: 3 [58240/110534 (53%)]\tClassification Loss: 1.6357\r\n",
      "Train Epoch: 3 [58880/110534 (53%)]\tClassification Loss: 1.2039\r\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 3 [60160/110534 (54%)]\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 3 [60800/110534 (55%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tClassification Loss: 1.5512\r\n",
      "Train Epoch: 3 [62080/110534 (56%)]\tClassification Loss: 1.5174\r\n",
      "Train Epoch: 3 [62720/110534 (57%)]\tClassification Loss: 1.6526\r\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tClassification Loss: 1.5857\r\n",
      "Train Epoch: 3 [64000/110534 (58%)]\tClassification Loss: 1.3484\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1000.pth.tar\r\n",
      "Train Epoch: 3 [64640/110534 (58%)]\tClassification Loss: 1.7876\r\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tClassification Loss: 1.4855\r\n",
      "Train Epoch: 3 [65920/110534 (60%)]\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 3 [66560/110534 (60%)]\tClassification Loss: 1.4322\r\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tClassification Loss: 1.3197\r\n",
      "Train Epoch: 3 [67840/110534 (61%)]\tClassification Loss: 1.4743\r\n",
      "Train Epoch: 3 [68480/110534 (62%)]\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 3 [69760/110534 (63%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 3 [70400/110534 (64%)]\tClassification Loss: 1.4778\r\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tClassification Loss: 1.6244\r\n",
      "Train Epoch: 3 [71680/110534 (65%)]\tClassification Loss: 1.8437\r\n",
      "Train Epoch: 3 [72320/110534 (65%)]\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tClassification Loss: 1.3549\r\n",
      "Train Epoch: 3 [73600/110534 (67%)]\tClassification Loss: 1.3318\r\n",
      "Train Epoch: 3 [74240/110534 (67%)]\tClassification Loss: 1.4040\r\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tClassification Loss: 1.7983\r\n",
      "Train Epoch: 3 [75520/110534 (68%)]\tClassification Loss: 1.6608\r\n",
      "Train Epoch: 3 [76160/110534 (69%)]\tClassification Loss: 1.3277\r\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tClassification Loss: 1.6592\r\n",
      "Train Epoch: 3 [77440/110534 (70%)]\tClassification Loss: 1.7562\r\n",
      "Train Epoch: 3 [78080/110534 (71%)]\tClassification Loss: 1.6733\r\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tClassification Loss: 1.5445\r\n",
      "Train Epoch: 3 [79360/110534 (72%)]\tClassification Loss: 1.5481\r\n",
      "Train Epoch: 3 [80000/110534 (72%)]\tClassification Loss: 1.5430\r\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tClassification Loss: 1.3367\r\n",
      "Train Epoch: 3 [81280/110534 (74%)]\tClassification Loss: 1.3669\r\n",
      "Train Epoch: 3 [81920/110534 (74%)]\tClassification Loss: 1.7951\r\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tClassification Loss: 1.6888\r\n",
      "Train Epoch: 3 [83200/110534 (75%)]\tClassification Loss: 1.6842\r\n",
      "Train Epoch: 3 [83840/110534 (76%)]\tClassification Loss: 1.4905\r\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tClassification Loss: 1.4512\r\n",
      "Train Epoch: 3 [85120/110534 (77%)]\tClassification Loss: 1.6430\r\n",
      "Train Epoch: 3 [85760/110534 (78%)]\tClassification Loss: 1.5192\r\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tClassification Loss: 1.4562\r\n",
      "Train Epoch: 3 [87040/110534 (79%)]\tClassification Loss: 1.5882\r\n",
      "Train Epoch: 3 [87680/110534 (79%)]\tClassification Loss: 1.9696\r\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 3 [88960/110534 (80%)]\tClassification Loss: 1.2689\r\n",
      "Train Epoch: 3 [89600/110534 (81%)]\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tClassification Loss: 1.6888\r\n",
      "Train Epoch: 3 [90880/110534 (82%)]\tClassification Loss: 1.5972\r\n",
      "Train Epoch: 3 [91520/110534 (83%)]\tClassification Loss: 1.6034\r\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tClassification Loss: 1.3751\r\n",
      "Train Epoch: 3 [92800/110534 (84%)]\tClassification Loss: 1.3595\r\n",
      "Train Epoch: 3 [93440/110534 (85%)]\tClassification Loss: 1.6824\r\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tClassification Loss: 1.4975\r\n",
      "Train Epoch: 3 [94720/110534 (86%)]\tClassification Loss: 1.4813\r\n",
      "Train Epoch: 3 [95360/110534 (86%)]\tClassification Loss: 1.7213\r\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tClassification Loss: 1.6338\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [96640/110534 (87%)]\tClassification Loss: 1.6685\r\n",
      "Train Epoch: 3 [97280/110534 (88%)]\tClassification Loss: 1.4994\r\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tClassification Loss: 1.7506\r\n",
      "Train Epoch: 3 [98560/110534 (89%)]\tClassification Loss: 1.6125\r\n",
      "Train Epoch: 3 [99200/110534 (90%)]\tClassification Loss: 1.2092\r\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tClassification Loss: 1.4792\r\n",
      "Train Epoch: 3 [100480/110534 (91%)]\tClassification Loss: 1.3494\r\n",
      "Train Epoch: 3 [101120/110534 (91%)]\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tClassification Loss: 1.1614\r\n",
      "Train Epoch: 3 [102400/110534 (93%)]\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 3 [103040/110534 (93%)]\tClassification Loss: 1.5365\r\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tClassification Loss: 1.6390\r\n",
      "Train Epoch: 3 [104320/110534 (94%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 3 [104960/110534 (95%)]\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tClassification Loss: 1.5861\r\n",
      "Train Epoch: 3 [106240/110534 (96%)]\tClassification Loss: 1.5702\r\n",
      "Train Epoch: 3 [106880/110534 (97%)]\tClassification Loss: 1.6867\r\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tClassification Loss: 1.5490\r\n",
      "Train Epoch: 3 [108160/110534 (98%)]\tClassification Loss: 1.4197\r\n",
      "Train Epoch: 3 [108800/110534 (98%)]\tClassification Loss: 1.2643\r\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 3 [110080/110534 (100%)]\tClassification Loss: 1.7394\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/110534 (0%)]\tClassification Loss: 1.9539\r\n",
      "\r\n",
      "Test set: Average loss: 1.4339, Accuracy: 23138/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 4 [640/110534 (1%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 4 [1280/110534 (1%)]\tClassification Loss: 1.4656\r\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tClassification Loss: 1.7549\r\n",
      "Train Epoch: 4 [2560/110534 (2%)]\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 4 [3200/110534 (3%)]\tClassification Loss: 1.5208\r\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tClassification Loss: 1.7634\r\n",
      "Train Epoch: 4 [4480/110534 (4%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 4 [5120/110534 (5%)]\tClassification Loss: 1.7439\r\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tClassification Loss: 1.5988\r\n",
      "Train Epoch: 4 [6400/110534 (6%)]\tClassification Loss: 1.7331\r\n",
      "Train Epoch: 4 [7040/110534 (6%)]\tClassification Loss: 1.4042\r\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tClassification Loss: 1.6208\r\n",
      "Train Epoch: 4 [8320/110534 (8%)]\tClassification Loss: 1.8898\r\n",
      "Train Epoch: 4 [8960/110534 (8%)]\tClassification Loss: 1.4494\r\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tClassification Loss: 1.4476\r\n",
      "Train Epoch: 4 [10240/110534 (9%)]\tClassification Loss: 1.5235\r\n",
      "Train Epoch: 4 [10880/110534 (10%)]\tClassification Loss: 1.2937\r\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tClassification Loss: 1.6367\r\n",
      "Train Epoch: 4 [12160/110534 (11%)]\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 4 [12800/110534 (12%)]\tClassification Loss: 1.6770\r\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tClassification Loss: 1.9755\r\n",
      "Train Epoch: 4 [14080/110534 (13%)]\tClassification Loss: 1.3813\r\n",
      "Train Epoch: 4 [14720/110534 (13%)]\tClassification Loss: 1.3416\r\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tClassification Loss: 1.3389\r\n",
      "Train Epoch: 4 [16000/110534 (14%)]\tClassification Loss: 1.7889\r\n",
      "Train Epoch: 4 [16640/110534 (15%)]\tClassification Loss: 1.5218\r\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tClassification Loss: 1.7327\r\n",
      "Train Epoch: 4 [17920/110534 (16%)]\tClassification Loss: 1.6969\r\n",
      "Train Epoch: 4 [18560/110534 (17%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tClassification Loss: 1.2543\r\n",
      "Train Epoch: 4 [19840/110534 (18%)]\tClassification Loss: 1.1719\r\n",
      "Train Epoch: 4 [20480/110534 (19%)]\tClassification Loss: 1.1789\r\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tClassification Loss: 1.4505\r\n",
      "Train Epoch: 4 [21760/110534 (20%)]\tClassification Loss: 1.5504\r\n",
      "Train Epoch: 4 [22400/110534 (20%)]\tClassification Loss: 1.5177\r\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tClassification Loss: 1.3771\r\n",
      "Train Epoch: 4 [23680/110534 (21%)]\tClassification Loss: 1.2606\r\n",
      "Train Epoch: 4 [24320/110534 (22%)]\tClassification Loss: 1.6289\r\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tClassification Loss: 1.3614\r\n",
      "Train Epoch: 4 [25600/110534 (23%)]\tClassification Loss: 1.5619\r\n",
      "Train Epoch: 4 [26240/110534 (24%)]\tClassification Loss: 1.6421\r\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tClassification Loss: 1.5964\r\n",
      "Train Epoch: 4 [27520/110534 (25%)]\tClassification Loss: 1.6126\r\n",
      "Train Epoch: 4 [28160/110534 (25%)]\tClassification Loss: 1.4112\r\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tClassification Loss: 1.5267\r\n",
      "Train Epoch: 4 [29440/110534 (27%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 4 [30080/110534 (27%)]\tClassification Loss: 1.8417\r\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tClassification Loss: 1.4775\r\n",
      "Train Epoch: 4 [31360/110534 (28%)]\tClassification Loss: 1.3518\r\n",
      "Train Epoch: 4 [32000/110534 (29%)]\tClassification Loss: 1.4582\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_500.pth.tar\r\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tClassification Loss: 2.0341\r\n",
      "Train Epoch: 4 [33280/110534 (30%)]\tClassification Loss: 1.5929\r\n",
      "Train Epoch: 4 [33920/110534 (31%)]\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 4 [35200/110534 (32%)]\tClassification Loss: 1.3751\r\n",
      "Train Epoch: 4 [35840/110534 (32%)]\tClassification Loss: 1.5990\r\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tClassification Loss: 1.5414\r\n",
      "Train Epoch: 4 [37120/110534 (34%)]\tClassification Loss: 1.3251\r\n",
      "Train Epoch: 4 [37760/110534 (34%)]\tClassification Loss: 1.5147\r\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 4 [39040/110534 (35%)]\tClassification Loss: 1.1165\r\n",
      "Train Epoch: 4 [39680/110534 (36%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tClassification Loss: 1.6753\r\n",
      "Train Epoch: 4 [40960/110534 (37%)]\tClassification Loss: 1.6868\r\n",
      "Train Epoch: 4 [41600/110534 (38%)]\tClassification Loss: 1.5084\r\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tClassification Loss: 1.7943\r\n",
      "Train Epoch: 4 [42880/110534 (39%)]\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 4 [43520/110534 (39%)]\tClassification Loss: 1.6151\r\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tClassification Loss: 1.5971\r\n",
      "Train Epoch: 4 [44800/110534 (41%)]\tClassification Loss: 1.3505\r\n",
      "Train Epoch: 4 [45440/110534 (41%)]\tClassification Loss: 1.5332\r\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tClassification Loss: 1.6495\r\n",
      "Train Epoch: 4 [46720/110534 (42%)]\tClassification Loss: 1.4009\r\n",
      "Train Epoch: 4 [47360/110534 (43%)]\tClassification Loss: 1.4046\r\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tClassification Loss: 1.6561\r\n",
      "Train Epoch: 4 [48640/110534 (44%)]\tClassification Loss: 1.5909\r\n",
      "Train Epoch: 4 [49280/110534 (45%)]\tClassification Loss: 1.5730\r\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tClassification Loss: 1.3122\r\n",
      "Train Epoch: 4 [50560/110534 (46%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 4 [51200/110534 (46%)]\tClassification Loss: 1.2292\r\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tClassification Loss: 1.4441\r\n",
      "Train Epoch: 4 [52480/110534 (47%)]\tClassification Loss: 1.6126\r\n",
      "Train Epoch: 4 [53120/110534 (48%)]\tClassification Loss: 1.9010\r\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tClassification Loss: 1.4318\r\n",
      "Train Epoch: 4 [54400/110534 (49%)]\tClassification Loss: 1.7050\r\n",
      "Train Epoch: 4 [55040/110534 (50%)]\tClassification Loss: 1.7040\r\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tClassification Loss: 1.9009\r\n",
      "Train Epoch: 4 [56320/110534 (51%)]\tClassification Loss: 1.5137\r\n",
      "Train Epoch: 4 [56960/110534 (52%)]\tClassification Loss: 1.5497\r\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tClassification Loss: 1.6979\r\n",
      "Train Epoch: 4 [58240/110534 (53%)]\tClassification Loss: 1.6483\r\n",
      "Train Epoch: 4 [58880/110534 (53%)]\tClassification Loss: 1.2320\r\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tClassification Loss: 1.5473\r\n",
      "Train Epoch: 4 [60160/110534 (54%)]\tClassification Loss: 1.3477\r\n",
      "Train Epoch: 4 [60800/110534 (55%)]\tClassification Loss: 1.4235\r\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tClassification Loss: 1.4804\r\n",
      "Train Epoch: 4 [62080/110534 (56%)]\tClassification Loss: 1.2926\r\n",
      "Train Epoch: 4 [62720/110534 (57%)]\tClassification Loss: 1.8061\r\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tClassification Loss: 1.3966\r\n",
      "Train Epoch: 4 [64000/110534 (58%)]\tClassification Loss: 1.3075\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1000.pth.tar\r\n",
      "Train Epoch: 4 [64640/110534 (58%)]\tClassification Loss: 1.5364\r\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tClassification Loss: 1.2802\r\n",
      "Train Epoch: 4 [65920/110534 (60%)]\tClassification Loss: 1.5771\r\n",
      "Train Epoch: 4 [66560/110534 (60%)]\tClassification Loss: 1.4238\r\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tClassification Loss: 1.3224\r\n",
      "Train Epoch: 4 [67840/110534 (61%)]\tClassification Loss: 1.5674\r\n",
      "Train Epoch: 4 [68480/110534 (62%)]\tClassification Loss: 1.4461\r\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tClassification Loss: 1.2877\r\n",
      "Train Epoch: 4 [69760/110534 (63%)]\tClassification Loss: 1.7451\r\n",
      "Train Epoch: 4 [70400/110534 (64%)]\tClassification Loss: 1.5713\r\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 4 [71680/110534 (65%)]\tClassification Loss: 1.8266\r\n",
      "Train Epoch: 4 [72320/110534 (65%)]\tClassification Loss: 1.6222\r\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tClassification Loss: 1.3850\r\n",
      "Train Epoch: 4 [73600/110534 (67%)]\tClassification Loss: 1.3456\r\n",
      "Train Epoch: 4 [74240/110534 (67%)]\tClassification Loss: 1.3337\r\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tClassification Loss: 1.5194\r\n",
      "Train Epoch: 4 [75520/110534 (68%)]\tClassification Loss: 1.5012\r\n",
      "Train Epoch: 4 [76160/110534 (69%)]\tClassification Loss: 1.2623\r\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 4 [77440/110534 (70%)]\tClassification Loss: 1.6611\r\n",
      "Train Epoch: 4 [78080/110534 (71%)]\tClassification Loss: 1.6562\r\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tClassification Loss: 1.5953\r\n",
      "Train Epoch: 4 [79360/110534 (72%)]\tClassification Loss: 1.5666\r\n",
      "Train Epoch: 4 [80000/110534 (72%)]\tClassification Loss: 1.5694\r\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tClassification Loss: 1.3762\r\n",
      "Train Epoch: 4 [81280/110534 (74%)]\tClassification Loss: 1.4346\r\n",
      "Train Epoch: 4 [81920/110534 (74%)]\tClassification Loss: 1.8135\r\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tClassification Loss: 1.7774\r\n",
      "Train Epoch: 4 [83200/110534 (75%)]\tClassification Loss: 1.5817\r\n",
      "Train Epoch: 4 [83840/110534 (76%)]\tClassification Loss: 1.5136\r\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 4 [85120/110534 (77%)]\tClassification Loss: 1.4176\r\n",
      "Train Epoch: 4 [85760/110534 (78%)]\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 4 [87040/110534 (79%)]\tClassification Loss: 1.6702\r\n",
      "Train Epoch: 4 [87680/110534 (79%)]\tClassification Loss: 1.9719\r\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tClassification Loss: 1.5443\r\n",
      "Train Epoch: 4 [88960/110534 (80%)]\tClassification Loss: 1.3815\r\n",
      "Train Epoch: 4 [89600/110534 (81%)]\tClassification Loss: 1.5223\r\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tClassification Loss: 1.6016\r\n",
      "Train Epoch: 4 [90880/110534 (82%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 4 [91520/110534 (83%)]\tClassification Loss: 1.6177\r\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tClassification Loss: 1.5429\r\n",
      "Train Epoch: 4 [92800/110534 (84%)]\tClassification Loss: 1.3486\r\n",
      "Train Epoch: 4 [93440/110534 (85%)]\tClassification Loss: 1.8430\r\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tClassification Loss: 1.5020\r\n",
      "Train Epoch: 4 [94720/110534 (86%)]\tClassification Loss: 1.4353\r\n",
      "Train Epoch: 4 [95360/110534 (86%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tClassification Loss: 1.4265\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [96640/110534 (87%)]\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 4 [97280/110534 (88%)]\tClassification Loss: 1.4196\r\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tClassification Loss: 1.7756\r\n",
      "Train Epoch: 4 [98560/110534 (89%)]\tClassification Loss: 1.7008\r\n",
      "Train Epoch: 4 [99200/110534 (90%)]\tClassification Loss: 1.2241\r\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 4 [100480/110534 (91%)]\tClassification Loss: 1.4166\r\n",
      "Train Epoch: 4 [101120/110534 (91%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tClassification Loss: 1.2067\r\n",
      "Train Epoch: 4 [102400/110534 (93%)]\tClassification Loss: 1.3464\r\n",
      "Train Epoch: 4 [103040/110534 (93%)]\tClassification Loss: 1.3954\r\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tClassification Loss: 1.5258\r\n",
      "Train Epoch: 4 [104320/110534 (94%)]\tClassification Loss: 1.5325\r\n",
      "Train Epoch: 4 [104960/110534 (95%)]\tClassification Loss: 1.7772\r\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tClassification Loss: 1.4839\r\n",
      "Train Epoch: 4 [106240/110534 (96%)]\tClassification Loss: 1.4135\r\n",
      "Train Epoch: 4 [106880/110534 (97%)]\tClassification Loss: 1.6547\r\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tClassification Loss: 1.6663\r\n",
      "Train Epoch: 4 [108160/110534 (98%)]\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 4 [108800/110534 (98%)]\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tClassification Loss: 1.5077\r\n",
      "Train Epoch: 4 [110080/110534 (100%)]\tClassification Loss: 1.8641\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/110534 (0%)]\tClassification Loss: 1.9265\r\n",
      "\r\n",
      "Test set: Average loss: 1.4230, Accuracy: 23211/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 5 [640/110534 (1%)]\tClassification Loss: 1.5368\r\n",
      "Train Epoch: 5 [1280/110534 (1%)]\tClassification Loss: 1.3051\r\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tClassification Loss: 1.6808\r\n",
      "Train Epoch: 5 [2560/110534 (2%)]\tClassification Loss: 1.5185\r\n",
      "Train Epoch: 5 [3200/110534 (3%)]\tClassification Loss: 1.4442\r\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tClassification Loss: 1.7278\r\n",
      "Train Epoch: 5 [4480/110534 (4%)]\tClassification Loss: 1.2457\r\n",
      "Train Epoch: 5 [5120/110534 (5%)]\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 5 [6400/110534 (6%)]\tClassification Loss: 1.7049\r\n",
      "Train Epoch: 5 [7040/110534 (6%)]\tClassification Loss: 1.3345\r\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 5 [8320/110534 (8%)]\tClassification Loss: 1.8461\r\n",
      "Train Epoch: 5 [8960/110534 (8%)]\tClassification Loss: 1.4428\r\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 5 [10240/110534 (9%)]\tClassification Loss: 1.3950\r\n",
      "Train Epoch: 5 [10880/110534 (10%)]\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 5 [12160/110534 (11%)]\tClassification Loss: 1.4828\r\n",
      "Train Epoch: 5 [12800/110534 (12%)]\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tClassification Loss: 1.7664\r\n",
      "Train Epoch: 5 [14080/110534 (13%)]\tClassification Loss: 1.3597\r\n",
      "Train Epoch: 5 [14720/110534 (13%)]\tClassification Loss: 1.3579\r\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tClassification Loss: 1.3246\r\n",
      "Train Epoch: 5 [16000/110534 (14%)]\tClassification Loss: 1.5524\r\n",
      "Train Epoch: 5 [16640/110534 (15%)]\tClassification Loss: 1.5051\r\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tClassification Loss: 1.6154\r\n",
      "Train Epoch: 5 [17920/110534 (16%)]\tClassification Loss: 1.7091\r\n",
      "Train Epoch: 5 [18560/110534 (17%)]\tClassification Loss: 1.5273\r\n",
      "Train Epoch: 5 [19200/110534 (17%)]\tClassification Loss: 1.4625\r\n",
      "Train Epoch: 5 [19840/110534 (18%)]\tClassification Loss: 1.3436\r\n",
      "Train Epoch: 5 [20480/110534 (19%)]\tClassification Loss: 1.1980\r\n",
      "Train Epoch: 5 [21120/110534 (19%)]\tClassification Loss: 1.4496\r\n",
      "Train Epoch: 5 [21760/110534 (20%)]\tClassification Loss: 1.8233\r\n",
      "Train Epoch: 5 [22400/110534 (20%)]\tClassification Loss: 1.6356\r\n",
      "Train Epoch: 5 [23040/110534 (21%)]\tClassification Loss: 1.3828\r\n",
      "Train Epoch: 5 [23680/110534 (21%)]\tClassification Loss: 1.3556\r\n",
      "Train Epoch: 5 [24320/110534 (22%)]\tClassification Loss: 1.8927\r\n",
      "Train Epoch: 5 [24960/110534 (23%)]\tClassification Loss: 1.4425\r\n",
      "Train Epoch: 5 [25600/110534 (23%)]\tClassification Loss: 1.6171\r\n",
      "Train Epoch: 5 [26240/110534 (24%)]\tClassification Loss: 1.7657\r\n",
      "Train Epoch: 5 [26880/110534 (24%)]\tClassification Loss: 1.4883\r\n",
      "Train Epoch: 5 [27520/110534 (25%)]\tClassification Loss: 1.6607\r\n",
      "Train Epoch: 5 [28160/110534 (25%)]\tClassification Loss: 1.5108\r\n",
      "Train Epoch: 5 [28800/110534 (26%)]\tClassification Loss: 1.5817\r\n",
      "Train Epoch: 5 [29440/110534 (27%)]\tClassification Loss: 1.5836\r\n",
      "Train Epoch: 5 [30080/110534 (27%)]\tClassification Loss: 1.6556\r\n",
      "Train Epoch: 5 [30720/110534 (28%)]\tClassification Loss: 1.3781\r\n",
      "Train Epoch: 5 [31360/110534 (28%)]\tClassification Loss: 1.3489\r\n",
      "Train Epoch: 5 [32000/110534 (29%)]\tClassification Loss: 1.4636\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_500.pth.tar\r\n",
      "Train Epoch: 5 [32640/110534 (30%)]\tClassification Loss: 2.0362\r\n",
      "Train Epoch: 5 [33280/110534 (30%)]\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 5 [33920/110534 (31%)]\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 5 [34560/110534 (31%)]\tClassification Loss: 1.3647\r\n",
      "Train Epoch: 5 [35200/110534 (32%)]\tClassification Loss: 1.3995\r\n",
      "Train Epoch: 5 [35840/110534 (32%)]\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 5 [36480/110534 (33%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 5 [37120/110534 (34%)]\tClassification Loss: 1.2288\r\n",
      "Train Epoch: 5 [37760/110534 (34%)]\tClassification Loss: 1.5733\r\n",
      "Train Epoch: 5 [38400/110534 (35%)]\tClassification Loss: 1.4474\r\n",
      "Train Epoch: 5 [39040/110534 (35%)]\tClassification Loss: 1.1416\r\n",
      "Train Epoch: 5 [39680/110534 (36%)]\tClassification Loss: 1.4680\r\n",
      "Train Epoch: 5 [40320/110534 (36%)]\tClassification Loss: 1.5544\r\n",
      "Train Epoch: 5 [40960/110534 (37%)]\tClassification Loss: 1.7929\r\n",
      "Train Epoch: 5 [41600/110534 (38%)]\tClassification Loss: 1.5358\r\n",
      "Train Epoch: 5 [42240/110534 (38%)]\tClassification Loss: 1.6176\r\n",
      "Train Epoch: 5 [42880/110534 (39%)]\tClassification Loss: 1.4183\r\n",
      "Train Epoch: 5 [43520/110534 (39%)]\tClassification Loss: 1.5232\r\n",
      "Train Epoch: 5 [44160/110534 (40%)]\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 5 [44800/110534 (41%)]\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 5 [45440/110534 (41%)]\tClassification Loss: 1.5689\r\n",
      "Train Epoch: 5 [46080/110534 (42%)]\tClassification Loss: 1.7084\r\n",
      "Train Epoch: 5 [46720/110534 (42%)]\tClassification Loss: 1.4191\r\n",
      "Train Epoch: 5 [47360/110534 (43%)]\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 5 [48000/110534 (43%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 5 [48640/110534 (44%)]\tClassification Loss: 1.6418\r\n",
      "Train Epoch: 5 [49280/110534 (45%)]\tClassification Loss: 1.4203\r\n",
      "Train Epoch: 5 [49920/110534 (45%)]\tClassification Loss: 1.2199\r\n",
      "Train Epoch: 5 [50560/110534 (46%)]\tClassification Loss: 1.6287\r\n",
      "Train Epoch: 5 [51200/110534 (46%)]\tClassification Loss: 1.3111\r\n",
      "Train Epoch: 5 [51840/110534 (47%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 5 [52480/110534 (47%)]\tClassification Loss: 1.4316\r\n",
      "Train Epoch: 5 [53120/110534 (48%)]\tClassification Loss: 1.8435\r\n",
      "Train Epoch: 5 [53760/110534 (49%)]\tClassification Loss: 1.4160\r\n",
      "Train Epoch: 5 [54400/110534 (49%)]\tClassification Loss: 1.7743\r\n",
      "Train Epoch: 5 [55040/110534 (50%)]\tClassification Loss: 1.7828\r\n",
      "Train Epoch: 5 [55680/110534 (50%)]\tClassification Loss: 1.9641\r\n",
      "Train Epoch: 5 [56320/110534 (51%)]\tClassification Loss: 1.3431\r\n",
      "Train Epoch: 5 [56960/110534 (52%)]\tClassification Loss: 1.6367\r\n",
      "Train Epoch: 5 [57600/110534 (52%)]\tClassification Loss: 1.7889\r\n",
      "Train Epoch: 5 [58240/110534 (53%)]\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 5 [58880/110534 (53%)]\tClassification Loss: 1.2266\r\n",
      "Train Epoch: 5 [59520/110534 (54%)]\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 5 [60160/110534 (54%)]\tClassification Loss: 1.2775\r\n",
      "Train Epoch: 5 [60800/110534 (55%)]\tClassification Loss: 1.4677\r\n",
      "Train Epoch: 5 [61440/110534 (56%)]\tClassification Loss: 1.5922\r\n",
      "Train Epoch: 5 [62080/110534 (56%)]\tClassification Loss: 1.4693\r\n",
      "Train Epoch: 5 [62720/110534 (57%)]\tClassification Loss: 1.5751\r\n",
      "Train Epoch: 5 [63360/110534 (57%)]\tClassification Loss: 1.6336\r\n",
      "Train Epoch: 5 [64000/110534 (58%)]\tClassification Loss: 1.2738\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1000.pth.tar\r\n",
      "Train Epoch: 5 [64640/110534 (58%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 5 [65280/110534 (59%)]\tClassification Loss: 1.3138\r\n",
      "Train Epoch: 5 [65920/110534 (60%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 5 [66560/110534 (60%)]\tClassification Loss: 1.4180\r\n",
      "Train Epoch: 5 [67200/110534 (61%)]\tClassification Loss: 1.1912\r\n",
      "Train Epoch: 5 [67840/110534 (61%)]\tClassification Loss: 1.5622\r\n",
      "Train Epoch: 5 [68480/110534 (62%)]\tClassification Loss: 1.5324\r\n",
      "Train Epoch: 5 [69120/110534 (63%)]\tClassification Loss: 1.2356\r\n",
      "Train Epoch: 5 [69760/110534 (63%)]\tClassification Loss: 1.6920\r\n",
      "Train Epoch: 5 [70400/110534 (64%)]\tClassification Loss: 1.3150\r\n",
      "Train Epoch: 5 [71040/110534 (64%)]\tClassification Loss: 1.6479\r\n",
      "Train Epoch: 5 [71680/110534 (65%)]\tClassification Loss: 1.9281\r\n",
      "Train Epoch: 5 [72320/110534 (65%)]\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 5 [72960/110534 (66%)]\tClassification Loss: 1.4854\r\n",
      "Train Epoch: 5 [73600/110534 (67%)]\tClassification Loss: 1.3230\r\n",
      "Train Epoch: 5 [74240/110534 (67%)]\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 5 [74880/110534 (68%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 5 [75520/110534 (68%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 5 [76160/110534 (69%)]\tClassification Loss: 1.2109\r\n",
      "Train Epoch: 5 [76800/110534 (69%)]\tClassification Loss: 1.5144\r\n",
      "Train Epoch: 5 [77440/110534 (70%)]\tClassification Loss: 1.6027\r\n",
      "Train Epoch: 5 [78080/110534 (71%)]\tClassification Loss: 1.6703\r\n",
      "Train Epoch: 5 [78720/110534 (71%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 5 [79360/110534 (72%)]\tClassification Loss: 1.5255\r\n",
      "Train Epoch: 5 [80000/110534 (72%)]\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 5 [80640/110534 (73%)]\tClassification Loss: 1.4224\r\n",
      "Train Epoch: 5 [81280/110534 (74%)]\tClassification Loss: 1.2959\r\n",
      "Train Epoch: 5 [81920/110534 (74%)]\tClassification Loss: 1.7363\r\n",
      "Train Epoch: 5 [82560/110534 (75%)]\tClassification Loss: 1.8227\r\n",
      "Train Epoch: 5 [83200/110534 (75%)]\tClassification Loss: 1.6784\r\n",
      "Train Epoch: 5 [83840/110534 (76%)]\tClassification Loss: 1.4911\r\n",
      "Train Epoch: 5 [84480/110534 (76%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 5 [85120/110534 (77%)]\tClassification Loss: 1.5791\r\n",
      "Train Epoch: 5 [85760/110534 (78%)]\tClassification Loss: 1.4877\r\n",
      "Train Epoch: 5 [86400/110534 (78%)]\tClassification Loss: 1.3182\r\n",
      "Train Epoch: 5 [87040/110534 (79%)]\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 5 [87680/110534 (79%)]\tClassification Loss: 1.9873\r\n",
      "Train Epoch: 5 [88320/110534 (80%)]\tClassification Loss: 1.3747\r\n",
      "Train Epoch: 5 [88960/110534 (80%)]\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 5 [89600/110534 (81%)]\tClassification Loss: 1.5966\r\n",
      "Train Epoch: 5 [90240/110534 (82%)]\tClassification Loss: 1.5757\r\n",
      "Train Epoch: 5 [90880/110534 (82%)]\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 5 [91520/110534 (83%)]\tClassification Loss: 1.6687\r\n",
      "Train Epoch: 5 [92160/110534 (83%)]\tClassification Loss: 1.3636\r\n",
      "Train Epoch: 5 [92800/110534 (84%)]\tClassification Loss: 1.3561\r\n",
      "Train Epoch: 5 [93440/110534 (85%)]\tClassification Loss: 1.8007\r\n",
      "Train Epoch: 5 [94080/110534 (85%)]\tClassification Loss: 1.3943\r\n",
      "Train Epoch: 5 [94720/110534 (86%)]\tClassification Loss: 1.4841\r\n",
      "Train Epoch: 5 [95360/110534 (86%)]\tClassification Loss: 1.4430\r\n",
      "Train Epoch: 5 [96000/110534 (87%)]\tClassification Loss: 1.6016\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [96640/110534 (87%)]\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 5 [97280/110534 (88%)]\tClassification Loss: 1.4412\r\n",
      "Train Epoch: 5 [97920/110534 (89%)]\tClassification Loss: 1.6507\r\n",
      "Train Epoch: 5 [98560/110534 (89%)]\tClassification Loss: 1.6530\r\n",
      "Train Epoch: 5 [99200/110534 (90%)]\tClassification Loss: 1.1232\r\n",
      "Train Epoch: 5 [99840/110534 (90%)]\tClassification Loss: 1.4915\r\n",
      "Train Epoch: 5 [100480/110534 (91%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 5 [101120/110534 (91%)]\tClassification Loss: 1.6652\r\n",
      "Train Epoch: 5 [101760/110534 (92%)]\tClassification Loss: 1.3539\r\n",
      "Train Epoch: 5 [102400/110534 (93%)]\tClassification Loss: 1.2734\r\n",
      "Train Epoch: 5 [103040/110534 (93%)]\tClassification Loss: 1.4224\r\n",
      "Train Epoch: 5 [103680/110534 (94%)]\tClassification Loss: 1.6483\r\n",
      "Train Epoch: 5 [104320/110534 (94%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 5 [104960/110534 (95%)]\tClassification Loss: 1.6232\r\n",
      "Train Epoch: 5 [105600/110534 (96%)]\tClassification Loss: 1.3947\r\n",
      "Train Epoch: 5 [106240/110534 (96%)]\tClassification Loss: 1.3146\r\n",
      "Train Epoch: 5 [106880/110534 (97%)]\tClassification Loss: 1.6862\r\n",
      "Train Epoch: 5 [107520/110534 (97%)]\tClassification Loss: 1.6502\r\n",
      "Train Epoch: 5 [108160/110534 (98%)]\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 5 [108800/110534 (98%)]\tClassification Loss: 1.3553\r\n",
      "Train Epoch: 5 [109440/110534 (99%)]\tClassification Loss: 1.6916\r\n",
      "Train Epoch: 5 [110080/110534 (100%)]\tClassification Loss: 1.6585\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/110534 (0%)]\tClassification Loss: 1.9336\r\n",
      "\r\n",
      "Test set: Average loss: 1.4187, Accuracy: 23161/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 6 [640/110534 (1%)]\tClassification Loss: 1.5372\r\n",
      "Train Epoch: 6 [1280/110534 (1%)]\tClassification Loss: 1.3690\r\n",
      "Train Epoch: 6 [1920/110534 (2%)]\tClassification Loss: 1.7107\r\n",
      "Train Epoch: 6 [2560/110534 (2%)]\tClassification Loss: 1.4957\r\n",
      "Train Epoch: 6 [3200/110534 (3%)]\tClassification Loss: 1.4192\r\n",
      "Train Epoch: 6 [3840/110534 (3%)]\tClassification Loss: 1.7990\r\n",
      "Train Epoch: 6 [4480/110534 (4%)]\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 6 [5120/110534 (5%)]\tClassification Loss: 1.6926\r\n",
      "Train Epoch: 6 [5760/110534 (5%)]\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 6 [6400/110534 (6%)]\tClassification Loss: 1.6133\r\n",
      "Train Epoch: 6 [7040/110534 (6%)]\tClassification Loss: 1.4035\r\n",
      "Train Epoch: 6 [7680/110534 (7%)]\tClassification Loss: 1.6318\r\n",
      "Train Epoch: 6 [8320/110534 (8%)]\tClassification Loss: 1.8391\r\n",
      "Train Epoch: 6 [8960/110534 (8%)]\tClassification Loss: 1.5742\r\n",
      "Train Epoch: 6 [9600/110534 (9%)]\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 6 [10240/110534 (9%)]\tClassification Loss: 1.4252\r\n",
      "Train Epoch: 6 [10880/110534 (10%)]\tClassification Loss: 1.3733\r\n",
      "Train Epoch: 6 [11520/110534 (10%)]\tClassification Loss: 1.4754\r\n",
      "Train Epoch: 6 [12160/110534 (11%)]\tClassification Loss: 1.5168\r\n",
      "Train Epoch: 6 [12800/110534 (12%)]\tClassification Loss: 1.5416\r\n",
      "Train Epoch: 6 [13440/110534 (12%)]\tClassification Loss: 1.8048\r\n",
      "Train Epoch: 6 [14080/110534 (13%)]\tClassification Loss: 1.3792\r\n",
      "Train Epoch: 6 [14720/110534 (13%)]\tClassification Loss: 1.4381\r\n",
      "Train Epoch: 6 [15360/110534 (14%)]\tClassification Loss: 1.2676\r\n",
      "Train Epoch: 6 [16000/110534 (14%)]\tClassification Loss: 1.6734\r\n",
      "Train Epoch: 6 [16640/110534 (15%)]\tClassification Loss: 1.6440\r\n",
      "Train Epoch: 6 [17280/110534 (16%)]\tClassification Loss: 1.6547\r\n",
      "Train Epoch: 6 [17920/110534 (16%)]\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 6 [18560/110534 (17%)]\tClassification Loss: 1.4463\r\n",
      "Train Epoch: 6 [19200/110534 (17%)]\tClassification Loss: 1.3846\r\n",
      "Train Epoch: 6 [19840/110534 (18%)]\tClassification Loss: 1.2084\r\n",
      "Train Epoch: 6 [20480/110534 (19%)]\tClassification Loss: 1.1688\r\n",
      "Train Epoch: 6 [21120/110534 (19%)]\tClassification Loss: 1.6048\r\n",
      "Train Epoch: 6 [21760/110534 (20%)]\tClassification Loss: 1.5759\r\n",
      "Train Epoch: 6 [22400/110534 (20%)]\tClassification Loss: 1.5309\r\n",
      "Train Epoch: 6 [23040/110534 (21%)]\tClassification Loss: 1.3531\r\n",
      "Train Epoch: 6 [23680/110534 (21%)]\tClassification Loss: 1.4354\r\n",
      "Train Epoch: 6 [24320/110534 (22%)]\tClassification Loss: 1.5888\r\n",
      "Train Epoch: 6 [24960/110534 (23%)]\tClassification Loss: 1.4985\r\n",
      "Train Epoch: 6 [25600/110534 (23%)]\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 6 [26240/110534 (24%)]\tClassification Loss: 1.7214\r\n",
      "Train Epoch: 6 [26880/110534 (24%)]\tClassification Loss: 1.4358\r\n",
      "Train Epoch: 6 [27520/110534 (25%)]\tClassification Loss: 1.7504\r\n",
      "Train Epoch: 6 [28160/110534 (25%)]\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 6 [28800/110534 (26%)]\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 6 [29440/110534 (27%)]\tClassification Loss: 1.6952\r\n",
      "Train Epoch: 6 [30080/110534 (27%)]\tClassification Loss: 1.6647\r\n",
      "Train Epoch: 6 [30720/110534 (28%)]\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 6 [31360/110534 (28%)]\tClassification Loss: 1.4004\r\n",
      "Train Epoch: 6 [32000/110534 (29%)]\tClassification Loss: 1.5156\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_500.pth.tar\r\n",
      "Train Epoch: 6 [32640/110534 (30%)]\tClassification Loss: 2.0178\r\n",
      "Train Epoch: 6 [33280/110534 (30%)]\tClassification Loss: 1.5425\r\n",
      "Train Epoch: 6 [33920/110534 (31%)]\tClassification Loss: 1.6313\r\n",
      "Train Epoch: 6 [34560/110534 (31%)]\tClassification Loss: 1.6344\r\n",
      "Train Epoch: 6 [35200/110534 (32%)]\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 6 [35840/110534 (32%)]\tClassification Loss: 1.6548\r\n",
      "Train Epoch: 6 [36480/110534 (33%)]\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 6 [37120/110534 (34%)]\tClassification Loss: 1.1793\r\n",
      "Train Epoch: 6 [37760/110534 (34%)]\tClassification Loss: 1.6802\r\n",
      "Train Epoch: 6 [38400/110534 (35%)]\tClassification Loss: 1.6052\r\n",
      "Train Epoch: 6 [39040/110534 (35%)]\tClassification Loss: 1.1944\r\n",
      "Train Epoch: 6 [39680/110534 (36%)]\tClassification Loss: 1.4589\r\n",
      "Train Epoch: 6 [40320/110534 (36%)]\tClassification Loss: 1.6797\r\n",
      "Train Epoch: 6 [40960/110534 (37%)]\tClassification Loss: 1.6695\r\n",
      "Train Epoch: 6 [41600/110534 (38%)]\tClassification Loss: 1.6079\r\n",
      "Train Epoch: 6 [42240/110534 (38%)]\tClassification Loss: 1.7658\r\n",
      "Train Epoch: 6 [42880/110534 (39%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 6 [43520/110534 (39%)]\tClassification Loss: 1.4826\r\n",
      "Train Epoch: 6 [44160/110534 (40%)]\tClassification Loss: 1.4011\r\n",
      "Train Epoch: 6 [44800/110534 (41%)]\tClassification Loss: 1.4061\r\n",
      "Train Epoch: 6 [45440/110534 (41%)]\tClassification Loss: 1.3802\r\n",
      "Train Epoch: 6 [46080/110534 (42%)]\tClassification Loss: 1.5868\r\n",
      "Train Epoch: 6 [46720/110534 (42%)]\tClassification Loss: 1.3395\r\n",
      "Train Epoch: 6 [47360/110534 (43%)]\tClassification Loss: 1.5472\r\n",
      "Train Epoch: 6 [48000/110534 (43%)]\tClassification Loss: 1.5096\r\n",
      "Train Epoch: 6 [48640/110534 (44%)]\tClassification Loss: 1.5856\r\n",
      "Train Epoch: 6 [49280/110534 (45%)]\tClassification Loss: 1.5965\r\n",
      "Train Epoch: 6 [49920/110534 (45%)]\tClassification Loss: 1.1784\r\n",
      "Train Epoch: 6 [50560/110534 (46%)]\tClassification Loss: 1.6495\r\n",
      "Train Epoch: 6 [51200/110534 (46%)]\tClassification Loss: 1.1648\r\n",
      "Train Epoch: 6 [51840/110534 (47%)]\tClassification Loss: 1.5414\r\n",
      "Train Epoch: 6 [52480/110534 (47%)]\tClassification Loss: 1.6279\r\n",
      "Train Epoch: 6 [53120/110534 (48%)]\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 6 [53760/110534 (49%)]\tClassification Loss: 1.4650\r\n",
      "Train Epoch: 6 [54400/110534 (49%)]\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 6 [55040/110534 (50%)]\tClassification Loss: 1.6138\r\n",
      "Train Epoch: 6 [55680/110534 (50%)]\tClassification Loss: 1.9782\r\n",
      "Train Epoch: 6 [56320/110534 (51%)]\tClassification Loss: 1.4378\r\n",
      "Train Epoch: 6 [56960/110534 (52%)]\tClassification Loss: 1.5852\r\n",
      "Train Epoch: 6 [57600/110534 (52%)]\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 6 [58240/110534 (53%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 6 [58880/110534 (53%)]\tClassification Loss: 1.1907\r\n",
      "Train Epoch: 6 [59520/110534 (54%)]\tClassification Loss: 1.5647\r\n",
      "Train Epoch: 6 [60160/110534 (54%)]\tClassification Loss: 1.3698\r\n",
      "Train Epoch: 6 [60800/110534 (55%)]\tClassification Loss: 1.5056\r\n",
      "Train Epoch: 6 [61440/110534 (56%)]\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 6 [62080/110534 (56%)]\tClassification Loss: 1.5509\r\n",
      "Train Epoch: 6 [62720/110534 (57%)]\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 6 [63360/110534 (57%)]\tClassification Loss: 1.5528\r\n",
      "Train Epoch: 6 [64000/110534 (58%)]\tClassification Loss: 1.3646\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1000.pth.tar\r\n",
      "Train Epoch: 6 [64640/110534 (58%)]\tClassification Loss: 1.5864\r\n",
      "Train Epoch: 6 [65280/110534 (59%)]\tClassification Loss: 1.5234\r\n",
      "Train Epoch: 6 [65920/110534 (60%)]\tClassification Loss: 1.5880\r\n",
      "Train Epoch: 6 [66560/110534 (60%)]\tClassification Loss: 1.5131\r\n",
      "Train Epoch: 6 [67200/110534 (61%)]\tClassification Loss: 1.3950\r\n",
      "Train Epoch: 6 [67840/110534 (61%)]\tClassification Loss: 1.4986\r\n",
      "Train Epoch: 6 [68480/110534 (62%)]\tClassification Loss: 1.5528\r\n",
      "Train Epoch: 6 [69120/110534 (63%)]\tClassification Loss: 1.1774\r\n",
      "Train Epoch: 6 [69760/110534 (63%)]\tClassification Loss: 1.6699\r\n",
      "Train Epoch: 6 [70400/110534 (64%)]\tClassification Loss: 1.3575\r\n",
      "Train Epoch: 6 [71040/110534 (64%)]\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 6 [71680/110534 (65%)]\tClassification Loss: 1.6934\r\n",
      "Train Epoch: 6 [72320/110534 (65%)]\tClassification Loss: 1.7004\r\n",
      "Train Epoch: 6 [72960/110534 (66%)]\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 6 [73600/110534 (67%)]\tClassification Loss: 1.3912\r\n",
      "Train Epoch: 6 [74240/110534 (67%)]\tClassification Loss: 1.3586\r\n",
      "Train Epoch: 6 [74880/110534 (68%)]\tClassification Loss: 1.6628\r\n",
      "Train Epoch: 6 [75520/110534 (68%)]\tClassification Loss: 1.4796\r\n",
      "Train Epoch: 6 [76160/110534 (69%)]\tClassification Loss: 1.2685\r\n",
      "Train Epoch: 6 [76800/110534 (69%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 6 [77440/110534 (70%)]\tClassification Loss: 1.6154\r\n",
      "Train Epoch: 6 [78080/110534 (71%)]\tClassification Loss: 1.5624\r\n",
      "Train Epoch: 6 [78720/110534 (71%)]\tClassification Loss: 1.5101\r\n",
      "Train Epoch: 6 [79360/110534 (72%)]\tClassification Loss: 1.6658\r\n",
      "Train Epoch: 6 [80000/110534 (72%)]\tClassification Loss: 1.6654\r\n",
      "Train Epoch: 6 [80640/110534 (73%)]\tClassification Loss: 1.4787\r\n",
      "Train Epoch: 6 [81280/110534 (74%)]\tClassification Loss: 1.2939\r\n",
      "Train Epoch: 6 [81920/110534 (74%)]\tClassification Loss: 1.7357\r\n",
      "Train Epoch: 6 [82560/110534 (75%)]\tClassification Loss: 1.8179\r\n",
      "Train Epoch: 6 [83200/110534 (75%)]\tClassification Loss: 1.7705\r\n",
      "Train Epoch: 6 [83840/110534 (76%)]\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 6 [84480/110534 (76%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 6 [85120/110534 (77%)]\tClassification Loss: 1.4823\r\n",
      "Train Epoch: 6 [85760/110534 (78%)]\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 6 [86400/110534 (78%)]\tClassification Loss: 1.3680\r\n",
      "Train Epoch: 6 [87040/110534 (79%)]\tClassification Loss: 1.5562\r\n",
      "Train Epoch: 6 [87680/110534 (79%)]\tClassification Loss: 1.8414\r\n",
      "Train Epoch: 6 [88320/110534 (80%)]\tClassification Loss: 1.3955\r\n",
      "Train Epoch: 6 [88960/110534 (80%)]\tClassification Loss: 1.2636\r\n",
      "Train Epoch: 6 [89600/110534 (81%)]\tClassification Loss: 1.4693\r\n",
      "Train Epoch: 6 [90240/110534 (82%)]\tClassification Loss: 1.5550\r\n",
      "Train Epoch: 6 [90880/110534 (82%)]\tClassification Loss: 1.6112\r\n",
      "Train Epoch: 6 [91520/110534 (83%)]\tClassification Loss: 1.6832\r\n",
      "Train Epoch: 6 [92160/110534 (83%)]\tClassification Loss: 1.3922\r\n",
      "Train Epoch: 6 [92800/110534 (84%)]\tClassification Loss: 1.3318\r\n",
      "Train Epoch: 6 [93440/110534 (85%)]\tClassification Loss: 1.6641\r\n",
      "Train Epoch: 6 [94080/110534 (85%)]\tClassification Loss: 1.4869\r\n",
      "Train Epoch: 6 [94720/110534 (86%)]\tClassification Loss: 1.4080\r\n",
      "Train Epoch: 6 [95360/110534 (86%)]\tClassification Loss: 1.4759\r\n",
      "Train Epoch: 6 [96000/110534 (87%)]\tClassification Loss: 1.4363\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [96640/110534 (87%)]\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 6 [97280/110534 (88%)]\tClassification Loss: 1.6604\r\n",
      "Train Epoch: 6 [97920/110534 (89%)]\tClassification Loss: 1.6666\r\n",
      "Train Epoch: 6 [98560/110534 (89%)]\tClassification Loss: 1.6085\r\n",
      "Train Epoch: 6 [99200/110534 (90%)]\tClassification Loss: 1.1631\r\n",
      "Train Epoch: 6 [99840/110534 (90%)]\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 6 [100480/110534 (91%)]\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 6 [101120/110534 (91%)]\tClassification Loss: 1.6874\r\n",
      "Train Epoch: 6 [101760/110534 (92%)]\tClassification Loss: 1.2808\r\n",
      "Train Epoch: 6 [102400/110534 (93%)]\tClassification Loss: 1.3520\r\n",
      "Train Epoch: 6 [103040/110534 (93%)]\tClassification Loss: 1.2807\r\n",
      "Train Epoch: 6 [103680/110534 (94%)]\tClassification Loss: 1.6918\r\n",
      "Train Epoch: 6 [104320/110534 (94%)]\tClassification Loss: 1.5577\r\n",
      "Train Epoch: 6 [104960/110534 (95%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 6 [105600/110534 (96%)]\tClassification Loss: 1.6145\r\n",
      "Train Epoch: 6 [106240/110534 (96%)]\tClassification Loss: 1.5997\r\n",
      "Train Epoch: 6 [106880/110534 (97%)]\tClassification Loss: 1.6870\r\n",
      "Train Epoch: 6 [107520/110534 (97%)]\tClassification Loss: 1.4924\r\n",
      "Train Epoch: 6 [108160/110534 (98%)]\tClassification Loss: 1.3739\r\n",
      "Train Epoch: 6 [108800/110534 (98%)]\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 6 [109440/110534 (99%)]\tClassification Loss: 1.6844\r\n",
      "Train Epoch: 6 [110080/110534 (100%)]\tClassification Loss: 1.7844\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_final.pth.tar\r\n",
      "Train Epoch: 7 [0/110534 (0%)]\tClassification Loss: 1.8691\r\n",
      "\r\n",
      "Test set: Average loss: 1.4111, Accuracy: 23258/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 7 [640/110534 (1%)]\tClassification Loss: 1.4310\r\n",
      "Train Epoch: 7 [1280/110534 (1%)]\tClassification Loss: 1.2534\r\n",
      "Train Epoch: 7 [1920/110534 (2%)]\tClassification Loss: 1.7544\r\n",
      "Train Epoch: 7 [2560/110534 (2%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 7 [3200/110534 (3%)]\tClassification Loss: 1.5224\r\n",
      "Train Epoch: 7 [3840/110534 (3%)]\tClassification Loss: 1.7278\r\n",
      "Train Epoch: 7 [4480/110534 (4%)]\tClassification Loss: 1.4672\r\n",
      "Train Epoch: 7 [5120/110534 (5%)]\tClassification Loss: 1.7231\r\n",
      "Train Epoch: 7 [5760/110534 (5%)]\tClassification Loss: 1.5933\r\n",
      "Train Epoch: 7 [6400/110534 (6%)]\tClassification Loss: 1.6767\r\n",
      "Train Epoch: 7 [7040/110534 (6%)]\tClassification Loss: 1.4078\r\n",
      "Train Epoch: 7 [7680/110534 (7%)]\tClassification Loss: 1.5557\r\n",
      "Train Epoch: 7 [8320/110534 (8%)]\tClassification Loss: 1.7110\r\n",
      "Train Epoch: 7 [8960/110534 (8%)]\tClassification Loss: 1.4736\r\n",
      "Train Epoch: 7 [9600/110534 (9%)]\tClassification Loss: 1.5532\r\n",
      "Train Epoch: 7 [10240/110534 (9%)]\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 7 [10880/110534 (10%)]\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 7 [11520/110534 (10%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 7 [12160/110534 (11%)]\tClassification Loss: 1.2518\r\n",
      "Train Epoch: 7 [12800/110534 (12%)]\tClassification Loss: 1.5819\r\n",
      "Train Epoch: 7 [13440/110534 (12%)]\tClassification Loss: 1.8715\r\n",
      "Train Epoch: 7 [14080/110534 (13%)]\tClassification Loss: 1.3942\r\n",
      "Train Epoch: 7 [14720/110534 (13%)]\tClassification Loss: 1.2683\r\n",
      "Train Epoch: 7 [15360/110534 (14%)]\tClassification Loss: 1.2747\r\n",
      "Train Epoch: 7 [16000/110534 (14%)]\tClassification Loss: 1.5695\r\n",
      "Train Epoch: 7 [16640/110534 (15%)]\tClassification Loss: 1.3011\r\n",
      "Train Epoch: 7 [17280/110534 (16%)]\tClassification Loss: 1.6353\r\n",
      "Train Epoch: 7 [17920/110534 (16%)]\tClassification Loss: 1.7753\r\n",
      "Train Epoch: 7 [18560/110534 (17%)]\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 7 [19200/110534 (17%)]\tClassification Loss: 1.4452\r\n",
      "Train Epoch: 7 [19840/110534 (18%)]\tClassification Loss: 1.3148\r\n",
      "Train Epoch: 7 [20480/110534 (19%)]\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 7 [21120/110534 (19%)]\tClassification Loss: 1.4738\r\n",
      "Train Epoch: 7 [21760/110534 (20%)]\tClassification Loss: 1.5534\r\n",
      "Train Epoch: 7 [22400/110534 (20%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 7 [23040/110534 (21%)]\tClassification Loss: 1.4745\r\n",
      "Train Epoch: 7 [23680/110534 (21%)]\tClassification Loss: 1.3104\r\n",
      "Train Epoch: 7 [24320/110534 (22%)]\tClassification Loss: 1.6549\r\n",
      "Train Epoch: 7 [24960/110534 (23%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 7 [25600/110534 (23%)]\tClassification Loss: 1.4079\r\n",
      "Train Epoch: 7 [26240/110534 (24%)]\tClassification Loss: 1.8611\r\n",
      "Train Epoch: 7 [26880/110534 (24%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 7 [27520/110534 (25%)]\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 7 [28160/110534 (25%)]\tClassification Loss: 1.3694\r\n",
      "Train Epoch: 7 [28800/110534 (26%)]\tClassification Loss: 1.4536\r\n",
      "Train Epoch: 7 [29440/110534 (27%)]\tClassification Loss: 1.3917\r\n",
      "Train Epoch: 7 [30080/110534 (27%)]\tClassification Loss: 1.6220\r\n",
      "Train Epoch: 7 [30720/110534 (28%)]\tClassification Loss: 1.4272\r\n",
      "Train Epoch: 7 [31360/110534 (28%)]\tClassification Loss: 1.3310\r\n",
      "Train Epoch: 7 [32000/110534 (29%)]\tClassification Loss: 1.3059\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_500.pth.tar\r\n",
      "Train Epoch: 7 [32640/110534 (30%)]\tClassification Loss: 1.9657\r\n",
      "Train Epoch: 7 [33280/110534 (30%)]\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 7 [33920/110534 (31%)]\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 7 [34560/110534 (31%)]\tClassification Loss: 1.5340\r\n",
      "Train Epoch: 7 [35200/110534 (32%)]\tClassification Loss: 1.4127\r\n",
      "Train Epoch: 7 [35840/110534 (32%)]\tClassification Loss: 1.6337\r\n",
      "Train Epoch: 7 [36480/110534 (33%)]\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 7 [37120/110534 (34%)]\tClassification Loss: 1.1336\r\n",
      "Train Epoch: 7 [37760/110534 (34%)]\tClassification Loss: 1.5110\r\n",
      "Train Epoch: 7 [38400/110534 (35%)]\tClassification Loss: 1.4690\r\n",
      "Train Epoch: 7 [39040/110534 (35%)]\tClassification Loss: 1.1320\r\n",
      "Train Epoch: 7 [39680/110534 (36%)]\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 7 [40320/110534 (36%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 7 [40960/110534 (37%)]\tClassification Loss: 1.7039\r\n",
      "Train Epoch: 7 [41600/110534 (38%)]\tClassification Loss: 1.4337\r\n",
      "Train Epoch: 7 [42240/110534 (38%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 7 [42880/110534 (39%)]\tClassification Loss: 1.6067\r\n",
      "Train Epoch: 7 [43520/110534 (39%)]\tClassification Loss: 1.6067\r\n",
      "Train Epoch: 7 [44160/110534 (40%)]\tClassification Loss: 1.5707\r\n",
      "Train Epoch: 7 [44800/110534 (41%)]\tClassification Loss: 1.3832\r\n",
      "Train Epoch: 7 [45440/110534 (41%)]\tClassification Loss: 1.4735\r\n",
      "Train Epoch: 7 [46080/110534 (42%)]\tClassification Loss: 1.5539\r\n",
      "Train Epoch: 7 [46720/110534 (42%)]\tClassification Loss: 1.2414\r\n",
      "Train Epoch: 7 [47360/110534 (43%)]\tClassification Loss: 1.5407\r\n",
      "Train Epoch: 7 [48000/110534 (43%)]\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 7 [48640/110534 (44%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 7 [49280/110534 (45%)]\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 7 [49920/110534 (45%)]\tClassification Loss: 1.1817\r\n",
      "Train Epoch: 7 [50560/110534 (46%)]\tClassification Loss: 1.5477\r\n",
      "Train Epoch: 7 [51200/110534 (46%)]\tClassification Loss: 1.1574\r\n",
      "Train Epoch: 7 [51840/110534 (47%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 7 [52480/110534 (47%)]\tClassification Loss: 1.3806\r\n",
      "Train Epoch: 7 [53120/110534 (48%)]\tClassification Loss: 1.7916\r\n",
      "Train Epoch: 7 [53760/110534 (49%)]\tClassification Loss: 1.5065\r\n",
      "Train Epoch: 7 [54400/110534 (49%)]\tClassification Loss: 1.6654\r\n",
      "Train Epoch: 7 [55040/110534 (50%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 7 [55680/110534 (50%)]\tClassification Loss: 2.0222\r\n",
      "Train Epoch: 7 [56320/110534 (51%)]\tClassification Loss: 1.4336\r\n",
      "Train Epoch: 7 [56960/110534 (52%)]\tClassification Loss: 1.4870\r\n",
      "Train Epoch: 7 [57600/110534 (52%)]\tClassification Loss: 1.7168\r\n",
      "Train Epoch: 7 [58240/110534 (53%)]\tClassification Loss: 1.4950\r\n",
      "Train Epoch: 7 [58880/110534 (53%)]\tClassification Loss: 1.0729\r\n",
      "Train Epoch: 7 [59520/110534 (54%)]\tClassification Loss: 1.4551\r\n",
      "Train Epoch: 7 [60160/110534 (54%)]\tClassification Loss: 1.2905\r\n",
      "Train Epoch: 7 [60800/110534 (55%)]\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 7 [61440/110534 (56%)]\tClassification Loss: 1.5314\r\n",
      "Train Epoch: 7 [62080/110534 (56%)]\tClassification Loss: 1.5913\r\n",
      "Train Epoch: 7 [62720/110534 (57%)]\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 7 [63360/110534 (57%)]\tClassification Loss: 1.4884\r\n",
      "Train Epoch: 7 [64000/110534 (58%)]\tClassification Loss: 1.4328\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1000.pth.tar\r\n",
      "Train Epoch: 7 [64640/110534 (58%)]\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 7 [65280/110534 (59%)]\tClassification Loss: 1.5021\r\n",
      "Train Epoch: 7 [65920/110534 (60%)]\tClassification Loss: 1.7509\r\n",
      "Train Epoch: 7 [66560/110534 (60%)]\tClassification Loss: 1.4560\r\n",
      "Train Epoch: 7 [67200/110534 (61%)]\tClassification Loss: 1.1684\r\n",
      "Train Epoch: 7 [67840/110534 (61%)]\tClassification Loss: 1.4464\r\n",
      "Train Epoch: 7 [68480/110534 (62%)]\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 7 [69120/110534 (63%)]\tClassification Loss: 1.4532\r\n",
      "Train Epoch: 7 [69760/110534 (63%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 7 [70400/110534 (64%)]\tClassification Loss: 1.3583\r\n",
      "Train Epoch: 7 [71040/110534 (64%)]\tClassification Loss: 1.7399\r\n",
      "Train Epoch: 7 [71680/110534 (65%)]\tClassification Loss: 1.8787\r\n",
      "Train Epoch: 7 [72320/110534 (65%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 7 [72960/110534 (66%)]\tClassification Loss: 1.3936\r\n",
      "Train Epoch: 7 [73600/110534 (67%)]\tClassification Loss: 1.3776\r\n",
      "Train Epoch: 7 [74240/110534 (67%)]\tClassification Loss: 1.3587\r\n",
      "Train Epoch: 7 [74880/110534 (68%)]\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 7 [75520/110534 (68%)]\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 7 [76160/110534 (69%)]\tClassification Loss: 1.2536\r\n",
      "Train Epoch: 7 [76800/110534 (69%)]\tClassification Loss: 1.6795\r\n",
      "Train Epoch: 7 [77440/110534 (70%)]\tClassification Loss: 1.6630\r\n",
      "Train Epoch: 7 [78080/110534 (71%)]\tClassification Loss: 1.7554\r\n",
      "Train Epoch: 7 [78720/110534 (71%)]\tClassification Loss: 1.4614\r\n",
      "Train Epoch: 7 [79360/110534 (72%)]\tClassification Loss: 1.6329\r\n",
      "Train Epoch: 7 [80000/110534 (72%)]\tClassification Loss: 1.5445\r\n",
      "Train Epoch: 7 [80640/110534 (73%)]\tClassification Loss: 1.4712\r\n",
      "Train Epoch: 7 [81280/110534 (74%)]\tClassification Loss: 1.3480\r\n",
      "Train Epoch: 7 [81920/110534 (74%)]\tClassification Loss: 1.6672\r\n",
      "Train Epoch: 7 [82560/110534 (75%)]\tClassification Loss: 1.7750\r\n",
      "Train Epoch: 7 [83200/110534 (75%)]\tClassification Loss: 1.6205\r\n",
      "Train Epoch: 7 [83840/110534 (76%)]\tClassification Loss: 1.6220\r\n",
      "Train Epoch: 7 [84480/110534 (76%)]\tClassification Loss: 1.7019\r\n",
      "Train Epoch: 7 [85120/110534 (77%)]\tClassification Loss: 1.4315\r\n",
      "Train Epoch: 7 [85760/110534 (78%)]\tClassification Loss: 1.3819\r\n",
      "Train Epoch: 7 [86400/110534 (78%)]\tClassification Loss: 1.2780\r\n",
      "Train Epoch: 7 [87040/110534 (79%)]\tClassification Loss: 1.7908\r\n",
      "Train Epoch: 7 [87680/110534 (79%)]\tClassification Loss: 1.9058\r\n",
      "Train Epoch: 7 [88320/110534 (80%)]\tClassification Loss: 1.4423\r\n",
      "Train Epoch: 7 [88960/110534 (80%)]\tClassification Loss: 1.4651\r\n",
      "Train Epoch: 7 [89600/110534 (81%)]\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 7 [90240/110534 (82%)]\tClassification Loss: 1.5629\r\n",
      "Train Epoch: 7 [90880/110534 (82%)]\tClassification Loss: 1.5284\r\n",
      "Train Epoch: 7 [91520/110534 (83%)]\tClassification Loss: 1.5597\r\n",
      "Train Epoch: 7 [92160/110534 (83%)]\tClassification Loss: 1.4530\r\n",
      "Train Epoch: 7 [92800/110534 (84%)]\tClassification Loss: 1.2221\r\n",
      "Train Epoch: 7 [93440/110534 (85%)]\tClassification Loss: 1.7146\r\n",
      "Train Epoch: 7 [94080/110534 (85%)]\tClassification Loss: 1.6531\r\n",
      "Train Epoch: 7 [94720/110534 (86%)]\tClassification Loss: 1.4133\r\n",
      "Train Epoch: 7 [95360/110534 (86%)]\tClassification Loss: 1.7289\r\n",
      "Train Epoch: 7 [96000/110534 (87%)]\tClassification Loss: 1.4516\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1500.pth.tar\r\n",
      "Train Epoch: 7 [96640/110534 (87%)]\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 7 [97280/110534 (88%)]\tClassification Loss: 1.5058\r\n",
      "Train Epoch: 7 [97920/110534 (89%)]\tClassification Loss: 1.6978\r\n",
      "Train Epoch: 7 [98560/110534 (89%)]\tClassification Loss: 1.5827\r\n",
      "Train Epoch: 7 [99200/110534 (90%)]\tClassification Loss: 1.2554\r\n",
      "Train Epoch: 7 [99840/110534 (90%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 7 [100480/110534 (91%)]\tClassification Loss: 1.3558\r\n",
      "Train Epoch: 7 [101120/110534 (91%)]\tClassification Loss: 1.5030\r\n",
      "Train Epoch: 7 [101760/110534 (92%)]\tClassification Loss: 1.3124\r\n",
      "Train Epoch: 7 [102400/110534 (93%)]\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 7 [103040/110534 (93%)]\tClassification Loss: 1.4232\r\n",
      "Train Epoch: 7 [103680/110534 (94%)]\tClassification Loss: 1.6451\r\n",
      "Train Epoch: 7 [104320/110534 (94%)]\tClassification Loss: 1.6180\r\n",
      "Train Epoch: 7 [104960/110534 (95%)]\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 7 [105600/110534 (96%)]\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 7 [106240/110534 (96%)]\tClassification Loss: 1.5633\r\n",
      "Train Epoch: 7 [106880/110534 (97%)]\tClassification Loss: 1.6327\r\n",
      "Train Epoch: 7 [107520/110534 (97%)]\tClassification Loss: 1.5349\r\n",
      "Train Epoch: 7 [108160/110534 (98%)]\tClassification Loss: 1.4890\r\n",
      "Train Epoch: 7 [108800/110534 (98%)]\tClassification Loss: 1.3885\r\n",
      "Train Epoch: 7 [109440/110534 (99%)]\tClassification Loss: 1.5711\r\n",
      "Train Epoch: 7 [110080/110534 (100%)]\tClassification Loss: 1.6164\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_final.pth.tar\r\n",
      "Train Epoch: 8 [0/110534 (0%)]\tClassification Loss: 1.9252\r\n",
      "\r\n",
      "Test set: Average loss: 1.4124, Accuracy: 23248/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 8 [640/110534 (1%)]\tClassification Loss: 1.5337\r\n",
      "Train Epoch: 8 [1280/110534 (1%)]\tClassification Loss: 1.3864\r\n",
      "Train Epoch: 8 [1920/110534 (2%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 8 [2560/110534 (2%)]\tClassification Loss: 1.6181\r\n",
      "Train Epoch: 8 [3200/110534 (3%)]\tClassification Loss: 1.4922\r\n",
      "Train Epoch: 8 [3840/110534 (3%)]\tClassification Loss: 1.7774\r\n",
      "Train Epoch: 8 [4480/110534 (4%)]\tClassification Loss: 1.2993\r\n",
      "Train Epoch: 8 [5120/110534 (5%)]\tClassification Loss: 1.8203\r\n",
      "Train Epoch: 8 [5760/110534 (5%)]\tClassification Loss: 1.6189\r\n",
      "Train Epoch: 8 [6400/110534 (6%)]\tClassification Loss: 1.6954\r\n",
      "Train Epoch: 8 [7040/110534 (6%)]\tClassification Loss: 1.2604\r\n",
      "Train Epoch: 8 [7680/110534 (7%)]\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 8 [8320/110534 (8%)]\tClassification Loss: 1.8024\r\n",
      "Train Epoch: 8 [8960/110534 (8%)]\tClassification Loss: 1.4072\r\n",
      "Train Epoch: 8 [9600/110534 (9%)]\tClassification Loss: 1.5450\r\n",
      "Train Epoch: 8 [10240/110534 (9%)]\tClassification Loss: 1.2782\r\n",
      "Train Epoch: 8 [10880/110534 (10%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 8 [11520/110534 (10%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 8 [12160/110534 (11%)]\tClassification Loss: 1.2394\r\n",
      "Train Epoch: 8 [12800/110534 (12%)]\tClassification Loss: 1.7819\r\n",
      "Train Epoch: 8 [13440/110534 (12%)]\tClassification Loss: 1.7589\r\n",
      "Train Epoch: 8 [14080/110534 (13%)]\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 8 [14720/110534 (13%)]\tClassification Loss: 1.3680\r\n",
      "Train Epoch: 8 [15360/110534 (14%)]\tClassification Loss: 1.4954\r\n",
      "Train Epoch: 8 [16000/110534 (14%)]\tClassification Loss: 1.5663\r\n",
      "Train Epoch: 8 [16640/110534 (15%)]\tClassification Loss: 1.4470\r\n",
      "Train Epoch: 8 [17280/110534 (16%)]\tClassification Loss: 1.6006\r\n",
      "Train Epoch: 8 [17920/110534 (16%)]\tClassification Loss: 1.6772\r\n",
      "Train Epoch: 8 [18560/110534 (17%)]\tClassification Loss: 1.5137\r\n",
      "Train Epoch: 8 [19200/110534 (17%)]\tClassification Loss: 1.3290\r\n",
      "Train Epoch: 8 [19840/110534 (18%)]\tClassification Loss: 1.2780\r\n",
      "Train Epoch: 8 [20480/110534 (19%)]\tClassification Loss: 1.2676\r\n",
      "Train Epoch: 8 [21120/110534 (19%)]\tClassification Loss: 1.4880\r\n",
      "Train Epoch: 8 [21760/110534 (20%)]\tClassification Loss: 1.7309\r\n",
      "Train Epoch: 8 [22400/110534 (20%)]\tClassification Loss: 1.5465\r\n",
      "Train Epoch: 8 [23040/110534 (21%)]\tClassification Loss: 1.3953\r\n",
      "Train Epoch: 8 [23680/110534 (21%)]\tClassification Loss: 1.2781\r\n",
      "Train Epoch: 8 [24320/110534 (22%)]\tClassification Loss: 1.5628\r\n",
      "Train Epoch: 8 [24960/110534 (23%)]\tClassification Loss: 1.3112\r\n",
      "Train Epoch: 8 [25600/110534 (23%)]\tClassification Loss: 1.5542\r\n",
      "Train Epoch: 8 [26240/110534 (24%)]\tClassification Loss: 1.7220\r\n",
      "Train Epoch: 8 [26880/110534 (24%)]\tClassification Loss: 1.4076\r\n",
      "Train Epoch: 8 [27520/110534 (25%)]\tClassification Loss: 1.5784\r\n",
      "Train Epoch: 8 [28160/110534 (25%)]\tClassification Loss: 1.4676\r\n",
      "Train Epoch: 8 [28800/110534 (26%)]\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 8 [29440/110534 (27%)]\tClassification Loss: 1.5240\r\n",
      "Train Epoch: 8 [30080/110534 (27%)]\tClassification Loss: 1.7044\r\n",
      "Train Epoch: 8 [30720/110534 (28%)]\tClassification Loss: 1.3101\r\n",
      "Train Epoch: 8 [31360/110534 (28%)]\tClassification Loss: 1.3624\r\n",
      "Train Epoch: 8 [32000/110534 (29%)]\tClassification Loss: 1.3467\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_500.pth.tar\r\n",
      "Train Epoch: 8 [32640/110534 (30%)]\tClassification Loss: 1.9312\r\n",
      "Train Epoch: 8 [33280/110534 (30%)]\tClassification Loss: 1.4867\r\n",
      "Train Epoch: 8 [33920/110534 (31%)]\tClassification Loss: 1.5209\r\n",
      "Train Epoch: 8 [34560/110534 (31%)]\tClassification Loss: 1.5219\r\n",
      "Train Epoch: 8 [35200/110534 (32%)]\tClassification Loss: 1.4628\r\n",
      "Train Epoch: 8 [35840/110534 (32%)]\tClassification Loss: 1.6687\r\n",
      "Train Epoch: 8 [36480/110534 (33%)]\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 8 [37120/110534 (34%)]\tClassification Loss: 1.3504\r\n",
      "Train Epoch: 8 [37760/110534 (34%)]\tClassification Loss: 1.6152\r\n",
      "Train Epoch: 8 [38400/110534 (35%)]\tClassification Loss: 1.4222\r\n",
      "Train Epoch: 8 [39040/110534 (35%)]\tClassification Loss: 1.1430\r\n",
      "Train Epoch: 8 [39680/110534 (36%)]\tClassification Loss: 1.3911\r\n",
      "Train Epoch: 8 [40320/110534 (36%)]\tClassification Loss: 1.6951\r\n",
      "Train Epoch: 8 [40960/110534 (37%)]\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 8 [41600/110534 (38%)]\tClassification Loss: 1.6169\r\n",
      "Train Epoch: 8 [42240/110534 (38%)]\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 8 [42880/110534 (39%)]\tClassification Loss: 1.3993\r\n",
      "Train Epoch: 8 [43520/110534 (39%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 8 [44160/110534 (40%)]\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 8 [44800/110534 (41%)]\tClassification Loss: 1.2948\r\n",
      "Train Epoch: 8 [45440/110534 (41%)]\tClassification Loss: 1.5518\r\n",
      "Train Epoch: 8 [46080/110534 (42%)]\tClassification Loss: 1.6035\r\n",
      "Train Epoch: 8 [46720/110534 (42%)]\tClassification Loss: 1.2742\r\n",
      "Train Epoch: 8 [47360/110534 (43%)]\tClassification Loss: 1.5101\r\n",
      "Train Epoch: 8 [48000/110534 (43%)]\tClassification Loss: 1.6114\r\n",
      "Train Epoch: 8 [48640/110534 (44%)]\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 8 [49280/110534 (45%)]\tClassification Loss: 1.6628\r\n",
      "Train Epoch: 8 [49920/110534 (45%)]\tClassification Loss: 1.2423\r\n",
      "Train Epoch: 8 [50560/110534 (46%)]\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 8 [51200/110534 (46%)]\tClassification Loss: 1.2097\r\n",
      "Train Epoch: 8 [51840/110534 (47%)]\tClassification Loss: 1.3837\r\n",
      "Train Epoch: 8 [52480/110534 (47%)]\tClassification Loss: 1.5637\r\n",
      "Train Epoch: 8 [53120/110534 (48%)]\tClassification Loss: 1.7258\r\n",
      "Train Epoch: 8 [53760/110534 (49%)]\tClassification Loss: 1.6250\r\n",
      "Train Epoch: 8 [54400/110534 (49%)]\tClassification Loss: 1.6416\r\n",
      "Train Epoch: 8 [55040/110534 (50%)]\tClassification Loss: 1.5179\r\n",
      "Train Epoch: 8 [55680/110534 (50%)]\tClassification Loss: 2.0212\r\n",
      "Train Epoch: 8 [56320/110534 (51%)]\tClassification Loss: 1.3814\r\n",
      "Train Epoch: 8 [56960/110534 (52%)]\tClassification Loss: 1.7461\r\n",
      "Train Epoch: 8 [57600/110534 (52%)]\tClassification Loss: 1.6810\r\n",
      "Train Epoch: 8 [58240/110534 (53%)]\tClassification Loss: 1.5430\r\n",
      "Train Epoch: 8 [58880/110534 (53%)]\tClassification Loss: 1.1412\r\n",
      "Train Epoch: 8 [59520/110534 (54%)]\tClassification Loss: 1.5209\r\n",
      "Train Epoch: 8 [60160/110534 (54%)]\tClassification Loss: 1.3656\r\n",
      "Train Epoch: 8 [60800/110534 (55%)]\tClassification Loss: 1.4058\r\n",
      "Train Epoch: 8 [61440/110534 (56%)]\tClassification Loss: 1.5201\r\n",
      "Train Epoch: 8 [62080/110534 (56%)]\tClassification Loss: 1.4628\r\n",
      "Train Epoch: 8 [62720/110534 (57%)]\tClassification Loss: 1.7317\r\n",
      "Train Epoch: 8 [63360/110534 (57%)]\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 8 [64000/110534 (58%)]\tClassification Loss: 1.4341\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1000.pth.tar\r\n",
      "Train Epoch: 8 [64640/110534 (58%)]\tClassification Loss: 1.5027\r\n",
      "Train Epoch: 8 [65280/110534 (59%)]\tClassification Loss: 1.4104\r\n",
      "Train Epoch: 8 [65920/110534 (60%)]\tClassification Loss: 1.6037\r\n",
      "Train Epoch: 8 [66560/110534 (60%)]\tClassification Loss: 1.4612\r\n",
      "Train Epoch: 8 [67200/110534 (61%)]\tClassification Loss: 1.3044\r\n",
      "Train Epoch: 8 [67840/110534 (61%)]\tClassification Loss: 1.5627\r\n",
      "Train Epoch: 8 [68480/110534 (62%)]\tClassification Loss: 1.5569\r\n",
      "Train Epoch: 8 [69120/110534 (63%)]\tClassification Loss: 1.3740\r\n",
      "Train Epoch: 8 [69760/110534 (63%)]\tClassification Loss: 1.7744\r\n",
      "Train Epoch: 8 [70400/110534 (64%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 8 [71040/110534 (64%)]\tClassification Loss: 1.7161\r\n",
      "Train Epoch: 8 [71680/110534 (65%)]\tClassification Loss: 1.8076\r\n",
      "Train Epoch: 8 [72320/110534 (65%)]\tClassification Loss: 1.5800\r\n",
      "Train Epoch: 8 [72960/110534 (66%)]\tClassification Loss: 1.4690\r\n",
      "Train Epoch: 8 [73600/110534 (67%)]\tClassification Loss: 1.5462\r\n",
      "Train Epoch: 8 [74240/110534 (67%)]\tClassification Loss: 1.2363\r\n",
      "Train Epoch: 8 [74880/110534 (68%)]\tClassification Loss: 1.5705\r\n",
      "Train Epoch: 8 [75520/110534 (68%)]\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 8 [76160/110534 (69%)]\tClassification Loss: 1.2677\r\n",
      "Train Epoch: 8 [76800/110534 (69%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 8 [77440/110534 (70%)]\tClassification Loss: 1.5337\r\n",
      "Train Epoch: 8 [78080/110534 (71%)]\tClassification Loss: 1.6460\r\n",
      "Train Epoch: 8 [78720/110534 (71%)]\tClassification Loss: 1.5759\r\n",
      "Train Epoch: 8 [79360/110534 (72%)]\tClassification Loss: 1.7478\r\n",
      "Train Epoch: 8 [80000/110534 (72%)]\tClassification Loss: 1.7031\r\n",
      "Train Epoch: 8 [80640/110534 (73%)]\tClassification Loss: 1.4538\r\n",
      "Train Epoch: 8 [81280/110534 (74%)]\tClassification Loss: 1.3348\r\n",
      "Train Epoch: 8 [81920/110534 (74%)]\tClassification Loss: 1.7953\r\n",
      "Train Epoch: 8 [82560/110534 (75%)]\tClassification Loss: 1.7796\r\n",
      "Train Epoch: 8 [83200/110534 (75%)]\tClassification Loss: 1.6338\r\n",
      "Train Epoch: 8 [83840/110534 (76%)]\tClassification Loss: 1.6457\r\n",
      "Train Epoch: 8 [84480/110534 (76%)]\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 8 [85120/110534 (77%)]\tClassification Loss: 1.4083\r\n",
      "Train Epoch: 8 [85760/110534 (78%)]\tClassification Loss: 1.4380\r\n",
      "Train Epoch: 8 [86400/110534 (78%)]\tClassification Loss: 1.2572\r\n",
      "Train Epoch: 8 [87040/110534 (79%)]\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 8 [87680/110534 (79%)]\tClassification Loss: 1.8209\r\n",
      "Train Epoch: 8 [88320/110534 (80%)]\tClassification Loss: 1.3934\r\n",
      "Train Epoch: 8 [88960/110534 (80%)]\tClassification Loss: 1.3474\r\n",
      "Train Epoch: 8 [89600/110534 (81%)]\tClassification Loss: 1.5238\r\n",
      "Train Epoch: 8 [90240/110534 (82%)]\tClassification Loss: 1.5344\r\n",
      "Train Epoch: 8 [90880/110534 (82%)]\tClassification Loss: 1.6446\r\n",
      "Train Epoch: 8 [91520/110534 (83%)]\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 8 [92160/110534 (83%)]\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 8 [92800/110534 (84%)]\tClassification Loss: 1.2198\r\n",
      "Train Epoch: 8 [93440/110534 (85%)]\tClassification Loss: 1.6729\r\n",
      "Train Epoch: 8 [94080/110534 (85%)]\tClassification Loss: 1.4870\r\n",
      "Train Epoch: 8 [94720/110534 (86%)]\tClassification Loss: 1.4128\r\n",
      "Train Epoch: 8 [95360/110534 (86%)]\tClassification Loss: 1.5321\r\n",
      "Train Epoch: 8 [96000/110534 (87%)]\tClassification Loss: 1.5030\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1500.pth.tar\r\n",
      "Train Epoch: 8 [96640/110534 (87%)]\tClassification Loss: 1.7112\r\n",
      "Train Epoch: 8 [97280/110534 (88%)]\tClassification Loss: 1.5754\r\n",
      "Train Epoch: 8 [97920/110534 (89%)]\tClassification Loss: 1.6625\r\n",
      "Train Epoch: 8 [98560/110534 (89%)]\tClassification Loss: 1.7384\r\n",
      "Train Epoch: 8 [99200/110534 (90%)]\tClassification Loss: 1.1886\r\n",
      "Train Epoch: 8 [99840/110534 (90%)]\tClassification Loss: 1.4498\r\n",
      "Train Epoch: 8 [100480/110534 (91%)]\tClassification Loss: 1.3295\r\n",
      "Train Epoch: 8 [101120/110534 (91%)]\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 8 [101760/110534 (92%)]\tClassification Loss: 1.3125\r\n",
      "Train Epoch: 8 [102400/110534 (93%)]\tClassification Loss: 1.3265\r\n",
      "Train Epoch: 8 [103040/110534 (93%)]\tClassification Loss: 1.4893\r\n",
      "Train Epoch: 8 [103680/110534 (94%)]\tClassification Loss: 1.5360\r\n",
      "Train Epoch: 8 [104320/110534 (94%)]\tClassification Loss: 1.4198\r\n",
      "Train Epoch: 8 [104960/110534 (95%)]\tClassification Loss: 1.7559\r\n",
      "Train Epoch: 8 [105600/110534 (96%)]\tClassification Loss: 1.4742\r\n",
      "Train Epoch: 8 [106240/110534 (96%)]\tClassification Loss: 1.3885\r\n",
      "Train Epoch: 8 [106880/110534 (97%)]\tClassification Loss: 1.5783\r\n",
      "Train Epoch: 8 [107520/110534 (97%)]\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 8 [108160/110534 (98%)]\tClassification Loss: 1.2300\r\n",
      "Train Epoch: 8 [108800/110534 (98%)]\tClassification Loss: 1.3070\r\n",
      "Train Epoch: 8 [109440/110534 (99%)]\tClassification Loss: 1.5277\r\n",
      "Train Epoch: 8 [110080/110534 (100%)]\tClassification Loss: 1.7697\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_final.pth.tar\r\n",
      "Train Epoch: 9 [0/110534 (0%)]\tClassification Loss: 1.8949\r\n",
      "\r\n",
      "Test set: Average loss: 1.4118, Accuracy: 23130/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 9 [640/110534 (1%)]\tClassification Loss: 1.5842\r\n",
      "Train Epoch: 9 [1280/110534 (1%)]\tClassification Loss: 1.4456\r\n",
      "Train Epoch: 9 [1920/110534 (2%)]\tClassification Loss: 1.5784\r\n",
      "Train Epoch: 9 [2560/110534 (2%)]\tClassification Loss: 1.5111\r\n",
      "Train Epoch: 9 [3200/110534 (3%)]\tClassification Loss: 1.3961\r\n",
      "Train Epoch: 9 [3840/110534 (3%)]\tClassification Loss: 1.9256\r\n",
      "Train Epoch: 9 [4480/110534 (4%)]\tClassification Loss: 1.5278\r\n",
      "Train Epoch: 9 [5120/110534 (5%)]\tClassification Loss: 1.5483\r\n",
      "Train Epoch: 9 [5760/110534 (5%)]\tClassification Loss: 1.5810\r\n",
      "Train Epoch: 9 [6400/110534 (6%)]\tClassification Loss: 1.7019\r\n",
      "Train Epoch: 9 [7040/110534 (6%)]\tClassification Loss: 1.3990\r\n",
      "Train Epoch: 9 [7680/110534 (7%)]\tClassification Loss: 1.5501\r\n",
      "Train Epoch: 9 [8320/110534 (8%)]\tClassification Loss: 1.8222\r\n",
      "Train Epoch: 9 [8960/110534 (8%)]\tClassification Loss: 1.5201\r\n",
      "Train Epoch: 9 [9600/110534 (9%)]\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 9 [10240/110534 (9%)]\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 9 [10880/110534 (10%)]\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 9 [11520/110534 (10%)]\tClassification Loss: 1.4249\r\n",
      "Train Epoch: 9 [12160/110534 (11%)]\tClassification Loss: 1.2807\r\n",
      "Train Epoch: 9 [12800/110534 (12%)]\tClassification Loss: 1.7907\r\n",
      "Train Epoch: 9 [13440/110534 (12%)]\tClassification Loss: 1.5813\r\n",
      "Train Epoch: 9 [14080/110534 (13%)]\tClassification Loss: 1.3006\r\n",
      "Train Epoch: 9 [14720/110534 (13%)]\tClassification Loss: 1.2026\r\n",
      "Train Epoch: 9 [15360/110534 (14%)]\tClassification Loss: 1.2650\r\n",
      "Train Epoch: 9 [16000/110534 (14%)]\tClassification Loss: 1.8832\r\n",
      "Train Epoch: 9 [16640/110534 (15%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 9 [17280/110534 (16%)]\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 9 [17920/110534 (16%)]\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 9 [18560/110534 (17%)]\tClassification Loss: 1.4610\r\n",
      "Train Epoch: 9 [19200/110534 (17%)]\tClassification Loss: 1.4961\r\n",
      "Train Epoch: 9 [19840/110534 (18%)]\tClassification Loss: 1.2444\r\n",
      "Train Epoch: 9 [20480/110534 (19%)]\tClassification Loss: 1.2140\r\n",
      "Train Epoch: 9 [21120/110534 (19%)]\tClassification Loss: 1.4567\r\n",
      "Train Epoch: 9 [21760/110534 (20%)]\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 9 [22400/110534 (20%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 9 [23040/110534 (21%)]\tClassification Loss: 1.4551\r\n",
      "Train Epoch: 9 [23680/110534 (21%)]\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 9 [24320/110534 (22%)]\tClassification Loss: 1.6554\r\n",
      "Train Epoch: 9 [24960/110534 (23%)]\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 9 [25600/110534 (23%)]\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 9 [26240/110534 (24%)]\tClassification Loss: 1.7936\r\n",
      "Train Epoch: 9 [26880/110534 (24%)]\tClassification Loss: 1.4637\r\n",
      "Train Epoch: 9 [27520/110534 (25%)]\tClassification Loss: 1.7617\r\n",
      "Train Epoch: 9 [28160/110534 (25%)]\tClassification Loss: 1.4025\r\n",
      "Train Epoch: 9 [28800/110534 (26%)]\tClassification Loss: 1.5544\r\n",
      "Train Epoch: 9 [29440/110534 (27%)]\tClassification Loss: 1.4914\r\n",
      "Train Epoch: 9 [30080/110534 (27%)]\tClassification Loss: 1.7186\r\n",
      "Train Epoch: 9 [30720/110534 (28%)]\tClassification Loss: 1.3744\r\n",
      "Train Epoch: 9 [31360/110534 (28%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 9 [32000/110534 (29%)]\tClassification Loss: 1.5206\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_500.pth.tar\r\n",
      "Train Epoch: 9 [32640/110534 (30%)]\tClassification Loss: 1.8338\r\n",
      "Train Epoch: 9 [33280/110534 (30%)]\tClassification Loss: 1.5540\r\n",
      "Train Epoch: 9 [33920/110534 (31%)]\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 9 [34560/110534 (31%)]\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 9 [35200/110534 (32%)]\tClassification Loss: 1.2414\r\n",
      "Train Epoch: 9 [35840/110534 (32%)]\tClassification Loss: 1.6361\r\n",
      "Train Epoch: 9 [36480/110534 (33%)]\tClassification Loss: 1.4530\r\n",
      "Train Epoch: 9 [37120/110534 (34%)]\tClassification Loss: 1.2252\r\n",
      "Train Epoch: 9 [37760/110534 (34%)]\tClassification Loss: 1.5899\r\n",
      "Train Epoch: 9 [38400/110534 (35%)]\tClassification Loss: 1.4031\r\n",
      "Train Epoch: 9 [39040/110534 (35%)]\tClassification Loss: 1.2240\r\n",
      "Train Epoch: 9 [39680/110534 (36%)]\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 9 [40320/110534 (36%)]\tClassification Loss: 1.7063\r\n",
      "Train Epoch: 9 [40960/110534 (37%)]\tClassification Loss: 1.6108\r\n",
      "Train Epoch: 9 [41600/110534 (38%)]\tClassification Loss: 1.5004\r\n",
      "Train Epoch: 9 [42240/110534 (38%)]\tClassification Loss: 1.5700\r\n",
      "Train Epoch: 9 [42880/110534 (39%)]\tClassification Loss: 1.3337\r\n",
      "Train Epoch: 9 [43520/110534 (39%)]\tClassification Loss: 1.5298\r\n",
      "Train Epoch: 9 [44160/110534 (40%)]\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 9 [44800/110534 (41%)]\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 9 [45440/110534 (41%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 9 [46080/110534 (42%)]\tClassification Loss: 1.6312\r\n",
      "Train Epoch: 9 [46720/110534 (42%)]\tClassification Loss: 1.2875\r\n",
      "Train Epoch: 9 [47360/110534 (43%)]\tClassification Loss: 1.5385\r\n",
      "Train Epoch: 9 [48000/110534 (43%)]\tClassification Loss: 1.5124\r\n",
      "Train Epoch: 9 [48640/110534 (44%)]\tClassification Loss: 1.5402\r\n",
      "Train Epoch: 9 [49280/110534 (45%)]\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 9 [49920/110534 (45%)]\tClassification Loss: 1.3982\r\n",
      "Train Epoch: 9 [50560/110534 (46%)]\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 9 [51200/110534 (46%)]\tClassification Loss: 1.1947\r\n",
      "Train Epoch: 9 [51840/110534 (47%)]\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 9 [52480/110534 (47%)]\tClassification Loss: 1.5022\r\n",
      "Train Epoch: 9 [53120/110534 (48%)]\tClassification Loss: 1.7100\r\n",
      "Train Epoch: 9 [53760/110534 (49%)]\tClassification Loss: 1.5323\r\n",
      "Train Epoch: 9 [54400/110534 (49%)]\tClassification Loss: 1.5598\r\n",
      "Train Epoch: 9 [55040/110534 (50%)]\tClassification Loss: 1.7385\r\n",
      "Train Epoch: 9 [55680/110534 (50%)]\tClassification Loss: 1.8622\r\n",
      "Train Epoch: 9 [56320/110534 (51%)]\tClassification Loss: 1.3761\r\n",
      "Train Epoch: 9 [56960/110534 (52%)]\tClassification Loss: 1.5950\r\n",
      "Train Epoch: 9 [57600/110534 (52%)]\tClassification Loss: 1.6488\r\n",
      "Train Epoch: 9 [58240/110534 (53%)]\tClassification Loss: 1.4533\r\n",
      "Train Epoch: 9 [58880/110534 (53%)]\tClassification Loss: 1.3054\r\n",
      "Train Epoch: 9 [59520/110534 (54%)]\tClassification Loss: 1.5946\r\n",
      "Train Epoch: 9 [60160/110534 (54%)]\tClassification Loss: 1.3570\r\n",
      "Train Epoch: 9 [60800/110534 (55%)]\tClassification Loss: 1.4057\r\n",
      "Train Epoch: 9 [61440/110534 (56%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 9 [62080/110534 (56%)]\tClassification Loss: 1.4776\r\n",
      "Train Epoch: 9 [62720/110534 (57%)]\tClassification Loss: 1.7143\r\n",
      "Train Epoch: 9 [63360/110534 (57%)]\tClassification Loss: 1.4466\r\n",
      "Train Epoch: 9 [64000/110534 (58%)]\tClassification Loss: 1.4129\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1000.pth.tar\r\n",
      "Train Epoch: 9 [64640/110534 (58%)]\tClassification Loss: 1.5297\r\n",
      "Train Epoch: 9 [65280/110534 (59%)]\tClassification Loss: 1.4607\r\n",
      "Train Epoch: 9 [65920/110534 (60%)]\tClassification Loss: 1.7443\r\n",
      "Train Epoch: 9 [66560/110534 (60%)]\tClassification Loss: 1.4544\r\n",
      "Train Epoch: 9 [67200/110534 (61%)]\tClassification Loss: 1.4178\r\n",
      "Train Epoch: 9 [67840/110534 (61%)]\tClassification Loss: 1.6584\r\n",
      "Train Epoch: 9 [68480/110534 (62%)]\tClassification Loss: 1.5743\r\n",
      "Train Epoch: 9 [69120/110534 (63%)]\tClassification Loss: 1.2195\r\n",
      "Train Epoch: 9 [69760/110534 (63%)]\tClassification Loss: 1.7197\r\n",
      "Train Epoch: 9 [70400/110534 (64%)]\tClassification Loss: 1.4729\r\n",
      "Train Epoch: 9 [71040/110534 (64%)]\tClassification Loss: 1.4938\r\n",
      "Train Epoch: 9 [71680/110534 (65%)]\tClassification Loss: 1.7836\r\n",
      "Train Epoch: 9 [72320/110534 (65%)]\tClassification Loss: 1.6012\r\n",
      "Train Epoch: 9 [72960/110534 (66%)]\tClassification Loss: 1.4495\r\n",
      "Train Epoch: 9 [73600/110534 (67%)]\tClassification Loss: 1.3263\r\n",
      "Train Epoch: 9 [74240/110534 (67%)]\tClassification Loss: 1.4660\r\n",
      "Train Epoch: 9 [74880/110534 (68%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 9 [75520/110534 (68%)]\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 9 [76160/110534 (69%)]\tClassification Loss: 1.2314\r\n",
      "Train Epoch: 9 [76800/110534 (69%)]\tClassification Loss: 1.6360\r\n",
      "Train Epoch: 9 [77440/110534 (70%)]\tClassification Loss: 1.5658\r\n",
      "Train Epoch: 9 [78080/110534 (71%)]\tClassification Loss: 1.6757\r\n",
      "Train Epoch: 9 [78720/110534 (71%)]\tClassification Loss: 1.5316\r\n",
      "Train Epoch: 9 [79360/110534 (72%)]\tClassification Loss: 1.7170\r\n",
      "Train Epoch: 9 [80000/110534 (72%)]\tClassification Loss: 1.6127\r\n",
      "Train Epoch: 9 [80640/110534 (73%)]\tClassification Loss: 1.4108\r\n",
      "Train Epoch: 9 [81280/110534 (74%)]\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 9 [81920/110534 (74%)]\tClassification Loss: 1.7260\r\n",
      "Train Epoch: 9 [82560/110534 (75%)]\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 9 [83200/110534 (75%)]\tClassification Loss: 1.5636\r\n",
      "Train Epoch: 9 [83840/110534 (76%)]\tClassification Loss: 1.5054\r\n",
      "Train Epoch: 9 [84480/110534 (76%)]\tClassification Loss: 1.2894\r\n",
      "Train Epoch: 9 [85120/110534 (77%)]\tClassification Loss: 1.3554\r\n",
      "Train Epoch: 9 [85760/110534 (78%)]\tClassification Loss: 1.4618\r\n",
      "Train Epoch: 9 [86400/110534 (78%)]\tClassification Loss: 1.2932\r\n",
      "Train Epoch: 9 [87040/110534 (79%)]\tClassification Loss: 1.6057\r\n",
      "Train Epoch: 9 [87680/110534 (79%)]\tClassification Loss: 2.0192\r\n",
      "Train Epoch: 9 [88320/110534 (80%)]\tClassification Loss: 1.5142\r\n",
      "Train Epoch: 9 [88960/110534 (80%)]\tClassification Loss: 1.3288\r\n",
      "Train Epoch: 9 [89600/110534 (81%)]\tClassification Loss: 1.3906\r\n",
      "Train Epoch: 9 [90240/110534 (82%)]\tClassification Loss: 1.4914\r\n",
      "Train Epoch: 9 [90880/110534 (82%)]\tClassification Loss: 1.5060\r\n",
      "Train Epoch: 9 [91520/110534 (83%)]\tClassification Loss: 1.5500\r\n",
      "Train Epoch: 9 [92160/110534 (83%)]\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 9 [92800/110534 (84%)]\tClassification Loss: 1.2640\r\n",
      "Train Epoch: 9 [93440/110534 (85%)]\tClassification Loss: 1.7230\r\n",
      "Train Epoch: 9 [94080/110534 (85%)]\tClassification Loss: 1.4299\r\n",
      "Train Epoch: 9 [94720/110534 (86%)]\tClassification Loss: 1.3126\r\n",
      "Train Epoch: 9 [95360/110534 (86%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 9 [96000/110534 (87%)]\tClassification Loss: 1.5438\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1500.pth.tar\r\n",
      "Train Epoch: 9 [96640/110534 (87%)]\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 9 [97280/110534 (88%)]\tClassification Loss: 1.5229\r\n",
      "Train Epoch: 9 [97920/110534 (89%)]\tClassification Loss: 1.9381\r\n",
      "Train Epoch: 9 [98560/110534 (89%)]\tClassification Loss: 1.6536\r\n",
      "Train Epoch: 9 [99200/110534 (90%)]\tClassification Loss: 1.0935\r\n",
      "Train Epoch: 9 [99840/110534 (90%)]\tClassification Loss: 1.6987\r\n",
      "Train Epoch: 9 [100480/110534 (91%)]\tClassification Loss: 1.4319\r\n",
      "Train Epoch: 9 [101120/110534 (91%)]\tClassification Loss: 1.5875\r\n",
      "Train Epoch: 9 [101760/110534 (92%)]\tClassification Loss: 1.2820\r\n",
      "Train Epoch: 9 [102400/110534 (93%)]\tClassification Loss: 1.2625\r\n",
      "Train Epoch: 9 [103040/110534 (93%)]\tClassification Loss: 1.3761\r\n",
      "Train Epoch: 9 [103680/110534 (94%)]\tClassification Loss: 1.4248\r\n",
      "Train Epoch: 9 [104320/110534 (94%)]\tClassification Loss: 1.4125\r\n",
      "Train Epoch: 9 [104960/110534 (95%)]\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 9 [105600/110534 (96%)]\tClassification Loss: 1.2596\r\n",
      "Train Epoch: 9 [106240/110534 (96%)]\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 9 [106880/110534 (97%)]\tClassification Loss: 1.5851\r\n",
      "Train Epoch: 9 [107520/110534 (97%)]\tClassification Loss: 1.5722\r\n",
      "Train Epoch: 9 [108160/110534 (98%)]\tClassification Loss: 1.3649\r\n",
      "Train Epoch: 9 [108800/110534 (98%)]\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 9 [109440/110534 (99%)]\tClassification Loss: 1.5380\r\n",
      "Train Epoch: 9 [110080/110534 (100%)]\tClassification Loss: 1.6795\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_final.pth.tar\r\n",
      "Train Epoch: 10 [0/110534 (0%)]\tClassification Loss: 1.8978\r\n",
      "\r\n",
      "Test set: Average loss: 1.4090, Accuracy: 23170/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 10 [640/110534 (1%)]\tClassification Loss: 1.5049\r\n",
      "Train Epoch: 10 [1280/110534 (1%)]\tClassification Loss: 1.2897\r\n",
      "Train Epoch: 10 [1920/110534 (2%)]\tClassification Loss: 1.7713\r\n",
      "Train Epoch: 10 [2560/110534 (2%)]\tClassification Loss: 1.5072\r\n",
      "Train Epoch: 10 [3200/110534 (3%)]\tClassification Loss: 1.3908\r\n",
      "Train Epoch: 10 [3840/110534 (3%)]\tClassification Loss: 1.7946\r\n",
      "Train Epoch: 10 [4480/110534 (4%)]\tClassification Loss: 1.3302\r\n",
      "Train Epoch: 10 [5120/110534 (5%)]\tClassification Loss: 1.7706\r\n",
      "Train Epoch: 10 [5760/110534 (5%)]\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 10 [6400/110534 (6%)]\tClassification Loss: 1.7088\r\n",
      "Train Epoch: 10 [7040/110534 (6%)]\tClassification Loss: 1.3456\r\n",
      "Train Epoch: 10 [7680/110534 (7%)]\tClassification Loss: 1.5230\r\n",
      "Train Epoch: 10 [8320/110534 (8%)]\tClassification Loss: 1.8772\r\n",
      "Train Epoch: 10 [8960/110534 (8%)]\tClassification Loss: 1.4338\r\n",
      "Train Epoch: 10 [9600/110534 (9%)]\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 10 [10240/110534 (9%)]\tClassification Loss: 1.2808\r\n",
      "Train Epoch: 10 [10880/110534 (10%)]\tClassification Loss: 1.3769\r\n",
      "Train Epoch: 10 [11520/110534 (10%)]\tClassification Loss: 1.6469\r\n",
      "Train Epoch: 10 [12160/110534 (11%)]\tClassification Loss: 1.3557\r\n",
      "Train Epoch: 10 [12800/110534 (12%)]\tClassification Loss: 1.7706\r\n",
      "Train Epoch: 10 [13440/110534 (12%)]\tClassification Loss: 1.7848\r\n",
      "Train Epoch: 10 [14080/110534 (13%)]\tClassification Loss: 1.4363\r\n",
      "Train Epoch: 10 [14720/110534 (13%)]\tClassification Loss: 1.4396\r\n",
      "Train Epoch: 10 [15360/110534 (14%)]\tClassification Loss: 1.2608\r\n",
      "Train Epoch: 10 [16000/110534 (14%)]\tClassification Loss: 1.5576\r\n",
      "Train Epoch: 10 [16640/110534 (15%)]\tClassification Loss: 1.4274\r\n",
      "Train Epoch: 10 [17280/110534 (16%)]\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 10 [17920/110534 (16%)]\tClassification Loss: 1.7935\r\n",
      "Train Epoch: 10 [18560/110534 (17%)]\tClassification Loss: 1.4582\r\n",
      "Train Epoch: 10 [19200/110534 (17%)]\tClassification Loss: 1.2812\r\n",
      "Train Epoch: 10 [19840/110534 (18%)]\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 10 [20480/110534 (19%)]\tClassification Loss: 1.2746\r\n",
      "Train Epoch: 10 [21120/110534 (19%)]\tClassification Loss: 1.4922\r\n",
      "Train Epoch: 10 [21760/110534 (20%)]\tClassification Loss: 1.6155\r\n",
      "Train Epoch: 10 [22400/110534 (20%)]\tClassification Loss: 1.5035\r\n",
      "Train Epoch: 10 [23040/110534 (21%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 10 [23680/110534 (21%)]\tClassification Loss: 1.3511\r\n",
      "Train Epoch: 10 [24320/110534 (22%)]\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 10 [24960/110534 (23%)]\tClassification Loss: 1.3578\r\n",
      "Train Epoch: 10 [25600/110534 (23%)]\tClassification Loss: 1.3962\r\n",
      "Train Epoch: 10 [26240/110534 (24%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 10 [26880/110534 (24%)]\tClassification Loss: 1.3610\r\n",
      "Train Epoch: 10 [27520/110534 (25%)]\tClassification Loss: 1.8498\r\n",
      "Train Epoch: 10 [28160/110534 (25%)]\tClassification Loss: 1.3882\r\n",
      "Train Epoch: 10 [28800/110534 (26%)]\tClassification Loss: 1.3857\r\n",
      "Train Epoch: 10 [29440/110534 (27%)]\tClassification Loss: 1.4821\r\n",
      "Train Epoch: 10 [30080/110534 (27%)]\tClassification Loss: 1.6441\r\n",
      "Train Epoch: 10 [30720/110534 (28%)]\tClassification Loss: 1.4898\r\n",
      "Train Epoch: 10 [31360/110534 (28%)]\tClassification Loss: 1.1966\r\n",
      "Train Epoch: 10 [32000/110534 (29%)]\tClassification Loss: 1.4237\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_500.pth.tar\r\n",
      "Train Epoch: 10 [32640/110534 (30%)]\tClassification Loss: 1.8687\r\n",
      "Train Epoch: 10 [33280/110534 (30%)]\tClassification Loss: 1.5374\r\n",
      "Train Epoch: 10 [33920/110534 (31%)]\tClassification Loss: 1.4950\r\n",
      "Train Epoch: 10 [34560/110534 (31%)]\tClassification Loss: 1.4992\r\n",
      "Train Epoch: 10 [35200/110534 (32%)]\tClassification Loss: 1.4770\r\n",
      "Train Epoch: 10 [35840/110534 (32%)]\tClassification Loss: 1.7163\r\n",
      "Train Epoch: 10 [36480/110534 (33%)]\tClassification Loss: 1.4814\r\n",
      "Train Epoch: 10 [37120/110534 (34%)]\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 10 [37760/110534 (34%)]\tClassification Loss: 1.5156\r\n",
      "Train Epoch: 10 [38400/110534 (35%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 10 [39040/110534 (35%)]\tClassification Loss: 1.1402\r\n",
      "Train Epoch: 10 [39680/110534 (36%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 10 [40320/110534 (36%)]\tClassification Loss: 1.6034\r\n",
      "Train Epoch: 10 [40960/110534 (37%)]\tClassification Loss: 1.6683\r\n",
      "Train Epoch: 10 [41600/110534 (38%)]\tClassification Loss: 1.6426\r\n",
      "Train Epoch: 10 [42240/110534 (38%)]\tClassification Loss: 1.7662\r\n",
      "Train Epoch: 10 [42880/110534 (39%)]\tClassification Loss: 1.5223\r\n",
      "Train Epoch: 10 [43520/110534 (39%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 10 [44160/110534 (40%)]\tClassification Loss: 1.4842\r\n",
      "Train Epoch: 10 [44800/110534 (41%)]\tClassification Loss: 1.2793\r\n",
      "Train Epoch: 10 [45440/110534 (41%)]\tClassification Loss: 1.3762\r\n",
      "Train Epoch: 10 [46080/110534 (42%)]\tClassification Loss: 1.6426\r\n",
      "Train Epoch: 10 [46720/110534 (42%)]\tClassification Loss: 1.3931\r\n",
      "Train Epoch: 10 [47360/110534 (43%)]\tClassification Loss: 1.5162\r\n",
      "Train Epoch: 10 [48000/110534 (43%)]\tClassification Loss: 1.4404\r\n",
      "Train Epoch: 10 [48640/110534 (44%)]\tClassification Loss: 1.6460\r\n",
      "Train Epoch: 10 [49280/110534 (45%)]\tClassification Loss: 1.5432\r\n",
      "Train Epoch: 10 [49920/110534 (45%)]\tClassification Loss: 1.2275\r\n",
      "Train Epoch: 10 [50560/110534 (46%)]\tClassification Loss: 1.5608\r\n",
      "Train Epoch: 10 [51200/110534 (46%)]\tClassification Loss: 1.2668\r\n",
      "Train Epoch: 10 [51840/110534 (47%)]\tClassification Loss: 1.3061\r\n",
      "Train Epoch: 10 [52480/110534 (47%)]\tClassification Loss: 1.5755\r\n",
      "Train Epoch: 10 [53120/110534 (48%)]\tClassification Loss: 1.8992\r\n",
      "Train Epoch: 10 [53760/110534 (49%)]\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 10 [54400/110534 (49%)]\tClassification Loss: 1.6609\r\n",
      "Train Epoch: 10 [55040/110534 (50%)]\tClassification Loss: 1.7168\r\n",
      "Train Epoch: 10 [55680/110534 (50%)]\tClassification Loss: 1.8533\r\n",
      "Train Epoch: 10 [56320/110534 (51%)]\tClassification Loss: 1.4042\r\n",
      "Train Epoch: 10 [56960/110534 (52%)]\tClassification Loss: 1.5351\r\n",
      "Train Epoch: 10 [57600/110534 (52%)]\tClassification Loss: 1.8482\r\n",
      "Train Epoch: 10 [58240/110534 (53%)]\tClassification Loss: 1.4571\r\n",
      "Train Epoch: 10 [58880/110534 (53%)]\tClassification Loss: 1.1865\r\n",
      "Train Epoch: 10 [59520/110534 (54%)]\tClassification Loss: 1.5250\r\n",
      "Train Epoch: 10 [60160/110534 (54%)]\tClassification Loss: 1.2989\r\n",
      "Train Epoch: 10 [60800/110534 (55%)]\tClassification Loss: 1.5741\r\n",
      "Train Epoch: 10 [61440/110534 (56%)]\tClassification Loss: 1.4945\r\n",
      "Train Epoch: 10 [62080/110534 (56%)]\tClassification Loss: 1.5147\r\n",
      "Train Epoch: 10 [62720/110534 (57%)]\tClassification Loss: 1.6000\r\n",
      "Train Epoch: 10 [63360/110534 (57%)]\tClassification Loss: 1.4310\r\n",
      "Train Epoch: 10 [64000/110534 (58%)]\tClassification Loss: 1.3331\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1000.pth.tar\r\n",
      "Train Epoch: 10 [64640/110534 (58%)]\tClassification Loss: 1.7097\r\n",
      "Train Epoch: 10 [65280/110534 (59%)]\tClassification Loss: 1.4228\r\n",
      "Train Epoch: 10 [65920/110534 (60%)]\tClassification Loss: 1.6080\r\n",
      "Train Epoch: 10 [66560/110534 (60%)]\tClassification Loss: 1.2741\r\n",
      "Train Epoch: 10 [67200/110534 (61%)]\tClassification Loss: 1.3418\r\n",
      "Train Epoch: 10 [67840/110534 (61%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 10 [68480/110534 (62%)]\tClassification Loss: 1.4570\r\n",
      "Train Epoch: 10 [69120/110534 (63%)]\tClassification Loss: 1.2330\r\n",
      "Train Epoch: 10 [69760/110534 (63%)]\tClassification Loss: 1.5585\r\n",
      "Train Epoch: 10 [70400/110534 (64%)]\tClassification Loss: 1.3601\r\n",
      "Train Epoch: 10 [71040/110534 (64%)]\tClassification Loss: 1.5805\r\n",
      "Train Epoch: 10 [71680/110534 (65%)]\tClassification Loss: 1.8255\r\n",
      "Train Epoch: 10 [72320/110534 (65%)]\tClassification Loss: 1.7556\r\n",
      "Train Epoch: 10 [72960/110534 (66%)]\tClassification Loss: 1.4838\r\n",
      "Train Epoch: 10 [73600/110534 (67%)]\tClassification Loss: 1.4558\r\n",
      "Train Epoch: 10 [74240/110534 (67%)]\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 10 [74880/110534 (68%)]\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 10 [75520/110534 (68%)]\tClassification Loss: 1.5271\r\n",
      "Train Epoch: 10 [76160/110534 (69%)]\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 10 [76800/110534 (69%)]\tClassification Loss: 1.6486\r\n",
      "Train Epoch: 10 [77440/110534 (70%)]\tClassification Loss: 1.7464\r\n",
      "Train Epoch: 10 [78080/110534 (71%)]\tClassification Loss: 1.6442\r\n",
      "Train Epoch: 10 [78720/110534 (71%)]\tClassification Loss: 1.6799\r\n",
      "Train Epoch: 10 [79360/110534 (72%)]\tClassification Loss: 1.4828\r\n",
      "Train Epoch: 10 [80000/110534 (72%)]\tClassification Loss: 1.5539\r\n",
      "Train Epoch: 10 [80640/110534 (73%)]\tClassification Loss: 1.3339\r\n",
      "Train Epoch: 10 [81280/110534 (74%)]\tClassification Loss: 1.2453\r\n",
      "Train Epoch: 10 [81920/110534 (74%)]\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 10 [82560/110534 (75%)]\tClassification Loss: 1.8690\r\n",
      "Train Epoch: 10 [83200/110534 (75%)]\tClassification Loss: 1.6793\r\n",
      "Train Epoch: 10 [83840/110534 (76%)]\tClassification Loss: 1.5380\r\n",
      "Train Epoch: 10 [84480/110534 (76%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 10 [85120/110534 (77%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 10 [85760/110534 (78%)]\tClassification Loss: 1.3550\r\n",
      "Train Epoch: 10 [86400/110534 (78%)]\tClassification Loss: 1.3622\r\n",
      "Train Epoch: 10 [87040/110534 (79%)]\tClassification Loss: 1.6721\r\n",
      "Train Epoch: 10 [87680/110534 (79%)]\tClassification Loss: 1.9012\r\n",
      "Train Epoch: 10 [88320/110534 (80%)]\tClassification Loss: 1.3806\r\n",
      "Train Epoch: 10 [88960/110534 (80%)]\tClassification Loss: 1.2947\r\n",
      "Train Epoch: 10 [89600/110534 (81%)]\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 10 [90240/110534 (82%)]\tClassification Loss: 1.5959\r\n",
      "Train Epoch: 10 [90880/110534 (82%)]\tClassification Loss: 1.5984\r\n",
      "Train Epoch: 10 [91520/110534 (83%)]\tClassification Loss: 1.6083\r\n",
      "Train Epoch: 10 [92160/110534 (83%)]\tClassification Loss: 1.2204\r\n",
      "Train Epoch: 10 [92800/110534 (84%)]\tClassification Loss: 1.3594\r\n",
      "Train Epoch: 10 [93440/110534 (85%)]\tClassification Loss: 1.5811\r\n",
      "Train Epoch: 10 [94080/110534 (85%)]\tClassification Loss: 1.3916\r\n",
      "Train Epoch: 10 [94720/110534 (86%)]\tClassification Loss: 1.4700\r\n",
      "Train Epoch: 10 [95360/110534 (86%)]\tClassification Loss: 1.6742\r\n",
      "Train Epoch: 10 [96000/110534 (87%)]\tClassification Loss: 1.5281\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1500.pth.tar\r\n",
      "Train Epoch: 10 [96640/110534 (87%)]\tClassification Loss: 1.7407\r\n",
      "Train Epoch: 10 [97280/110534 (88%)]\tClassification Loss: 1.5236\r\n",
      "Train Epoch: 10 [97920/110534 (89%)]\tClassification Loss: 1.8357\r\n",
      "Train Epoch: 10 [98560/110534 (89%)]\tClassification Loss: 1.6625\r\n",
      "Train Epoch: 10 [99200/110534 (90%)]\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 10 [99840/110534 (90%)]\tClassification Loss: 1.5631\r\n",
      "Train Epoch: 10 [100480/110534 (91%)]\tClassification Loss: 1.3823\r\n",
      "Train Epoch: 10 [101120/110534 (91%)]\tClassification Loss: 1.5819\r\n",
      "Train Epoch: 10 [101760/110534 (92%)]\tClassification Loss: 1.2302\r\n",
      "Train Epoch: 10 [102400/110534 (93%)]\tClassification Loss: 1.2910\r\n",
      "Train Epoch: 10 [103040/110534 (93%)]\tClassification Loss: 1.3368\r\n",
      "Train Epoch: 10 [103680/110534 (94%)]\tClassification Loss: 1.5193\r\n",
      "Train Epoch: 10 [104320/110534 (94%)]\tClassification Loss: 1.4933\r\n",
      "Train Epoch: 10 [104960/110534 (95%)]\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 10 [105600/110534 (96%)]\tClassification Loss: 1.4286\r\n",
      "Train Epoch: 10 [106240/110534 (96%)]\tClassification Loss: 1.4203\r\n",
      "Train Epoch: 10 [106880/110534 (97%)]\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 10 [107520/110534 (97%)]\tClassification Loss: 1.6795\r\n",
      "Train Epoch: 10 [108160/110534 (98%)]\tClassification Loss: 1.5003\r\n",
      "Train Epoch: 10 [108800/110534 (98%)]\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 10 [109440/110534 (99%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 10 [110080/110534 (100%)]\tClassification Loss: 1.6824\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_final.pth.tar\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g5b-KDF5GUd7",
    "colab_type": "code",
    "outputId": "40ecd8a2-32bd-46e0-bb17-ff2429062648",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1584699253819,
     "user_tz": -300,
     "elapsed": 64469,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# From scratch. Freeze=True (2). LR=0.01\n",
    "# model_5_500.pth.tar trained for 4.16 epochs. Stable at 58% accuracy and loss around 1.6 but loss was still going down slightly\n",
    "! python train.py"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100% 97.8M/97.8M [00:00<00:00, 168MB/s]\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "\n",
      "Test set: Average loss: 3.4765, Accuracy: 45/960 (5%)\n",
      "\n",
      "Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 4.3483\tTriple Loss(1): 0.5163\tClassification Loss: 3.3158\n",
      "Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 4.3431\tTriple Loss(1): 0.6884\tClassification Loss: 2.9662\n",
      "Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 5.8130\tTriple Loss(0): 1.5165\tClassification Loss: 2.7800\n",
      "Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 3.2336\tTriple Loss(1): 0.2471\tClassification Loss: 2.7393\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 2.7347\tTriple Loss(0): 0.0000\tClassification Loss: 2.7347\n",
      "Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 2.5127\tTriple Loss(0): 0.0000\tClassification Loss: 2.5127\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 3.7189\tTriple Loss(1): 0.5633\tClassification Loss: 2.5924\n",
      "Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 3.3676\tTriple Loss(1): 0.4493\tClassification Loss: 2.4691\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 3.1038\tTriple Loss(1): 0.3192\tClassification Loss: 2.4655\n",
      "Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 2.9974\tTriple Loss(1): 0.2652\tClassification Loss: 2.4670\n",
      "\n",
      "Test set: Average loss: 2.5878, Accuracy: 239/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 3.7002\tTriple Loss(1): 0.5625\tClassification Loss: 2.5753\n",
      "Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 2.1138\tTriple Loss(0): 0.0000\tClassification Loss: 2.1138\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 3.3530\tTriple Loss(1): 0.4862\tClassification Loss: 2.3806\n",
      "Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 3.3885\tTriple Loss(1): 0.4694\tClassification Loss: 2.4498\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 3.1319\tTriple Loss(1): 0.3365\tClassification Loss: 2.4590\n",
      "Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 3.2836\tTriple Loss(1): 0.4319\tClassification Loss: 2.4199\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 3.4411\tTriple Loss(1): 0.5314\tClassification Loss: 2.3783\n",
      "Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 2.4496\tTriple Loss(0): 0.0000\tClassification Loss: 2.4496\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 3.2715\tTriple Loss(1): 0.5521\tClassification Loss: 2.1672\n",
      "Train Epoch: 1 [6080/110534 (5%)]\tAll Loss: 3.2655\tTriple Loss(1): 0.4961\tClassification Loss: 2.2734\n",
      "\n",
      "Test set: Average loss: 2.4070, Accuracy: 293/960 (31%)\n",
      "\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 3.4184\tTriple Loss(1): 0.4781\tClassification Loss: 2.4621\n",
      "Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 3.3485\tTriple Loss(1): 0.4487\tClassification Loss: 2.4512\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 3.9483\tTriple Loss(1): 0.7355\tClassification Loss: 2.4773\n",
      "Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 2.8470\tTriple Loss(1): 0.4186\tClassification Loss: 2.0097\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 3.0134\tTriple Loss(1): 0.4657\tClassification Loss: 2.0820\n",
      "Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 1.9897\tTriple Loss(0): 0.0000\tClassification Loss: 1.9897\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 2.1861\tTriple Loss(0): 0.0000\tClassification Loss: 2.1861\n",
      "Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 2.0087\tTriple Loss(0): 0.0000\tClassification Loss: 2.0087\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 2.9460\tTriple Loss(1): 0.4324\tClassification Loss: 2.0813\n",
      "Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 3.2819\tTriple Loss(1): 0.6515\tClassification Loss: 1.9788\n",
      "\n",
      "Test set: Average loss: 2.2947, Accuracy: 356/960 (37%)\n",
      "\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 2.3249\tTriple Loss(0): 0.0000\tClassification Loss: 2.3249\n",
      "Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 2.9201\tTriple Loss(1): 0.3725\tClassification Loss: 2.1750\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 2.2955\tTriple Loss(0): 0.0000\tClassification Loss: 2.2955\n",
      "Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 4.7827\tTriple Loss(0): 1.2674\tClassification Loss: 2.2480\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 2.4254\tTriple Loss(0): 0.0000\tClassification Loss: 2.4254\n",
      "Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 2.4969\tTriple Loss(1): 0.3294\tClassification Loss: 1.8382\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 2.8650\tTriple Loss(1): 0.3630\tClassification Loss: 2.1389\n",
      "Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 2.5245\tTriple Loss(1): 0.2584\tClassification Loss: 2.0077\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 2.3520\tTriple Loss(0): 0.0000\tClassification Loss: 2.3520\n",
      "Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 2.9566\tTriple Loss(1): 0.4006\tClassification Loss: 2.1554\n",
      "\n",
      "Test set: Average loss: 2.2093, Accuracy: 353/960 (37%)\n",
      "\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 2.8315\tTriple Loss(1): 0.3449\tClassification Loss: 2.1417\n",
      "Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 2.6978\tTriple Loss(1): 0.4072\tClassification Loss: 1.8835\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 3.2743\tTriple Loss(1): 0.4901\tClassification Loss: 2.2942\n",
      "Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 2.9040\tTriple Loss(1): 0.3723\tClassification Loss: 2.1593\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 2.4449\tTriple Loss(1): 0.1961\tClassification Loss: 2.0528\n",
      "Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 3.0787\tTriple Loss(1): 0.3534\tClassification Loss: 2.3719\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 2.5468\tTriple Loss(1): 0.1878\tClassification Loss: 2.1712\n",
      "Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 2.8062\tTriple Loss(1): 0.4043\tClassification Loss: 1.9977\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 3.1750\tTriple Loss(1): 0.6566\tClassification Loss: 1.8619\n",
      "Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 1.9020\tTriple Loss(0): 0.0000\tClassification Loss: 1.9020\n",
      "\n",
      "Test set: Average loss: 2.1455, Accuracy: 414/960 (43%)\n",
      "\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 3.3954\tTriple Loss(1): 0.6302\tClassification Loss: 2.1351\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n",
      "Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 1.8333\tTriple Loss(0): 0.0000\tClassification Loss: 1.8333\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 3.3811\tTriple Loss(1): 0.6631\tClassification Loss: 2.0548\n",
      "Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 2.9412\tTriple Loss(1): 0.3009\tClassification Loss: 2.3394\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 2.6089\tTriple Loss(1): 0.3809\tClassification Loss: 1.8472\n",
      "Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 2.7778\tTriple Loss(1): 0.3351\tClassification Loss: 2.1077\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 3.1198\tTriple Loss(1): 0.5424\tClassification Loss: 2.0351\n",
      "Train Epoch: 1 [18240/110534 (16%)]\tAll Loss: 3.1858\tTriple Loss(1): 0.5191\tClassification Loss: 2.1476\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 2.7241\tTriple Loss(1): 0.3870\tClassification Loss: 1.9502\n",
      "Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 2.1246\tTriple Loss(0): 0.0000\tClassification Loss: 2.1246\n",
      "\n",
      "Test set: Average loss: 2.0926, Accuracy: 451/960 (47%)\n",
      "\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 2.9398\tTriple Loss(1): 0.5479\tClassification Loss: 1.8440\n",
      "Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 3.1365\tTriple Loss(1): 0.4846\tClassification Loss: 2.1672\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 2.7402\tTriple Loss(1): 0.3493\tClassification Loss: 2.0415\n",
      "Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 2.4842\tTriple Loss(1): 0.2579\tClassification Loss: 1.9684\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 2.9076\tTriple Loss(1): 0.5844\tClassification Loss: 1.7389\n",
      "Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 2.3740\tTriple Loss(0): 0.0000\tClassification Loss: 2.3740\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 2.5532\tTriple Loss(1): 0.3270\tClassification Loss: 1.8992\n",
      "Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 2.9301\tTriple Loss(1): 0.4902\tClassification Loss: 1.9498\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 2.9281\tTriple Loss(1): 0.4011\tClassification Loss: 2.1259\n",
      "Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 2.1255\tTriple Loss(0): 0.0000\tClassification Loss: 2.1255\n",
      "\n",
      "Test set: Average loss: 2.0552, Accuracy: 442/960 (46%)\n",
      "\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 2.4062\tTriple Loss(1): 0.3059\tClassification Loss: 1.7944\n",
      "Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 2.0716\tTriple Loss(0): 0.0000\tClassification Loss: 2.0716\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 2.6025\tTriple Loss(1): 0.3060\tClassification Loss: 1.9905\n",
      "Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 2.9490\tTriple Loss(1): 0.4043\tClassification Loss: 2.1405\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 3.5304\tTriple Loss(1): 0.6128\tClassification Loss: 2.3049\n",
      "Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 2.7842\tTriple Loss(1): 0.4443\tClassification Loss: 1.8957\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 2.9956\tTriple Loss(1): 0.4409\tClassification Loss: 2.1139\n",
      "Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 3.2313\tTriple Loss(1): 0.5145\tClassification Loss: 2.2024\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 2.6267\tTriple Loss(1): 0.3616\tClassification Loss: 1.9035\n",
      "Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 2.5326\tTriple Loss(1): 0.2467\tClassification Loss: 2.0391\n",
      "\n",
      "Test set: Average loss: 2.0189, Accuracy: 445/960 (46%)\n",
      "\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 2.8240\tTriple Loss(1): 0.3765\tClassification Loss: 2.0709\n",
      "Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 2.7654\tTriple Loss(1): 0.2802\tClassification Loss: 2.2051\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 2.6757\tTriple Loss(1): 0.4651\tClassification Loss: 1.7454\n",
      "Train Epoch: 1 [26560/110534 (24%)]\tAll Loss: 2.7239\tTriple Loss(1): 0.2846\tClassification Loss: 2.1547\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tAll Loss: 2.8952\tTriple Loss(1): 0.3654\tClassification Loss: 2.1643\n",
      "Train Epoch: 1 [27200/110534 (25%)]\tAll Loss: 2.5882\tTriple Loss(1): 0.2133\tClassification Loss: 2.1617\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tAll Loss: 2.9765\tTriple Loss(1): 0.4522\tClassification Loss: 2.0720\n",
      "Train Epoch: 1 [27840/110534 (25%)]\tAll Loss: 2.6444\tTriple Loss(1): 0.4858\tClassification Loss: 1.6727\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tAll Loss: 2.1910\tTriple Loss(0): 0.0000\tClassification Loss: 2.1910\n",
      "Train Epoch: 1 [28480/110534 (26%)]\tAll Loss: 2.6191\tTriple Loss(1): 0.2397\tClassification Loss: 2.1397\n",
      "\n",
      "Test set: Average loss: 1.9852, Accuracy: 464/960 (48%)\n",
      "\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tAll Loss: 2.8718\tTriple Loss(1): 0.5096\tClassification Loss: 1.8527\n",
      "Train Epoch: 1 [29120/110534 (26%)]\tAll Loss: 1.9934\tTriple Loss(0): 0.0000\tClassification Loss: 1.9934\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tAll Loss: 1.8415\tTriple Loss(0): 0.0000\tClassification Loss: 1.8415\n",
      "Train Epoch: 1 [29760/110534 (27%)]\tAll Loss: 2.8349\tTriple Loss(1): 0.5418\tClassification Loss: 1.7513\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tAll Loss: 2.8767\tTriple Loss(1): 0.5367\tClassification Loss: 1.8032\n",
      "Train Epoch: 1 [30400/110534 (27%)]\tAll Loss: 1.8885\tTriple Loss(0): 0.0000\tClassification Loss: 1.8885\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tAll Loss: 2.3419\tTriple Loss(1): 0.3519\tClassification Loss: 1.6381\n",
      "Train Epoch: 1 [31040/110534 (28%)]\tAll Loss: 2.0692\tTriple Loss(0): 0.0000\tClassification Loss: 2.0692\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tAll Loss: 2.4971\tTriple Loss(1): 0.1483\tClassification Loss: 2.2005\n",
      "Train Epoch: 1 [31680/110534 (29%)]\tAll Loss: 2.8840\tTriple Loss(1): 0.5302\tClassification Loss: 1.8236\n",
      "\n",
      "Test set: Average loss: 1.9634, Accuracy: 495/960 (52%)\n",
      "\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tAll Loss: 2.7454\tTriple Loss(1): 0.3377\tClassification Loss: 2.0701\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n",
      "Train Epoch: 1 [32320/110534 (29%)]\tAll Loss: 2.7000\tTriple Loss(1): 0.4641\tClassification Loss: 1.7718\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tAll Loss: 2.9430\tTriple Loss(1): 0.4569\tClassification Loss: 2.0292\n",
      "Train Epoch: 1 [32960/110534 (30%)]\tAll Loss: 2.1577\tTriple Loss(0): 0.0000\tClassification Loss: 2.1577\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tAll Loss: 2.9199\tTriple Loss(1): 0.4382\tClassification Loss: 2.0434\n",
      "Train Epoch: 1 [33600/110534 (30%)]\tAll Loss: 2.3569\tTriple Loss(1): 0.2624\tClassification Loss: 1.8322\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tAll Loss: 2.4958\tTriple Loss(1): 0.3111\tClassification Loss: 1.8736\n",
      "Train Epoch: 1 [34240/110534 (31%)]\tAll Loss: 2.3389\tTriple Loss(1): 0.1821\tClassification Loss: 1.9747\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tAll Loss: 2.2353\tTriple Loss(1): 0.2653\tClassification Loss: 1.7047\n",
      "Train Epoch: 1 [34880/110534 (32%)]\tAll Loss: 2.4089\tTriple Loss(1): 0.2764\tClassification Loss: 1.8561\n",
      "\n",
      "Test set: Average loss: 1.9388, Accuracy: 509/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tAll Loss: 3.0277\tTriple Loss(1): 0.4425\tClassification Loss: 2.1426\n",
      "Train Epoch: 1 [35520/110534 (32%)]\tAll Loss: 2.5437\tTriple Loss(1): 0.3404\tClassification Loss: 1.8630\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tAll Loss: 2.4272\tTriple Loss(1): 0.3055\tClassification Loss: 1.8162\n",
      "Train Epoch: 1 [36160/110534 (33%)]\tAll Loss: 1.7582\tTriple Loss(0): 0.0000\tClassification Loss: 1.7582\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tAll Loss: 2.0010\tTriple Loss(0): 0.0000\tClassification Loss: 2.0010\n",
      "Train Epoch: 1 [36800/110534 (33%)]\tAll Loss: 2.5182\tTriple Loss(1): 0.3421\tClassification Loss: 1.8340\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tAll Loss: 2.0998\tTriple Loss(0): 0.0000\tClassification Loss: 2.0998\n",
      "Train Epoch: 1 [37440/110534 (34%)]\tAll Loss: 2.1519\tTriple Loss(0): 0.0000\tClassification Loss: 2.1519\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tAll Loss: 2.2487\tTriple Loss(1): 0.2287\tClassification Loss: 1.7913\n",
      "Train Epoch: 1 [38080/110534 (34%)]\tAll Loss: 2.7475\tTriple Loss(1): 0.3595\tClassification Loss: 2.0285\n",
      "\n",
      "Test set: Average loss: 1.9134, Accuracy: 506/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tAll Loss: 2.1651\tTriple Loss(1): 0.2237\tClassification Loss: 1.7178\n",
      "Train Epoch: 1 [38720/110534 (35%)]\tAll Loss: 2.3181\tTriple Loss(1): 0.2214\tClassification Loss: 1.8752\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tAll Loss: 1.7000\tTriple Loss(0): 0.0000\tClassification Loss: 1.7000\n",
      "Train Epoch: 1 [39360/110534 (36%)]\tAll Loss: 2.3534\tTriple Loss(1): 0.3298\tClassification Loss: 1.6938\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tAll Loss: 2.1633\tTriple Loss(0): 0.0000\tClassification Loss: 2.1633\n",
      "Train Epoch: 1 [40000/110534 (36%)]\tAll Loss: 2.1082\tTriple Loss(1): 0.3570\tClassification Loss: 1.3942\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tAll Loss: 1.9464\tTriple Loss(0): 0.0000\tClassification Loss: 1.9464\n",
      "Train Epoch: 1 [40640/110534 (37%)]\tAll Loss: 2.4581\tTriple Loss(1): 0.2881\tClassification Loss: 1.8819\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tAll Loss: 2.4596\tTriple Loss(1): 0.3763\tClassification Loss: 1.7070\n",
      "Train Epoch: 1 [41280/110534 (37%)]\tAll Loss: 2.8360\tTriple Loss(1): 0.3933\tClassification Loss: 2.0495\n",
      "\n",
      "Test set: Average loss: 1.8913, Accuracy: 501/960 (52%)\n",
      "\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tAll Loss: 2.6485\tTriple Loss(1): 0.2794\tClassification Loss: 2.0896\n",
      "Train Epoch: 1 [41920/110534 (38%)]\tAll Loss: 1.8013\tTriple Loss(0): 0.0000\tClassification Loss: 1.8013\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tAll Loss: 2.3413\tTriple Loss(1): 0.2126\tClassification Loss: 1.9160\n",
      "Train Epoch: 1 [42560/110534 (38%)]\tAll Loss: 2.4925\tTriple Loss(1): 0.3506\tClassification Loss: 1.7913\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tAll Loss: 2.5602\tTriple Loss(1): 0.3528\tClassification Loss: 1.8546\n",
      "Train Epoch: 1 [43200/110534 (39%)]\tAll Loss: 2.9310\tTriple Loss(1): 0.4195\tClassification Loss: 2.0920\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tAll Loss: 2.2854\tTriple Loss(1): 0.2855\tClassification Loss: 1.7144\n",
      "Train Epoch: 1 [43840/110534 (40%)]\tAll Loss: 2.2022\tTriple Loss(1): 0.2600\tClassification Loss: 1.6823\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tAll Loss: 2.6150\tTriple Loss(1): 0.2260\tClassification Loss: 2.1630\n",
      "Train Epoch: 1 [44480/110534 (40%)]\tAll Loss: 2.5634\tTriple Loss(1): 0.2866\tClassification Loss: 1.9902\n",
      "\n",
      "Test set: Average loss: 1.8762, Accuracy: 507/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tAll Loss: 2.0189\tTriple Loss(1): 0.2534\tClassification Loss: 1.5121\n",
      "Train Epoch: 1 [45120/110534 (41%)]\tAll Loss: 2.7420\tTriple Loss(1): 0.4076\tClassification Loss: 1.9269\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tAll Loss: 2.1491\tTriple Loss(1): 0.1709\tClassification Loss: 1.8072\n",
      "Train Epoch: 1 [45760/110534 (41%)]\tAll Loss: 2.4320\tTriple Loss(1): 0.4516\tClassification Loss: 1.5288\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tAll Loss: 1.5956\tTriple Loss(0): 0.0000\tClassification Loss: 1.5956\n",
      "Train Epoch: 1 [46400/110534 (42%)]\tAll Loss: 2.4420\tTriple Loss(1): 0.1715\tClassification Loss: 2.0991\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tAll Loss: 2.8519\tTriple Loss(1): 0.4245\tClassification Loss: 2.0029\n",
      "Train Epoch: 1 [47040/110534 (43%)]\tAll Loss: 2.5904\tTriple Loss(1): 0.3942\tClassification Loss: 1.8020\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tAll Loss: 2.6433\tTriple Loss(1): 0.3151\tClassification Loss: 2.0130\n",
      "Train Epoch: 1 [47680/110534 (43%)]\tAll Loss: 1.6539\tTriple Loss(1): 0.0781\tClassification Loss: 1.4976\n",
      "\n",
      "Test set: Average loss: 1.8631, Accuracy: 505/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tAll Loss: 3.2785\tTriple Loss(1): 0.5653\tClassification Loss: 2.1478\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n",
      "Train Epoch: 1 [48320/110534 (44%)]\tAll Loss: 1.8732\tTriple Loss(0): 0.0000\tClassification Loss: 1.8732\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tAll Loss: 2.5874\tTriple Loss(1): 0.2613\tClassification Loss: 2.0649\n",
      "Train Epoch: 1 [48960/110534 (44%)]\tAll Loss: 2.1897\tTriple Loss(1): 0.2048\tClassification Loss: 1.7800\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tAll Loss: 2.7718\tTriple Loss(1): 0.4770\tClassification Loss: 1.8178\n",
      "Train Epoch: 1 [49600/110534 (45%)]\tAll Loss: 2.2213\tTriple Loss(1): 0.1181\tClassification Loss: 1.9852\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tAll Loss: 2.5682\tTriple Loss(1): 0.4894\tClassification Loss: 1.5893\n",
      "Train Epoch: 1 [50240/110534 (45%)]\tAll Loss: 2.1994\tTriple Loss(1): 0.3703\tClassification Loss: 1.4588\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tAll Loss: 2.9597\tTriple Loss(1): 0.4937\tClassification Loss: 1.9723\n",
      "Train Epoch: 1 [50880/110534 (46%)]\tAll Loss: 2.4249\tTriple Loss(1): 0.3435\tClassification Loss: 1.7379\n",
      "\n",
      "Test set: Average loss: 1.8459, Accuracy: 514/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tAll Loss: 2.6998\tTriple Loss(1): 0.4711\tClassification Loss: 1.7577\n",
      "Train Epoch: 1 [51520/110534 (47%)]\tAll Loss: 2.5335\tTriple Loss(1): 0.4784\tClassification Loss: 1.5767\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tAll Loss: 2.4529\tTriple Loss(1): 0.3118\tClassification Loss: 1.8293\n",
      "Train Epoch: 1 [52160/110534 (47%)]\tAll Loss: 1.8344\tTriple Loss(0): 0.0000\tClassification Loss: 1.8344\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tAll Loss: 2.6516\tTriple Loss(1): 0.1587\tClassification Loss: 2.3342\n",
      "Train Epoch: 1 [52800/110534 (48%)]\tAll Loss: 1.7014\tTriple Loss(0): 0.0000\tClassification Loss: 1.7014\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tAll Loss: 1.7610\tTriple Loss(0): 0.0000\tClassification Loss: 1.7610\n",
      "Train Epoch: 1 [53440/110534 (48%)]\tAll Loss: 2.1598\tTriple Loss(1): 0.1105\tClassification Loss: 1.9388\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tAll Loss: 2.5967\tTriple Loss(1): 0.3039\tClassification Loss: 1.9889\n",
      "Train Epoch: 1 [54080/110534 (49%)]\tAll Loss: 2.7736\tTriple Loss(1): 0.4240\tClassification Loss: 1.9255\n",
      "\n",
      "Test set: Average loss: 1.8410, Accuracy: 508/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tAll Loss: 2.8189\tTriple Loss(1): 0.4549\tClassification Loss: 1.9091\n",
      "Train Epoch: 1 [54720/110534 (49%)]\tAll Loss: 2.6048\tTriple Loss(1): 0.3069\tClassification Loss: 1.9909\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tAll Loss: 2.5006\tTriple Loss(1): 0.1884\tClassification Loss: 2.1238\n",
      "Train Epoch: 1 [55360/110534 (50%)]\tAll Loss: 1.9566\tTriple Loss(1): 0.1422\tClassification Loss: 1.6722\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tAll Loss: 2.2964\tTriple Loss(1): 0.1742\tClassification Loss: 1.9481\n",
      "Train Epoch: 1 [56000/110534 (51%)]\tAll Loss: 3.4284\tTriple Loss(1): 0.5743\tClassification Loss: 2.2799\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tAll Loss: 2.3117\tTriple Loss(1): 0.1571\tClassification Loss: 1.9976\n",
      "Train Epoch: 1 [56640/110534 (51%)]\tAll Loss: 2.4158\tTriple Loss(1): 0.5048\tClassification Loss: 1.4063\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tAll Loss: 2.8628\tTriple Loss(1): 0.5304\tClassification Loss: 1.8021\n",
      "Train Epoch: 1 [57280/110534 (52%)]\tAll Loss: 3.0746\tTriple Loss(1): 0.5206\tClassification Loss: 2.0335\n",
      "\n",
      "Test set: Average loss: 1.8273, Accuracy: 521/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tAll Loss: 2.3462\tTriple Loss(1): 0.1881\tClassification Loss: 1.9701\n",
      "Train Epoch: 1 [57920/110534 (52%)]\tAll Loss: 2.9457\tTriple Loss(1): 0.4668\tClassification Loss: 2.0122\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tAll Loss: 2.2595\tTriple Loss(1): 0.2970\tClassification Loss: 1.6654\n",
      "Train Epoch: 1 [58560/110534 (53%)]\tAll Loss: 1.8096\tTriple Loss(0): 0.0000\tClassification Loss: 1.8096\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tAll Loss: 1.9453\tTriple Loss(0): 0.0000\tClassification Loss: 1.9453\n",
      "Train Epoch: 1 [59200/110534 (54%)]\tAll Loss: 2.0179\tTriple Loss(0): 0.0000\tClassification Loss: 2.0179\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tAll Loss: 1.9886\tTriple Loss(1): 0.1341\tClassification Loss: 1.7204\n",
      "Train Epoch: 1 [59840/110534 (54%)]\tAll Loss: 1.6498\tTriple Loss(0): 0.0000\tClassification Loss: 1.6498\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tAll Loss: 2.3359\tTriple Loss(1): 0.3142\tClassification Loss: 1.7074\n",
      "Train Epoch: 1 [60480/110534 (55%)]\tAll Loss: 1.7006\tTriple Loss(0): 0.0000\tClassification Loss: 1.7006\n",
      "\n",
      "Test set: Average loss: 1.8186, Accuracy: 516/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tAll Loss: 1.9742\tTriple Loss(0): 0.0000\tClassification Loss: 1.9742\n",
      "Train Epoch: 1 [61120/110534 (55%)]\tAll Loss: 2.8295\tTriple Loss(1): 0.4873\tClassification Loss: 1.8549\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tAll Loss: 2.5856\tTriple Loss(1): 0.6225\tClassification Loss: 1.3406\n",
      "Train Epoch: 1 [61760/110534 (56%)]\tAll Loss: 2.5430\tTriple Loss(1): 0.2986\tClassification Loss: 1.9459\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tAll Loss: 2.4627\tTriple Loss(1): 0.3963\tClassification Loss: 1.6701\n",
      "Train Epoch: 1 [62400/110534 (56%)]\tAll Loss: 2.0829\tTriple Loss(1): 0.1892\tClassification Loss: 1.7045\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tAll Loss: 1.4925\tTriple Loss(0): 0.0000\tClassification Loss: 1.4925\n",
      "Train Epoch: 1 [63040/110534 (57%)]\tAll Loss: 2.7423\tTriple Loss(1): 0.1598\tClassification Loss: 2.4227\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tAll Loss: 2.3618\tTriple Loss(1): 0.2468\tClassification Loss: 1.8681\n",
      "Train Epoch: 1 [63680/110534 (58%)]\tAll Loss: 2.3278\tTriple Loss(1): 0.4459\tClassification Loss: 1.4361\n",
      "\n",
      "Test set: Average loss: 1.8103, Accuracy: 506/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tAll Loss: 2.8265\tTriple Loss(1): 0.3108\tClassification Loss: 2.2049\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2000.pth.tar\n",
      "Train Epoch: 1 [64320/110534 (58%)]\tAll Loss: 1.6694\tTriple Loss(0): 0.0000\tClassification Loss: 1.6694\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tAll Loss: 2.8607\tTriple Loss(1): 0.1525\tClassification Loss: 2.5558\n",
      "Train Epoch: 1 [64960/110534 (59%)]\tAll Loss: 2.3631\tTriple Loss(1): 0.3434\tClassification Loss: 1.6764\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tAll Loss: 2.6446\tTriple Loss(1): 0.3635\tClassification Loss: 1.9175\n",
      "Train Epoch: 1 [65600/110534 (59%)]\tAll Loss: 4.7827\tTriple Loss(1): 1.4682\tClassification Loss: 1.8464\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tAll Loss: 2.0072\tTriple Loss(1): 0.2383\tClassification Loss: 1.5306\n",
      "Train Epoch: 1 [66240/110534 (60%)]\tAll Loss: 2.4007\tTriple Loss(1): 0.3395\tClassification Loss: 1.7217\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tAll Loss: 2.8311\tTriple Loss(1): 0.5403\tClassification Loss: 1.7505\n",
      "Train Epoch: 1 [66880/110534 (60%)]\tAll Loss: 2.5571\tTriple Loss(1): 0.2954\tClassification Loss: 1.9662\n",
      "\n",
      "Test set: Average loss: 1.8084, Accuracy: 507/960 (53%)\n",
      "\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tAll Loss: 2.4412\tTriple Loss(1): 0.2294\tClassification Loss: 1.9824\n",
      "Train Epoch: 1 [67520/110534 (61%)]\tAll Loss: 2.6413\tTriple Loss(1): 0.3574\tClassification Loss: 1.9264\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tAll Loss: 2.9034\tTriple Loss(1): 0.4123\tClassification Loss: 2.0788\n",
      "Train Epoch: 1 [68160/110534 (62%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.1701\tClassification Loss: 1.4205\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tAll Loss: 2.0638\tTriple Loss(1): 0.2492\tClassification Loss: 1.5654\n",
      "Train Epoch: 1 [68800/110534 (62%)]\tAll Loss: 2.3183\tTriple Loss(1): 0.3316\tClassification Loss: 1.6551\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tAll Loss: 2.3030\tTriple Loss(1): 0.2948\tClassification Loss: 1.7135\n",
      "Train Epoch: 1 [69440/110534 (63%)]\tAll Loss: 2.4529\tTriple Loss(1): 0.2853\tClassification Loss: 1.8823\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tAll Loss: 2.0452\tTriple Loss(1): 0.1375\tClassification Loss: 1.7701\n",
      "Train Epoch: 1 [70080/110534 (63%)]\tAll Loss: 1.6478\tTriple Loss(1): 0.0251\tClassification Loss: 1.5975\n",
      "\n",
      "Test set: Average loss: 1.7928, Accuracy: 519/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tAll Loss: 2.3296\tTriple Loss(1): 0.3641\tClassification Loss: 1.6013\n",
      "Train Epoch: 1 [70720/110534 (64%)]\tAll Loss: 1.8982\tTriple Loss(1): 0.2154\tClassification Loss: 1.4675\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tAll Loss: 2.2778\tTriple Loss(1): 0.2835\tClassification Loss: 1.7109\n",
      "Train Epoch: 1 [71360/110534 (65%)]\tAll Loss: 2.1244\tTriple Loss(1): 0.1839\tClassification Loss: 1.7567\n",
      "Train Epoch: 1 [71680/110534 (65%)]\tAll Loss: 2.4158\tTriple Loss(1): 0.2634\tClassification Loss: 1.8889\n",
      "Train Epoch: 1 [72000/110534 (65%)]\tAll Loss: 2.6531\tTriple Loss(1): 0.4082\tClassification Loss: 1.8368\n",
      "Train Epoch: 1 [72320/110534 (65%)]\tAll Loss: 2.3675\tTriple Loss(1): 0.3691\tClassification Loss: 1.6293\n",
      "Train Epoch: 1 [72640/110534 (66%)]\tAll Loss: 2.2782\tTriple Loss(1): 0.4456\tClassification Loss: 1.3870\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tAll Loss: 2.0959\tTriple Loss(1): 0.2819\tClassification Loss: 1.5322\n",
      "Train Epoch: 1 [73280/110534 (66%)]\tAll Loss: 2.0327\tTriple Loss(1): 0.3087\tClassification Loss: 1.4154\n",
      "\n",
      "Test set: Average loss: 1.7856, Accuracy: 525/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [73600/110534 (67%)]\tAll Loss: 2.8245\tTriple Loss(1): 0.3028\tClassification Loss: 2.2190\n",
      "Train Epoch: 1 [73920/110534 (67%)]\tAll Loss: 2.5612\tTriple Loss(1): 0.3287\tClassification Loss: 1.9038\n",
      "Train Epoch: 1 [74240/110534 (67%)]\tAll Loss: 2.0773\tTriple Loss(1): 0.2912\tClassification Loss: 1.4949\n",
      "Train Epoch: 1 [74560/110534 (67%)]\tAll Loss: 2.9874\tTriple Loss(1): 0.3831\tClassification Loss: 2.2212\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tAll Loss: 2.4087\tTriple Loss(1): 0.2402\tClassification Loss: 1.9284\n",
      "Train Epoch: 1 [75200/110534 (68%)]\tAll Loss: 2.0415\tTriple Loss(1): 0.1795\tClassification Loss: 1.6826\n",
      "Train Epoch: 1 [75520/110534 (68%)]\tAll Loss: 2.2684\tTriple Loss(1): 0.3777\tClassification Loss: 1.5129\n",
      "Train Epoch: 1 [75840/110534 (69%)]\tAll Loss: 2.3190\tTriple Loss(1): 0.2496\tClassification Loss: 1.8198\n",
      "Train Epoch: 1 [76160/110534 (69%)]\tAll Loss: 2.1836\tTriple Loss(1): 0.1938\tClassification Loss: 1.7961\n",
      "Train Epoch: 1 [76480/110534 (69%)]\tAll Loss: 2.0495\tTriple Loss(1): 0.2256\tClassification Loss: 1.5982\n",
      "\n",
      "Test set: Average loss: 1.7764, Accuracy: 532/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tAll Loss: 1.7576\tTriple Loss(0): 0.0000\tClassification Loss: 1.7576\n",
      "Train Epoch: 1 [77120/110534 (70%)]\tAll Loss: 1.7004\tTriple Loss(0): 0.0000\tClassification Loss: 1.7004\n",
      "Train Epoch: 1 [77440/110534 (70%)]\tAll Loss: 1.7737\tTriple Loss(0): 0.0000\tClassification Loss: 1.7737\n",
      "Train Epoch: 1 [77760/110534 (70%)]\tAll Loss: 1.5699\tTriple Loss(0): 0.0000\tClassification Loss: 1.5699\n",
      "Train Epoch: 1 [78080/110534 (71%)]\tAll Loss: 2.1513\tTriple Loss(0): 0.0000\tClassification Loss: 2.1513\n",
      "Train Epoch: 1 [78400/110534 (71%)]\tAll Loss: 1.8238\tTriple Loss(0): 0.0000\tClassification Loss: 1.8238\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tAll Loss: 1.9410\tTriple Loss(1): 0.2228\tClassification Loss: 1.4953\n",
      "Train Epoch: 1 [79040/110534 (71%)]\tAll Loss: 2.2730\tTriple Loss(1): 0.2462\tClassification Loss: 1.7806\n",
      "Train Epoch: 1 [79360/110534 (72%)]\tAll Loss: 2.7604\tTriple Loss(1): 0.3362\tClassification Loss: 2.0881\n",
      "Train Epoch: 1 [79680/110534 (72%)]\tAll Loss: 2.2181\tTriple Loss(1): 0.2943\tClassification Loss: 1.6295\n",
      "\n",
      "Test set: Average loss: 1.7714, Accuracy: 522/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [80000/110534 (72%)]\tAll Loss: 2.3018\tTriple Loss(1): 0.2581\tClassification Loss: 1.7856\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2500.pth.tar\n",
      "Train Epoch: 1 [80320/110534 (73%)]\tAll Loss: 1.8587\tTriple Loss(1): 0.2124\tClassification Loss: 1.4340\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tAll Loss: 1.8738\tTriple Loss(1): 0.1223\tClassification Loss: 1.6291\n",
      "Train Epoch: 1 [80960/110534 (73%)]\tAll Loss: 1.6995\tTriple Loss(0): 0.0000\tClassification Loss: 1.6995\n",
      "Train Epoch: 1 [81280/110534 (74%)]\tAll Loss: 2.2450\tTriple Loss(1): 0.3222\tClassification Loss: 1.6006\n",
      "Train Epoch: 1 [81600/110534 (74%)]\tAll Loss: 1.4992\tTriple Loss(0): 0.0000\tClassification Loss: 1.4992\n",
      "Train Epoch: 1 [81920/110534 (74%)]\tAll Loss: 2.1801\tTriple Loss(1): 0.2521\tClassification Loss: 1.6758\n",
      "Train Epoch: 1 [82240/110534 (74%)]\tAll Loss: 2.8596\tTriple Loss(1): 0.5311\tClassification Loss: 1.7975\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tAll Loss: 1.7594\tTriple Loss(0): 0.0000\tClassification Loss: 1.7594\n",
      "Train Epoch: 1 [82880/110534 (75%)]\tAll Loss: 1.9521\tTriple Loss(0): 0.0000\tClassification Loss: 1.9521\n",
      "\n",
      "Test set: Average loss: 1.7686, Accuracy: 524/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [83200/110534 (75%)]\tAll Loss: 2.5890\tTriple Loss(1): 0.4552\tClassification Loss: 1.6786\n",
      "Train Epoch: 1 [83520/110534 (76%)]\tAll Loss: 2.0330\tTriple Loss(1): 0.1459\tClassification Loss: 1.7413\n",
      "Train Epoch: 1 [83840/110534 (76%)]\tAll Loss: 2.1185\tTriple Loss(1): 0.2446\tClassification Loss: 1.6293\n",
      "Train Epoch: 1 [84160/110534 (76%)]\tAll Loss: 2.1832\tTriple Loss(1): 0.3422\tClassification Loss: 1.4988\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tAll Loss: 1.7955\tTriple Loss(1): 0.1631\tClassification Loss: 1.4693\n",
      "Train Epoch: 1 [84800/110534 (77%)]\tAll Loss: 2.3203\tTriple Loss(1): 0.1800\tClassification Loss: 1.9603\n",
      "Train Epoch: 1 [85120/110534 (77%)]\tAll Loss: 2.3963\tTriple Loss(1): 0.4111\tClassification Loss: 1.5741\n",
      "Train Epoch: 1 [85440/110534 (77%)]\tAll Loss: 1.9818\tTriple Loss(1): 0.2462\tClassification Loss: 1.4894\n",
      "Train Epoch: 1 [85760/110534 (78%)]\tAll Loss: 2.2152\tTriple Loss(1): 0.2293\tClassification Loss: 1.7566\n",
      "Train Epoch: 1 [86080/110534 (78%)]\tAll Loss: 2.4546\tTriple Loss(1): 0.3882\tClassification Loss: 1.6782\n",
      "\n",
      "Test set: Average loss: 1.7602, Accuracy: 529/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tAll Loss: 2.5731\tTriple Loss(1): 0.1879\tClassification Loss: 2.1973\n",
      "Train Epoch: 1 [86720/110534 (78%)]\tAll Loss: 1.9862\tTriple Loss(1): 0.2532\tClassification Loss: 1.4797\n",
      "Train Epoch: 1 [87040/110534 (79%)]\tAll Loss: 2.3193\tTriple Loss(1): 0.3745\tClassification Loss: 1.5703\n",
      "Train Epoch: 1 [87360/110534 (79%)]\tAll Loss: 2.4982\tTriple Loss(1): 0.3791\tClassification Loss: 1.7399\n",
      "Train Epoch: 1 [87680/110534 (79%)]\tAll Loss: 2.1686\tTriple Loss(1): 0.2151\tClassification Loss: 1.7384\n",
      "Train Epoch: 1 [88000/110534 (80%)]\tAll Loss: 3.0999\tTriple Loss(1): 0.3523\tClassification Loss: 2.3954\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tAll Loss: 2.3341\tTriple Loss(1): 0.3855\tClassification Loss: 1.5630\n",
      "Train Epoch: 1 [88640/110534 (80%)]\tAll Loss: 3.2444\tTriple Loss(1): 0.5510\tClassification Loss: 2.1423\n",
      "Train Epoch: 1 [88960/110534 (80%)]\tAll Loss: 2.5136\tTriple Loss(1): 0.3174\tClassification Loss: 1.8788\n",
      "Train Epoch: 1 [89280/110534 (81%)]\tAll Loss: 6.7501\tTriple Loss(0): 2.5056\tClassification Loss: 1.7390\n",
      "\n",
      "Test set: Average loss: 1.7580, Accuracy: 533/960 (56%)\n",
      "\n",
      "Train Epoch: 1 [89600/110534 (81%)]\tAll Loss: 2.7892\tTriple Loss(1): 0.3187\tClassification Loss: 2.1518\n",
      "Train Epoch: 1 [89920/110534 (81%)]\tAll Loss: 2.8351\tTriple Loss(1): 0.3775\tClassification Loss: 2.0800\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tAll Loss: 2.0197\tTriple Loss(1): 0.2858\tClassification Loss: 1.4481\n",
      "Train Epoch: 1 [90560/110534 (82%)]\tAll Loss: 1.7117\tTriple Loss(1): 0.1398\tClassification Loss: 1.4321\n",
      "Train Epoch: 1 [90880/110534 (82%)]\tAll Loss: 1.7422\tTriple Loss(0): 0.0000\tClassification Loss: 1.7422\n",
      "Train Epoch: 1 [91200/110534 (82%)]\tAll Loss: 2.0855\tTriple Loss(1): 0.1686\tClassification Loss: 1.7483\n",
      "Train Epoch: 1 [91520/110534 (83%)]\tAll Loss: 2.3808\tTriple Loss(1): 0.3919\tClassification Loss: 1.5970\n",
      "Train Epoch: 1 [91840/110534 (83%)]\tAll Loss: 1.7672\tTriple Loss(0): 0.0000\tClassification Loss: 1.7672\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tAll Loss: 2.5439\tTriple Loss(1): 0.2743\tClassification Loss: 1.9953\n",
      "Train Epoch: 1 [92480/110534 (84%)]\tAll Loss: 2.3065\tTriple Loss(1): 0.3451\tClassification Loss: 1.6163\n",
      "\n",
      "Test set: Average loss: 1.7517, Accuracy: 532/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [92800/110534 (84%)]\tAll Loss: 1.6471\tTriple Loss(0): 0.0000\tClassification Loss: 1.6471\n",
      "Train Epoch: 1 [93120/110534 (84%)]\tAll Loss: 1.9948\tTriple Loss(0): 0.0000\tClassification Loss: 1.9948\n",
      "Train Epoch: 1 [93440/110534 (85%)]\tAll Loss: 2.1370\tTriple Loss(1): 0.1822\tClassification Loss: 1.7726\n",
      "Train Epoch: 1 [93760/110534 (85%)]\tAll Loss: 2.3059\tTriple Loss(1): 0.4311\tClassification Loss: 1.4438\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tAll Loss: 2.5431\tTriple Loss(1): 0.4537\tClassification Loss: 1.6357\n",
      "Train Epoch: 1 [94400/110534 (85%)]\tAll Loss: 1.8640\tTriple Loss(1): 0.0899\tClassification Loss: 1.6843\n",
      "Train Epoch: 1 [94720/110534 (86%)]\tAll Loss: 2.0161\tTriple Loss(1): 0.1419\tClassification Loss: 1.7324\n",
      "Train Epoch: 1 [95040/110534 (86%)]\tAll Loss: 2.9017\tTriple Loss(1): 0.4830\tClassification Loss: 1.9358\n",
      "Train Epoch: 1 [95360/110534 (86%)]\tAll Loss: 2.5064\tTriple Loss(1): 0.3799\tClassification Loss: 1.7467\n",
      "Train Epoch: 1 [95680/110534 (87%)]\tAll Loss: 6.8117\tTriple Loss(0): 2.5527\tClassification Loss: 1.7063\n",
      "\n",
      "Test set: Average loss: 1.7453, Accuracy: 544/960 (57%)\n",
      "\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tAll Loss: 1.4101\tTriple Loss(0): 0.0000\tClassification Loss: 1.4101\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_3000.pth.tar\n",
      "Train Epoch: 1 [96320/110534 (87%)]\tAll Loss: 1.8353\tTriple Loss(1): 0.1840\tClassification Loss: 1.4673\n",
      "Train Epoch: 1 [96640/110534 (87%)]\tAll Loss: 2.2191\tTriple Loss(1): 0.1817\tClassification Loss: 1.8557\n",
      "Train Epoch: 1 [96960/110534 (88%)]\tAll Loss: 1.3168\tTriple Loss(0): 0.0000\tClassification Loss: 1.3168\n",
      "Train Epoch: 1 [97280/110534 (88%)]\tAll Loss: 1.9920\tTriple Loss(1): 0.2517\tClassification Loss: 1.4887\n",
      "Train Epoch: 1 [97600/110534 (88%)]\tAll Loss: 2.3372\tTriple Loss(1): 0.1470\tClassification Loss: 2.0431\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tAll Loss: 1.6884\tTriple Loss(1): 0.2043\tClassification Loss: 1.2799\n",
      "Train Epoch: 1 [98240/110534 (89%)]\tAll Loss: 2.0529\tTriple Loss(1): 0.3436\tClassification Loss: 1.3658\n",
      "Train Epoch: 1 [98560/110534 (89%)]\tAll Loss: 2.7592\tTriple Loss(1): 0.5253\tClassification Loss: 1.7086\n",
      "Train Epoch: 1 [98880/110534 (89%)]\tAll Loss: 2.2714\tTriple Loss(1): 0.3334\tClassification Loss: 1.6045\n",
      "\n",
      "Test set: Average loss: 1.7466, Accuracy: 531/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [99200/110534 (90%)]\tAll Loss: 1.4943\tTriple Loss(0): 0.0000\tClassification Loss: 1.4943\n",
      "Train Epoch: 1 [99520/110534 (90%)]\tAll Loss: 2.2640\tTriple Loss(1): 0.2768\tClassification Loss: 1.7104\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tAll Loss: 2.4267\tTriple Loss(1): 0.3344\tClassification Loss: 1.7579\n",
      "Train Epoch: 1 [100160/110534 (91%)]\tAll Loss: 1.9971\tTriple Loss(0): 0.0000\tClassification Loss: 1.9971\n",
      "Train Epoch: 1 [100480/110534 (91%)]\tAll Loss: 2.1782\tTriple Loss(1): 0.2529\tClassification Loss: 1.6725\n",
      "Train Epoch: 1 [100800/110534 (91%)]\tAll Loss: 2.7019\tTriple Loss(1): 0.1748\tClassification Loss: 2.3522\n",
      "Train Epoch: 1 [101120/110534 (91%)]\tAll Loss: 1.4711\tTriple Loss(0): 0.0000\tClassification Loss: 1.4711\n",
      "Train Epoch: 1 [101440/110534 (92%)]\tAll Loss: 2.0412\tTriple Loss(1): 0.3027\tClassification Loss: 1.4359\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tAll Loss: 1.8082\tTriple Loss(1): 0.0902\tClassification Loss: 1.6278\n",
      "Train Epoch: 1 [102080/110534 (92%)]\tAll Loss: 2.6796\tTriple Loss(1): 0.2749\tClassification Loss: 2.1299\n",
      "\n",
      "Test set: Average loss: 1.7372, Accuracy: 530/960 (55%)\n",
      "\n",
      "Train Epoch: 1 [102400/110534 (93%)]\tAll Loss: 2.4854\tTriple Loss(1): 0.3328\tClassification Loss: 1.8197\n",
      "Train Epoch: 1 [102720/110534 (93%)]\tAll Loss: 2.5434\tTriple Loss(1): 0.3558\tClassification Loss: 1.8319\n",
      "Train Epoch: 1 [103040/110534 (93%)]\tAll Loss: 2.0624\tTriple Loss(1): 0.2261\tClassification Loss: 1.6101\n",
      "Train Epoch: 1 [103360/110534 (93%)]\tAll Loss: 2.2817\tTriple Loss(1): 0.2097\tClassification Loss: 1.8623\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tAll Loss: 2.1291\tTriple Loss(1): 0.1359\tClassification Loss: 1.8572\n",
      "Train Epoch: 1 [104000/110534 (94%)]\tAll Loss: 1.6326\tTriple Loss(0): 0.0000\tClassification Loss: 1.6326\n",
      "Train Epoch: 1 [104320/110534 (94%)]\tAll Loss: 2.5769\tTriple Loss(1): 0.2595\tClassification Loss: 2.0580\n",
      "Train Epoch: 1 [104640/110534 (95%)]\tAll Loss: 1.9418\tTriple Loss(0): 0.0000\tClassification Loss: 1.9418\n",
      "Train Epoch: 1 [104960/110534 (95%)]\tAll Loss: 2.2493\tTriple Loss(1): 0.3140\tClassification Loss: 1.6213\n",
      "Train Epoch: 1 [105280/110534 (95%)]\tAll Loss: 2.4247\tTriple Loss(1): 0.3199\tClassification Loss: 1.7849\n",
      "\n",
      "Test set: Average loss: 1.7575, Accuracy: 517/960 (54%)\n",
      "\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tAll Loss: 2.3755\tTriple Loss(1): 0.3085\tClassification Loss: 1.7585\n",
      "Train Epoch: 1 [105920/110534 (96%)]\tAll Loss: 2.2612\tTriple Loss(1): 0.2635\tClassification Loss: 1.7341\n",
      "Train Epoch: 1 [106240/110534 (96%)]\tAll Loss: 2.2239\tTriple Loss(1): 0.3401\tClassification Loss: 1.5436\n",
      "Train Epoch: 1 [106560/110534 (96%)]\tAll Loss: 2.0847\tTriple Loss(1): 0.2904\tClassification Loss: 1.5039\n",
      "Train Epoch: 1 [106880/110534 (97%)]\tAll Loss: 2.3152\tTriple Loss(1): 0.1991\tClassification Loss: 1.9169\n",
      "Train Epoch: 1 [107200/110534 (97%)]\tAll Loss: 2.6478\tTriple Loss(1): 0.4249\tClassification Loss: 1.7981\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tAll Loss: 2.9075\tTriple Loss(1): 0.4104\tClassification Loss: 2.0868\n",
      "Train Epoch: 1 [107840/110534 (98%)]\tAll Loss: 2.3895\tTriple Loss(1): 0.4341\tClassification Loss: 1.5214\n",
      "Train Epoch: 1 [108160/110534 (98%)]\tAll Loss: 2.0814\tTriple Loss(1): 0.2965\tClassification Loss: 1.4883\n",
      "Train Epoch: 1 [108480/110534 (98%)]\tAll Loss: 2.5762\tTriple Loss(1): 0.4362\tClassification Loss: 1.7038\n",
      "\n",
      "Test set: Average loss: 1.7241, Accuracy: 535/960 (56%)\n",
      "\n",
      "Train Epoch: 1 [108800/110534 (98%)]\tAll Loss: 2.6288\tTriple Loss(1): 0.4828\tClassification Loss: 1.6632\n",
      "Train Epoch: 1 [109120/110534 (99%)]\tAll Loss: 2.3780\tTriple Loss(1): 0.3193\tClassification Loss: 1.7395\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tAll Loss: 1.6101\tTriple Loss(0): 0.0000\tClassification Loss: 1.6101\n",
      "Train Epoch: 1 [109760/110534 (99%)]\tAll Loss: 2.0934\tTriple Loss(1): 0.2363\tClassification Loss: 1.6209\n",
      "Train Epoch: 1 [110080/110534 (100%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.2957\tClassification Loss: 1.5857\n",
      "Train Epoch: 1 [110400/110534 (100%)]\tAll Loss: 2.1458\tTriple Loss(1): 0.2415\tClassification Loss: 1.6629\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_final.pth.tar\n",
      "\n",
      "Test set: Average loss: 1.7276, Accuracy: 535/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [0/110534 (0%)]\tAll Loss: 2.4888\tTriple Loss(1): 0.4119\tClassification Loss: 1.6651\n",
      "Train Epoch: 2 [320/110534 (0%)]\tAll Loss: 1.9379\tTriple Loss(1): 0.1140\tClassification Loss: 1.7098\n",
      "Train Epoch: 2 [640/110534 (1%)]\tAll Loss: 3.1061\tTriple Loss(0): 1.0354\tClassification Loss: 1.0353\n",
      "Train Epoch: 2 [960/110534 (1%)]\tAll Loss: 2.6403\tTriple Loss(1): 0.3697\tClassification Loss: 1.9010\n",
      "Train Epoch: 2 [1280/110534 (1%)]\tAll Loss: 2.5230\tTriple Loss(1): 0.4533\tClassification Loss: 1.6164\n",
      "Train Epoch: 2 [1600/110534 (1%)]\tAll Loss: 2.4727\tTriple Loss(1): 0.3389\tClassification Loss: 1.7949\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tAll Loss: 1.8828\tTriple Loss(1): 0.0665\tClassification Loss: 1.7497\n",
      "Train Epoch: 2 [2240/110534 (2%)]\tAll Loss: 2.1663\tTriple Loss(1): 0.2365\tClassification Loss: 1.6934\n",
      "Train Epoch: 2 [2560/110534 (2%)]\tAll Loss: 1.6943\tTriple Loss(0): 0.0000\tClassification Loss: 1.6943\n",
      "Train Epoch: 2 [2880/110534 (3%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.1955\tClassification Loss: 1.9384\n",
      "\n",
      "Test set: Average loss: 1.7290, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [3200/110534 (3%)]\tAll Loss: 2.8168\tTriple Loss(1): 0.2830\tClassification Loss: 2.2508\n",
      "Train Epoch: 2 [3520/110534 (3%)]\tAll Loss: 1.6901\tTriple Loss(1): 0.2318\tClassification Loss: 1.2266\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tAll Loss: 2.3281\tTriple Loss(1): 0.3439\tClassification Loss: 1.6402\n",
      "Train Epoch: 2 [4160/110534 (4%)]\tAll Loss: 2.7172\tTriple Loss(1): 0.5102\tClassification Loss: 1.6969\n",
      "Train Epoch: 2 [4480/110534 (4%)]\tAll Loss: 2.1177\tTriple Loss(1): 0.3157\tClassification Loss: 1.4864\n",
      "Train Epoch: 2 [4800/110534 (4%)]\tAll Loss: 1.9950\tTriple Loss(1): 0.2141\tClassification Loss: 1.5669\n",
      "Train Epoch: 2 [5120/110534 (5%)]\tAll Loss: 2.8651\tTriple Loss(1): 0.4585\tClassification Loss: 1.9481\n",
      "Train Epoch: 2 [5440/110534 (5%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.1962\tClassification Loss: 1.7502\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tAll Loss: 1.9910\tTriple Loss(1): 0.2595\tClassification Loss: 1.4720\n",
      "Train Epoch: 2 [6080/110534 (5%)]\tAll Loss: 1.6211\tTriple Loss(0): 0.0000\tClassification Loss: 1.6211\n",
      "\n",
      "Test set: Average loss: 1.7271, Accuracy: 538/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [6400/110534 (6%)]\tAll Loss: 2.5488\tTriple Loss(1): 0.2871\tClassification Loss: 1.9746\n",
      "Train Epoch: 2 [6720/110534 (6%)]\tAll Loss: 2.0933\tTriple Loss(1): 0.1458\tClassification Loss: 1.8016\n",
      "Train Epoch: 2 [7040/110534 (6%)]\tAll Loss: 2.2270\tTriple Loss(1): 0.2193\tClassification Loss: 1.7884\n",
      "Train Epoch: 2 [7360/110534 (7%)]\tAll Loss: 1.7071\tTriple Loss(0): 0.0000\tClassification Loss: 1.7071\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tAll Loss: 2.0665\tTriple Loss(1): 0.2850\tClassification Loss: 1.4966\n",
      "Train Epoch: 2 [8000/110534 (7%)]\tAll Loss: 1.9871\tTriple Loss(1): 0.3577\tClassification Loss: 1.2716\n",
      "Train Epoch: 2 [8320/110534 (8%)]\tAll Loss: 1.6625\tTriple Loss(0): 0.0000\tClassification Loss: 1.6625\n",
      "Train Epoch: 2 [8640/110534 (8%)]\tAll Loss: 1.9762\tTriple Loss(1): 0.2428\tClassification Loss: 1.4906\n",
      "Train Epoch: 2 [8960/110534 (8%)]\tAll Loss: 1.8285\tTriple Loss(1): 0.1404\tClassification Loss: 1.5476\n",
      "Train Epoch: 2 [9280/110534 (8%)]\tAll Loss: 5.3859\tTriple Loss(0): 1.9683\tClassification Loss: 1.4494\n",
      "\n",
      "Test set: Average loss: 1.7165, Accuracy: 544/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tAll Loss: 2.5125\tTriple Loss(1): 0.2882\tClassification Loss: 1.9361\n",
      "Train Epoch: 2 [9920/110534 (9%)]\tAll Loss: 2.1329\tTriple Loss(1): 0.1695\tClassification Loss: 1.7939\n",
      "Train Epoch: 2 [10240/110534 (9%)]\tAll Loss: 1.8770\tTriple Loss(0): 0.0000\tClassification Loss: 1.8770\n",
      "Train Epoch: 2 [10560/110534 (10%)]\tAll Loss: 2.6242\tTriple Loss(1): 0.3769\tClassification Loss: 1.8704\n",
      "Train Epoch: 2 [10880/110534 (10%)]\tAll Loss: 2.4676\tTriple Loss(1): 0.2982\tClassification Loss: 1.8711\n",
      "Train Epoch: 2 [11200/110534 (10%)]\tAll Loss: 1.9896\tTriple Loss(1): 0.2266\tClassification Loss: 1.5365\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tAll Loss: 2.1402\tTriple Loss(1): 0.2774\tClassification Loss: 1.5854\n",
      "Train Epoch: 2 [11840/110534 (11%)]\tAll Loss: 1.9730\tTriple Loss(1): 0.1723\tClassification Loss: 1.6283\n",
      "Train Epoch: 2 [12160/110534 (11%)]\tAll Loss: 2.3370\tTriple Loss(1): 0.3032\tClassification Loss: 1.7306\n",
      "Train Epoch: 2 [12480/110534 (11%)]\tAll Loss: 1.7091\tTriple Loss(0): 0.0000\tClassification Loss: 1.7091\n",
      "\n",
      "Test set: Average loss: 1.7131, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [12800/110534 (12%)]\tAll Loss: 1.8863\tTriple Loss(1): 0.1178\tClassification Loss: 1.6507\n",
      "Train Epoch: 2 [13120/110534 (12%)]\tAll Loss: 1.7322\tTriple Loss(1): 0.2247\tClassification Loss: 1.2828\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tAll Loss: 1.8523\tTriple Loss(0): 0.0000\tClassification Loss: 1.8523\n",
      "Train Epoch: 2 [13760/110534 (12%)]\tAll Loss: 2.2200\tTriple Loss(1): 0.2416\tClassification Loss: 1.7368\n",
      "Train Epoch: 2 [14080/110534 (13%)]\tAll Loss: 1.5162\tTriple Loss(0): 0.0000\tClassification Loss: 1.5162\n",
      "Train Epoch: 2 [14400/110534 (13%)]\tAll Loss: 2.7163\tTriple Loss(1): 0.2974\tClassification Loss: 2.1216\n",
      "Train Epoch: 2 [14720/110534 (13%)]\tAll Loss: 1.7607\tTriple Loss(0): 0.0000\tClassification Loss: 1.7607\n",
      "Train Epoch: 2 [15040/110534 (14%)]\tAll Loss: 2.2110\tTriple Loss(0): 0.2735\tClassification Loss: 1.6640\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tAll Loss: 2.1503\tTriple Loss(1): 0.2862\tClassification Loss: 1.5779\n",
      "Train Epoch: 2 [15680/110534 (14%)]\tAll Loss: 2.1043\tTriple Loss(1): 0.2909\tClassification Loss: 1.5226\n",
      "\n",
      "Test set: Average loss: 1.7034, Accuracy: 541/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [16000/110534 (14%)]\tAll Loss: 2.3310\tTriple Loss(1): 0.2466\tClassification Loss: 1.8378\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_500.pth.tar\n",
      "Train Epoch: 2 [16320/110534 (15%)]\tAll Loss: 2.0527\tTriple Loss(1): 0.3712\tClassification Loss: 1.3103\n",
      "Train Epoch: 2 [16640/110534 (15%)]\tAll Loss: 2.6672\tTriple Loss(1): 0.3718\tClassification Loss: 1.9235\n",
      "Train Epoch: 2 [16960/110534 (15%)]\tAll Loss: 2.6399\tTriple Loss(1): 0.3046\tClassification Loss: 2.0308\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tAll Loss: 1.6875\tTriple Loss(1): 0.0874\tClassification Loss: 1.5126\n",
      "Train Epoch: 2 [17600/110534 (16%)]\tAll Loss: 2.4644\tTriple Loss(1): 0.4327\tClassification Loss: 1.5990\n",
      "Train Epoch: 2 [17920/110534 (16%)]\tAll Loss: 1.8505\tTriple Loss(0): 0.0000\tClassification Loss: 1.8505\n",
      "Train Epoch: 2 [18240/110534 (16%)]\tAll Loss: 2.0013\tTriple Loss(0): 0.0000\tClassification Loss: 2.0013\n",
      "Train Epoch: 2 [18560/110534 (17%)]\tAll Loss: 1.5661\tTriple Loss(0): 0.0000\tClassification Loss: 1.5661\n",
      "Train Epoch: 2 [18880/110534 (17%)]\tAll Loss: 2.3244\tTriple Loss(1): 0.3121\tClassification Loss: 1.7002\n",
      "\n",
      "Test set: Average loss: 1.7087, Accuracy: 538/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tAll Loss: 1.9395\tTriple Loss(1): 0.2556\tClassification Loss: 1.4283\n",
      "Train Epoch: 2 [19520/110534 (18%)]\tAll Loss: 2.4110\tTriple Loss(1): 0.2813\tClassification Loss: 1.8483\n",
      "Train Epoch: 2 [19840/110534 (18%)]\tAll Loss: 2.1709\tTriple Loss(1): 0.2929\tClassification Loss: 1.5851\n",
      "Train Epoch: 2 [20160/110534 (18%)]\tAll Loss: 2.0533\tTriple Loss(1): 0.2123\tClassification Loss: 1.6286\n",
      "Train Epoch: 2 [20480/110534 (19%)]\tAll Loss: 2.1315\tTriple Loss(1): 0.2673\tClassification Loss: 1.5970\n",
      "Train Epoch: 2 [20800/110534 (19%)]\tAll Loss: 2.6052\tTriple Loss(1): 0.2684\tClassification Loss: 2.0684\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tAll Loss: 1.6049\tTriple Loss(0): 0.0000\tClassification Loss: 1.6049\n",
      "Train Epoch: 2 [21440/110534 (19%)]\tAll Loss: 2.0198\tTriple Loss(1): 0.2723\tClassification Loss: 1.4751\n",
      "Train Epoch: 2 [21760/110534 (20%)]\tAll Loss: 2.1268\tTriple Loss(0): 0.0000\tClassification Loss: 2.1268\n",
      "Train Epoch: 2 [22080/110534 (20%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.3423\tClassification Loss: 1.6990\n",
      "\n",
      "Test set: Average loss: 1.7116, Accuracy: 519/960 (54%)\n",
      "\n",
      "Train Epoch: 2 [22400/110534 (20%)]\tAll Loss: 2.2553\tTriple Loss(1): 0.3756\tClassification Loss: 1.5042\n",
      "Train Epoch: 2 [22720/110534 (21%)]\tAll Loss: 1.7874\tTriple Loss(0): 0.0000\tClassification Loss: 1.7874\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tAll Loss: 2.2437\tTriple Loss(1): 0.2144\tClassification Loss: 1.8149\n",
      "Train Epoch: 2 [23360/110534 (21%)]\tAll Loss: 2.0689\tTriple Loss(1): 0.1629\tClassification Loss: 1.7432\n",
      "Train Epoch: 2 [23680/110534 (21%)]\tAll Loss: 2.5378\tTriple Loss(1): 0.3068\tClassification Loss: 1.9241\n",
      "Train Epoch: 2 [24000/110534 (22%)]\tAll Loss: 2.6956\tTriple Loss(1): 0.5635\tClassification Loss: 1.5685\n",
      "Train Epoch: 2 [24320/110534 (22%)]\tAll Loss: 2.0538\tTriple Loss(1): 0.1398\tClassification Loss: 1.7742\n",
      "Train Epoch: 2 [24640/110534 (22%)]\tAll Loss: 2.5665\tTriple Loss(1): 0.1888\tClassification Loss: 2.1890\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tAll Loss: 1.8926\tTriple Loss(1): 0.1344\tClassification Loss: 1.6238\n",
      "Train Epoch: 2 [25280/110534 (23%)]\tAll Loss: 2.3642\tTriple Loss(1): 0.3091\tClassification Loss: 1.7461\n",
      "\n",
      "Test set: Average loss: 1.6995, Accuracy: 537/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [25600/110534 (23%)]\tAll Loss: 2.0844\tTriple Loss(1): 0.1718\tClassification Loss: 1.7408\n",
      "Train Epoch: 2 [25920/110534 (23%)]\tAll Loss: 1.8263\tTriple Loss(0): 0.0000\tClassification Loss: 1.8263\n",
      "Train Epoch: 2 [26240/110534 (24%)]\tAll Loss: 1.8329\tTriple Loss(1): 0.0921\tClassification Loss: 1.6486\n",
      "Train Epoch: 2 [26560/110534 (24%)]\tAll Loss: 2.8687\tTriple Loss(1): 0.4491\tClassification Loss: 1.9704\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tAll Loss: 2.3955\tTriple Loss(1): 0.2196\tClassification Loss: 1.9563\n",
      "Train Epoch: 2 [27200/110534 (25%)]\tAll Loss: 3.0978\tTriple Loss(1): 0.5665\tClassification Loss: 1.9648\n",
      "Train Epoch: 2 [27520/110534 (25%)]\tAll Loss: 2.1992\tTriple Loss(1): 0.2747\tClassification Loss: 1.6498\n",
      "Train Epoch: 2 [27840/110534 (25%)]\tAll Loss: 2.0458\tTriple Loss(1): 0.2615\tClassification Loss: 1.5229\n",
      "Train Epoch: 2 [28160/110534 (25%)]\tAll Loss: 2.6621\tTriple Loss(1): 0.3085\tClassification Loss: 2.0452\n",
      "Train Epoch: 2 [28480/110534 (26%)]\tAll Loss: 2.4246\tTriple Loss(0): 0.2080\tClassification Loss: 2.0085\n",
      "\n",
      "Test set: Average loss: 1.6955, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tAll Loss: 1.4340\tTriple Loss(0): 0.0000\tClassification Loss: 1.4340\n",
      "Train Epoch: 2 [29120/110534 (26%)]\tAll Loss: 1.9399\tTriple Loss(1): 0.0777\tClassification Loss: 1.7846\n",
      "Train Epoch: 2 [29440/110534 (27%)]\tAll Loss: 2.5564\tTriple Loss(1): 0.4713\tClassification Loss: 1.6137\n",
      "Train Epoch: 2 [29760/110534 (27%)]\tAll Loss: 1.7505\tTriple Loss(1): 0.1493\tClassification Loss: 1.4520\n",
      "Train Epoch: 2 [30080/110534 (27%)]\tAll Loss: 1.9909\tTriple Loss(1): 0.2490\tClassification Loss: 1.4928\n",
      "Train Epoch: 2 [30400/110534 (27%)]\tAll Loss: 1.5536\tTriple Loss(0): 0.0000\tClassification Loss: 1.5536\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tAll Loss: 2.2994\tTriple Loss(1): 0.4106\tClassification Loss: 1.4783\n",
      "Train Epoch: 2 [31040/110534 (28%)]\tAll Loss: 2.3579\tTriple Loss(1): 0.2380\tClassification Loss: 1.8820\n",
      "Train Epoch: 2 [31360/110534 (28%)]\tAll Loss: 2.3309\tTriple Loss(1): 0.2575\tClassification Loss: 1.8158\n",
      "Train Epoch: 2 [31680/110534 (29%)]\tAll Loss: 2.0479\tTriple Loss(1): 0.1818\tClassification Loss: 1.6843\n",
      "\n",
      "Test set: Average loss: 1.6989, Accuracy: 533/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [32000/110534 (29%)]\tAll Loss: 2.2790\tTriple Loss(1): 0.2495\tClassification Loss: 1.7801\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1000.pth.tar\n",
      "Train Epoch: 2 [32320/110534 (29%)]\tAll Loss: 2.2293\tTriple Loss(1): 0.3772\tClassification Loss: 1.4748\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tAll Loss: 2.1376\tTriple Loss(1): 0.1651\tClassification Loss: 1.8075\n",
      "Train Epoch: 2 [32960/110534 (30%)]\tAll Loss: 2.6559\tTriple Loss(1): 0.4026\tClassification Loss: 1.8507\n",
      "Train Epoch: 2 [33280/110534 (30%)]\tAll Loss: 2.2985\tTriple Loss(1): 0.2356\tClassification Loss: 1.8273\n",
      "Train Epoch: 2 [33600/110534 (30%)]\tAll Loss: 1.9524\tTriple Loss(1): 0.1834\tClassification Loss: 1.5856\n",
      "Train Epoch: 2 [33920/110534 (31%)]\tAll Loss: 7.8739\tTriple Loss(0): 3.1530\tClassification Loss: 1.5679\n",
      "Train Epoch: 2 [34240/110534 (31%)]\tAll Loss: 2.7509\tTriple Loss(1): 0.5702\tClassification Loss: 1.6105\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tAll Loss: 2.4724\tTriple Loss(1): 0.4983\tClassification Loss: 1.4758\n",
      "Train Epoch: 2 [34880/110534 (32%)]\tAll Loss: 1.4401\tTriple Loss(0): 0.0000\tClassification Loss: 1.4401\n",
      "\n",
      "Test set: Average loss: 1.6903, Accuracy: 539/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [35200/110534 (32%)]\tAll Loss: 2.7976\tTriple Loss(1): 0.3994\tClassification Loss: 1.9987\n",
      "Train Epoch: 2 [35520/110534 (32%)]\tAll Loss: 2.1194\tTriple Loss(1): 0.1015\tClassification Loss: 1.9164\n",
      "Train Epoch: 2 [35840/110534 (32%)]\tAll Loss: 1.7293\tTriple Loss(1): 0.1504\tClassification Loss: 1.4285\n",
      "Train Epoch: 2 [36160/110534 (33%)]\tAll Loss: 1.8832\tTriple Loss(1): 0.1570\tClassification Loss: 1.5693\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tAll Loss: 2.5406\tTriple Loss(1): 0.3438\tClassification Loss: 1.8531\n",
      "Train Epoch: 2 [36800/110534 (33%)]\tAll Loss: 2.0727\tTriple Loss(1): 0.2235\tClassification Loss: 1.6256\n",
      "Train Epoch: 2 [37120/110534 (34%)]\tAll Loss: 2.2950\tTriple Loss(1): 0.2119\tClassification Loss: 1.8713\n",
      "Train Epoch: 2 [37440/110534 (34%)]\tAll Loss: 2.0083\tTriple Loss(0): 0.0000\tClassification Loss: 2.0083\n",
      "Train Epoch: 2 [37760/110534 (34%)]\tAll Loss: 2.5735\tTriple Loss(1): 0.4206\tClassification Loss: 1.7324\n",
      "Train Epoch: 2 [38080/110534 (34%)]\tAll Loss: 2.3746\tTriple Loss(1): 0.3393\tClassification Loss: 1.6959\n",
      "\n",
      "Test set: Average loss: 1.6885, Accuracy: 535/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tAll Loss: 1.7433\tTriple Loss(1): 0.1770\tClassification Loss: 1.3892\n",
      "Train Epoch: 2 [38720/110534 (35%)]\tAll Loss: 1.8362\tTriple Loss(0): 0.0000\tClassification Loss: 1.8362\n",
      "Train Epoch: 2 [39040/110534 (35%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.1941\tClassification Loss: 1.4656\n",
      "Train Epoch: 2 [39360/110534 (36%)]\tAll Loss: 2.1808\tTriple Loss(1): 0.2845\tClassification Loss: 1.6119\n",
      "Train Epoch: 2 [39680/110534 (36%)]\tAll Loss: 2.3698\tTriple Loss(1): 0.1858\tClassification Loss: 1.9981\n",
      "Train Epoch: 2 [40000/110534 (36%)]\tAll Loss: 1.9526\tTriple Loss(1): 0.3570\tClassification Loss: 1.2386\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tAll Loss: 2.5555\tTriple Loss(1): 0.3686\tClassification Loss: 1.8184\n",
      "Train Epoch: 2 [40640/110534 (37%)]\tAll Loss: 2.5359\tTriple Loss(1): 0.3273\tClassification Loss: 1.8813\n",
      "Train Epoch: 2 [40960/110534 (37%)]\tAll Loss: 2.0179\tTriple Loss(1): 0.2364\tClassification Loss: 1.5450\n",
      "Train Epoch: 2 [41280/110534 (37%)]\tAll Loss: 2.3799\tTriple Loss(1): 0.2943\tClassification Loss: 1.7914\n",
      "\n",
      "Test set: Average loss: 1.6858, Accuracy: 535/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [41600/110534 (38%)]\tAll Loss: 1.8126\tTriple Loss(0): 0.0000\tClassification Loss: 1.8126\n",
      "Train Epoch: 2 [41920/110534 (38%)]\tAll Loss: 1.8717\tTriple Loss(1): 0.1132\tClassification Loss: 1.6454\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tAll Loss: 2.3829\tTriple Loss(1): 0.3631\tClassification Loss: 1.6566\n",
      "Train Epoch: 2 [42560/110534 (38%)]\tAll Loss: 1.8253\tTriple Loss(0): 0.0000\tClassification Loss: 1.8253\n",
      "Train Epoch: 2 [42880/110534 (39%)]\tAll Loss: 1.8487\tTriple Loss(0): 0.0000\tClassification Loss: 1.8487\n",
      "Train Epoch: 2 [43200/110534 (39%)]\tAll Loss: 2.2524\tTriple Loss(1): 0.2966\tClassification Loss: 1.6592\n",
      "Train Epoch: 2 [43520/110534 (39%)]\tAll Loss: 1.9347\tTriple Loss(1): 0.1892\tClassification Loss: 1.5563\n",
      "Train Epoch: 2 [43840/110534 (40%)]\tAll Loss: 2.0369\tTriple Loss(1): 0.2353\tClassification Loss: 1.5662\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tAll Loss: 2.7770\tTriple Loss(1): 0.3737\tClassification Loss: 2.0296\n",
      "Train Epoch: 2 [44480/110534 (40%)]\tAll Loss: 1.6665\tTriple Loss(0): 0.0000\tClassification Loss: 1.6665\n",
      "\n",
      "Test set: Average loss: 1.6857, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [44800/110534 (41%)]\tAll Loss: 1.2702\tTriple Loss(0): 0.0000\tClassification Loss: 1.2702\n",
      "Train Epoch: 2 [45120/110534 (41%)]\tAll Loss: 2.2318\tTriple Loss(1): 0.3212\tClassification Loss: 1.5894\n",
      "Train Epoch: 2 [45440/110534 (41%)]\tAll Loss: 2.1643\tTriple Loss(1): 0.2339\tClassification Loss: 1.6964\n",
      "Train Epoch: 2 [45760/110534 (41%)]\tAll Loss: 1.4610\tTriple Loss(0): 0.0000\tClassification Loss: 1.4610\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tAll Loss: 1.2680\tTriple Loss(0): 0.0000\tClassification Loss: 1.2680\n",
      "Train Epoch: 2 [46400/110534 (42%)]\tAll Loss: 2.4933\tTriple Loss(1): 0.3189\tClassification Loss: 1.8555\n",
      "Train Epoch: 2 [46720/110534 (42%)]\tAll Loss: 1.7976\tTriple Loss(0): 0.0000\tClassification Loss: 1.7976\n",
      "Train Epoch: 2 [47040/110534 (43%)]\tAll Loss: 2.4857\tTriple Loss(1): 0.3821\tClassification Loss: 1.7216\n",
      "Train Epoch: 2 [47360/110534 (43%)]\tAll Loss: 2.1261\tTriple Loss(1): 0.2403\tClassification Loss: 1.6455\n",
      "Train Epoch: 2 [47680/110534 (43%)]\tAll Loss: 1.6445\tTriple Loss(1): 0.1569\tClassification Loss: 1.3307\n",
      "\n",
      "Test set: Average loss: 1.6887, Accuracy: 539/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tAll Loss: 4.4430\tTriple Loss(0): 1.2317\tClassification Loss: 1.9797\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1500.pth.tar\n",
      "Train Epoch: 2 [48320/110534 (44%)]\tAll Loss: 2.3963\tTriple Loss(1): 0.3189\tClassification Loss: 1.7585\n",
      "Train Epoch: 2 [48640/110534 (44%)]\tAll Loss: 1.8761\tTriple Loss(0): 0.0000\tClassification Loss: 1.8761\n",
      "Train Epoch: 2 [48960/110534 (44%)]\tAll Loss: 2.5714\tTriple Loss(1): 0.3741\tClassification Loss: 1.8231\n",
      "Train Epoch: 2 [49280/110534 (45%)]\tAll Loss: 2.1606\tTriple Loss(1): 0.3252\tClassification Loss: 1.5103\n",
      "Train Epoch: 2 [49600/110534 (45%)]\tAll Loss: 2.5213\tTriple Loss(1): 0.3579\tClassification Loss: 1.8055\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tAll Loss: 1.9394\tTriple Loss(1): 0.2814\tClassification Loss: 1.3767\n",
      "Train Epoch: 2 [50240/110534 (45%)]\tAll Loss: 1.8231\tTriple Loss(1): 0.1122\tClassification Loss: 1.5988\n",
      "Train Epoch: 2 [50560/110534 (46%)]\tAll Loss: 2.0492\tTriple Loss(1): 0.1633\tClassification Loss: 1.7226\n",
      "Train Epoch: 2 [50880/110534 (46%)]\tAll Loss: 2.4563\tTriple Loss(1): 0.3262\tClassification Loss: 1.8039\n",
      "\n",
      "Test set: Average loss: 1.6805, Accuracy: 544/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [51200/110534 (46%)]\tAll Loss: 5.4395\tTriple Loss(0): 1.9318\tClassification Loss: 1.5760\n",
      "Train Epoch: 2 [51520/110534 (47%)]\tAll Loss: 2.5007\tTriple Loss(1): 0.4199\tClassification Loss: 1.6609\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tAll Loss: 2.3323\tTriple Loss(1): 0.3514\tClassification Loss: 1.6295\n",
      "Train Epoch: 2 [52160/110534 (47%)]\tAll Loss: 2.8511\tTriple Loss(1): 0.4236\tClassification Loss: 2.0039\n",
      "Train Epoch: 2 [52480/110534 (47%)]\tAll Loss: 2.3624\tTriple Loss(1): 0.0829\tClassification Loss: 2.1966\n",
      "Train Epoch: 2 [52800/110534 (48%)]\tAll Loss: 1.9530\tTriple Loss(1): 0.2018\tClassification Loss: 1.5494\n",
      "Train Epoch: 2 [53120/110534 (48%)]\tAll Loss: 1.4486\tTriple Loss(0): 0.0000\tClassification Loss: 1.4486\n",
      "Train Epoch: 2 [53440/110534 (48%)]\tAll Loss: 8.3666\tTriple Loss(0): 3.2471\tClassification Loss: 1.8724\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tAll Loss: 2.3406\tTriple Loss(1): 0.2747\tClassification Loss: 1.7912\n",
      "Train Epoch: 2 [54080/110534 (49%)]\tAll Loss: 1.7550\tTriple Loss(0): 0.0000\tClassification Loss: 1.7550\n",
      "\n",
      "Test set: Average loss: 1.6849, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [54400/110534 (49%)]\tAll Loss: 2.6238\tTriple Loss(1): 0.4617\tClassification Loss: 1.7004\n",
      "Train Epoch: 2 [54720/110534 (49%)]\tAll Loss: 2.3234\tTriple Loss(1): 0.3148\tClassification Loss: 1.6937\n",
      "Train Epoch: 2 [55040/110534 (50%)]\tAll Loss: 2.7421\tTriple Loss(1): 0.3915\tClassification Loss: 1.9591\n",
      "Train Epoch: 2 [55360/110534 (50%)]\tAll Loss: 1.4227\tTriple Loss(0): 0.0000\tClassification Loss: 1.4227\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tAll Loss: 2.3809\tTriple Loss(1): 0.2776\tClassification Loss: 1.8257\n",
      "Train Epoch: 2 [56000/110534 (51%)]\tAll Loss: 2.5533\tTriple Loss(1): 0.2423\tClassification Loss: 2.0687\n",
      "Train Epoch: 2 [56320/110534 (51%)]\tAll Loss: 1.8539\tTriple Loss(0): 0.0000\tClassification Loss: 1.8539\n",
      "Train Epoch: 2 [56640/110534 (51%)]\tAll Loss: 1.8054\tTriple Loss(1): 0.2720\tClassification Loss: 1.2613\n",
      "Train Epoch: 2 [56960/110534 (52%)]\tAll Loss: 2.1948\tTriple Loss(1): 0.2064\tClassification Loss: 1.7820\n",
      "Train Epoch: 2 [57280/110534 (52%)]\tAll Loss: 2.4135\tTriple Loss(1): 0.2962\tClassification Loss: 1.8211\n",
      "\n",
      "Test set: Average loss: 1.6769, Accuracy: 552/960 (58%)\n",
      "\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tAll Loss: 2.3116\tTriple Loss(1): 0.2254\tClassification Loss: 1.8609\n",
      "Train Epoch: 2 [57920/110534 (52%)]\tAll Loss: 2.5277\tTriple Loss(1): 0.3809\tClassification Loss: 1.7659\n",
      "Train Epoch: 2 [58240/110534 (53%)]\tAll Loss: 1.8676\tTriple Loss(1): 0.2463\tClassification Loss: 1.3750\n",
      "Train Epoch: 2 [58560/110534 (53%)]\tAll Loss: 1.5860\tTriple Loss(0): 0.0000\tClassification Loss: 1.5860\n",
      "Train Epoch: 2 [58880/110534 (53%)]\tAll Loss: 2.2577\tTriple Loss(1): 0.2192\tClassification Loss: 1.8193\n",
      "Train Epoch: 2 [59200/110534 (54%)]\tAll Loss: 2.2929\tTriple Loss(1): 0.2316\tClassification Loss: 1.8298\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tAll Loss: 1.3778\tTriple Loss(0): 0.0000\tClassification Loss: 1.3778\n",
      "Train Epoch: 2 [59840/110534 (54%)]\tAll Loss: 2.3819\tTriple Loss(1): 0.4661\tClassification Loss: 1.4497\n",
      "Train Epoch: 2 [60160/110534 (54%)]\tAll Loss: 2.0080\tTriple Loss(1): 0.2097\tClassification Loss: 1.5886\n",
      "Train Epoch: 2 [60480/110534 (55%)]\tAll Loss: 1.7734\tTriple Loss(1): 0.0735\tClassification Loss: 1.6264\n",
      "\n",
      "Test set: Average loss: 1.6746, Accuracy: 540/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [60800/110534 (55%)]\tAll Loss: 1.9851\tTriple Loss(1): 0.0935\tClassification Loss: 1.7982\n",
      "Train Epoch: 2 [61120/110534 (55%)]\tAll Loss: 2.4575\tTriple Loss(1): 0.3659\tClassification Loss: 1.7257\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tAll Loss: 1.8211\tTriple Loss(1): 0.3792\tClassification Loss: 1.0626\n",
      "Train Epoch: 2 [61760/110534 (56%)]\tAll Loss: 2.1943\tTriple Loss(1): 0.2128\tClassification Loss: 1.7688\n",
      "Train Epoch: 2 [62080/110534 (56%)]\tAll Loss: 2.3574\tTriple Loss(1): 0.3177\tClassification Loss: 1.7219\n",
      "Train Epoch: 2 [62400/110534 (56%)]\tAll Loss: 2.4445\tTriple Loss(0): 0.4804\tClassification Loss: 1.4837\n",
      "Train Epoch: 2 [62720/110534 (57%)]\tAll Loss: 1.4021\tTriple Loss(0): 0.0000\tClassification Loss: 1.4021\n",
      "Train Epoch: 2 [63040/110534 (57%)]\tAll Loss: 3.1305\tTriple Loss(1): 0.5339\tClassification Loss: 2.0627\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tAll Loss: 2.1954\tTriple Loss(1): 0.2520\tClassification Loss: 1.6914\n",
      "Train Epoch: 2 [63680/110534 (58%)]\tAll Loss: 1.4770\tTriple Loss(0): 0.0000\tClassification Loss: 1.4770\n",
      "\n",
      "Test set: Average loss: 1.6816, Accuracy: 541/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [64000/110534 (58%)]\tAll Loss: 2.4947\tTriple Loss(1): 0.1955\tClassification Loss: 2.1038\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2000.pth.tar\n",
      "Train Epoch: 2 [64320/110534 (58%)]\tAll Loss: 2.1477\tTriple Loss(1): 0.3229\tClassification Loss: 1.5019\n",
      "Train Epoch: 2 [64640/110534 (58%)]\tAll Loss: 4.8599\tTriple Loss(0): 1.3121\tClassification Loss: 2.2358\n",
      "Train Epoch: 2 [64960/110534 (59%)]\tAll Loss: 2.3308\tTriple Loss(1): 0.3097\tClassification Loss: 1.7113\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tAll Loss: 1.8602\tTriple Loss(1): 0.0377\tClassification Loss: 1.7848\n",
      "Train Epoch: 2 [65600/110534 (59%)]\tAll Loss: 1.9804\tTriple Loss(1): 0.1440\tClassification Loss: 1.6924\n",
      "Train Epoch: 2 [65920/110534 (60%)]\tAll Loss: 1.9103\tTriple Loss(1): 0.1011\tClassification Loss: 1.7081\n",
      "Train Epoch: 2 [66240/110534 (60%)]\tAll Loss: 1.7736\tTriple Loss(0): 0.0000\tClassification Loss: 1.7736\n",
      "Train Epoch: 2 [66560/110534 (60%)]\tAll Loss: 1.9558\tTriple Loss(1): 0.2270\tClassification Loss: 1.5019\n",
      "Train Epoch: 2 [66880/110534 (60%)]\tAll Loss: 1.7467\tTriple Loss(0): 0.0000\tClassification Loss: 1.7467\n",
      "\n",
      "Test set: Average loss: 1.6729, Accuracy: 533/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.2022\tClassification Loss: 1.8409\n",
      "Train Epoch: 2 [67520/110534 (61%)]\tAll Loss: 2.4317\tTriple Loss(1): 0.2716\tClassification Loss: 1.8886\n",
      "Train Epoch: 2 [67840/110534 (61%)]\tAll Loss: 2.3753\tTriple Loss(1): 0.2353\tClassification Loss: 1.9047\n",
      "Train Epoch: 2 [68160/110534 (62%)]\tAll Loss: 2.1274\tTriple Loss(1): 0.3165\tClassification Loss: 1.4944\n",
      "Train Epoch: 2 [68480/110534 (62%)]\tAll Loss: 1.4172\tTriple Loss(0): 0.0000\tClassification Loss: 1.4172\n",
      "Train Epoch: 2 [68800/110534 (62%)]\tAll Loss: 1.9005\tTriple Loss(1): 0.1482\tClassification Loss: 1.6042\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tAll Loss: 2.1510\tTriple Loss(0): 0.2272\tClassification Loss: 1.6966\n",
      "Train Epoch: 2 [69440/110534 (63%)]\tAll Loss: 2.2385\tTriple Loss(1): 0.2939\tClassification Loss: 1.6507\n",
      "Train Epoch: 2 [69760/110534 (63%)]\tAll Loss: 2.4104\tTriple Loss(1): 0.3484\tClassification Loss: 1.7135\n",
      "Train Epoch: 2 [70080/110534 (63%)]\tAll Loss: 2.0236\tTriple Loss(1): 0.3436\tClassification Loss: 1.3364\n",
      "\n",
      "Test set: Average loss: 1.6682, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [70400/110534 (64%)]\tAll Loss: 2.3131\tTriple Loss(1): 0.3779\tClassification Loss: 1.5573\n",
      "Train Epoch: 2 [70720/110534 (64%)]\tAll Loss: 1.7775\tTriple Loss(0): 0.2021\tClassification Loss: 1.3732\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tAll Loss: 2.4137\tTriple Loss(1): 0.4493\tClassification Loss: 1.5151\n",
      "Train Epoch: 2 [71360/110534 (65%)]\tAll Loss: 2.4237\tTriple Loss(1): 0.2843\tClassification Loss: 1.8552\n",
      "Train Epoch: 2 [71680/110534 (65%)]\tAll Loss: 2.5274\tTriple Loss(1): 0.3993\tClassification Loss: 1.7289\n",
      "Train Epoch: 2 [72000/110534 (65%)]\tAll Loss: 2.0057\tTriple Loss(1): 0.2138\tClassification Loss: 1.5780\n",
      "Train Epoch: 2 [72320/110534 (65%)]\tAll Loss: 1.9913\tTriple Loss(1): 0.2056\tClassification Loss: 1.5800\n",
      "Train Epoch: 2 [72640/110534 (66%)]\tAll Loss: 1.9437\tTriple Loss(1): 0.2373\tClassification Loss: 1.4691\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tAll Loss: 1.6841\tTriple Loss(1): 0.1537\tClassification Loss: 1.3767\n",
      "Train Epoch: 2 [73280/110534 (66%)]\tAll Loss: 1.2847\tTriple Loss(0): 0.0000\tClassification Loss: 1.2847\n",
      "\n",
      "Test set: Average loss: 1.6652, Accuracy: 545/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [73600/110534 (67%)]\tAll Loss: 2.8814\tTriple Loss(1): 0.3406\tClassification Loss: 2.2002\n",
      "Train Epoch: 2 [73920/110534 (67%)]\tAll Loss: 2.2439\tTriple Loss(1): 0.1486\tClassification Loss: 1.9466\n",
      "Train Epoch: 2 [74240/110534 (67%)]\tAll Loss: 1.8759\tTriple Loss(1): 0.2104\tClassification Loss: 1.4550\n",
      "Train Epoch: 2 [74560/110534 (67%)]\tAll Loss: 2.3478\tTriple Loss(1): 0.1687\tClassification Loss: 2.0105\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tAll Loss: 2.6002\tTriple Loss(1): 0.4244\tClassification Loss: 1.7513\n",
      "Train Epoch: 2 [75200/110534 (68%)]\tAll Loss: 1.5798\tTriple Loss(0): 0.0000\tClassification Loss: 1.5798\n",
      "Train Epoch: 2 [75520/110534 (68%)]\tAll Loss: 3.7064\tTriple Loss(0): 1.1333\tClassification Loss: 1.4397\n",
      "Train Epoch: 2 [75840/110534 (69%)]\tAll Loss: 2.1227\tTriple Loss(1): 0.2768\tClassification Loss: 1.5691\n",
      "Train Epoch: 2 [76160/110534 (69%)]\tAll Loss: 2.2644\tTriple Loss(1): 0.2883\tClassification Loss: 1.6877\n",
      "Train Epoch: 2 [76480/110534 (69%)]\tAll Loss: 2.0898\tTriple Loss(1): 0.1945\tClassification Loss: 1.7009\n",
      "\n",
      "Test set: Average loss: 1.6621, Accuracy: 542/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tAll Loss: 1.9161\tTriple Loss(1): 0.2286\tClassification Loss: 1.4590\n",
      "Train Epoch: 2 [77120/110534 (70%)]\tAll Loss: 2.3094\tTriple Loss(1): 0.3386\tClassification Loss: 1.6322\n",
      "Train Epoch: 2 [77440/110534 (70%)]\tAll Loss: 2.4436\tTriple Loss(1): 0.4432\tClassification Loss: 1.5572\n",
      "Train Epoch: 2 [77760/110534 (70%)]\tAll Loss: 1.6590\tTriple Loss(0): 0.0000\tClassification Loss: 1.6590\n",
      "Train Epoch: 2 [78080/110534 (71%)]\tAll Loss: 2.3398\tTriple Loss(1): 0.1994\tClassification Loss: 1.9410\n",
      "Train Epoch: 2 [78400/110534 (71%)]\tAll Loss: 2.2804\tTriple Loss(1): 0.3473\tClassification Loss: 1.5858\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tAll Loss: 1.9721\tTriple Loss(1): 0.2674\tClassification Loss: 1.4373\n",
      "Train Epoch: 2 [79040/110534 (71%)]\tAll Loss: 1.8302\tTriple Loss(0): 0.0000\tClassification Loss: 1.8302\n",
      "Train Epoch: 2 [79360/110534 (72%)]\tAll Loss: 3.2934\tTriple Loss(1): 0.6495\tClassification Loss: 1.9943\n",
      "Train Epoch: 2 [79680/110534 (72%)]\tAll Loss: 1.7346\tTriple Loss(0): 0.1125\tClassification Loss: 1.5096\n",
      "\n",
      "Test set: Average loss: 1.6705, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [80000/110534 (72%)]\tAll Loss: 1.7868\tTriple Loss(1): 0.1214\tClassification Loss: 1.5439\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2500.pth.tar\n",
      "Train Epoch: 2 [80320/110534 (73%)]\tAll Loss: 1.7362\tTriple Loss(1): 0.2156\tClassification Loss: 1.3050\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tAll Loss: 2.0687\tTriple Loss(1): 0.2768\tClassification Loss: 1.5151\n",
      "Train Epoch: 2 [80960/110534 (73%)]\tAll Loss: 2.0660\tTriple Loss(1): 0.1528\tClassification Loss: 1.7603\n",
      "Train Epoch: 2 [81280/110534 (74%)]\tAll Loss: 2.2158\tTriple Loss(1): 0.3962\tClassification Loss: 1.4235\n",
      "Train Epoch: 2 [81600/110534 (74%)]\tAll Loss: 2.0526\tTriple Loss(1): 0.3006\tClassification Loss: 1.4513\n",
      "Train Epoch: 2 [81920/110534 (74%)]\tAll Loss: 1.7403\tTriple Loss(0): 0.0000\tClassification Loss: 1.7403\n",
      "Train Epoch: 2 [82240/110534 (74%)]\tAll Loss: 2.0119\tTriple Loss(0): 0.0000\tClassification Loss: 2.0119\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tAll Loss: 2.1282\tTriple Loss(1): 0.2630\tClassification Loss: 1.6023\n",
      "Train Epoch: 2 [82880/110534 (75%)]\tAll Loss: 2.9401\tTriple Loss(1): 0.4661\tClassification Loss: 2.0079\n",
      "\n",
      "Test set: Average loss: 1.6642, Accuracy: 547/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [83200/110534 (75%)]\tAll Loss: 2.6491\tTriple Loss(1): 0.4791\tClassification Loss: 1.6909\n",
      "Train Epoch: 2 [83520/110534 (76%)]\tAll Loss: 2.1388\tTriple Loss(1): 0.2522\tClassification Loss: 1.6344\n",
      "Train Epoch: 2 [83840/110534 (76%)]\tAll Loss: 1.5501\tTriple Loss(1): 0.1077\tClassification Loss: 1.3348\n",
      "Train Epoch: 2 [84160/110534 (76%)]\tAll Loss: 1.5154\tTriple Loss(1): 0.0899\tClassification Loss: 1.3357\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.4090\tClassification Loss: 1.3153\n",
      "Train Epoch: 2 [84800/110534 (77%)]\tAll Loss: 2.4755\tTriple Loss(1): 0.3304\tClassification Loss: 1.8147\n",
      "Train Epoch: 2 [85120/110534 (77%)]\tAll Loss: 1.6974\tTriple Loss(1): 0.1493\tClassification Loss: 1.3989\n",
      "Train Epoch: 2 [85440/110534 (77%)]\tAll Loss: 1.3685\tTriple Loss(0): 0.0000\tClassification Loss: 1.3685\n",
      "Train Epoch: 2 [85760/110534 (78%)]\tAll Loss: 2.0540\tTriple Loss(1): 0.1693\tClassification Loss: 1.7154\n",
      "Train Epoch: 2 [86080/110534 (78%)]\tAll Loss: 2.1369\tTriple Loss(1): 0.2520\tClassification Loss: 1.6329\n",
      "\n",
      "Test set: Average loss: 1.6589, Accuracy: 542/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tAll Loss: 2.7954\tTriple Loss(1): 0.3633\tClassification Loss: 2.0689\n",
      "Train Epoch: 2 [86720/110534 (78%)]\tAll Loss: 2.0131\tTriple Loss(1): 0.2909\tClassification Loss: 1.4313\n",
      "Train Epoch: 2 [87040/110534 (79%)]\tAll Loss: 1.9479\tTriple Loss(1): 0.2629\tClassification Loss: 1.4221\n",
      "Train Epoch: 2 [87360/110534 (79%)]\tAll Loss: 2.2029\tTriple Loss(1): 0.3202\tClassification Loss: 1.5624\n",
      "Train Epoch: 2 [87680/110534 (79%)]\tAll Loss: 1.9986\tTriple Loss(1): 0.1565\tClassification Loss: 1.6856\n",
      "Train Epoch: 2 [88000/110534 (80%)]\tAll Loss: 2.7150\tTriple Loss(1): 0.2276\tClassification Loss: 2.2598\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tAll Loss: 2.0369\tTriple Loss(1): 0.1559\tClassification Loss: 1.7251\n",
      "Train Epoch: 2 [88640/110534 (80%)]\tAll Loss: 2.4387\tTriple Loss(1): 0.2317\tClassification Loss: 1.9752\n",
      "Train Epoch: 2 [88960/110534 (80%)]\tAll Loss: 1.7794\tTriple Loss(0): 0.0000\tClassification Loss: 1.7794\n",
      "Train Epoch: 2 [89280/110534 (81%)]\tAll Loss: 2.6597\tTriple Loss(1): 0.4533\tClassification Loss: 1.7531\n",
      "\n",
      "Test set: Average loss: 1.6613, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [89600/110534 (81%)]\tAll Loss: 2.5266\tTriple Loss(1): 0.1955\tClassification Loss: 2.1357\n",
      "Train Epoch: 2 [89920/110534 (81%)]\tAll Loss: 2.5861\tTriple Loss(1): 0.4182\tClassification Loss: 1.7497\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tAll Loss: 2.0395\tTriple Loss(1): 0.1985\tClassification Loss: 1.6424\n",
      "Train Epoch: 2 [90560/110534 (82%)]\tAll Loss: 1.6235\tTriple Loss(1): 0.1489\tClassification Loss: 1.3257\n",
      "Train Epoch: 2 [90880/110534 (82%)]\tAll Loss: 2.1334\tTriple Loss(1): 0.2267\tClassification Loss: 1.6801\n",
      "Train Epoch: 2 [91200/110534 (82%)]\tAll Loss: 1.8708\tTriple Loss(1): 0.2210\tClassification Loss: 1.4289\n",
      "Train Epoch: 2 [91520/110534 (83%)]\tAll Loss: 1.6401\tTriple Loss(0): 0.0000\tClassification Loss: 1.6401\n",
      "Train Epoch: 2 [91840/110534 (83%)]\tAll Loss: 2.6120\tTriple Loss(1): 0.5362\tClassification Loss: 1.5396\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tAll Loss: 2.7132\tTriple Loss(1): 0.3545\tClassification Loss: 2.0042\n",
      "Train Epoch: 2 [92480/110534 (84%)]\tAll Loss: 1.8850\tTriple Loss(1): 0.1350\tClassification Loss: 1.6151\n",
      "\n",
      "Test set: Average loss: 1.6691, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [92800/110534 (84%)]\tAll Loss: 1.9706\tTriple Loss(1): 0.1624\tClassification Loss: 1.6457\n",
      "Train Epoch: 2 [93120/110534 (84%)]\tAll Loss: 1.9647\tTriple Loss(1): 0.0951\tClassification Loss: 1.7745\n",
      "Train Epoch: 2 [93440/110534 (85%)]\tAll Loss: 3.0173\tTriple Loss(0): 0.6316\tClassification Loss: 1.7541\n",
      "Train Epoch: 2 [93760/110534 (85%)]\tAll Loss: 2.2309\tTriple Loss(1): 0.3585\tClassification Loss: 1.5138\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tAll Loss: 1.7631\tTriple Loss(1): 0.1097\tClassification Loss: 1.5437\n",
      "Train Epoch: 2 [94400/110534 (85%)]\tAll Loss: 2.2216\tTriple Loss(1): 0.2614\tClassification Loss: 1.6988\n",
      "Train Epoch: 2 [94720/110534 (86%)]\tAll Loss: 1.7035\tTriple Loss(0): 0.0000\tClassification Loss: 1.7035\n",
      "Train Epoch: 2 [95040/110534 (86%)]\tAll Loss: 1.9317\tTriple Loss(1): 0.1473\tClassification Loss: 1.6370\n",
      "Train Epoch: 2 [95360/110534 (86%)]\tAll Loss: 2.1842\tTriple Loss(1): 0.2473\tClassification Loss: 1.6895\n",
      "Train Epoch: 2 [95680/110534 (87%)]\tAll Loss: 1.8858\tTriple Loss(1): 0.1593\tClassification Loss: 1.5672\n",
      "\n",
      "Test set: Average loss: 1.6595, Accuracy: 542/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tAll Loss: 1.8131\tTriple Loss(1): 0.1966\tClassification Loss: 1.4199\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_3000.pth.tar\n",
      "Train Epoch: 2 [96320/110534 (87%)]\tAll Loss: 1.8410\tTriple Loss(1): 0.1606\tClassification Loss: 1.5199\n",
      "Train Epoch: 2 [96640/110534 (87%)]\tAll Loss: 1.7218\tTriple Loss(0): 0.0000\tClassification Loss: 1.7218\n",
      "Train Epoch: 2 [96960/110534 (88%)]\tAll Loss: 1.3942\tTriple Loss(1): 0.0839\tClassification Loss: 1.2265\n",
      "Train Epoch: 2 [97280/110534 (88%)]\tAll Loss: 2.0034\tTriple Loss(1): 0.3788\tClassification Loss: 1.2457\n",
      "Train Epoch: 2 [97600/110534 (88%)]\tAll Loss: 1.9115\tTriple Loss(0): 0.0000\tClassification Loss: 1.9115\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tAll Loss: 1.1860\tTriple Loss(0): 0.0000\tClassification Loss: 1.1860\n",
      "Train Epoch: 2 [98240/110534 (89%)]\tAll Loss: 2.2573\tTriple Loss(0): 0.4676\tClassification Loss: 1.3220\n",
      "Train Epoch: 2 [98560/110534 (89%)]\tAll Loss: 1.9454\tTriple Loss(1): 0.2523\tClassification Loss: 1.4408\n",
      "Train Epoch: 2 [98880/110534 (89%)]\tAll Loss: 2.0493\tTriple Loss(1): 0.2397\tClassification Loss: 1.5700\n",
      "\n",
      "Test set: Average loss: 1.6667, Accuracy: 537/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [99200/110534 (90%)]\tAll Loss: 1.9327\tTriple Loss(1): 0.2137\tClassification Loss: 1.5053\n",
      "Train Epoch: 2 [99520/110534 (90%)]\tAll Loss: 2.1136\tTriple Loss(1): 0.2694\tClassification Loss: 1.5748\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tAll Loss: 2.6177\tTriple Loss(1): 0.4630\tClassification Loss: 1.6916\n",
      "Train Epoch: 2 [100160/110534 (91%)]\tAll Loss: 2.4375\tTriple Loss(1): 0.2947\tClassification Loss: 1.8480\n",
      "Train Epoch: 2 [100480/110534 (91%)]\tAll Loss: 2.3060\tTriple Loss(1): 0.3379\tClassification Loss: 1.6301\n",
      "Train Epoch: 2 [100800/110534 (91%)]\tAll Loss: 2.9281\tTriple Loss(1): 0.3593\tClassification Loss: 2.2094\n",
      "Train Epoch: 2 [101120/110534 (91%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.3471\tClassification Loss: 1.4484\n",
      "Train Epoch: 2 [101440/110534 (92%)]\tAll Loss: 1.8111\tTriple Loss(1): 0.1888\tClassification Loss: 1.4334\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tAll Loss: 2.3895\tTriple Loss(1): 0.3468\tClassification Loss: 1.6958\n",
      "Train Epoch: 2 [102080/110534 (92%)]\tAll Loss: 2.2525\tTriple Loss(1): 0.1657\tClassification Loss: 1.9212\n",
      "\n",
      "Test set: Average loss: 1.6635, Accuracy: 541/960 (56%)\n",
      "\n",
      "Train Epoch: 2 [102400/110534 (93%)]\tAll Loss: 2.3588\tTriple Loss(1): 0.2714\tClassification Loss: 1.8161\n",
      "Train Epoch: 2 [102720/110534 (93%)]\tAll Loss: 2.5350\tTriple Loss(1): 0.3770\tClassification Loss: 1.7811\n",
      "Train Epoch: 2 [103040/110534 (93%)]\tAll Loss: 2.1051\tTriple Loss(1): 0.3401\tClassification Loss: 1.4248\n",
      "Train Epoch: 2 [103360/110534 (93%)]\tAll Loss: 1.9985\tTriple Loss(1): 0.0618\tClassification Loss: 1.8750\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tAll Loss: 2.6616\tTriple Loss(1): 0.5040\tClassification Loss: 1.6536\n",
      "Train Epoch: 2 [104000/110534 (94%)]\tAll Loss: 1.5379\tTriple Loss(0): 0.0000\tClassification Loss: 1.5379\n",
      "Train Epoch: 2 [104320/110534 (94%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.1496\tClassification Loss: 1.9494\n",
      "Train Epoch: 2 [104640/110534 (95%)]\tAll Loss: 2.3869\tTriple Loss(1): 0.1854\tClassification Loss: 2.0161\n",
      "Train Epoch: 2 [104960/110534 (95%)]\tAll Loss: 1.9741\tTriple Loss(1): 0.2553\tClassification Loss: 1.4635\n",
      "Train Epoch: 2 [105280/110534 (95%)]\tAll Loss: 2.1159\tTriple Loss(1): 0.2185\tClassification Loss: 1.6789\n",
      "\n",
      "Test set: Average loss: 1.6629, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tAll Loss: 2.0852\tTriple Loss(1): 0.1616\tClassification Loss: 1.7619\n",
      "Train Epoch: 2 [105920/110534 (96%)]\tAll Loss: 2.0030\tTriple Loss(1): 0.1505\tClassification Loss: 1.7021\n",
      "Train Epoch: 2 [106240/110534 (96%)]\tAll Loss: 1.4003\tTriple Loss(0): 0.0000\tClassification Loss: 1.4003\n",
      "Train Epoch: 2 [106560/110534 (96%)]\tAll Loss: 1.6857\tTriple Loss(0): 0.0000\tClassification Loss: 1.6857\n",
      "Train Epoch: 2 [106880/110534 (97%)]\tAll Loss: 2.1564\tTriple Loss(0): 0.0000\tClassification Loss: 2.1564\n",
      "Train Epoch: 2 [107200/110534 (97%)]\tAll Loss: 3.1570\tTriple Loss(0): 0.6883\tClassification Loss: 1.7805\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tAll Loss: 2.1602\tTriple Loss(1): 0.1613\tClassification Loss: 1.8377\n",
      "Train Epoch: 2 [107840/110534 (98%)]\tAll Loss: 2.2330\tTriple Loss(1): 0.3723\tClassification Loss: 1.4883\n",
      "Train Epoch: 2 [108160/110534 (98%)]\tAll Loss: 1.6095\tTriple Loss(1): 0.1682\tClassification Loss: 1.2731\n",
      "Train Epoch: 2 [108480/110534 (98%)]\tAll Loss: 2.2882\tTriple Loss(1): 0.3563\tClassification Loss: 1.5756\n",
      "\n",
      "Test set: Average loss: 1.6581, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 2 [108800/110534 (98%)]\tAll Loss: 3.6565\tTriple Loss(0): 1.0185\tClassification Loss: 1.6195\n",
      "Train Epoch: 2 [109120/110534 (99%)]\tAll Loss: 2.5592\tTriple Loss(1): 0.2760\tClassification Loss: 2.0072\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tAll Loss: 1.6302\tTriple Loss(0): 0.1109\tClassification Loss: 1.4083\n",
      "Train Epoch: 2 [109760/110534 (99%)]\tAll Loss: 1.8644\tTriple Loss(1): 0.1104\tClassification Loss: 1.6436\n",
      "Train Epoch: 2 [110080/110534 (100%)]\tAll Loss: 2.2714\tTriple Loss(1): 0.2779\tClassification Loss: 1.7155\n",
      "Train Epoch: 2 [110400/110534 (100%)]\tAll Loss: 2.0645\tTriple Loss(1): 0.2581\tClassification Loss: 1.5484\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_final.pth.tar\n",
      "\n",
      "Test set: Average loss: 1.6647, Accuracy: 532/960 (55%)\n",
      "\n",
      "Train Epoch: 3 [0/110534 (0%)]\tAll Loss: 1.9444\tTriple Loss(1): 0.1538\tClassification Loss: 1.6367\n",
      "Train Epoch: 3 [320/110534 (0%)]\tAll Loss: 1.9898\tTriple Loss(1): 0.2035\tClassification Loss: 1.5829\n",
      "Train Epoch: 3 [640/110534 (1%)]\tAll Loss: 1.8764\tTriple Loss(1): 0.4137\tClassification Loss: 1.0490\n",
      "Train Epoch: 3 [960/110534 (1%)]\tAll Loss: 2.5638\tTriple Loss(1): 0.2467\tClassification Loss: 2.0703\n",
      "Train Epoch: 3 [1280/110534 (1%)]\tAll Loss: 2.4402\tTriple Loss(1): 0.4695\tClassification Loss: 1.5012\n",
      "Train Epoch: 3 [1600/110534 (1%)]\tAll Loss: 2.3215\tTriple Loss(1): 0.2782\tClassification Loss: 1.7652\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tAll Loss: 2.1047\tTriple Loss(1): 0.1371\tClassification Loss: 1.8304\n",
      "Train Epoch: 3 [2240/110534 (2%)]\tAll Loss: 1.7270\tTriple Loss(0): 0.0000\tClassification Loss: 1.7270\n",
      "Train Epoch: 3 [2560/110534 (2%)]\tAll Loss: 2.3596\tTriple Loss(1): 0.3298\tClassification Loss: 1.6999\n",
      "Train Epoch: 3 [2880/110534 (3%)]\tAll Loss: 2.0704\tTriple Loss(1): 0.1513\tClassification Loss: 1.7679\n",
      "\n",
      "Test set: Average loss: 1.6640, Accuracy: 541/960 (56%)\n",
      "\n",
      "Train Epoch: 3 [3200/110534 (3%)]\tAll Loss: 1.9580\tTriple Loss(0): 0.0000\tClassification Loss: 1.9580\n",
      "Train Epoch: 3 [3520/110534 (3%)]\tAll Loss: 1.6650\tTriple Loss(1): 0.1892\tClassification Loss: 1.2867\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tAll Loss: 1.9112\tTriple Loss(1): 0.1232\tClassification Loss: 1.6649\n",
      "Train Epoch: 3 [4160/110534 (4%)]\tAll Loss: 2.2322\tTriple Loss(1): 0.3181\tClassification Loss: 1.5960\n",
      "Train Epoch: 3 [4480/110534 (4%)]\tAll Loss: 2.0191\tTriple Loss(1): 0.2481\tClassification Loss: 1.5228\n",
      "Train Epoch: 3 [4800/110534 (4%)]\tAll Loss: 1.5659\tTriple Loss(0): 0.0000\tClassification Loss: 1.5659\n",
      "Train Epoch: 3 [5120/110534 (5%)]\tAll Loss: 1.9423\tTriple Loss(1): 0.1386\tClassification Loss: 1.6650\n",
      "Train Epoch: 3 [5440/110534 (5%)]\tAll Loss: 2.2206\tTriple Loss(1): 0.2513\tClassification Loss: 1.7181\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tAll Loss: 1.9764\tTriple Loss(1): 0.2503\tClassification Loss: 1.4758\n",
      "Train Epoch: 3 [6080/110534 (5%)]\tAll Loss: 2.1138\tTriple Loss(1): 0.2574\tClassification Loss: 1.5990\n",
      "\n",
      "Test set: Average loss: 1.6543, Accuracy: 543/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [6400/110534 (6%)]\tAll Loss: 2.2415\tTriple Loss(1): 0.2315\tClassification Loss: 1.7785\n",
      "Train Epoch: 3 [6720/110534 (6%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.1847\tClassification Loss: 1.5440\n",
      "Train Epoch: 3 [7040/110534 (6%)]\tAll Loss: 1.8251\tTriple Loss(0): 0.0000\tClassification Loss: 1.8251\n",
      "Train Epoch: 3 [7360/110534 (7%)]\tAll Loss: 2.0961\tTriple Loss(1): 0.2942\tClassification Loss: 1.5077\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tAll Loss: 1.2267\tTriple Loss(0): 0.0000\tClassification Loss: 1.2267\n",
      "Train Epoch: 3 [8000/110534 (7%)]\tAll Loss: 2.3039\tTriple Loss(1): 0.4791\tClassification Loss: 1.3456\n",
      "Train Epoch: 3 [8320/110534 (8%)]\tAll Loss: 1.7519\tTriple Loss(1): 0.1040\tClassification Loss: 1.5439\n",
      "Train Epoch: 3 [8640/110534 (8%)]\tAll Loss: 1.9910\tTriple Loss(1): 0.3813\tClassification Loss: 1.2284\n",
      "Train Epoch: 3 [8960/110534 (8%)]\tAll Loss: 1.9546\tTriple Loss(1): 0.2521\tClassification Loss: 1.4503\n",
      "Train Epoch: 3 [9280/110534 (8%)]\tAll Loss: 1.7522\tTriple Loss(1): 0.2007\tClassification Loss: 1.3508\n",
      "\n",
      "Test set: Average loss: 1.6493, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tAll Loss: 2.3714\tTriple Loss(1): 0.3017\tClassification Loss: 1.7680\n",
      "Train Epoch: 3 [9920/110534 (9%)]\tAll Loss: 2.1995\tTriple Loss(1): 0.2789\tClassification Loss: 1.6416\n",
      "Train Epoch: 3 [10240/110534 (9%)]\tAll Loss: 2.3498\tTriple Loss(1): 0.2461\tClassification Loss: 1.8576\n",
      "Train Epoch: 3 [10560/110534 (10%)]\tAll Loss: 2.1456\tTriple Loss(1): 0.1719\tClassification Loss: 1.8018\n",
      "Train Epoch: 3 [10880/110534 (10%)]\tAll Loss: 2.6118\tTriple Loss(1): 0.3696\tClassification Loss: 1.8726\n",
      "Train Epoch: 3 [11200/110534 (10%)]\tAll Loss: 1.7860\tTriple Loss(1): 0.2594\tClassification Loss: 1.2672\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tAll Loss: 1.5803\tTriple Loss(0): 0.0000\tClassification Loss: 1.5803\n",
      "Train Epoch: 3 [11840/110534 (11%)]\tAll Loss: 2.0584\tTriple Loss(1): 0.1707\tClassification Loss: 1.7170\n",
      "Train Epoch: 3 [12160/110534 (11%)]\tAll Loss: 1.8099\tTriple Loss(0): 0.0000\tClassification Loss: 1.8099\n",
      "Train Epoch: 3 [12480/110534 (11%)]\tAll Loss: 2.2176\tTriple Loss(1): 0.2333\tClassification Loss: 1.7510\n",
      "\n",
      "Test set: Average loss: 1.6532, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [12800/110534 (12%)]\tAll Loss: 2.1384\tTriple Loss(1): 0.3221\tClassification Loss: 1.4943\n",
      "Train Epoch: 3 [13120/110534 (12%)]\tAll Loss: 1.3973\tTriple Loss(0): 0.0000\tClassification Loss: 1.3973\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tAll Loss: 1.9962\tTriple Loss(0): 0.0000\tClassification Loss: 1.9962\n",
      "Train Epoch: 3 [13760/110534 (12%)]\tAll Loss: 2.0375\tTriple Loss(1): 0.1760\tClassification Loss: 1.6855\n",
      "Train Epoch: 3 [14080/110534 (13%)]\tAll Loss: 2.3512\tTriple Loss(1): 0.3113\tClassification Loss: 1.7287\n",
      "Train Epoch: 3 [14400/110534 (13%)]\tAll Loss: 2.6253\tTriple Loss(1): 0.2240\tClassification Loss: 2.1773\n",
      "Train Epoch: 3 [14720/110534 (13%)]\tAll Loss: 2.2869\tTriple Loss(1): 0.2412\tClassification Loss: 1.8046\n",
      "Train Epoch: 3 [15040/110534 (14%)]\tAll Loss: 2.0509\tTriple Loss(1): 0.3294\tClassification Loss: 1.3922\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tAll Loss: 1.8597\tTriple Loss(1): 0.1985\tClassification Loss: 1.4626\n",
      "Train Epoch: 3 [15680/110534 (14%)]\tAll Loss: 1.4384\tTriple Loss(0): 0.0000\tClassification Loss: 1.4384\n",
      "\n",
      "Test set: Average loss: 1.6466, Accuracy: 544/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [16000/110534 (14%)]\tAll Loss: 5.7329\tTriple Loss(0): 1.9399\tClassification Loss: 1.8530\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_500.pth.tar\n",
      "Train Epoch: 3 [16320/110534 (15%)]\tAll Loss: 1.8269\tTriple Loss(1): 0.2669\tClassification Loss: 1.2931\n",
      "Train Epoch: 3 [16640/110534 (15%)]\tAll Loss: 1.9150\tTriple Loss(1): 0.1821\tClassification Loss: 1.5508\n",
      "Train Epoch: 3 [16960/110534 (15%)]\tAll Loss: 1.9914\tTriple Loss(0): 0.0000\tClassification Loss: 1.9914\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tAll Loss: 1.6781\tTriple Loss(1): 0.1296\tClassification Loss: 1.4189\n",
      "Train Epoch: 3 [17600/110534 (16%)]\tAll Loss: 2.4527\tTriple Loss(1): 0.3734\tClassification Loss: 1.7059\n",
      "Train Epoch: 3 [17920/110534 (16%)]\tAll Loss: 2.0821\tTriple Loss(1): 0.1518\tClassification Loss: 1.7784\n",
      "Train Epoch: 3 [18240/110534 (16%)]\tAll Loss: 2.0339\tTriple Loss(1): 0.0604\tClassification Loss: 1.9131\n",
      "Train Epoch: 3 [18560/110534 (17%)]\tAll Loss: 1.9909\tTriple Loss(1): 0.2292\tClassification Loss: 1.5326\n",
      "Train Epoch: 3 [18880/110534 (17%)]\tAll Loss: 2.1103\tTriple Loss(1): 0.2532\tClassification Loss: 1.6040\n",
      "\n",
      "Test set: Average loss: 1.6454, Accuracy: 552/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tAll Loss: 1.4262\tTriple Loss(0): 0.0000\tClassification Loss: 1.4262\n",
      "Train Epoch: 3 [19520/110534 (18%)]\tAll Loss: 2.1427\tTriple Loss(1): 0.2295\tClassification Loss: 1.6836\n",
      "Train Epoch: 3 [19840/110534 (18%)]\tAll Loss: 2.3921\tTriple Loss(1): 0.3755\tClassification Loss: 1.6411\n",
      "Train Epoch: 3 [20160/110534 (18%)]\tAll Loss: 1.9291\tTriple Loss(1): 0.1303\tClassification Loss: 1.6686\n",
      "Train Epoch: 3 [20480/110534 (19%)]\tAll Loss: 1.9943\tTriple Loss(1): 0.2876\tClassification Loss: 1.4191\n",
      "Train Epoch: 3 [20800/110534 (19%)]\tAll Loss: 3.3002\tTriple Loss(1): 0.5478\tClassification Loss: 2.2047\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tAll Loss: 1.9359\tTriple Loss(1): 0.2321\tClassification Loss: 1.4717\n",
      "Train Epoch: 3 [21440/110534 (19%)]\tAll Loss: 2.3040\tTriple Loss(1): 0.4554\tClassification Loss: 1.3932\n",
      "Train Epoch: 3 [21760/110534 (20%)]\tAll Loss: 2.4705\tTriple Loss(1): 0.2163\tClassification Loss: 2.0378\n",
      "Train Epoch: 3 [22080/110534 (20%)]\tAll Loss: 2.0128\tTriple Loss(1): 0.1486\tClassification Loss: 1.7155\n",
      "\n",
      "Test set: Average loss: 1.6504, Accuracy: 529/960 (55%)\n",
      "\n",
      "Train Epoch: 3 [22400/110534 (20%)]\tAll Loss: 2.2688\tTriple Loss(1): 0.3205\tClassification Loss: 1.6279\n",
      "Train Epoch: 3 [22720/110534 (21%)]\tAll Loss: 1.6511\tTriple Loss(0): 0.0000\tClassification Loss: 1.6511\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tAll Loss: 1.6268\tTriple Loss(0): 0.0000\tClassification Loss: 1.6268\n",
      "Train Epoch: 3 [23360/110534 (21%)]\tAll Loss: 1.6899\tTriple Loss(1): 0.0578\tClassification Loss: 1.5742\n",
      "Train Epoch: 3 [23680/110534 (21%)]\tAll Loss: 2.2949\tTriple Loss(1): 0.2079\tClassification Loss: 1.8791\n",
      "Train Epoch: 3 [24000/110534 (22%)]\tAll Loss: 2.3342\tTriple Loss(1): 0.4290\tClassification Loss: 1.4762\n",
      "Train Epoch: 3 [24320/110534 (22%)]\tAll Loss: 2.2253\tTriple Loss(1): 0.2651\tClassification Loss: 1.6951\n",
      "Train Epoch: 3 [24640/110534 (22%)]\tAll Loss: 2.0580\tTriple Loss(0): 0.0000\tClassification Loss: 2.0580\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tAll Loss: 1.9726\tTriple Loss(1): 0.2246\tClassification Loss: 1.5233\n",
      "Train Epoch: 3 [25280/110534 (23%)]\tAll Loss: 2.2613\tTriple Loss(1): 0.3067\tClassification Loss: 1.6479\n",
      "\n",
      "Test set: Average loss: 1.6415, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [25600/110534 (23%)]\tAll Loss: 1.5874\tTriple Loss(0): 0.0000\tClassification Loss: 1.5874\n",
      "Train Epoch: 3 [25920/110534 (23%)]\tAll Loss: 2.3986\tTriple Loss(1): 0.3505\tClassification Loss: 1.6977\n",
      "Train Epoch: 3 [26240/110534 (24%)]\tAll Loss: 2.6749\tTriple Loss(1): 0.5807\tClassification Loss: 1.5135\n",
      "Train Epoch: 3 [26560/110534 (24%)]\tAll Loss: 2.4483\tTriple Loss(1): 0.3346\tClassification Loss: 1.7791\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tAll Loss: 1.8399\tTriple Loss(0): 0.0000\tClassification Loss: 1.8399\n",
      "Train Epoch: 3 [27200/110534 (25%)]\tAll Loss: 2.4215\tTriple Loss(1): 0.2254\tClassification Loss: 1.9707\n",
      "Train Epoch: 3 [27520/110534 (25%)]\tAll Loss: 1.8356\tTriple Loss(1): 0.1488\tClassification Loss: 1.5380\n",
      "Train Epoch: 3 [27840/110534 (25%)]\tAll Loss: 1.8149\tTriple Loss(1): 0.1800\tClassification Loss: 1.4548\n",
      "Train Epoch: 3 [28160/110534 (25%)]\tAll Loss: 2.3969\tTriple Loss(1): 0.2692\tClassification Loss: 1.8585\n",
      "Train Epoch: 3 [28480/110534 (26%)]\tAll Loss: 2.3744\tTriple Loss(1): 0.2819\tClassification Loss: 1.8106\n",
      "\n",
      "Test set: Average loss: 1.6448, Accuracy: 557/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tAll Loss: 1.6487\tTriple Loss(1): 0.1860\tClassification Loss: 1.2767\n",
      "Train Epoch: 3 [29120/110534 (26%)]\tAll Loss: 2.2537\tTriple Loss(1): 0.2628\tClassification Loss: 1.7282\n",
      "Train Epoch: 3 [29440/110534 (27%)]\tAll Loss: 1.4078\tTriple Loss(0): 0.0000\tClassification Loss: 1.4078\n",
      "Train Epoch: 3 [29760/110534 (27%)]\tAll Loss: 1.6080\tTriple Loss(1): 0.1991\tClassification Loss: 1.2098\n",
      "Train Epoch: 3 [30080/110534 (27%)]\tAll Loss: 1.5150\tTriple Loss(1): 0.0758\tClassification Loss: 1.3634\n",
      "Train Epoch: 3 [30400/110534 (27%)]\tAll Loss: 1.7987\tTriple Loss(1): 0.2249\tClassification Loss: 1.3489\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tAll Loss: 2.1277\tTriple Loss(1): 0.4177\tClassification Loss: 1.2922\n",
      "Train Epoch: 3 [31040/110534 (28%)]\tAll Loss: 2.1116\tTriple Loss(1): 0.1590\tClassification Loss: 1.7937\n",
      "Train Epoch: 3 [31360/110534 (28%)]\tAll Loss: 2.2025\tTriple Loss(1): 0.2723\tClassification Loss: 1.6578\n",
      "Train Epoch: 3 [31680/110534 (29%)]\tAll Loss: 1.8642\tTriple Loss(1): 0.1463\tClassification Loss: 1.5716\n",
      "\n",
      "Test set: Average loss: 1.6380, Accuracy: 550/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [32000/110534 (29%)]\tAll Loss: 1.9004\tTriple Loss(1): 0.0000\tClassification Loss: 1.9004\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1000.pth.tar\n",
      "Train Epoch: 3 [32320/110534 (29%)]\tAll Loss: 1.9250\tTriple Loss(1): 0.1913\tClassification Loss: 1.5425\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tAll Loss: 1.9585\tTriple Loss(1): 0.2242\tClassification Loss: 1.5100\n",
      "Train Epoch: 3 [32960/110534 (30%)]\tAll Loss: 1.9648\tTriple Loss(1): 0.1368\tClassification Loss: 1.6911\n",
      "Train Epoch: 3 [33280/110534 (30%)]\tAll Loss: 2.2617\tTriple Loss(1): 0.2602\tClassification Loss: 1.7412\n",
      "Train Epoch: 3 [33600/110534 (30%)]\tAll Loss: 1.5912\tTriple Loss(0): 0.0000\tClassification Loss: 1.5912\n",
      "Train Epoch: 3 [33920/110534 (31%)]\tAll Loss: 2.0048\tTriple Loss(1): 0.2263\tClassification Loss: 1.5522\n",
      "Train Epoch: 3 [34240/110534 (31%)]\tAll Loss: 2.5789\tTriple Loss(1): 0.3772\tClassification Loss: 1.8244\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tAll Loss: 2.0613\tTriple Loss(1): 0.3154\tClassification Loss: 1.4305\n",
      "Train Epoch: 3 [34880/110534 (32%)]\tAll Loss: 1.7031\tTriple Loss(1): 0.0676\tClassification Loss: 1.5680\n",
      "\n",
      "Test set: Average loss: 1.6343, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [35200/110534 (32%)]\tAll Loss: 2.4243\tTriple Loss(1): 0.1646\tClassification Loss: 2.0950\n",
      "Train Epoch: 3 [35520/110534 (32%)]\tAll Loss: 1.6794\tTriple Loss(0): 0.0000\tClassification Loss: 1.6794\n",
      "Train Epoch: 3 [35840/110534 (32%)]\tAll Loss: 1.6273\tTriple Loss(1): 0.1516\tClassification Loss: 1.3240\n",
      "Train Epoch: 3 [36160/110534 (33%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.3345\tClassification Loss: 1.5762\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tAll Loss: 2.0733\tTriple Loss(1): 0.1947\tClassification Loss: 1.6839\n",
      "Train Epoch: 3 [36800/110534 (33%)]\tAll Loss: 2.4671\tTriple Loss(1): 0.3269\tClassification Loss: 1.8133\n",
      "Train Epoch: 3 [37120/110534 (34%)]\tAll Loss: 2.2441\tTriple Loss(1): 0.1563\tClassification Loss: 1.9315\n",
      "Train Epoch: 3 [37440/110534 (34%)]\tAll Loss: 1.9050\tTriple Loss(1): 0.1263\tClassification Loss: 1.6523\n",
      "Train Epoch: 3 [37760/110534 (34%)]\tAll Loss: 1.6936\tTriple Loss(1): 0.0529\tClassification Loss: 1.5879\n",
      "Train Epoch: 3 [38080/110534 (34%)]\tAll Loss: 1.9887\tTriple Loss(1): 0.0907\tClassification Loss: 1.8073\n",
      "\n",
      "Test set: Average loss: 1.6415, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tAll Loss: 1.3891\tTriple Loss(0): 0.0000\tClassification Loss: 1.3891\n",
      "Train Epoch: 3 [38720/110534 (35%)]\tAll Loss: 2.5951\tTriple Loss(1): 0.3112\tClassification Loss: 1.9727\n",
      "Train Epoch: 3 [39040/110534 (35%)]\tAll Loss: 1.4274\tTriple Loss(0): 0.0000\tClassification Loss: 1.4274\n",
      "Train Epoch: 3 [39360/110534 (36%)]\tAll Loss: 1.3725\tTriple Loss(0): 0.0000\tClassification Loss: 1.3725\n",
      "Train Epoch: 3 [39680/110534 (36%)]\tAll Loss: 2.1414\tTriple Loss(1): 0.1849\tClassification Loss: 1.7715\n",
      "Train Epoch: 3 [40000/110534 (36%)]\tAll Loss: 1.7204\tTriple Loss(1): 0.2208\tClassification Loss: 1.2789\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.1190\tClassification Loss: 1.7991\n",
      "Train Epoch: 3 [40640/110534 (37%)]\tAll Loss: 2.2603\tTriple Loss(1): 0.2372\tClassification Loss: 1.7858\n",
      "Train Epoch: 3 [40960/110534 (37%)]\tAll Loss: 1.5412\tTriple Loss(0): 0.0000\tClassification Loss: 1.5412\n",
      "Train Epoch: 3 [41280/110534 (37%)]\tAll Loss: 2.0678\tTriple Loss(1): 0.2143\tClassification Loss: 1.6391\n",
      "\n",
      "Test set: Average loss: 1.6333, Accuracy: 547/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [41600/110534 (38%)]\tAll Loss: 1.6474\tTriple Loss(1): 0.0368\tClassification Loss: 1.5737\n",
      "Train Epoch: 3 [41920/110534 (38%)]\tAll Loss: 1.9990\tTriple Loss(1): 0.1882\tClassification Loss: 1.6227\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tAll Loss: 1.7048\tTriple Loss(0): 0.0000\tClassification Loss: 1.7048\n",
      "Train Epoch: 3 [42560/110534 (38%)]\tAll Loss: 1.7313\tTriple Loss(1): 0.1665\tClassification Loss: 1.3983\n",
      "Train Epoch: 3 [42880/110534 (39%)]\tAll Loss: 1.6392\tTriple Loss(0): 0.0000\tClassification Loss: 1.6392\n",
      "Train Epoch: 3 [43200/110534 (39%)]\tAll Loss: 1.7118\tTriple Loss(0): 0.0000\tClassification Loss: 1.7118\n",
      "Train Epoch: 3 [43520/110534 (39%)]\tAll Loss: 1.9168\tTriple Loss(1): 0.2374\tClassification Loss: 1.4420\n",
      "Train Epoch: 3 [43840/110534 (40%)]\tAll Loss: 2.2104\tTriple Loss(1): 0.2632\tClassification Loss: 1.6840\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tAll Loss: 2.5213\tTriple Loss(1): 0.2113\tClassification Loss: 2.0987\n",
      "Train Epoch: 3 [44480/110534 (40%)]\tAll Loss: 1.6990\tTriple Loss(0): 0.0000\tClassification Loss: 1.6990\n",
      "\n",
      "Test set: Average loss: 1.6327, Accuracy: 562/960 (59%)\n",
      "\n",
      "Train Epoch: 3 [44800/110534 (41%)]\tAll Loss: 1.8694\tTriple Loss(1): 0.2851\tClassification Loss: 1.2992\n",
      "Train Epoch: 3 [45120/110534 (41%)]\tAll Loss: 1.8155\tTriple Loss(1): 0.0417\tClassification Loss: 1.7322\n",
      "Train Epoch: 3 [45440/110534 (41%)]\tAll Loss: 1.6589\tTriple Loss(1): 0.1390\tClassification Loss: 1.3808\n",
      "Train Epoch: 3 [45760/110534 (41%)]\tAll Loss: 1.6444\tTriple Loss(1): 0.1431\tClassification Loss: 1.3582\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tAll Loss: 1.8285\tTriple Loss(1): 0.2568\tClassification Loss: 1.3149\n",
      "Train Epoch: 3 [46400/110534 (42%)]\tAll Loss: 2.0277\tTriple Loss(0): 0.0000\tClassification Loss: 2.0277\n",
      "Train Epoch: 3 [46720/110534 (42%)]\tAll Loss: 2.4568\tTriple Loss(1): 0.2478\tClassification Loss: 1.9612\n",
      "Train Epoch: 3 [47040/110534 (43%)]\tAll Loss: 2.2700\tTriple Loss(1): 0.3820\tClassification Loss: 1.5059\n",
      "Train Epoch: 3 [47360/110534 (43%)]\tAll Loss: 2.3403\tTriple Loss(1): 0.3601\tClassification Loss: 1.6201\n",
      "Train Epoch: 3 [47680/110534 (43%)]\tAll Loss: 1.6766\tTriple Loss(1): 0.2061\tClassification Loss: 1.2645\n",
      "\n",
      "Test set: Average loss: 1.6363, Accuracy: 541/960 (56%)\n",
      "\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tAll Loss: 2.3369\tTriple Loss(1): 0.1846\tClassification Loss: 1.9678\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1500.pth.tar\n",
      "Train Epoch: 3 [48320/110534 (44%)]\tAll Loss: 2.0344\tTriple Loss(1): 0.2491\tClassification Loss: 1.5362\n",
      "Train Epoch: 3 [48640/110534 (44%)]\tAll Loss: 2.5994\tTriple Loss(1): 0.3193\tClassification Loss: 1.9608\n",
      "Train Epoch: 3 [48960/110534 (44%)]\tAll Loss: 2.0912\tTriple Loss(1): 0.3516\tClassification Loss: 1.3881\n",
      "Train Epoch: 3 [49280/110534 (45%)]\tAll Loss: 1.9475\tTriple Loss(1): 0.2019\tClassification Loss: 1.5438\n",
      "Train Epoch: 3 [49600/110534 (45%)]\tAll Loss: 3.9717\tTriple Loss(0): 1.1959\tClassification Loss: 1.5799\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tAll Loss: 1.9204\tTriple Loss(1): 0.3578\tClassification Loss: 1.2049\n",
      "Train Epoch: 3 [50240/110534 (45%)]\tAll Loss: 1.9564\tTriple Loss(1): 0.3390\tClassification Loss: 1.2785\n",
      "Train Epoch: 3 [50560/110534 (46%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.3028\tClassification Loss: 1.7781\n",
      "Train Epoch: 3 [50880/110534 (46%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.3235\tClassification Loss: 1.6824\n",
      "\n",
      "Test set: Average loss: 1.6281, Accuracy: 560/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [51200/110534 (46%)]\tAll Loss: 1.4515\tTriple Loss(0): 0.0000\tClassification Loss: 1.4515\n",
      "Train Epoch: 3 [51520/110534 (47%)]\tAll Loss: 2.0303\tTriple Loss(1): 0.3244\tClassification Loss: 1.3816\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tAll Loss: 1.9317\tTriple Loss(1): 0.1772\tClassification Loss: 1.5773\n",
      "Train Epoch: 3 [52160/110534 (47%)]\tAll Loss: 2.1229\tTriple Loss(1): 0.2072\tClassification Loss: 1.7084\n",
      "Train Epoch: 3 [52480/110534 (47%)]\tAll Loss: 2.8164\tTriple Loss(1): 0.2314\tClassification Loss: 2.3536\n",
      "Train Epoch: 3 [52800/110534 (48%)]\tAll Loss: 1.3932\tTriple Loss(0): 0.0000\tClassification Loss: 1.3932\n",
      "Train Epoch: 3 [53120/110534 (48%)]\tAll Loss: 2.0384\tTriple Loss(1): 0.2116\tClassification Loss: 1.6152\n",
      "Train Epoch: 3 [53440/110534 (48%)]\tAll Loss: 2.2906\tTriple Loss(1): 0.2513\tClassification Loss: 1.7880\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tAll Loss: 1.9553\tTriple Loss(1): 0.1013\tClassification Loss: 1.7526\n",
      "Train Epoch: 3 [54080/110534 (49%)]\tAll Loss: 2.4975\tTriple Loss(1): 0.2097\tClassification Loss: 2.0781\n",
      "\n",
      "Test set: Average loss: 1.6343, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [54400/110534 (49%)]\tAll Loss: 2.2396\tTriple Loss(1): 0.1844\tClassification Loss: 1.8708\n",
      "Train Epoch: 3 [54720/110534 (49%)]\tAll Loss: 2.0620\tTriple Loss(1): 0.1597\tClassification Loss: 1.7426\n",
      "Train Epoch: 3 [55040/110534 (50%)]\tAll Loss: 2.2182\tTriple Loss(1): 0.1242\tClassification Loss: 1.9698\n",
      "Train Epoch: 3 [55360/110534 (50%)]\tAll Loss: 2.2291\tTriple Loss(1): 0.3018\tClassification Loss: 1.6255\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tAll Loss: 2.3113\tTriple Loss(1): 0.2009\tClassification Loss: 1.9096\n",
      "Train Epoch: 3 [56000/110534 (51%)]\tAll Loss: 2.5859\tTriple Loss(1): 0.2457\tClassification Loss: 2.0945\n",
      "Train Epoch: 3 [56320/110534 (51%)]\tAll Loss: 2.3593\tTriple Loss(1): 0.3034\tClassification Loss: 1.7525\n",
      "Train Epoch: 3 [56640/110534 (51%)]\tAll Loss: 1.6820\tTriple Loss(1): 0.2259\tClassification Loss: 1.2303\n",
      "Train Epoch: 3 [56960/110534 (52%)]\tAll Loss: 1.5436\tTriple Loss(0): 0.0000\tClassification Loss: 1.5436\n",
      "Train Epoch: 3 [57280/110534 (52%)]\tAll Loss: 2.0458\tTriple Loss(1): 0.0676\tClassification Loss: 1.9107\n",
      "\n",
      "Test set: Average loss: 1.6301, Accuracy: 555/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tAll Loss: 2.2502\tTriple Loss(1): 0.2137\tClassification Loss: 1.8229\n",
      "Train Epoch: 3 [57920/110534 (52%)]\tAll Loss: 1.7763\tTriple Loss(1): 0.0711\tClassification Loss: 1.6341\n",
      "Train Epoch: 3 [58240/110534 (53%)]\tAll Loss: 2.2550\tTriple Loss(1): 0.4140\tClassification Loss: 1.4270\n",
      "Train Epoch: 3 [58560/110534 (53%)]\tAll Loss: 2.1723\tTriple Loss(1): 0.3453\tClassification Loss: 1.4817\n",
      "Train Epoch: 3 [58880/110534 (53%)]\tAll Loss: 1.6535\tTriple Loss(0): 0.0000\tClassification Loss: 1.6535\n",
      "Train Epoch: 3 [59200/110534 (54%)]\tAll Loss: 2.2643\tTriple Loss(1): 0.1647\tClassification Loss: 1.9349\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tAll Loss: 2.0145\tTriple Loss(0): 0.2686\tClassification Loss: 1.4773\n",
      "Train Epoch: 3 [59840/110534 (54%)]\tAll Loss: 2.2136\tTriple Loss(1): 0.3250\tClassification Loss: 1.5636\n",
      "Train Epoch: 3 [60160/110534 (54%)]\tAll Loss: 2.4810\tTriple Loss(0): 0.5369\tClassification Loss: 1.4072\n",
      "Train Epoch: 3 [60480/110534 (55%)]\tAll Loss: 1.6357\tTriple Loss(0): 0.0000\tClassification Loss: 1.6357\n",
      "\n",
      "Test set: Average loss: 1.6252, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [60800/110534 (55%)]\tAll Loss: 2.0973\tTriple Loss(1): 0.1624\tClassification Loss: 1.7725\n",
      "Train Epoch: 3 [61120/110534 (55%)]\tAll Loss: 2.3301\tTriple Loss(1): 0.2503\tClassification Loss: 1.8295\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tAll Loss: 1.4669\tTriple Loss(1): 0.1950\tClassification Loss: 1.0768\n",
      "Train Epoch: 3 [61760/110534 (56%)]\tAll Loss: 2.2899\tTriple Loss(1): 0.2767\tClassification Loss: 1.7364\n",
      "Train Epoch: 3 [62080/110534 (56%)]\tAll Loss: 2.1694\tTriple Loss(1): 0.3168\tClassification Loss: 1.5358\n",
      "Train Epoch: 3 [62400/110534 (56%)]\tAll Loss: 2.1228\tTriple Loss(1): 0.1726\tClassification Loss: 1.7776\n",
      "Train Epoch: 3 [62720/110534 (57%)]\tAll Loss: 2.0449\tTriple Loss(1): 0.2825\tClassification Loss: 1.4800\n",
      "Train Epoch: 3 [63040/110534 (57%)]\tAll Loss: 2.3214\tTriple Loss(1): 0.2305\tClassification Loss: 1.8605\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tAll Loss: 1.5930\tTriple Loss(0): 0.0000\tClassification Loss: 1.5930\n",
      "Train Epoch: 3 [63680/110534 (58%)]\tAll Loss: 1.6754\tTriple Loss(1): 0.0742\tClassification Loss: 1.5271\n",
      "\n",
      "Test set: Average loss: 1.6292, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [64000/110534 (58%)]\tAll Loss: 3.0538\tTriple Loss(1): 0.3916\tClassification Loss: 2.2706\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2000.pth.tar\n",
      "Train Epoch: 3 [64320/110534 (58%)]\tAll Loss: 1.7263\tTriple Loss(1): 0.2333\tClassification Loss: 1.2598\n",
      "Train Epoch: 3 [64640/110534 (58%)]\tAll Loss: 3.1154\tTriple Loss(1): 0.3322\tClassification Loss: 2.4511\n",
      "Train Epoch: 3 [64960/110534 (59%)]\tAll Loss: 2.0358\tTriple Loss(1): 0.2198\tClassification Loss: 1.5961\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tAll Loss: 2.3008\tTriple Loss(1): 0.2593\tClassification Loss: 1.7821\n",
      "Train Epoch: 3 [65600/110534 (59%)]\tAll Loss: 1.9665\tTriple Loss(1): 0.1795\tClassification Loss: 1.6074\n",
      "Train Epoch: 3 [65920/110534 (60%)]\tAll Loss: 2.3238\tTriple Loss(1): 0.3190\tClassification Loss: 1.6857\n",
      "Train Epoch: 3 [66240/110534 (60%)]\tAll Loss: 1.5704\tTriple Loss(0): 0.0000\tClassification Loss: 1.5704\n",
      "Train Epoch: 3 [66560/110534 (60%)]\tAll Loss: 1.5376\tTriple Loss(1): 0.1062\tClassification Loss: 1.3253\n",
      "Train Epoch: 3 [66880/110534 (60%)]\tAll Loss: 2.0282\tTriple Loss(1): 0.1186\tClassification Loss: 1.7910\n",
      "\n",
      "Test set: Average loss: 1.6257, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tAll Loss: 2.1903\tTriple Loss(1): 0.2226\tClassification Loss: 1.7452\n",
      "Train Epoch: 3 [67520/110534 (61%)]\tAll Loss: 2.4715\tTriple Loss(1): 0.2599\tClassification Loss: 1.9517\n",
      "Train Epoch: 3 [67840/110534 (61%)]\tAll Loss: 1.9117\tTriple Loss(0): 0.0000\tClassification Loss: 1.9117\n",
      "Train Epoch: 3 [68160/110534 (62%)]\tAll Loss: 1.5613\tTriple Loss(1): 0.0740\tClassification Loss: 1.4133\n",
      "Train Epoch: 3 [68480/110534 (62%)]\tAll Loss: 1.8814\tTriple Loss(1): 0.2353\tClassification Loss: 1.4107\n",
      "Train Epoch: 3 [68800/110534 (62%)]\tAll Loss: 2.3777\tTriple Loss(0): 0.5468\tClassification Loss: 1.2841\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tAll Loss: 11.8949\tTriple Loss(0): 5.1917\tClassification Loss: 1.5115\n",
      "Train Epoch: 3 [69440/110534 (63%)]\tAll Loss: 2.0683\tTriple Loss(1): 0.2568\tClassification Loss: 1.5547\n",
      "Train Epoch: 3 [69760/110534 (63%)]\tAll Loss: 2.0952\tTriple Loss(1): 0.1475\tClassification Loss: 1.8003\n",
      "Train Epoch: 3 [70080/110534 (63%)]\tAll Loss: 1.5824\tTriple Loss(0): 0.0000\tClassification Loss: 1.5824\n",
      "\n",
      "Test set: Average loss: 1.6151, Accuracy: 560/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [70400/110534 (64%)]\tAll Loss: 1.7463\tTriple Loss(1): 0.1076\tClassification Loss: 1.5311\n",
      "Train Epoch: 3 [70720/110534 (64%)]\tAll Loss: 1.3627\tTriple Loss(0): 0.0000\tClassification Loss: 1.3627\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.1860\tClassification Loss: 1.5414\n",
      "Train Epoch: 3 [71360/110534 (65%)]\tAll Loss: 2.1963\tTriple Loss(1): 0.2752\tClassification Loss: 1.6460\n",
      "Train Epoch: 3 [71680/110534 (65%)]\tAll Loss: 1.8167\tTriple Loss(1): 0.1437\tClassification Loss: 1.5293\n",
      "Train Epoch: 3 [72000/110534 (65%)]\tAll Loss: 2.1869\tTriple Loss(1): 0.2321\tClassification Loss: 1.7227\n",
      "Train Epoch: 3 [72320/110534 (65%)]\tAll Loss: 1.4786\tTriple Loss(0): 0.0000\tClassification Loss: 1.4786\n",
      "Train Epoch: 3 [72640/110534 (66%)]\tAll Loss: 1.8234\tTriple Loss(1): 0.2448\tClassification Loss: 1.3338\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tAll Loss: 1.6790\tTriple Loss(1): 0.1109\tClassification Loss: 1.4571\n",
      "Train Epoch: 3 [73280/110534 (66%)]\tAll Loss: 1.6978\tTriple Loss(1): 0.2081\tClassification Loss: 1.2816\n",
      "\n",
      "Test set: Average loss: 1.6185, Accuracy: 550/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [73600/110534 (67%)]\tAll Loss: 2.8201\tTriple Loss(1): 0.2554\tClassification Loss: 2.3093\n",
      "Train Epoch: 3 [73920/110534 (67%)]\tAll Loss: 2.2153\tTriple Loss(1): 0.1019\tClassification Loss: 2.0114\n",
      "Train Epoch: 3 [74240/110534 (67%)]\tAll Loss: 1.3148\tTriple Loss(0): 0.0000\tClassification Loss: 1.3148\n",
      "Train Epoch: 3 [74560/110534 (67%)]\tAll Loss: 2.0693\tTriple Loss(1): 0.2190\tClassification Loss: 1.6312\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tAll Loss: 1.8192\tTriple Loss(0): 0.0000\tClassification Loss: 1.8192\n",
      "Train Epoch: 3 [75200/110534 (68%)]\tAll Loss: 1.9503\tTriple Loss(1): 0.2487\tClassification Loss: 1.4530\n",
      "Train Epoch: 3 [75520/110534 (68%)]\tAll Loss: 1.8273\tTriple Loss(1): 0.1392\tClassification Loss: 1.5489\n",
      "Train Epoch: 3 [75840/110534 (69%)]\tAll Loss: 1.6608\tTriple Loss(0): 0.0000\tClassification Loss: 1.6608\n",
      "Train Epoch: 3 [76160/110534 (69%)]\tAll Loss: 1.8226\tTriple Loss(1): 0.1005\tClassification Loss: 1.6215\n",
      "Train Epoch: 3 [76480/110534 (69%)]\tAll Loss: 2.0707\tTriple Loss(1): 0.3536\tClassification Loss: 1.3635\n",
      "\n",
      "Test set: Average loss: 1.6147, Accuracy: 554/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tAll Loss: 2.1622\tTriple Loss(1): 0.3161\tClassification Loss: 1.5301\n",
      "Train Epoch: 3 [77120/110534 (70%)]\tAll Loss: 1.9641\tTriple Loss(1): 0.1861\tClassification Loss: 1.5919\n",
      "Train Epoch: 3 [77440/110534 (70%)]\tAll Loss: 2.0775\tTriple Loss(1): 0.1514\tClassification Loss: 1.7746\n",
      "Train Epoch: 3 [77760/110534 (70%)]\tAll Loss: 1.8946\tTriple Loss(1): 0.2338\tClassification Loss: 1.4271\n",
      "Train Epoch: 3 [78080/110534 (71%)]\tAll Loss: 2.7158\tTriple Loss(1): 0.3252\tClassification Loss: 2.0653\n",
      "Train Epoch: 3 [78400/110534 (71%)]\tAll Loss: 1.8315\tTriple Loss(1): 0.1976\tClassification Loss: 1.4363\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tAll Loss: 1.2295\tTriple Loss(0): 0.0000\tClassification Loss: 1.2295\n",
      "Train Epoch: 3 [79040/110534 (71%)]\tAll Loss: 2.2809\tTriple Loss(1): 0.2479\tClassification Loss: 1.7852\n",
      "Train Epoch: 3 [79360/110534 (72%)]\tAll Loss: 1.9516\tTriple Loss(0): 0.0000\tClassification Loss: 1.9516\n",
      "Train Epoch: 3 [79680/110534 (72%)]\tAll Loss: 1.5644\tTriple Loss(1): 0.0594\tClassification Loss: 1.4456\n",
      "\n",
      "Test set: Average loss: 1.6144, Accuracy: 547/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [80000/110534 (72%)]\tAll Loss: 2.0505\tTriple Loss(1): 0.2476\tClassification Loss: 1.5553\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2500.pth.tar\n",
      "Train Epoch: 3 [80320/110534 (73%)]\tAll Loss: 1.1478\tTriple Loss(0): 0.0000\tClassification Loss: 1.1478\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tAll Loss: 1.7865\tTriple Loss(1): 0.1950\tClassification Loss: 1.3965\n",
      "Train Epoch: 3 [80960/110534 (73%)]\tAll Loss: 2.0770\tTriple Loss(1): 0.2439\tClassification Loss: 1.5893\n",
      "Train Epoch: 3 [81280/110534 (74%)]\tAll Loss: 1.6787\tTriple Loss(1): 0.1545\tClassification Loss: 1.3697\n",
      "Train Epoch: 3 [81600/110534 (74%)]\tAll Loss: 1.9507\tTriple Loss(1): 0.2588\tClassification Loss: 1.4331\n",
      "Train Epoch: 3 [81920/110534 (74%)]\tAll Loss: 2.1382\tTriple Loss(1): 0.1786\tClassification Loss: 1.7810\n",
      "Train Epoch: 3 [82240/110534 (74%)]\tAll Loss: 1.9644\tTriple Loss(1): 0.1600\tClassification Loss: 1.6444\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tAll Loss: 1.9499\tTriple Loss(1): 0.1929\tClassification Loss: 1.5642\n",
      "Train Epoch: 3 [82880/110534 (75%)]\tAll Loss: 2.2308\tTriple Loss(1): 0.1992\tClassification Loss: 1.8324\n",
      "\n",
      "Test set: Average loss: 1.6172, Accuracy: 550/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [83200/110534 (75%)]\tAll Loss: 2.1292\tTriple Loss(1): 0.2177\tClassification Loss: 1.6938\n",
      "Train Epoch: 3 [83520/110534 (76%)]\tAll Loss: 1.4743\tTriple Loss(0): 0.0000\tClassification Loss: 1.4743\n",
      "Train Epoch: 3 [83840/110534 (76%)]\tAll Loss: 2.4492\tTriple Loss(1): 0.4035\tClassification Loss: 1.6422\n",
      "Train Epoch: 3 [84160/110534 (76%)]\tAll Loss: 1.7572\tTriple Loss(1): 0.2667\tClassification Loss: 1.2238\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tAll Loss: 1.4341\tTriple Loss(0): 0.0000\tClassification Loss: 1.4341\n",
      "Train Epoch: 3 [84800/110534 (77%)]\tAll Loss: 2.0831\tTriple Loss(1): 0.1803\tClassification Loss: 1.7226\n",
      "Train Epoch: 3 [85120/110534 (77%)]\tAll Loss: 2.1895\tTriple Loss(1): 0.4047\tClassification Loss: 1.3800\n",
      "Train Epoch: 3 [85440/110534 (77%)]\tAll Loss: 1.1450\tTriple Loss(0): 0.0000\tClassification Loss: 1.1450\n",
      "Train Epoch: 3 [85760/110534 (78%)]\tAll Loss: 2.0557\tTriple Loss(1): 0.2168\tClassification Loss: 1.6220\n",
      "Train Epoch: 3 [86080/110534 (78%)]\tAll Loss: 1.7505\tTriple Loss(0): 0.0000\tClassification Loss: 1.7505\n",
      "\n",
      "Test set: Average loss: 1.6118, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tAll Loss: 2.3783\tTriple Loss(1): 0.1696\tClassification Loss: 2.0392\n",
      "Train Epoch: 3 [86720/110534 (78%)]\tAll Loss: 1.2384\tTriple Loss(0): 0.0000\tClassification Loss: 1.2384\n",
      "Train Epoch: 3 [87040/110534 (79%)]\tAll Loss: 1.5438\tTriple Loss(0): 0.0000\tClassification Loss: 1.5438\n",
      "Train Epoch: 3 [87360/110534 (79%)]\tAll Loss: 2.3095\tTriple Loss(1): 0.2727\tClassification Loss: 1.7641\n",
      "Train Epoch: 3 [87680/110534 (79%)]\tAll Loss: 1.9177\tTriple Loss(1): 0.2096\tClassification Loss: 1.4984\n",
      "Train Epoch: 3 [88000/110534 (80%)]\tAll Loss: 2.0831\tTriple Loss(0): 0.0000\tClassification Loss: 2.0831\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tAll Loss: 1.8857\tTriple Loss(1): 0.1446\tClassification Loss: 1.5965\n",
      "Train Epoch: 3 [88640/110534 (80%)]\tAll Loss: 1.9323\tTriple Loss(0): 0.0000\tClassification Loss: 1.9323\n",
      "Train Epoch: 3 [88960/110534 (80%)]\tAll Loss: 2.5407\tTriple Loss(1): 0.3541\tClassification Loss: 1.8324\n",
      "Train Epoch: 3 [89280/110534 (81%)]\tAll Loss: 1.5860\tTriple Loss(1): 0.0843\tClassification Loss: 1.4173\n",
      "\n",
      "Test set: Average loss: 1.6076, Accuracy: 555/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [89600/110534 (81%)]\tAll Loss: 2.0985\tTriple Loss(0): 0.0000\tClassification Loss: 2.0985\n",
      "Train Epoch: 3 [89920/110534 (81%)]\tAll Loss: 2.3212\tTriple Loss(1): 0.2344\tClassification Loss: 1.8523\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tAll Loss: 1.6913\tTriple Loss(1): 0.1502\tClassification Loss: 1.3908\n",
      "Train Epoch: 3 [90560/110534 (82%)]\tAll Loss: 1.7004\tTriple Loss(1): 0.1354\tClassification Loss: 1.4296\n",
      "Train Epoch: 3 [90880/110534 (82%)]\tAll Loss: 2.5980\tTriple Loss(1): 0.4080\tClassification Loss: 1.7819\n",
      "Train Epoch: 3 [91200/110534 (82%)]\tAll Loss: 2.1935\tTriple Loss(1): 0.3464\tClassification Loss: 1.5006\n",
      "Train Epoch: 3 [91520/110534 (83%)]\tAll Loss: 2.7347\tTriple Loss(1): 0.5830\tClassification Loss: 1.5688\n",
      "Train Epoch: 3 [91840/110534 (83%)]\tAll Loss: 2.1010\tTriple Loss(1): 0.2340\tClassification Loss: 1.6331\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tAll Loss: 2.0085\tTriple Loss(0): 0.0000\tClassification Loss: 2.0085\n",
      "Train Epoch: 3 [92480/110534 (84%)]\tAll Loss: 1.9461\tTriple Loss(1): 0.1932\tClassification Loss: 1.5598\n",
      "\n",
      "Test set: Average loss: 1.6155, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [92800/110534 (84%)]\tAll Loss: 1.5008\tTriple Loss(0): 0.0000\tClassification Loss: 1.5008\n",
      "Train Epoch: 3 [93120/110534 (84%)]\tAll Loss: 2.3300\tTriple Loss(1): 0.2869\tClassification Loss: 1.7563\n",
      "Train Epoch: 3 [93440/110534 (85%)]\tAll Loss: 2.2402\tTriple Loss(1): 0.3373\tClassification Loss: 1.5655\n",
      "Train Epoch: 3 [93760/110534 (85%)]\tAll Loss: 1.4289\tTriple Loss(0): 0.0000\tClassification Loss: 1.4289\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tAll Loss: 1.9068\tTriple Loss(1): 0.1801\tClassification Loss: 1.5466\n",
      "Train Epoch: 3 [94400/110534 (85%)]\tAll Loss: 2.2672\tTriple Loss(1): 0.2430\tClassification Loss: 1.7813\n",
      "Train Epoch: 3 [94720/110534 (86%)]\tAll Loss: 1.9643\tTriple Loss(1): 0.1864\tClassification Loss: 1.5915\n",
      "Train Epoch: 3 [95040/110534 (86%)]\tAll Loss: 2.0011\tTriple Loss(1): 0.1701\tClassification Loss: 1.6609\n",
      "Train Epoch: 3 [95360/110534 (86%)]\tAll Loss: 2.4574\tTriple Loss(1): 0.4822\tClassification Loss: 1.4931\n",
      "Train Epoch: 3 [95680/110534 (87%)]\tAll Loss: 2.1804\tTriple Loss(1): 0.2329\tClassification Loss: 1.7146\n",
      "\n",
      "Test set: Average loss: 1.6164, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tAll Loss: 1.6168\tTriple Loss(1): 0.1377\tClassification Loss: 1.3414\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_3000.pth.tar\n",
      "Train Epoch: 3 [96320/110534 (87%)]\tAll Loss: 1.4745\tTriple Loss(0): 0.0000\tClassification Loss: 1.4745\n",
      "Train Epoch: 3 [96640/110534 (87%)]\tAll Loss: 1.8507\tTriple Loss(1): 0.0667\tClassification Loss: 1.7172\n",
      "Train Epoch: 3 [96960/110534 (88%)]\tAll Loss: 1.8136\tTriple Loss(1): 0.2362\tClassification Loss: 1.3411\n",
      "Train Epoch: 3 [97280/110534 (88%)]\tAll Loss: 1.6458\tTriple Loss(1): 0.1205\tClassification Loss: 1.4048\n",
      "Train Epoch: 3 [97600/110534 (88%)]\tAll Loss: 1.7890\tTriple Loss(0): 0.0763\tClassification Loss: 1.6364\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tAll Loss: 1.2719\tTriple Loss(0): 0.0000\tClassification Loss: 1.2719\n",
      "Train Epoch: 3 [98240/110534 (89%)]\tAll Loss: 1.4392\tTriple Loss(1): 0.0829\tClassification Loss: 1.2735\n",
      "Train Epoch: 3 [98560/110534 (89%)]\tAll Loss: 1.3552\tTriple Loss(0): 0.0000\tClassification Loss: 1.3552\n",
      "Train Epoch: 3 [98880/110534 (89%)]\tAll Loss: 2.1221\tTriple Loss(1): 0.3400\tClassification Loss: 1.4421\n",
      "\n",
      "Test set: Average loss: 1.6239, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [99200/110534 (90%)]\tAll Loss: 2.6382\tTriple Loss(1): 0.5282\tClassification Loss: 1.5817\n",
      "Train Epoch: 3 [99520/110534 (90%)]\tAll Loss: 1.8999\tTriple Loss(1): 0.1222\tClassification Loss: 1.6554\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.2075\tClassification Loss: 1.6221\n",
      "Train Epoch: 3 [100160/110534 (91%)]\tAll Loss: 2.5638\tTriple Loss(1): 0.3251\tClassification Loss: 1.9136\n",
      "Train Epoch: 3 [100480/110534 (91%)]\tAll Loss: 2.0411\tTriple Loss(1): 0.3208\tClassification Loss: 1.3995\n",
      "Train Epoch: 3 [100800/110534 (91%)]\tAll Loss: 2.6828\tTriple Loss(1): 0.3388\tClassification Loss: 2.0052\n",
      "Train Epoch: 3 [101120/110534 (91%)]\tAll Loss: 1.8462\tTriple Loss(1): 0.1878\tClassification Loss: 1.4707\n",
      "Train Epoch: 3 [101440/110534 (92%)]\tAll Loss: 1.6994\tTriple Loss(1): 0.1926\tClassification Loss: 1.3142\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tAll Loss: 2.0764\tTriple Loss(1): 0.1943\tClassification Loss: 1.6878\n",
      "Train Epoch: 3 [102080/110534 (92%)]\tAll Loss: 2.6985\tTriple Loss(1): 0.3695\tClassification Loss: 1.9595\n",
      "\n",
      "Test set: Average loss: 1.6157, Accuracy: 542/960 (56%)\n",
      "\n",
      "Train Epoch: 3 [102400/110534 (93%)]\tAll Loss: 1.9388\tTriple Loss(1): 0.0979\tClassification Loss: 1.7431\n",
      "Train Epoch: 3 [102720/110534 (93%)]\tAll Loss: 2.2306\tTriple Loss(1): 0.2005\tClassification Loss: 1.8295\n",
      "Train Epoch: 3 [103040/110534 (93%)]\tAll Loss: 2.1076\tTriple Loss(1): 0.2692\tClassification Loss: 1.5693\n",
      "Train Epoch: 3 [103360/110534 (93%)]\tAll Loss: 1.9942\tTriple Loss(1): 0.1480\tClassification Loss: 1.6983\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tAll Loss: 2.4466\tTriple Loss(1): 0.3272\tClassification Loss: 1.7922\n",
      "Train Epoch: 3 [104000/110534 (94%)]\tAll Loss: 2.1344\tTriple Loss(1): 0.3220\tClassification Loss: 1.4903\n",
      "Train Epoch: 3 [104320/110534 (94%)]\tAll Loss: 2.1739\tTriple Loss(0): 0.0000\tClassification Loss: 2.1739\n",
      "Train Epoch: 3 [104640/110534 (95%)]\tAll Loss: 1.9766\tTriple Loss(0): 0.0000\tClassification Loss: 1.9766\n",
      "Train Epoch: 3 [104960/110534 (95%)]\tAll Loss: 1.9880\tTriple Loss(1): 0.2906\tClassification Loss: 1.4069\n",
      "Train Epoch: 3 [105280/110534 (95%)]\tAll Loss: 2.5649\tTriple Loss(1): 0.4854\tClassification Loss: 1.5940\n",
      "\n",
      "Test set: Average loss: 1.6166, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tAll Loss: 2.0278\tTriple Loss(1): 0.1926\tClassification Loss: 1.6426\n",
      "Train Epoch: 3 [105920/110534 (96%)]\tAll Loss: 2.0284\tTriple Loss(1): 0.1864\tClassification Loss: 1.6557\n",
      "Train Epoch: 3 [106240/110534 (96%)]\tAll Loss: 1.5826\tTriple Loss(1): 0.1594\tClassification Loss: 1.2639\n",
      "Train Epoch: 3 [106560/110534 (96%)]\tAll Loss: 2.1635\tTriple Loss(1): 0.2631\tClassification Loss: 1.6373\n",
      "Train Epoch: 3 [106880/110534 (97%)]\tAll Loss: 2.3258\tTriple Loss(1): 0.1864\tClassification Loss: 1.9529\n",
      "Train Epoch: 3 [107200/110534 (97%)]\tAll Loss: 1.7433\tTriple Loss(0): 0.0000\tClassification Loss: 1.7433\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tAll Loss: 1.7301\tTriple Loss(0): 0.0000\tClassification Loss: 1.7301\n",
      "Train Epoch: 3 [107840/110534 (98%)]\tAll Loss: 1.4623\tTriple Loss(0): 0.0000\tClassification Loss: 1.4623\n",
      "Train Epoch: 3 [108160/110534 (98%)]\tAll Loss: 1.7753\tTriple Loss(1): 0.2587\tClassification Loss: 1.2580\n",
      "Train Epoch: 3 [108480/110534 (98%)]\tAll Loss: 2.4207\tTriple Loss(1): 0.4230\tClassification Loss: 1.5748\n",
      "\n",
      "Test set: Average loss: 1.6027, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 3 [108800/110534 (98%)]\tAll Loss: 1.6664\tTriple Loss(1): 0.1057\tClassification Loss: 1.4551\n",
      "Train Epoch: 3 [109120/110534 (99%)]\tAll Loss: 2.1947\tTriple Loss(1): 0.1654\tClassification Loss: 1.8640\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tAll Loss: 1.6944\tTriple Loss(1): 0.1878\tClassification Loss: 1.3188\n",
      "Train Epoch: 3 [109760/110534 (99%)]\tAll Loss: 1.9214\tTriple Loss(1): 0.1521\tClassification Loss: 1.6173\n",
      "Train Epoch: 3 [110080/110534 (100%)]\tAll Loss: 1.8071\tTriple Loss(1): 0.1050\tClassification Loss: 1.5970\n",
      "Train Epoch: 3 [110400/110534 (100%)]\tAll Loss: 2.4242\tTriple Loss(1): 0.3726\tClassification Loss: 1.6790\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_final.pth.tar\n",
      "\n",
      "Test set: Average loss: 1.6079, Accuracy: 550/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [0/110534 (0%)]\tAll Loss: 1.6143\tTriple Loss(0): 0.0000\tClassification Loss: 1.6143\n",
      "Train Epoch: 4 [320/110534 (0%)]\tAll Loss: 1.5282\tTriple Loss(0): 0.0000\tClassification Loss: 1.5282\n",
      "Train Epoch: 4 [640/110534 (1%)]\tAll Loss: 1.4718\tTriple Loss(1): 0.2255\tClassification Loss: 1.0208\n",
      "Train Epoch: 4 [960/110534 (1%)]\tAll Loss: 2.0784\tTriple Loss(0): 0.0000\tClassification Loss: 2.0784\n",
      "Train Epoch: 4 [1280/110534 (1%)]\tAll Loss: 2.0765\tTriple Loss(1): 0.2275\tClassification Loss: 1.6215\n",
      "Train Epoch: 4 [1600/110534 (1%)]\tAll Loss: 2.2020\tTriple Loss(1): 0.1992\tClassification Loss: 1.8037\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tAll Loss: 2.2470\tTriple Loss(1): 0.1846\tClassification Loss: 1.8777\n",
      "Train Epoch: 4 [2240/110534 (2%)]\tAll Loss: 2.0864\tTriple Loss(1): 0.2609\tClassification Loss: 1.5647\n",
      "Train Epoch: 4 [2560/110534 (2%)]\tAll Loss: 1.7710\tTriple Loss(1): 0.0825\tClassification Loss: 1.6060\n",
      "Train Epoch: 4 [2880/110534 (3%)]\tAll Loss: 2.4915\tTriple Loss(1): 0.3874\tClassification Loss: 1.7167\n",
      "\n",
      "Test set: Average loss: 1.6108, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [3200/110534 (3%)]\tAll Loss: 2.2865\tTriple Loss(1): 0.1099\tClassification Loss: 2.0667\n",
      "Train Epoch: 4 [3520/110534 (3%)]\tAll Loss: 1.6985\tTriple Loss(1): 0.2248\tClassification Loss: 1.2490\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tAll Loss: 1.9458\tTriple Loss(1): 0.1119\tClassification Loss: 1.7220\n",
      "Train Epoch: 4 [4160/110534 (4%)]\tAll Loss: 1.9680\tTriple Loss(1): 0.1429\tClassification Loss: 1.6822\n",
      "Train Epoch: 4 [4480/110534 (4%)]\tAll Loss: 1.9162\tTriple Loss(1): 0.2272\tClassification Loss: 1.4618\n",
      "Train Epoch: 4 [4800/110534 (4%)]\tAll Loss: 1.9294\tTriple Loss(1): 0.2557\tClassification Loss: 1.4180\n",
      "Train Epoch: 4 [5120/110534 (5%)]\tAll Loss: 2.0178\tTriple Loss(1): 0.0698\tClassification Loss: 1.8782\n",
      "Train Epoch: 4 [5440/110534 (5%)]\tAll Loss: 2.5363\tTriple Loss(1): 0.5070\tClassification Loss: 1.5224\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tAll Loss: 1.7934\tTriple Loss(1): 0.2674\tClassification Loss: 1.2586\n",
      "Train Epoch: 4 [6080/110534 (5%)]\tAll Loss: 2.6046\tTriple Loss(1): 0.4923\tClassification Loss: 1.6200\n",
      "\n",
      "Test set: Average loss: 1.6040, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [6400/110534 (6%)]\tAll Loss: 2.2226\tTriple Loss(1): 0.3075\tClassification Loss: 1.6076\n",
      "Train Epoch: 4 [6720/110534 (6%)]\tAll Loss: 2.1916\tTriple Loss(1): 0.2620\tClassification Loss: 1.6677\n",
      "Train Epoch: 4 [7040/110534 (6%)]\tAll Loss: 2.1920\tTriple Loss(1): 0.2000\tClassification Loss: 1.7921\n",
      "Train Epoch: 4 [7360/110534 (7%)]\tAll Loss: 1.8703\tTriple Loss(1): 0.2057\tClassification Loss: 1.4590\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tAll Loss: 1.3664\tTriple Loss(1): 0.0864\tClassification Loss: 1.1935\n",
      "Train Epoch: 4 [8000/110534 (7%)]\tAll Loss: 1.9324\tTriple Loss(1): 0.3025\tClassification Loss: 1.3275\n",
      "Train Epoch: 4 [8320/110534 (8%)]\tAll Loss: 1.5880\tTriple Loss(1): 0.0443\tClassification Loss: 1.4994\n",
      "Train Epoch: 4 [8640/110534 (8%)]\tAll Loss: 1.2107\tTriple Loss(0): 0.0000\tClassification Loss: 1.2107\n",
      "Train Epoch: 4 [8960/110534 (8%)]\tAll Loss: 1.2963\tTriple Loss(0): 0.0000\tClassification Loss: 1.2963\n",
      "Train Epoch: 4 [9280/110534 (8%)]\tAll Loss: 1.3921\tTriple Loss(1): 0.0349\tClassification Loss: 1.3223\n",
      "\n",
      "Test set: Average loss: 1.6016, Accuracy: 554/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tAll Loss: 1.9484\tTriple Loss(1): 0.0618\tClassification Loss: 1.8248\n",
      "Train Epoch: 4 [9920/110534 (9%)]\tAll Loss: 1.6882\tTriple Loss(0): 0.0000\tClassification Loss: 1.6882\n",
      "Train Epoch: 4 [10240/110534 (9%)]\tAll Loss: 2.3149\tTriple Loss(1): 0.2613\tClassification Loss: 1.7924\n",
      "Train Epoch: 4 [10560/110534 (10%)]\tAll Loss: 2.0079\tTriple Loss(1): 0.1805\tClassification Loss: 1.6469\n",
      "Train Epoch: 4 [10880/110534 (10%)]\tAll Loss: 2.1541\tTriple Loss(1): 0.1742\tClassification Loss: 1.8058\n",
      "Train Epoch: 4 [11200/110534 (10%)]\tAll Loss: 1.2090\tTriple Loss(0): 0.0000\tClassification Loss: 1.2090\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tAll Loss: 1.6949\tTriple Loss(1): 0.1355\tClassification Loss: 1.4240\n",
      "Train Epoch: 4 [11840/110534 (11%)]\tAll Loss: 2.2563\tTriple Loss(1): 0.2764\tClassification Loss: 1.7035\n",
      "Train Epoch: 4 [12160/110534 (11%)]\tAll Loss: 1.9195\tTriple Loss(1): 0.1727\tClassification Loss: 1.5741\n",
      "Train Epoch: 4 [12480/110534 (11%)]\tAll Loss: 1.7369\tTriple Loss(0): 0.0000\tClassification Loss: 1.7369\n",
      "\n",
      "Test set: Average loss: 1.6025, Accuracy: 547/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [12800/110534 (12%)]\tAll Loss: 1.9491\tTriple Loss(1): 0.2992\tClassification Loss: 1.3506\n",
      "Train Epoch: 4 [13120/110534 (12%)]\tAll Loss: 1.6719\tTriple Loss(1): 0.1450\tClassification Loss: 1.3819\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tAll Loss: 2.2059\tTriple Loss(1): 0.1689\tClassification Loss: 1.8681\n",
      "Train Epoch: 4 [13760/110534 (12%)]\tAll Loss: 1.9861\tTriple Loss(1): 0.1073\tClassification Loss: 1.7715\n",
      "Train Epoch: 4 [14080/110534 (13%)]\tAll Loss: 1.8783\tTriple Loss(0): 0.0000\tClassification Loss: 1.8783\n",
      "Train Epoch: 4 [14400/110534 (13%)]\tAll Loss: 2.7916\tTriple Loss(1): 0.3747\tClassification Loss: 2.0423\n",
      "Train Epoch: 4 [14720/110534 (13%)]\tAll Loss: 1.9012\tTriple Loss(1): 0.1365\tClassification Loss: 1.6282\n",
      "Train Epoch: 4 [15040/110534 (14%)]\tAll Loss: 2.0235\tTriple Loss(1): 0.2773\tClassification Loss: 1.4689\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tAll Loss: 2.2557\tTriple Loss(1): 0.3413\tClassification Loss: 1.5732\n",
      "Train Epoch: 4 [15680/110534 (14%)]\tAll Loss: 1.6727\tTriple Loss(1): 0.2108\tClassification Loss: 1.2511\n",
      "\n",
      "Test set: Average loss: 1.6021, Accuracy: 544/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [16000/110534 (14%)]\tAll Loss: 1.9459\tTriple Loss(1): 0.1248\tClassification Loss: 1.6963\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_500.pth.tar\n",
      "Train Epoch: 4 [16320/110534 (15%)]\tAll Loss: 1.2675\tTriple Loss(0): 0.0000\tClassification Loss: 1.2675\n",
      "Train Epoch: 4 [16640/110534 (15%)]\tAll Loss: 1.9627\tTriple Loss(1): 0.1616\tClassification Loss: 1.6395\n",
      "Train Epoch: 4 [16960/110534 (15%)]\tAll Loss: 2.0852\tTriple Loss(1): 0.1504\tClassification Loss: 1.7844\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tAll Loss: 1.7734\tTriple Loss(1): 0.1713\tClassification Loss: 1.4308\n",
      "Train Epoch: 4 [17600/110534 (16%)]\tAll Loss: 2.0336\tTriple Loss(1): 0.1905\tClassification Loss: 1.6527\n",
      "Train Epoch: 4 [17920/110534 (16%)]\tAll Loss: 2.0750\tTriple Loss(1): 0.1942\tClassification Loss: 1.6865\n",
      "Train Epoch: 4 [18240/110534 (16%)]\tAll Loss: 7.0800\tTriple Loss(0): 2.5153\tClassification Loss: 2.0494\n",
      "Train Epoch: 4 [18560/110534 (17%)]\tAll Loss: 1.5806\tTriple Loss(0): 0.0000\tClassification Loss: 1.5806\n",
      "Train Epoch: 4 [18880/110534 (17%)]\tAll Loss: 2.3235\tTriple Loss(1): 0.2706\tClassification Loss: 1.7824\n",
      "\n",
      "Test set: Average loss: 1.6059, Accuracy: 556/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tAll Loss: 1.9615\tTriple Loss(1): 0.2957\tClassification Loss: 1.3700\n",
      "Train Epoch: 4 [19520/110534 (18%)]\tAll Loss: 1.5681\tTriple Loss(0): 0.0000\tClassification Loss: 1.5681\n",
      "Train Epoch: 4 [19840/110534 (18%)]\tAll Loss: 2.7177\tTriple Loss(0): 0.5612\tClassification Loss: 1.5954\n",
      "Train Epoch: 4 [20160/110534 (18%)]\tAll Loss: 1.3343\tTriple Loss(0): 0.0000\tClassification Loss: 1.3343\n",
      "Train Epoch: 4 [20480/110534 (19%)]\tAll Loss: 2.0395\tTriple Loss(1): 0.2844\tClassification Loss: 1.4708\n",
      "Train Epoch: 4 [20800/110534 (19%)]\tAll Loss: 2.4581\tTriple Loss(1): 0.2440\tClassification Loss: 1.9700\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tAll Loss: 2.0580\tTriple Loss(1): 0.2701\tClassification Loss: 1.5178\n",
      "Train Epoch: 4 [21440/110534 (19%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.1790\tClassification Loss: 1.4296\n",
      "Train Epoch: 4 [21760/110534 (20%)]\tAll Loss: 2.6684\tTriple Loss(1): 0.3029\tClassification Loss: 2.0626\n",
      "Train Epoch: 4 [22080/110534 (20%)]\tAll Loss: 1.9059\tTriple Loss(1): 0.1170\tClassification Loss: 1.6719\n",
      "\n",
      "Test set: Average loss: 1.6078, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [22400/110534 (20%)]\tAll Loss: 1.5224\tTriple Loss(0): 0.0000\tClassification Loss: 1.5224\n",
      "Train Epoch: 4 [22720/110534 (21%)]\tAll Loss: 1.9223\tTriple Loss(1): 0.1637\tClassification Loss: 1.5950\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tAll Loss: 2.0793\tTriple Loss(1): 0.1404\tClassification Loss: 1.7986\n",
      "Train Epoch: 4 [23360/110534 (21%)]\tAll Loss: 1.8507\tTriple Loss(1): 0.2032\tClassification Loss: 1.4443\n",
      "Train Epoch: 4 [23680/110534 (21%)]\tAll Loss: 2.2691\tTriple Loss(1): 0.2072\tClassification Loss: 1.8546\n",
      "Train Epoch: 4 [24000/110534 (22%)]\tAll Loss: 1.9754\tTriple Loss(1): 0.1977\tClassification Loss: 1.5801\n",
      "Train Epoch: 4 [24320/110534 (22%)]\tAll Loss: 2.3338\tTriple Loss(1): 0.3313\tClassification Loss: 1.6712\n",
      "Train Epoch: 4 [24640/110534 (22%)]\tAll Loss: 2.4249\tTriple Loss(1): 0.2195\tClassification Loss: 1.9858\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tAll Loss: 1.8664\tTriple Loss(1): 0.2435\tClassification Loss: 1.3795\n",
      "Train Epoch: 4 [25280/110534 (23%)]\tAll Loss: 1.6723\tTriple Loss(1): 0.0686\tClassification Loss: 1.5351\n",
      "\n",
      "Test set: Average loss: 1.6048, Accuracy: 555/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [25600/110534 (23%)]\tAll Loss: 1.5907\tTriple Loss(0): 0.0000\tClassification Loss: 1.5907\n",
      "Train Epoch: 4 [25920/110534 (23%)]\tAll Loss: 2.3169\tTriple Loss(1): 0.2936\tClassification Loss: 1.7297\n",
      "Train Epoch: 4 [26240/110534 (24%)]\tAll Loss: 2.2649\tTriple Loss(1): 0.3589\tClassification Loss: 1.5471\n",
      "Train Epoch: 4 [26560/110534 (24%)]\tAll Loss: 2.1117\tTriple Loss(1): 0.1522\tClassification Loss: 1.8073\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tAll Loss: 2.2916\tTriple Loss(1): 0.2714\tClassification Loss: 1.7487\n",
      "Train Epoch: 4 [27200/110534 (25%)]\tAll Loss: 2.2312\tTriple Loss(1): 0.1760\tClassification Loss: 1.8793\n",
      "Train Epoch: 4 [27520/110534 (25%)]\tAll Loss: 1.7053\tTriple Loss(1): 0.0000\tClassification Loss: 1.7053\n",
      "Train Epoch: 4 [27840/110534 (25%)]\tAll Loss: 1.5078\tTriple Loss(0): 0.0000\tClassification Loss: 1.5078\n",
      "Train Epoch: 4 [28160/110534 (25%)]\tAll Loss: 2.1821\tTriple Loss(1): 0.1941\tClassification Loss: 1.7939\n",
      "Train Epoch: 4 [28480/110534 (26%)]\tAll Loss: 1.9387\tTriple Loss(1): 0.2361\tClassification Loss: 1.4665\n",
      "\n",
      "Test set: Average loss: 1.6056, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tAll Loss: 1.4205\tTriple Loss(0): 0.0000\tClassification Loss: 1.4205\n",
      "Train Epoch: 4 [29120/110534 (26%)]\tAll Loss: 2.0927\tTriple Loss(1): 0.2764\tClassification Loss: 1.5399\n",
      "Train Epoch: 4 [29440/110534 (27%)]\tAll Loss: 1.3972\tTriple Loss(0): 0.0000\tClassification Loss: 1.3972\n",
      "Train Epoch: 4 [29760/110534 (27%)]\tAll Loss: 1.7598\tTriple Loss(1): 0.2205\tClassification Loss: 1.3188\n",
      "Train Epoch: 4 [30080/110534 (27%)]\tAll Loss: 1.7648\tTriple Loss(1): 0.1728\tClassification Loss: 1.4191\n",
      "Train Epoch: 4 [30400/110534 (27%)]\tAll Loss: 2.0758\tTriple Loss(1): 0.1662\tClassification Loss: 1.7433\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tAll Loss: 1.6051\tTriple Loss(1): 0.1708\tClassification Loss: 1.2636\n",
      "Train Epoch: 4 [31040/110534 (28%)]\tAll Loss: 2.6178\tTriple Loss(1): 0.3544\tClassification Loss: 1.9090\n",
      "Train Epoch: 4 [31360/110534 (28%)]\tAll Loss: 1.7841\tTriple Loss(1): 0.0783\tClassification Loss: 1.6275\n",
      "Train Epoch: 4 [31680/110534 (29%)]\tAll Loss: 2.1278\tTriple Loss(1): 0.2509\tClassification Loss: 1.6260\n",
      "\n",
      "Test set: Average loss: 1.5984, Accuracy: 560/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [32000/110534 (29%)]\tAll Loss: 2.3783\tTriple Loss(1): 0.3408\tClassification Loss: 1.6968\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1000.pth.tar\n",
      "Train Epoch: 4 [32320/110534 (29%)]\tAll Loss: 1.5942\tTriple Loss(1): 0.1384\tClassification Loss: 1.3174\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tAll Loss: 1.6929\tTriple Loss(0): 0.0000\tClassification Loss: 1.6929\n",
      "Train Epoch: 4 [32960/110534 (30%)]\tAll Loss: 2.1175\tTriple Loss(1): 0.1803\tClassification Loss: 1.7569\n",
      "Train Epoch: 4 [33280/110534 (30%)]\tAll Loss: 2.5035\tTriple Loss(1): 0.2684\tClassification Loss: 1.9666\n",
      "Train Epoch: 4 [33600/110534 (30%)]\tAll Loss: 1.8624\tTriple Loss(1): 0.1163\tClassification Loss: 1.6297\n",
      "Train Epoch: 4 [33920/110534 (31%)]\tAll Loss: 1.5929\tTriple Loss(0): 0.0000\tClassification Loss: 1.5929\n",
      "Train Epoch: 4 [34240/110534 (31%)]\tAll Loss: 1.9849\tTriple Loss(1): 0.2240\tClassification Loss: 1.5369\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tAll Loss: 2.0687\tTriple Loss(1): 0.2806\tClassification Loss: 1.5074\n",
      "Train Epoch: 4 [34880/110534 (32%)]\tAll Loss: 1.7505\tTriple Loss(1): 0.2626\tClassification Loss: 1.2252\n",
      "\n",
      "Test set: Average loss: 1.6050, Accuracy: 557/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [35200/110534 (32%)]\tAll Loss: 2.4097\tTriple Loss(1): 0.2287\tClassification Loss: 1.9523\n",
      "Train Epoch: 4 [35520/110534 (32%)]\tAll Loss: 2.0083\tTriple Loss(1): 0.1816\tClassification Loss: 1.6451\n",
      "Train Epoch: 4 [35840/110534 (32%)]\tAll Loss: 1.6150\tTriple Loss(1): 0.1367\tClassification Loss: 1.3416\n",
      "Train Epoch: 4 [36160/110534 (33%)]\tAll Loss: 2.1299\tTriple Loss(1): 0.1693\tClassification Loss: 1.7913\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tAll Loss: 2.1198\tTriple Loss(1): 0.1773\tClassification Loss: 1.7653\n",
      "Train Epoch: 4 [36800/110534 (33%)]\tAll Loss: 2.0883\tTriple Loss(1): 0.2867\tClassification Loss: 1.5149\n",
      "Train Epoch: 4 [37120/110534 (34%)]\tAll Loss: 2.3692\tTriple Loss(1): 0.2538\tClassification Loss: 1.8615\n",
      "Train Epoch: 4 [37440/110534 (34%)]\tAll Loss: 2.1092\tTriple Loss(1): 0.2229\tClassification Loss: 1.6635\n",
      "Train Epoch: 4 [37760/110534 (34%)]\tAll Loss: 1.7955\tTriple Loss(1): 0.1184\tClassification Loss: 1.5586\n",
      "Train Epoch: 4 [38080/110534 (34%)]\tAll Loss: 2.2771\tTriple Loss(0): 0.2525\tClassification Loss: 1.7720\n",
      "\n",
      "Test set: Average loss: 1.5971, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tAll Loss: 1.9470\tTriple Loss(1): 0.2871\tClassification Loss: 1.3728\n",
      "Train Epoch: 4 [38720/110534 (35%)]\tAll Loss: 1.7556\tTriple Loss(0): 0.0000\tClassification Loss: 1.7556\n",
      "Train Epoch: 4 [39040/110534 (35%)]\tAll Loss: 1.7056\tTriple Loss(1): 0.1890\tClassification Loss: 1.3276\n",
      "Train Epoch: 4 [39360/110534 (36%)]\tAll Loss: 1.5900\tTriple Loss(1): 0.0680\tClassification Loss: 1.4541\n",
      "Train Epoch: 4 [39680/110534 (36%)]\tAll Loss: 2.4338\tTriple Loss(1): 0.2951\tClassification Loss: 1.8436\n",
      "Train Epoch: 4 [40000/110534 (36%)]\tAll Loss: 1.2722\tTriple Loss(0): 0.0000\tClassification Loss: 1.2722\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tAll Loss: 2.4052\tTriple Loss(1): 0.2999\tClassification Loss: 1.8054\n",
      "Train Epoch: 4 [40640/110534 (37%)]\tAll Loss: 1.9097\tTriple Loss(1): 0.1354\tClassification Loss: 1.6389\n",
      "Train Epoch: 4 [40960/110534 (37%)]\tAll Loss: 1.4005\tTriple Loss(0): 0.0000\tClassification Loss: 1.4005\n",
      "Train Epoch: 4 [41280/110534 (37%)]\tAll Loss: 2.0128\tTriple Loss(1): 0.2322\tClassification Loss: 1.5485\n",
      "\n",
      "Test set: Average loss: 1.5955, Accuracy: 557/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [41600/110534 (38%)]\tAll Loss: 1.6653\tTriple Loss(0): 0.0000\tClassification Loss: 1.6653\n",
      "Train Epoch: 4 [41920/110534 (38%)]\tAll Loss: 2.0519\tTriple Loss(1): 0.2597\tClassification Loss: 1.5326\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tAll Loss: 1.9780\tTriple Loss(1): 0.1133\tClassification Loss: 1.7514\n",
      "Train Epoch: 4 [42560/110534 (38%)]\tAll Loss: 1.9826\tTriple Loss(1): 0.2425\tClassification Loss: 1.4977\n",
      "Train Epoch: 4 [42880/110534 (39%)]\tAll Loss: 2.1550\tTriple Loss(1): 0.2615\tClassification Loss: 1.6320\n",
      "Train Epoch: 4 [43200/110534 (39%)]\tAll Loss: 2.3601\tTriple Loss(1): 0.3899\tClassification Loss: 1.5803\n",
      "Train Epoch: 4 [43520/110534 (39%)]\tAll Loss: 1.3831\tTriple Loss(1): 0.0393\tClassification Loss: 1.3044\n",
      "Train Epoch: 4 [43840/110534 (40%)]\tAll Loss: 2.1792\tTriple Loss(1): 0.2076\tClassification Loss: 1.7639\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tAll Loss: 2.4917\tTriple Loss(1): 0.3037\tClassification Loss: 1.8843\n",
      "Train Epoch: 4 [44480/110534 (40%)]\tAll Loss: 2.0384\tTriple Loss(1): 0.2393\tClassification Loss: 1.5597\n",
      "\n",
      "Test set: Average loss: 1.5973, Accuracy: 556/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [44800/110534 (41%)]\tAll Loss: 1.1836\tTriple Loss(1): 0.0557\tClassification Loss: 1.0722\n",
      "Train Epoch: 4 [45120/110534 (41%)]\tAll Loss: 1.8973\tTriple Loss(1): 0.1777\tClassification Loss: 1.5420\n",
      "Train Epoch: 4 [45440/110534 (41%)]\tAll Loss: 1.8518\tTriple Loss(1): 0.1733\tClassification Loss: 1.5051\n",
      "Train Epoch: 4 [45760/110534 (41%)]\tAll Loss: 1.8246\tTriple Loss(1): 0.1285\tClassification Loss: 1.5676\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tAll Loss: 1.8610\tTriple Loss(1): 0.3239\tClassification Loss: 1.2131\n",
      "Train Epoch: 4 [46400/110534 (42%)]\tAll Loss: 2.9003\tTriple Loss(1): 0.4369\tClassification Loss: 2.0265\n",
      "Train Epoch: 4 [46720/110534 (42%)]\tAll Loss: 2.4275\tTriple Loss(1): 0.2438\tClassification Loss: 1.9398\n",
      "Train Epoch: 4 [47040/110534 (43%)]\tAll Loss: 2.2211\tTriple Loss(1): 0.3706\tClassification Loss: 1.4798\n",
      "Train Epoch: 4 [47360/110534 (43%)]\tAll Loss: 2.1354\tTriple Loss(1): 0.2548\tClassification Loss: 1.6257\n",
      "Train Epoch: 4 [47680/110534 (43%)]\tAll Loss: 1.1829\tTriple Loss(0): 0.0000\tClassification Loss: 1.1829\n",
      "\n",
      "Test set: Average loss: 1.6000, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tAll Loss: 2.3523\tTriple Loss(1): 0.2275\tClassification Loss: 1.8974\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1500.pth.tar\n",
      "Train Epoch: 4 [48320/110534 (44%)]\tAll Loss: 2.6035\tTriple Loss(1): 0.3631\tClassification Loss: 1.8773\n",
      "Train Epoch: 4 [48640/110534 (44%)]\tAll Loss: 2.3342\tTriple Loss(1): 0.2337\tClassification Loss: 1.8669\n",
      "Train Epoch: 4 [48960/110534 (44%)]\tAll Loss: 2.1643\tTriple Loss(1): 0.3470\tClassification Loss: 1.4702\n",
      "Train Epoch: 4 [49280/110534 (45%)]\tAll Loss: 2.5824\tTriple Loss(1): 0.5587\tClassification Loss: 1.4649\n",
      "Train Epoch: 4 [49600/110534 (45%)]\tAll Loss: 1.7900\tTriple Loss(0): 0.0000\tClassification Loss: 1.7900\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tAll Loss: 1.6180\tTriple Loss(1): 0.1099\tClassification Loss: 1.3982\n",
      "Train Epoch: 4 [50240/110534 (45%)]\tAll Loss: 1.8281\tTriple Loss(1): 0.2966\tClassification Loss: 1.2350\n",
      "Train Epoch: 4 [50560/110534 (46%)]\tAll Loss: 2.1639\tTriple Loss(1): 0.1529\tClassification Loss: 1.8580\n",
      "Train Epoch: 4 [50880/110534 (46%)]\tAll Loss: 2.8016\tTriple Loss(1): 0.4540\tClassification Loss: 1.8936\n",
      "\n",
      "Test set: Average loss: 1.5952, Accuracy: 552/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [51200/110534 (46%)]\tAll Loss: 2.0792\tTriple Loss(1): 0.2656\tClassification Loss: 1.5480\n",
      "Train Epoch: 4 [51520/110534 (47%)]\tAll Loss: 1.3915\tTriple Loss(0): 0.0000\tClassification Loss: 1.3915\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tAll Loss: 1.6864\tTriple Loss(0): 0.0000\tClassification Loss: 1.6864\n",
      "Train Epoch: 4 [52160/110534 (47%)]\tAll Loss: 2.3159\tTriple Loss(1): 0.2844\tClassification Loss: 1.7472\n",
      "Train Epoch: 4 [52480/110534 (47%)]\tAll Loss: 2.4306\tTriple Loss(1): 0.1389\tClassification Loss: 2.1529\n",
      "Train Epoch: 4 [52800/110534 (48%)]\tAll Loss: 2.0399\tTriple Loss(1): 0.2355\tClassification Loss: 1.5690\n",
      "Train Epoch: 4 [53120/110534 (48%)]\tAll Loss: 2.1957\tTriple Loss(1): 0.2986\tClassification Loss: 1.5985\n",
      "Train Epoch: 4 [53440/110534 (48%)]\tAll Loss: 2.3623\tTriple Loss(1): 0.2389\tClassification Loss: 1.8846\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tAll Loss: 2.0617\tTriple Loss(1): 0.1761\tClassification Loss: 1.7096\n",
      "Train Epoch: 4 [54080/110534 (49%)]\tAll Loss: 2.5485\tTriple Loss(1): 0.2507\tClassification Loss: 2.0471\n",
      "\n",
      "Test set: Average loss: 1.5985, Accuracy: 554/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [54400/110534 (49%)]\tAll Loss: 2.7689\tTriple Loss(1): 0.5512\tClassification Loss: 1.6665\n",
      "Train Epoch: 4 [54720/110534 (49%)]\tAll Loss: 2.0710\tTriple Loss(1): 0.1850\tClassification Loss: 1.7009\n",
      "Train Epoch: 4 [55040/110534 (50%)]\tAll Loss: 2.5161\tTriple Loss(1): 0.2355\tClassification Loss: 2.0451\n",
      "Train Epoch: 4 [55360/110534 (50%)]\tAll Loss: 1.4399\tTriple Loss(0): 0.0435\tClassification Loss: 1.3529\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tAll Loss: 2.3011\tTriple Loss(1): 0.2409\tClassification Loss: 1.8192\n",
      "Train Epoch: 4 [56000/110534 (51%)]\tAll Loss: 2.2018\tTriple Loss(0): 0.0000\tClassification Loss: 2.2018\n",
      "Train Epoch: 4 [56320/110534 (51%)]\tAll Loss: 1.8208\tTriple Loss(0): 0.0000\tClassification Loss: 1.8208\n",
      "Train Epoch: 4 [56640/110534 (51%)]\tAll Loss: 1.0498\tTriple Loss(0): 0.0000\tClassification Loss: 1.0498\n",
      "Train Epoch: 4 [56960/110534 (52%)]\tAll Loss: 2.2406\tTriple Loss(1): 0.2633\tClassification Loss: 1.7141\n",
      "Train Epoch: 4 [57280/110534 (52%)]\tAll Loss: 2.1007\tTriple Loss(1): 0.1892\tClassification Loss: 1.7223\n",
      "\n",
      "Test set: Average loss: 1.5972, Accuracy: 554/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tAll Loss: 1.8066\tTriple Loss(0): 0.0000\tClassification Loss: 1.8066\n",
      "Train Epoch: 4 [57920/110534 (52%)]\tAll Loss: 1.9905\tTriple Loss(1): 0.1581\tClassification Loss: 1.6743\n",
      "Train Epoch: 4 [58240/110534 (53%)]\tAll Loss: 1.4896\tTriple Loss(1): 0.1581\tClassification Loss: 1.1733\n",
      "Train Epoch: 4 [58560/110534 (53%)]\tAll Loss: 1.2871\tTriple Loss(1): 0.0000\tClassification Loss: 1.2871\n",
      "Train Epoch: 4 [58880/110534 (53%)]\tAll Loss: 2.4145\tTriple Loss(1): 0.3666\tClassification Loss: 1.6814\n",
      "Train Epoch: 4 [59200/110534 (54%)]\tAll Loss: 2.0576\tTriple Loss(1): 0.0994\tClassification Loss: 1.8588\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tAll Loss: 1.8965\tTriple Loss(1): 0.1339\tClassification Loss: 1.6287\n",
      "Train Epoch: 4 [59840/110534 (54%)]\tAll Loss: 1.9590\tTriple Loss(1): 0.2384\tClassification Loss: 1.4822\n",
      "Train Epoch: 4 [60160/110534 (54%)]\tAll Loss: 1.5201\tTriple Loss(0): 0.0000\tClassification Loss: 1.5201\n",
      "Train Epoch: 4 [60480/110534 (55%)]\tAll Loss: 1.5668\tTriple Loss(1): 0.0986\tClassification Loss: 1.3697\n",
      "\n",
      "Test set: Average loss: 1.5963, Accuracy: 554/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [60800/110534 (55%)]\tAll Loss: 2.0602\tTriple Loss(1): 0.1946\tClassification Loss: 1.6709\n",
      "Train Epoch: 4 [61120/110534 (55%)]\tAll Loss: 2.1042\tTriple Loss(1): 0.2008\tClassification Loss: 1.7026\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tAll Loss: 1.6207\tTriple Loss(1): 0.2812\tClassification Loss: 1.0583\n",
      "Train Epoch: 4 [61760/110534 (56%)]\tAll Loss: 2.1675\tTriple Loss(1): 0.2654\tClassification Loss: 1.6368\n",
      "Train Epoch: 4 [62080/110534 (56%)]\tAll Loss: 2.1382\tTriple Loss(1): 0.3114\tClassification Loss: 1.5154\n",
      "Train Epoch: 4 [62400/110534 (56%)]\tAll Loss: 1.2154\tTriple Loss(0): 0.0000\tClassification Loss: 1.2154\n",
      "Train Epoch: 4 [62720/110534 (57%)]\tAll Loss: 1.6270\tTriple Loss(1): 0.1103\tClassification Loss: 1.4063\n",
      "Train Epoch: 4 [63040/110534 (57%)]\tAll Loss: 2.0711\tTriple Loss(1): 0.1132\tClassification Loss: 1.8447\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tAll Loss: 1.8845\tTriple Loss(1): 0.2035\tClassification Loss: 1.4775\n",
      "Train Epoch: 4 [63680/110534 (58%)]\tAll Loss: 1.7647\tTriple Loss(1): 0.1691\tClassification Loss: 1.4265\n",
      "\n",
      "Test set: Average loss: 1.6047, Accuracy: 547/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [64000/110534 (58%)]\tAll Loss: 2.3576\tTriple Loss(1): 0.0760\tClassification Loss: 2.2056\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2000.pth.tar\n",
      "Train Epoch: 4 [64320/110534 (58%)]\tAll Loss: 1.6223\tTriple Loss(1): 0.1551\tClassification Loss: 1.3121\n",
      "Train Epoch: 4 [64640/110534 (58%)]\tAll Loss: 2.2186\tTriple Loss(0): 0.0000\tClassification Loss: 2.2186\n",
      "Train Epoch: 4 [64960/110534 (59%)]\tAll Loss: 2.2444\tTriple Loss(1): 0.3298\tClassification Loss: 1.5847\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tAll Loss: 2.2175\tTriple Loss(1): 0.2350\tClassification Loss: 1.7475\n",
      "Train Epoch: 4 [65600/110534 (59%)]\tAll Loss: 2.2488\tTriple Loss(1): 0.2247\tClassification Loss: 1.7994\n",
      "Train Epoch: 4 [65920/110534 (60%)]\tAll Loss: 1.5568\tTriple Loss(0): 0.0000\tClassification Loss: 1.5568\n",
      "Train Epoch: 4 [66240/110534 (60%)]\tAll Loss: 1.7678\tTriple Loss(1): 0.0361\tClassification Loss: 1.6956\n",
      "Train Epoch: 4 [66560/110534 (60%)]\tAll Loss: 1.7393\tTriple Loss(1): 0.1750\tClassification Loss: 1.3892\n",
      "Train Epoch: 4 [66880/110534 (60%)]\tAll Loss: 2.3561\tTriple Loss(1): 0.3850\tClassification Loss: 1.5861\n",
      "\n",
      "Test set: Average loss: 1.6003, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tAll Loss: 2.1307\tTriple Loss(1): 0.1551\tClassification Loss: 1.8205\n",
      "Train Epoch: 4 [67520/110534 (61%)]\tAll Loss: 1.8261\tTriple Loss(0): 0.0000\tClassification Loss: 1.8261\n",
      "Train Epoch: 4 [67840/110534 (61%)]\tAll Loss: 2.3735\tTriple Loss(1): 0.2610\tClassification Loss: 1.8516\n",
      "Train Epoch: 4 [68160/110534 (62%)]\tAll Loss: 1.4197\tTriple Loss(0): 0.0000\tClassification Loss: 1.4197\n",
      "Train Epoch: 4 [68480/110534 (62%)]\tAll Loss: 2.1250\tTriple Loss(1): 0.3298\tClassification Loss: 1.4655\n",
      "Train Epoch: 4 [68800/110534 (62%)]\tAll Loss: 1.8819\tTriple Loss(1): 0.2419\tClassification Loss: 1.3981\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tAll Loss: 1.9601\tTriple Loss(1): 0.1386\tClassification Loss: 1.6828\n",
      "Train Epoch: 4 [69440/110534 (63%)]\tAll Loss: 1.4799\tTriple Loss(0): 0.0000\tClassification Loss: 1.4799\n",
      "Train Epoch: 4 [69760/110534 (63%)]\tAll Loss: 1.9940\tTriple Loss(1): 0.2697\tClassification Loss: 1.4545\n",
      "Train Epoch: 4 [70080/110534 (63%)]\tAll Loss: 1.8492\tTriple Loss(1): 0.1692\tClassification Loss: 1.5109\n",
      "\n",
      "Test set: Average loss: 1.5942, Accuracy: 563/960 (59%)\n",
      "\n",
      "Train Epoch: 4 [70400/110534 (64%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.1457\tClassification Loss: 1.4963\n",
      "Train Epoch: 4 [70720/110534 (64%)]\tAll Loss: 1.9347\tTriple Loss(1): 0.2729\tClassification Loss: 1.3889\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tAll Loss: 1.8869\tTriple Loss(1): 0.2414\tClassification Loss: 1.4041\n",
      "Train Epoch: 4 [71360/110534 (65%)]\tAll Loss: 2.2123\tTriple Loss(1): 0.2217\tClassification Loss: 1.7688\n",
      "Train Epoch: 4 [71680/110534 (65%)]\tAll Loss: 2.1073\tTriple Loss(1): 0.2413\tClassification Loss: 1.6246\n",
      "Train Epoch: 4 [72000/110534 (65%)]\tAll Loss: 2.4729\tTriple Loss(1): 0.3830\tClassification Loss: 1.7069\n",
      "Train Epoch: 4 [72320/110534 (65%)]\tAll Loss: 1.5412\tTriple Loss(0): 0.0000\tClassification Loss: 1.5412\n",
      "Train Epoch: 4 [72640/110534 (66%)]\tAll Loss: 1.5324\tTriple Loss(1): 0.1142\tClassification Loss: 1.3040\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tAll Loss: 1.5052\tTriple Loss(0): 0.0000\tClassification Loss: 1.5052\n",
      "Train Epoch: 4 [73280/110534 (66%)]\tAll Loss: 1.6882\tTriple Loss(1): 0.2256\tClassification Loss: 1.2370\n",
      "\n",
      "Test set: Average loss: 1.5875, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [73600/110534 (67%)]\tAll Loss: 2.4388\tTriple Loss(1): 0.1108\tClassification Loss: 2.2172\n",
      "Train Epoch: 4 [73920/110534 (67%)]\tAll Loss: 2.2566\tTriple Loss(1): 0.2039\tClassification Loss: 1.8487\n",
      "Train Epoch: 4 [74240/110534 (67%)]\tAll Loss: 2.0129\tTriple Loss(1): 0.3629\tClassification Loss: 1.2870\n",
      "Train Epoch: 4 [74560/110534 (67%)]\tAll Loss: 1.9548\tTriple Loss(1): 0.1028\tClassification Loss: 1.7491\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tAll Loss: 1.8811\tTriple Loss(1): 0.1631\tClassification Loss: 1.5549\n",
      "Train Epoch: 4 [75200/110534 (68%)]\tAll Loss: 1.4489\tTriple Loss(0): 0.0000\tClassification Loss: 1.4489\n",
      "Train Epoch: 4 [75520/110534 (68%)]\tAll Loss: 1.9243\tTriple Loss(1): 0.1764\tClassification Loss: 1.5715\n",
      "Train Epoch: 4 [75840/110534 (69%)]\tAll Loss: 2.3496\tTriple Loss(1): 0.4115\tClassification Loss: 1.5265\n",
      "Train Epoch: 4 [76160/110534 (69%)]\tAll Loss: 1.9182\tTriple Loss(1): 0.1355\tClassification Loss: 1.6472\n",
      "Train Epoch: 4 [76480/110534 (69%)]\tAll Loss: 1.6719\tTriple Loss(1): 0.1429\tClassification Loss: 1.3862\n",
      "\n",
      "Test set: Average loss: 1.5884, Accuracy: 552/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tAll Loss: 1.4688\tTriple Loss(0): 0.0000\tClassification Loss: 1.4688\n",
      "Train Epoch: 4 [77120/110534 (70%)]\tAll Loss: 1.7325\tTriple Loss(1): 0.0892\tClassification Loss: 1.5541\n",
      "Train Epoch: 4 [77440/110534 (70%)]\tAll Loss: 2.1859\tTriple Loss(1): 0.3290\tClassification Loss: 1.5278\n",
      "Train Epoch: 4 [77760/110534 (70%)]\tAll Loss: 1.6206\tTriple Loss(0): 0.0000\tClassification Loss: 1.6206\n",
      "Train Epoch: 4 [78080/110534 (71%)]\tAll Loss: 2.4688\tTriple Loss(1): 0.1314\tClassification Loss: 2.2060\n",
      "Train Epoch: 4 [78400/110534 (71%)]\tAll Loss: 2.0627\tTriple Loss(1): 0.2360\tClassification Loss: 1.5907\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tAll Loss: 1.6236\tTriple Loss(1): 0.2676\tClassification Loss: 1.0885\n",
      "Train Epoch: 4 [79040/110534 (71%)]\tAll Loss: 6.2384\tTriple Loss(0): 2.2640\tClassification Loss: 1.7103\n",
      "Train Epoch: 4 [79360/110534 (72%)]\tAll Loss: 2.5946\tTriple Loss(1): 0.3844\tClassification Loss: 1.8257\n",
      "Train Epoch: 4 [79680/110534 (72%)]\tAll Loss: 2.3705\tTriple Loss(1): 0.4390\tClassification Loss: 1.4924\n",
      "\n",
      "Test set: Average loss: 1.5919, Accuracy: 546/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [80000/110534 (72%)]\tAll Loss: 1.6160\tTriple Loss(1): 0.1650\tClassification Loss: 1.2859\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2500.pth.tar\n",
      "Train Epoch: 4 [80320/110534 (73%)]\tAll Loss: 1.5677\tTriple Loss(1): 0.2212\tClassification Loss: 1.1252\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tAll Loss: 1.7319\tTriple Loss(1): 0.2649\tClassification Loss: 1.2021\n",
      "Train Epoch: 4 [80960/110534 (73%)]\tAll Loss: 2.0456\tTriple Loss(1): 0.3152\tClassification Loss: 1.4152\n",
      "Train Epoch: 4 [81280/110534 (74%)]\tAll Loss: 1.9366\tTriple Loss(1): 0.2964\tClassification Loss: 1.3437\n",
      "Train Epoch: 4 [81600/110534 (74%)]\tAll Loss: 1.6872\tTriple Loss(1): 0.1420\tClassification Loss: 1.4032\n",
      "Train Epoch: 4 [81920/110534 (74%)]\tAll Loss: 2.2540\tTriple Loss(1): 0.3059\tClassification Loss: 1.6423\n",
      "Train Epoch: 4 [82240/110534 (74%)]\tAll Loss: 2.6418\tTriple Loss(1): 0.3864\tClassification Loss: 1.8690\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tAll Loss: 2.2861\tTriple Loss(1): 0.3289\tClassification Loss: 1.6283\n",
      "Train Epoch: 4 [82880/110534 (75%)]\tAll Loss: 2.8237\tTriple Loss(1): 0.3248\tClassification Loss: 2.1741\n",
      "\n",
      "Test set: Average loss: 1.5875, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [83200/110534 (75%)]\tAll Loss: 1.8082\tTriple Loss(1): 0.1407\tClassification Loss: 1.5268\n",
      "Train Epoch: 4 [83520/110534 (76%)]\tAll Loss: 1.5433\tTriple Loss(0): 0.0252\tClassification Loss: 1.4929\n",
      "Train Epoch: 4 [83840/110534 (76%)]\tAll Loss: 1.3094\tTriple Loss(1): 0.0280\tClassification Loss: 1.2534\n",
      "Train Epoch: 4 [84160/110534 (76%)]\tAll Loss: 1.7579\tTriple Loss(1): 0.3026\tClassification Loss: 1.1527\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tAll Loss: 1.6448\tTriple Loss(1): 0.1885\tClassification Loss: 1.2678\n",
      "Train Epoch: 4 [84800/110534 (77%)]\tAll Loss: 2.3269\tTriple Loss(1): 0.3589\tClassification Loss: 1.6091\n",
      "Train Epoch: 4 [85120/110534 (77%)]\tAll Loss: 1.4429\tTriple Loss(0): 0.0000\tClassification Loss: 1.4429\n",
      "Train Epoch: 4 [85440/110534 (77%)]\tAll Loss: 1.8710\tTriple Loss(1): 0.3793\tClassification Loss: 1.1124\n",
      "Train Epoch: 4 [85760/110534 (78%)]\tAll Loss: 1.6768\tTriple Loss(0): 0.0000\tClassification Loss: 1.6768\n",
      "Train Epoch: 4 [86080/110534 (78%)]\tAll Loss: 1.9893\tTriple Loss(0): 0.0000\tClassification Loss: 1.9893\n",
      "\n",
      "Test set: Average loss: 1.5859, Accuracy: 559/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tAll Loss: 2.3442\tTriple Loss(1): 0.1932\tClassification Loss: 1.9577\n",
      "Train Epoch: 4 [86720/110534 (78%)]\tAll Loss: 1.7570\tTriple Loss(1): 0.1755\tClassification Loss: 1.4060\n",
      "Train Epoch: 4 [87040/110534 (79%)]\tAll Loss: 2.0247\tTriple Loss(1): 0.2774\tClassification Loss: 1.4700\n",
      "Train Epoch: 4 [87360/110534 (79%)]\tAll Loss: 2.2441\tTriple Loss(1): 0.2272\tClassification Loss: 1.7898\n",
      "Train Epoch: 4 [87680/110534 (79%)]\tAll Loss: 2.0789\tTriple Loss(1): 0.1759\tClassification Loss: 1.7271\n",
      "Train Epoch: 4 [88000/110534 (80%)]\tAll Loss: 2.9043\tTriple Loss(1): 0.2816\tClassification Loss: 2.3411\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tAll Loss: 1.6383\tTriple Loss(1): 0.1182\tClassification Loss: 1.4020\n",
      "Train Epoch: 4 [88640/110534 (80%)]\tAll Loss: 2.0327\tTriple Loss(0): 0.0000\tClassification Loss: 2.0327\n",
      "Train Epoch: 4 [88960/110534 (80%)]\tAll Loss: 2.8804\tTriple Loss(1): 0.5624\tClassification Loss: 1.7556\n",
      "Train Epoch: 4 [89280/110534 (81%)]\tAll Loss: 2.4753\tTriple Loss(1): 0.4196\tClassification Loss: 1.6360\n",
      "\n",
      "Test set: Average loss: 1.5858, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [89600/110534 (81%)]\tAll Loss: 2.2284\tTriple Loss(0): 0.0000\tClassification Loss: 2.2284\n",
      "Train Epoch: 4 [89920/110534 (81%)]\tAll Loss: 2.3901\tTriple Loss(1): 0.3145\tClassification Loss: 1.7610\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tAll Loss: 2.0021\tTriple Loss(1): 0.3006\tClassification Loss: 1.4008\n",
      "Train Epoch: 4 [90560/110534 (82%)]\tAll Loss: 1.6123\tTriple Loss(1): 0.1997\tClassification Loss: 1.2130\n",
      "Train Epoch: 4 [90880/110534 (82%)]\tAll Loss: 2.2769\tTriple Loss(1): 0.3266\tClassification Loss: 1.6237\n",
      "Train Epoch: 4 [91200/110534 (82%)]\tAll Loss: 1.7026\tTriple Loss(1): 0.1548\tClassification Loss: 1.3931\n",
      "Train Epoch: 4 [91520/110534 (83%)]\tAll Loss: 2.6605\tTriple Loss(1): 0.5365\tClassification Loss: 1.5876\n",
      "Train Epoch: 4 [91840/110534 (83%)]\tAll Loss: 1.4709\tTriple Loss(0): 0.0000\tClassification Loss: 1.4709\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tAll Loss: 2.6662\tTriple Loss(1): 0.3617\tClassification Loss: 1.9427\n",
      "Train Epoch: 4 [92480/110534 (84%)]\tAll Loss: 1.7738\tTriple Loss(1): 0.0993\tClassification Loss: 1.5752\n",
      "\n",
      "Test set: Average loss: 1.5925, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [92800/110534 (84%)]\tAll Loss: 1.9677\tTriple Loss(1): 0.1546\tClassification Loss: 1.6585\n",
      "Train Epoch: 4 [93120/110534 (84%)]\tAll Loss: 1.7432\tTriple Loss(0): 0.0000\tClassification Loss: 1.7432\n",
      "Train Epoch: 4 [93440/110534 (85%)]\tAll Loss: 1.8013\tTriple Loss(1): 0.1865\tClassification Loss: 1.4282\n",
      "Train Epoch: 4 [93760/110534 (85%)]\tAll Loss: 5.7776\tTriple Loss(0): 2.1513\tClassification Loss: 1.4751\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tAll Loss: 6.7598\tTriple Loss(0): 2.6934\tClassification Loss: 1.3730\n",
      "Train Epoch: 4 [94400/110534 (85%)]\tAll Loss: 1.9653\tTriple Loss(1): 0.1789\tClassification Loss: 1.6075\n",
      "Train Epoch: 4 [94720/110534 (86%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.1857\tClassification Loss: 1.6493\n",
      "Train Epoch: 4 [95040/110534 (86%)]\tAll Loss: 1.8099\tTriple Loss(1): 0.0704\tClassification Loss: 1.6692\n",
      "Train Epoch: 4 [95360/110534 (86%)]\tAll Loss: 2.0932\tTriple Loss(1): 0.2776\tClassification Loss: 1.5380\n",
      "Train Epoch: 4 [95680/110534 (87%)]\tAll Loss: 2.0644\tTriple Loss(1): 0.2134\tClassification Loss: 1.6375\n",
      "\n",
      "Test set: Average loss: 1.5857, Accuracy: 549/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tAll Loss: 1.9258\tTriple Loss(1): 0.2760\tClassification Loss: 1.3739\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_3000.pth.tar\n",
      "Train Epoch: 4 [96320/110534 (87%)]\tAll Loss: 1.7938\tTriple Loss(1): 0.1750\tClassification Loss: 1.4438\n",
      "Train Epoch: 4 [96640/110534 (87%)]\tAll Loss: 1.7434\tTriple Loss(0): 0.0000\tClassification Loss: 1.7434\n",
      "Train Epoch: 4 [96960/110534 (88%)]\tAll Loss: 1.9789\tTriple Loss(1): 0.2907\tClassification Loss: 1.3976\n",
      "Train Epoch: 4 [97280/110534 (88%)]\tAll Loss: 1.5676\tTriple Loss(1): 0.1746\tClassification Loss: 1.2184\n",
      "Train Epoch: 4 [97600/110534 (88%)]\tAll Loss: 2.1359\tTriple Loss(1): 0.2092\tClassification Loss: 1.7174\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tAll Loss: 1.1293\tTriple Loss(0): 0.0000\tClassification Loss: 1.1293\n",
      "Train Epoch: 4 [98240/110534 (89%)]\tAll Loss: 1.6372\tTriple Loss(1): 0.2127\tClassification Loss: 1.2119\n",
      "Train Epoch: 4 [98560/110534 (89%)]\tAll Loss: 1.1898\tTriple Loss(0): 0.0000\tClassification Loss: 1.1898\n",
      "Train Epoch: 4 [98880/110534 (89%)]\tAll Loss: 1.4673\tTriple Loss(0): 0.0000\tClassification Loss: 1.4673\n",
      "\n",
      "Test set: Average loss: 1.6001, Accuracy: 555/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [99200/110534 (90%)]\tAll Loss: 1.9585\tTriple Loss(1): 0.2448\tClassification Loss: 1.4690\n",
      "Train Epoch: 4 [99520/110534 (90%)]\tAll Loss: 1.8197\tTriple Loss(1): 0.1254\tClassification Loss: 1.5689\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tAll Loss: 2.0553\tTriple Loss(1): 0.1998\tClassification Loss: 1.6556\n",
      "Train Epoch: 4 [100160/110534 (91%)]\tAll Loss: 4.3528\tTriple Loss(0): 1.2153\tClassification Loss: 1.9223\n",
      "Train Epoch: 4 [100480/110534 (91%)]\tAll Loss: 2.0049\tTriple Loss(1): 0.2379\tClassification Loss: 1.5290\n",
      "Train Epoch: 4 [100800/110534 (91%)]\tAll Loss: 3.4454\tTriple Loss(0): 0.8193\tClassification Loss: 1.8068\n",
      "Train Epoch: 4 [101120/110534 (91%)]\tAll Loss: 1.8647\tTriple Loss(1): 0.2451\tClassification Loss: 1.3744\n",
      "Train Epoch: 4 [101440/110534 (92%)]\tAll Loss: 2.0374\tTriple Loss(1): 0.3677\tClassification Loss: 1.3021\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tAll Loss: 1.7292\tTriple Loss(1): 0.0663\tClassification Loss: 1.5965\n",
      "Train Epoch: 4 [102080/110534 (92%)]\tAll Loss: 2.8579\tTriple Loss(1): 0.3550\tClassification Loss: 2.1479\n",
      "\n",
      "Test set: Average loss: 1.5898, Accuracy: 548/960 (57%)\n",
      "\n",
      "Train Epoch: 4 [102400/110534 (93%)]\tAll Loss: 1.7546\tTriple Loss(1): 0.1591\tClassification Loss: 1.4363\n",
      "Train Epoch: 4 [102720/110534 (93%)]\tAll Loss: 2.2690\tTriple Loss(1): 0.3175\tClassification Loss: 1.6341\n",
      "Train Epoch: 4 [103040/110534 (93%)]\tAll Loss: 2.0679\tTriple Loss(1): 0.2930\tClassification Loss: 1.4818\n",
      "Train Epoch: 4 [103360/110534 (93%)]\tAll Loss: 1.9058\tTriple Loss(0): 0.0000\tClassification Loss: 1.9058\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tAll Loss: 2.3772\tTriple Loss(1): 0.3019\tClassification Loss: 1.7735\n",
      "Train Epoch: 4 [104000/110534 (94%)]\tAll Loss: 1.7711\tTriple Loss(1): 0.1266\tClassification Loss: 1.5179\n",
      "Train Epoch: 4 [104320/110534 (94%)]\tAll Loss: 1.9167\tTriple Loss(0): 0.0000\tClassification Loss: 1.9167\n",
      "Train Epoch: 4 [104640/110534 (95%)]\tAll Loss: 2.2616\tTriple Loss(1): 0.2138\tClassification Loss: 1.8341\n",
      "Train Epoch: 4 [104960/110534 (95%)]\tAll Loss: 1.5719\tTriple Loss(0): 0.0000\tClassification Loss: 1.5719\n",
      "Train Epoch: 4 [105280/110534 (95%)]\tAll Loss: 1.9879\tTriple Loss(1): 0.1800\tClassification Loss: 1.6280\n",
      "\n",
      "Test set: Average loss: 1.5918, Accuracy: 560/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tAll Loss: 2.0039\tTriple Loss(1): 0.1176\tClassification Loss: 1.7687\n",
      "Train Epoch: 4 [105920/110534 (96%)]\tAll Loss: 1.7262\tTriple Loss(0): 0.0000\tClassification Loss: 1.7262\n",
      "Train Epoch: 4 [106240/110534 (96%)]\tAll Loss: 1.6088\tTriple Loss(1): 0.1576\tClassification Loss: 1.2937\n",
      "Train Epoch: 4 [106560/110534 (96%)]\tAll Loss: 1.8804\tTriple Loss(1): 0.1281\tClassification Loss: 1.6243\n",
      "Train Epoch: 4 [106880/110534 (97%)]\tAll Loss: 2.9208\tTriple Loss(1): 0.4500\tClassification Loss: 2.0208\n",
      "Train Epoch: 4 [107200/110534 (97%)]\tAll Loss: 1.8484\tTriple Loss(0): 0.0000\tClassification Loss: 1.8484\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tAll Loss: 1.9060\tTriple Loss(1): 0.1496\tClassification Loss: 1.6068\n",
      "Train Epoch: 4 [107840/110534 (98%)]\tAll Loss: 1.5530\tTriple Loss(0): 0.0000\tClassification Loss: 1.5530\n",
      "Train Epoch: 4 [108160/110534 (98%)]\tAll Loss: 1.5517\tTriple Loss(1): 0.1518\tClassification Loss: 1.2481\n",
      "Train Epoch: 4 [108480/110534 (98%)]\tAll Loss: 1.8281\tTriple Loss(1): 0.0946\tClassification Loss: 1.6390\n",
      "\n",
      "Test set: Average loss: 1.5840, Accuracy: 557/960 (58%)\n",
      "\n",
      "Train Epoch: 4 [108800/110534 (98%)]\tAll Loss: 1.6886\tTriple Loss(0): 0.0000\tClassification Loss: 1.6886\n",
      "Train Epoch: 4 [109120/110534 (99%)]\tAll Loss: 2.2969\tTriple Loss(1): 0.2016\tClassification Loss: 1.8937\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tAll Loss: 1.8607\tTriple Loss(1): 0.2129\tClassification Loss: 1.4349\n",
      "Train Epoch: 4 [109760/110534 (99%)]\tAll Loss: 1.5134\tTriple Loss(0): 0.0000\tClassification Loss: 1.5134\n",
      "Train Epoch: 4 [110080/110534 (100%)]\tAll Loss: 2.1339\tTriple Loss(1): 0.2114\tClassification Loss: 1.7110\n",
      "Train Epoch: 4 [110400/110534 (100%)]\tAll Loss: 1.9492\tTriple Loss(1): 0.1944\tClassification Loss: 1.5605\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_final.pth.tar\n",
      "\n",
      "Test set: Average loss: 1.5863, Accuracy: 556/960 (58%)\n",
      "\n",
      "Train Epoch: 5 [0/110534 (0%)]\tAll Loss: 1.9346\tTriple Loss(1): 0.2073\tClassification Loss: 1.5200\n",
      "Train Epoch: 5 [320/110534 (0%)]\tAll Loss: 2.3173\tTriple Loss(1): 0.4272\tClassification Loss: 1.4629\n",
      "Train Epoch: 5 [640/110534 (1%)]\tAll Loss: 1.3295\tTriple Loss(1): 0.1510\tClassification Loss: 1.0275\n",
      "Train Epoch: 5 [960/110534 (1%)]\tAll Loss: 2.3740\tTriple Loss(1): 0.2690\tClassification Loss: 1.8360\n",
      "Train Epoch: 5 [1280/110534 (1%)]\tAll Loss: 1.6855\tTriple Loss(1): 0.1196\tClassification Loss: 1.4463\n",
      "Train Epoch: 5 [1600/110534 (1%)]\tAll Loss: 2.0910\tTriple Loss(1): 0.1966\tClassification Loss: 1.6978\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tAll Loss: 1.8110\tTriple Loss(1): 0.0815\tClassification Loss: 1.6479\n",
      "Train Epoch: 5 [2240/110534 (2%)]\tAll Loss: 1.8682\tTriple Loss(1): 0.1483\tClassification Loss: 1.5715\n",
      "Train Epoch: 5 [2560/110534 (2%)]\tAll Loss: 1.8474\tTriple Loss(1): 0.1536\tClassification Loss: 1.5401\n",
      "Train Epoch: 5 [2880/110534 (3%)]\tAll Loss: 2.1520\tTriple Loss(1): 0.0997\tClassification Loss: 1.9525\n",
      "\n",
      "Test set: Average loss: 1.5915, Accuracy: 557/960 (58%)\n",
      "\n",
      "Train Epoch: 5 [3200/110534 (3%)]\tAll Loss: 2.2608\tTriple Loss(1): 0.0764\tClassification Loss: 2.1080\n",
      "Train Epoch: 5 [3520/110534 (3%)]\tAll Loss: 1.2783\tTriple Loss(0): 0.0000\tClassification Loss: 1.2783\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tAll Loss: 2.3105\tTriple Loss(1): 0.3141\tClassification Loss: 1.6823\n",
      "Train Epoch: 5 [4160/110534 (4%)]\tAll Loss: 1.9516\tTriple Loss(1): 0.1785\tClassification Loss: 1.5946\n",
      "Train Epoch: 5 [4480/110534 (4%)]\tAll Loss: 1.9282\tTriple Loss(1): 0.2533\tClassification Loss: 1.4215\n",
      "Train Epoch: 5 [4800/110534 (4%)]\tAll Loss: 1.6674\tTriple Loss(1): 0.1065\tClassification Loss: 1.4543\n",
      "Train Epoch: 5 [5120/110534 (5%)]\tAll Loss: 2.0875\tTriple Loss(1): 0.1981\tClassification Loss: 1.6913\n",
      "Train Epoch: 5 [5440/110534 (5%)]\tAll Loss: 2.0870\tTriple Loss(1): 0.1956\tClassification Loss: 1.6959\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tAll Loss: 1.8451\tTriple Loss(1): 0.3835\tClassification Loss: 1.0781\n",
      "Train Epoch: 5 [6080/110534 (5%)]\tAll Loss: 1.4283\tTriple Loss(0): 0.0000\tClassification Loss: 1.4283\n",
      "\n",
      "Test set: Average loss: 1.5851, Accuracy: 552/960 (58%)\n",
      "\n",
      "Train Epoch: 5 [6400/110534 (6%)]\tAll Loss: 2.6505\tTriple Loss(1): 0.4293\tClassification Loss: 1.7919\n",
      "Train Epoch: 5 [6720/110534 (6%)]\tAll Loss: 1.6735\tTriple Loss(0): 0.0000\tClassification Loss: 1.6735\n",
      "Train Epoch: 5 [7040/110534 (6%)]\tAll Loss: 2.4707\tTriple Loss(1): 0.2980\tClassification Loss: 1.8746\n",
      "Train Epoch: 5 [7360/110534 (7%)]\tAll Loss: 2.0783\tTriple Loss(1): 0.3011\tClassification Loss: 1.4761\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tAll Loss: 1.7756\tTriple Loss(1): 0.2950\tClassification Loss: 1.1856\n",
      "Train Epoch: 5 [8000/110534 (7%)]\tAll Loss: 1.5714\tTriple Loss(1): 0.1494\tClassification Loss: 1.2726\n",
      "Train Epoch: 5 [8320/110534 (8%)]\tAll Loss: 2.2170\tTriple Loss(1): 0.3513\tClassification Loss: 1.5143\n",
      "Train Epoch: 5 [8640/110534 (8%)]\tAll Loss: 1.6244\tTriple Loss(1): 0.1581\tClassification Loss: 1.3083\n",
      "Train Epoch: 5 [8960/110534 (8%)]\tAll Loss: 13.5483\tTriple Loss(0): 6.0888\tClassification Loss: 1.3707\n",
      "Train Epoch: 5 [9280/110534 (8%)]\tAll Loss: 1.2703\tTriple Loss(0): 0.0000\tClassification Loss: 1.2703\n",
      "\n",
      "Test set: Average loss: 1.5826, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tAll Loss: 2.4587\tTriple Loss(1): 0.2710\tClassification Loss: 1.9167\n",
      "Train Epoch: 5 [9920/110534 (9%)]\tAll Loss: 1.6368\tTriple Loss(1): 0.0935\tClassification Loss: 1.4497\n",
      "Train Epoch: 5 [10240/110534 (9%)]\tAll Loss: 1.8395\tTriple Loss(0): 0.0000\tClassification Loss: 1.8395\n",
      "Train Epoch: 5 [10560/110534 (10%)]\tAll Loss: 1.7452\tTriple Loss(0): 0.0000\tClassification Loss: 1.7452\n",
      "Train Epoch: 5 [10880/110534 (10%)]\tAll Loss: 1.9553\tTriple Loss(1): 0.1227\tClassification Loss: 1.7098\n",
      "Train Epoch: 5 [11200/110534 (10%)]\tAll Loss: 1.5803\tTriple Loss(1): 0.1332\tClassification Loss: 1.3139\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tAll Loss: 2.0895\tTriple Loss(1): 0.2813\tClassification Loss: 1.5270\n",
      "Train Epoch: 5 [11840/110534 (11%)]\tAll Loss: 2.0519\tTriple Loss(1): 0.2374\tClassification Loss: 1.5772\n",
      "Train Epoch: 5 [12160/110534 (11%)]\tAll Loss: 2.3497\tTriple Loss(1): 0.3123\tClassification Loss: 1.7251\n",
      "Train Epoch: 5 [12480/110534 (11%)]\tAll Loss: 2.0096\tTriple Loss(1): 0.1812\tClassification Loss: 1.6472\n",
      "\n",
      "Test set: Average loss: 1.5827, Accuracy: 553/960 (58%)\n",
      "\n",
      "Train Epoch: 5 [12800/110534 (12%)]\tAll Loss: 1.4241\tTriple Loss(0): 0.0000\tClassification Loss: 1.4241\n",
      "Train Epoch: 5 [13120/110534 (12%)]\tAll Loss: 1.3893\tTriple Loss(0): 0.0000\tClassification Loss: 1.3893\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tAll Loss: 2.5202\tTriple Loss(1): 0.1951\tClassification Loss: 2.1300\n",
      "Train Epoch: 5 [13760/110534 (12%)]\tAll Loss: 3.0111\tTriple Loss(1): 0.6506\tClassification Loss: 1.7100\n",
      "Train Epoch: 5 [14080/110534 (13%)]\tAll Loss: 1.9223\tTriple Loss(1): 0.1785\tClassification Loss: 1.5652\n",
      "Train Epoch: 5 [14400/110534 (13%)]\tAll Loss: 2.2244\tTriple Loss(1): 0.0635\tClassification Loss: 2.0973\n",
      "Train Epoch: 5 [14720/110534 (13%)]\tAll Loss: 2.0267\tTriple Loss(1): 0.1697\tClassification Loss: 1.6873\n",
      "Train Epoch: 5 [15040/110534 (14%)]\tAll Loss: 1.7934\tTriple Loss(1): 0.1364\tClassification Loss: 1.5206\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tAll Loss: 1.7923\tTriple Loss(1): 0.1004\tClassification Loss: 1.5915\n",
      "Train Epoch: 5 [15680/110534 (14%)]\tAll Loss: 1.8293\tTriple Loss(1): 0.2257\tClassification Loss: 1.3779\n",
      "\n",
      "Test set: Average loss: 1.5820, Accuracy: 551/960 (57%)\n",
      "\n",
      "Train Epoch: 5 [16000/110534 (14%)]\tAll Loss: 3.5146\tTriple Loss(0): 0.9210\tClassification Loss: 1.6725\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_5_500.pth.tar\n",
      "Train Epoch: 5 [16320/110534 (15%)]\tAll Loss: 1.8803\tTriple Loss(1): 0.2743\tClassification Loss: 1.3317\n",
      "Train Epoch: 5 [16640/110534 (15%)]\tAll Loss: 2.0335\tTriple Loss(1): 0.1485\tClassification Loss: 1.7366\n",
      "Train Epoch: 5 [16960/110534 (15%)]\tAll Loss: 2.3462\tTriple Loss(1): 0.1616\tClassification Loss: 2.0230\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tAll Loss: 1.7425\tTriple Loss(1): 0.1579\tClassification Loss: 1.4266\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WQN1zCcsuzua",
    "colab_type": "code",
    "outputId": "56702ff0-f0a8-40bb-f7f0-d2a4bba654e0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# From scratch. FREEZE = False. LR=0.01\n",
    "! python train.py"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  \"please use transforms.RandomResizedCrop instead.\")\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "\n",
      "Test set: Average loss: 3.4774, Accuracy: 58/960 (6%)\n",
      "\n",
      "Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 6.3927\tTriple Loss(0): 1.5768\tClassification Loss: 3.2391\n",
      "Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 7.2953\tTriple Loss(0): 1.1788\tClassification Loss: 4.9378\n",
      "Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 4.4997\tTriple Loss(1): 0.8896\tClassification Loss: 2.7204\n",
      "Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 2.4944\tTriple Loss(0): 0.0000\tClassification Loss: 2.4944\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 4.5276\tTriple Loss(1): 0.9104\tClassification Loss: 2.7068\n",
      "Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 4.2570\tTriple Loss(1): 0.7107\tClassification Loss: 2.8356\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 5.3400\tTriple Loss(0): 1.5198\tClassification Loss: 2.3005\n",
      "Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 2.5340\tTriple Loss(0): 0.0000\tClassification Loss: 2.5340\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 4.3180\tTriple Loss(1): 0.9147\tClassification Loss: 2.4887\n",
      "Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 4.2571\tTriple Loss(1): 0.9409\tClassification Loss: 2.3753\n",
      "\n",
      "Test set: Average loss: 2.6772, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 4.1243\tTriple Loss(1): 0.7547\tClassification Loss: 2.6149\n",
      "Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 4.4523\tTriple Loss(1): 0.9022\tClassification Loss: 2.6479\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 4.3290\tTriple Loss(1): 0.9194\tClassification Loss: 2.4903\n",
      "Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 6.0292\tTriple Loss(0): 1.6875\tClassification Loss: 2.6541\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 5.3994\tTriple Loss(0): 1.4073\tClassification Loss: 2.5848\n",
      "Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 4.2473\tTriple Loss(1): 0.9426\tClassification Loss: 2.3621\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 4.4257\tTriple Loss(1): 0.9199\tClassification Loss: 2.5860\n",
      "Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 5.1151\tTriple Loss(1): 0.9369\tClassification Loss: 3.2413\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 4.1016\tTriple Loss(1): 0.7977\tClassification Loss: 2.5062\n",
      "Train Epoch: 1 [6080/110534 (5%)]\tAll Loss: 3.8709\tTriple Loss(1): 0.7892\tClassification Loss: 2.2924\n",
      "\n",
      "Test set: Average loss: 2.6767, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 4.0436\tTriple Loss(1): 0.7719\tClassification Loss: 2.4999\n",
      "Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 3.9948\tTriple Loss(1): 0.7379\tClassification Loss: 2.5191\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 3.9228\tTriple Loss(1): 0.8700\tClassification Loss: 2.1828\n",
      "Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 4.3355\tTriple Loss(0): 0.8877\tClassification Loss: 2.5601\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 4.0670\tTriple Loss(1): 0.8836\tClassification Loss: 2.2997\n",
      "Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 4.2738\tTriple Loss(1): 0.9201\tClassification Loss: 2.4335\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 4.7312\tTriple Loss(0): 1.0168\tClassification Loss: 2.6977\n",
      "Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 4.3484\tTriple Loss(1): 0.9238\tClassification Loss: 2.5008\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 4.0268\tTriple Loss(1): 0.7372\tClassification Loss: 2.5524\n",
      "Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 7.4774\tTriple Loss(0): 2.4656\tClassification Loss: 2.5462\n",
      "\n",
      "Test set: Average loss: 2.8447, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 4.7577\tTriple Loss(1): 0.9924\tClassification Loss: 2.7729\n",
      "Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 4.8479\tTriple Loss(0): 1.1232\tClassification Loss: 2.6015\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 4.1653\tTriple Loss(1): 0.8588\tClassification Loss: 2.4477\n",
      "Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 4.1695\tTriple Loss(1): 0.7784\tClassification Loss: 2.6127\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 3.3668\tTriple Loss(1): 0.6423\tClassification Loss: 2.0822\n",
      "Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 4.0505\tTriple Loss(1): 0.7542\tClassification Loss: 2.5421\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 4.2915\tTriple Loss(1): 0.8644\tClassification Loss: 2.5628\n",
      "Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 4.5104\tTriple Loss(1): 0.9290\tClassification Loss: 2.6525\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 4.0403\tTriple Loss(1): 0.8604\tClassification Loss: 2.3194\n",
      "Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 4.4361\tTriple Loss(1): 0.9970\tClassification Loss: 2.4421\n",
      "\n",
      "Test set: Average loss: 2.6686, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 4.2094\tTriple Loss(1): 0.8399\tClassification Loss: 2.5296\n",
      "Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 4.4838\tTriple Loss(1): 1.0555\tClassification Loss: 2.3728\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 4.5072\tTriple Loss(1): 0.9801\tClassification Loss: 2.5470\n",
      "Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 3.4201\tTriple Loss(0): 0.4881\tClassification Loss: 2.4438\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 4.2018\tTriple Loss(1): 0.8508\tClassification Loss: 2.5002\n",
      "Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 4.2747\tTriple Loss(1): 0.9568\tClassification Loss: 2.3611\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 8.5445\tTriple Loss(0): 3.1487\tClassification Loss: 2.2470\n",
      "Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 4.1940\tTriple Loss(1): 0.9116\tClassification Loss: 2.3708\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 4.3293\tTriple Loss(1): 0.8798\tClassification Loss: 2.5697\n",
      "Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 4.8355\tTriple Loss(0): 1.0902\tClassification Loss: 2.6550\n",
      "\n",
      "Test set: Average loss: 2.6613, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 4.3279\tTriple Loss(1): 0.9446\tClassification Loss: 2.4386\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n",
      "Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 4.5907\tTriple Loss(1): 0.9076\tClassification Loss: 2.7754\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 5.5341\tTriple Loss(0): 1.4708\tClassification Loss: 2.5926\n",
      "Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 2.4883\tTriple Loss(0): 0.0000\tClassification Loss: 2.4883\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 4.1963\tTriple Loss(1): 0.8973\tClassification Loss: 2.4017\n",
      "Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 3.9172\tTriple Loss(1): 0.6945\tClassification Loss: 2.5282\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 4.1890\tTriple Loss(1): 0.8385\tClassification Loss: 2.5119\n",
      "Train Epoch: 1 [18240/110534 (16%)]\tAll Loss: 4.4191\tTriple Loss(1): 0.9389\tClassification Loss: 2.5413\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 4.6381\tTriple Loss(1): 0.9491\tClassification Loss: 2.7399\n",
      "Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 4.3851\tTriple Loss(1): 0.8349\tClassification Loss: 2.7153\n",
      "\n",
      "Test set: Average loss: 2.6528, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 4.4306\tTriple Loss(1): 0.9319\tClassification Loss: 2.5668\n",
      "Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 2.5804\tTriple Loss(0): 0.0000\tClassification Loss: 2.5804\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 4.1453\tTriple Loss(1): 0.8753\tClassification Loss: 2.3946\n",
      "Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 4.0131\tTriple Loss(1): 0.8551\tClassification Loss: 2.3028\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 4.2994\tTriple Loss(1): 0.8849\tClassification Loss: 2.5296\n",
      "Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 4.4528\tTriple Loss(1): 0.9643\tClassification Loss: 2.5241\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 4.5310\tTriple Loss(1): 0.8650\tClassification Loss: 2.8009\n",
      "Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 4.4944\tTriple Loss(1): 0.9457\tClassification Loss: 2.6029\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 4.1482\tTriple Loss(1): 0.7665\tClassification Loss: 2.6153\n",
      "Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 4.1255\tTriple Loss(1): 0.7416\tClassification Loss: 2.6424\n",
      "\n",
      "Test set: Average loss: 2.6748, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 4.1520\tTriple Loss(1): 0.7861\tClassification Loss: 2.5798\n",
      "Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 4.3277\tTriple Loss(1): 0.8814\tClassification Loss: 2.5649\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 4.1481\tTriple Loss(1): 0.8958\tClassification Loss: 2.3566\n",
      "Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 3.9202\tTriple Loss(1): 0.7689\tClassification Loss: 2.3824\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 8.2987\tTriple Loss(0): 2.8458\tClassification Loss: 2.6071\n",
      "Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 4.5714\tTriple Loss(1): 0.9867\tClassification Loss: 2.5980\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 4.1334\tTriple Loss(1): 0.7681\tClassification Loss: 2.5972\n",
      "Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 3.8031\tTriple Loss(1): 0.7846\tClassification Loss: 2.2338\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 3.7348\tTriple Loss(1): 0.6410\tClassification Loss: 2.4527\n",
      "Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 4.1063\tTriple Loss(1): 0.8097\tClassification Loss: 2.4869\n",
      "\n",
      "Test set: Average loss: 2.6601, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 3.9465\tTriple Loss(1): 0.7276\tClassification Loss: 2.4914\n",
      "Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 5.7636\tTriple Loss(0): 1.5520\tClassification Loss: 2.6596\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 3.8178\tTriple Loss(1): 0.8391\tClassification Loss: 2.1396\n",
      "Train Epoch: 1 [26560/110534 (24%)]\tAll Loss: 3.7539\tTriple Loss(1): 0.5853\tClassification Loss: 2.5832\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tAll Loss: 3.8732\tTriple Loss(1): 0.6777\tClassification Loss: 2.5177\n",
      "Train Epoch: 1 [27200/110534 (25%)]\tAll Loss: 4.8000\tTriple Loss(0): 1.1752\tClassification Loss: 2.4496\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tAll Loss: 4.1132\tTriple Loss(1): 0.7470\tClassification Loss: 2.6192\n",
      "Train Epoch: 1 [27840/110534 (25%)]\tAll Loss: 4.1018\tTriple Loss(1): 0.8068\tClassification Loss: 2.4883\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tAll Loss: 7.0002\tTriple Loss(0): 2.2984\tClassification Loss: 2.4034\n",
      "Train Epoch: 1 [28480/110534 (26%)]\tAll Loss: 3.9736\tTriple Loss(1): 0.7858\tClassification Loss: 2.4020\n",
      "\n",
      "Test set: Average loss: 2.6639, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tAll Loss: 3.7584\tTriple Loss(1): 0.6495\tClassification Loss: 2.4594\n",
      "Train Epoch: 1 [29120/110534 (26%)]\tAll Loss: 4.0681\tTriple Loss(1): 0.6573\tClassification Loss: 2.7535\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tAll Loss: 4.4222\tTriple Loss(1): 0.8457\tClassification Loss: 2.7307\n",
      "Train Epoch: 1 [29760/110534 (27%)]\tAll Loss: 4.9471\tTriple Loss(0): 0.9854\tClassification Loss: 2.9762\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tAll Loss: 3.7203\tTriple Loss(1): 0.6304\tClassification Loss: 2.4595\n",
      "Train Epoch: 1 [30400/110534 (27%)]\tAll Loss: 3.6259\tTriple Loss(1): 0.5765\tClassification Loss: 2.4729\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tAll Loss: 4.2980\tTriple Loss(1): 0.8256\tClassification Loss: 2.6469\n",
      "Train Epoch: 1 [31040/110534 (28%)]\tAll Loss: 3.6839\tTriple Loss(1): 0.6456\tClassification Loss: 2.3928\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tAll Loss: 4.0375\tTriple Loss(1): 0.8145\tClassification Loss: 2.4084\n",
      "Train Epoch: 1 [31680/110534 (29%)]\tAll Loss: 4.4286\tTriple Loss(1): 0.8084\tClassification Loss: 2.8117\n",
      "\n",
      "Test set: Average loss: 2.6668, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tAll Loss: 3.9235\tTriple Loss(1): 0.6228\tClassification Loss: 2.6779\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n",
      "Train Epoch: 1 [32320/110534 (29%)]\tAll Loss: 4.0697\tTriple Loss(1): 0.8481\tClassification Loss: 2.3736\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tAll Loss: 3.7210\tTriple Loss(1): 0.6839\tClassification Loss: 2.3531\n",
      "Train Epoch: 1 [32960/110534 (30%)]\tAll Loss: 3.4258\tTriple Loss(0): 0.4476\tClassification Loss: 2.5306\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tAll Loss: 4.2453\tTriple Loss(1): 0.8276\tClassification Loss: 2.5901\n",
      "Train Epoch: 1 [33600/110534 (30%)]\tAll Loss: 3.7739\tTriple Loss(1): 0.7161\tClassification Loss: 2.3417\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tAll Loss: 3.8801\tTriple Loss(1): 0.6531\tClassification Loss: 2.5739\n",
      "Train Epoch: 1 [34240/110534 (31%)]\tAll Loss: 3.6513\tTriple Loss(1): 0.7075\tClassification Loss: 2.2363\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tAll Loss: 3.6162\tTriple Loss(0): 0.5754\tClassification Loss: 2.4654\n",
      "Train Epoch: 1 [34880/110534 (32%)]\tAll Loss: 4.5402\tTriple Loss(0): 0.9832\tClassification Loss: 2.5738\n",
      "\n",
      "Test set: Average loss: 2.6561, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tAll Loss: 3.9922\tTriple Loss(1): 0.6541\tClassification Loss: 2.6840\n",
      "Train Epoch: 1 [35520/110534 (32%)]\tAll Loss: 3.9051\tTriple Loss(0): 0.7399\tClassification Loss: 2.4252\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tAll Loss: 4.0492\tTriple Loss(1): 0.8603\tClassification Loss: 2.3287\n",
      "Train Epoch: 1 [36160/110534 (33%)]\tAll Loss: 4.4272\tTriple Loss(0): 0.9735\tClassification Loss: 2.4803\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tAll Loss: 4.1439\tTriple Loss(1): 0.8121\tClassification Loss: 2.5197\n",
      "Train Epoch: 1 [36800/110534 (33%)]\tAll Loss: 2.7120\tTriple Loss(0): 0.0000\tClassification Loss: 2.7120\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tAll Loss: 4.2137\tTriple Loss(1): 0.7058\tClassification Loss: 2.8021\n",
      "Train Epoch: 1 [37440/110534 (34%)]\tAll Loss: 4.2286\tTriple Loss(1): 0.7150\tClassification Loss: 2.7986\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tAll Loss: 3.9963\tTriple Loss(1): 0.6717\tClassification Loss: 2.6529\n",
      "Train Epoch: 1 [38080/110534 (34%)]\tAll Loss: 4.2253\tTriple Loss(1): 0.8379\tClassification Loss: 2.5496\n",
      "\n",
      "Test set: Average loss: 2.6629, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tAll Loss: 3.7411\tTriple Loss(1): 0.6992\tClassification Loss: 2.3426\n",
      "Train Epoch: 1 [38720/110534 (35%)]\tAll Loss: 3.3764\tTriple Loss(1): 0.4097\tClassification Loss: 2.5570\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tAll Loss: 4.1509\tTriple Loss(1): 0.8638\tClassification Loss: 2.4232\n",
      "Train Epoch: 1 [39360/110534 (36%)]\tAll Loss: 4.2628\tTriple Loss(1): 0.9046\tClassification Loss: 2.4536\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tAll Loss: 4.2391\tTriple Loss(1): 0.7547\tClassification Loss: 2.7298\n",
      "Train Epoch: 1 [40000/110534 (36%)]\tAll Loss: 2.6321\tTriple Loss(0): 0.0000\tClassification Loss: 2.6321\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tAll Loss: 3.8283\tTriple Loss(1): 0.7154\tClassification Loss: 2.3975\n",
      "Train Epoch: 1 [40640/110534 (37%)]\tAll Loss: 3.8307\tTriple Loss(1): 0.7573\tClassification Loss: 2.3162\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tAll Loss: 3.9246\tTriple Loss(1): 0.7688\tClassification Loss: 2.3869\n",
      "Train Epoch: 1 [41280/110534 (37%)]\tAll Loss: 4.4439\tTriple Loss(1): 0.8794\tClassification Loss: 2.6850\n",
      "\n",
      "Test set: Average loss: 2.7437, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tAll Loss: 3.8282\tTriple Loss(1): 0.7124\tClassification Loss: 2.4033\n",
      "Train Epoch: 1 [41920/110534 (38%)]\tAll Loss: 3.9216\tTriple Loss(1): 0.7975\tClassification Loss: 2.3266\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tAll Loss: 3.9059\tTriple Loss(1): 0.7218\tClassification Loss: 2.4622\n",
      "Train Epoch: 1 [42560/110534 (38%)]\tAll Loss: 4.2621\tTriple Loss(1): 0.8165\tClassification Loss: 2.6290\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tAll Loss: 3.9384\tTriple Loss(1): 0.6800\tClassification Loss: 2.5784\n",
      "Train Epoch: 1 [43200/110534 (39%)]\tAll Loss: 4.0537\tTriple Loss(1): 0.7590\tClassification Loss: 2.5357\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tAll Loss: 3.8508\tTriple Loss(1): 0.6374\tClassification Loss: 2.5761\n",
      "Train Epoch: 1 [43840/110534 (40%)]\tAll Loss: 3.9722\tTriple Loss(1): 0.7587\tClassification Loss: 2.4548\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tAll Loss: 4.4477\tTriple Loss(1): 0.9011\tClassification Loss: 2.6455\n",
      "Train Epoch: 1 [44480/110534 (40%)]\tAll Loss: 3.9055\tTriple Loss(1): 0.8302\tClassification Loss: 2.2450\n",
      "\n",
      "Test set: Average loss: 2.6481, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tAll Loss: 4.1332\tTriple Loss(1): 0.7882\tClassification Loss: 2.5567\n",
      "Train Epoch: 1 [45120/110534 (41%)]\tAll Loss: 3.7593\tTriple Loss(1): 0.7328\tClassification Loss: 2.2937\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tAll Loss: 3.8391\tTriple Loss(1): 0.7869\tClassification Loss: 2.2652\n",
      "Train Epoch: 1 [45760/110534 (41%)]\tAll Loss: 4.1812\tTriple Loss(1): 0.8330\tClassification Loss: 2.5152\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tAll Loss: 4.0878\tTriple Loss(1): 0.8500\tClassification Loss: 2.3877\n",
      "Train Epoch: 1 [46400/110534 (42%)]\tAll Loss: 4.2643\tTriple Loss(1): 0.9003\tClassification Loss: 2.4637\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tAll Loss: 3.5854\tTriple Loss(1): 0.7438\tClassification Loss: 2.0979\n",
      "Train Epoch: 1 [47040/110534 (43%)]\tAll Loss: 3.7833\tTriple Loss(1): 0.6576\tClassification Loss: 2.4681\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tAll Loss: 3.9693\tTriple Loss(1): 0.8942\tClassification Loss: 2.1810\n",
      "Train Epoch: 1 [47680/110534 (43%)]\tAll Loss: 3.7576\tTriple Loss(1): 0.5797\tClassification Loss: 2.5983\n",
      "\n",
      "Test set: Average loss: 2.6315, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tAll Loss: 8.5209\tTriple Loss(0): 3.1098\tClassification Loss: 2.3013\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n",
      "Train Epoch: 1 [48320/110534 (44%)]\tAll Loss: 4.0806\tTriple Loss(1): 0.7510\tClassification Loss: 2.5787\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tAll Loss: 3.6743\tTriple Loss(1): 0.6843\tClassification Loss: 2.3057\n",
      "Train Epoch: 1 [48960/110534 (44%)]\tAll Loss: 4.1766\tTriple Loss(1): 0.9917\tClassification Loss: 2.1933\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tAll Loss: 4.1863\tTriple Loss(1): 0.8764\tClassification Loss: 2.4335\n",
      "Train Epoch: 1 [49600/110534 (45%)]\tAll Loss: 4.3330\tTriple Loss(1): 0.7780\tClassification Loss: 2.7771\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tAll Loss: 4.0618\tTriple Loss(1): 0.7741\tClassification Loss: 2.5135\n",
      "Train Epoch: 1 [50240/110534 (45%)]\tAll Loss: 3.6110\tTriple Loss(1): 0.5645\tClassification Loss: 2.4819\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tAll Loss: 4.0616\tTriple Loss(1): 0.8380\tClassification Loss: 2.3856\n",
      "Train Epoch: 1 [50880/110534 (46%)]\tAll Loss: 4.4979\tTriple Loss(0): 0.9976\tClassification Loss: 2.5027\n",
      "\n",
      "Test set: Average loss: 2.6499, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tAll Loss: 4.2981\tTriple Loss(1): 0.8312\tClassification Loss: 2.6357\n",
      "Train Epoch: 1 [51520/110534 (47%)]\tAll Loss: 12.6927\tTriple Loss(0): 5.1161\tClassification Loss: 2.4605\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tAll Loss: 4.2003\tTriple Loss(1): 0.8325\tClassification Loss: 2.5353\n",
      "Train Epoch: 1 [52160/110534 (47%)]\tAll Loss: 3.9495\tTriple Loss(1): 0.7075\tClassification Loss: 2.5345\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tAll Loss: 4.1363\tTriple Loss(1): 0.8198\tClassification Loss: 2.4968\n",
      "Train Epoch: 1 [52800/110534 (48%)]\tAll Loss: 3.8626\tTriple Loss(1): 0.7561\tClassification Loss: 2.3505\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tAll Loss: 4.5070\tTriple Loss(1): 1.0835\tClassification Loss: 2.3401\n",
      "Train Epoch: 1 [53440/110534 (48%)]\tAll Loss: 3.9245\tTriple Loss(1): 0.7964\tClassification Loss: 2.3317\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tAll Loss: 3.6909\tTriple Loss(1): 0.5869\tClassification Loss: 2.5171\n",
      "Train Epoch: 1 [54080/110534 (49%)]\tAll Loss: 3.9717\tTriple Loss(1): 0.8395\tClassification Loss: 2.2927\n",
      "\n",
      "Test set: Average loss: 2.7334, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tAll Loss: 5.2474\tTriple Loss(0): 1.4444\tClassification Loss: 2.3587\n",
      "Train Epoch: 1 [54720/110534 (49%)]\tAll Loss: 4.4407\tTriple Loss(1): 0.8215\tClassification Loss: 2.7978\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tAll Loss: 3.7474\tTriple Loss(1): 0.6906\tClassification Loss: 2.3662\n",
      "Train Epoch: 1 [55360/110534 (50%)]\tAll Loss: 4.1838\tTriple Loss(0): 0.7159\tClassification Loss: 2.7520\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tAll Loss: 2.4238\tTriple Loss(0): 0.0000\tClassification Loss: 2.4238\n",
      "Train Epoch: 1 [56000/110534 (51%)]\tAll Loss: 3.2547\tTriple Loss(1): 0.4922\tClassification Loss: 2.2703\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tAll Loss: 4.0683\tTriple Loss(1): 0.8153\tClassification Loss: 2.4377\n",
      "Train Epoch: 1 [56640/110534 (51%)]\tAll Loss: 4.3793\tTriple Loss(1): 0.8783\tClassification Loss: 2.6226\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tAll Loss: 4.1905\tTriple Loss(1): 0.7679\tClassification Loss: 2.6548\n",
      "Train Epoch: 1 [57280/110534 (52%)]\tAll Loss: 3.9424\tTriple Loss(1): 0.8441\tClassification Loss: 2.2543\n",
      "\n",
      "Test set: Average loss: 2.6730, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tAll Loss: 3.8567\tTriple Loss(1): 0.7495\tClassification Loss: 2.3577\n",
      "Train Epoch: 1 [57920/110534 (52%)]\tAll Loss: 3.5804\tTriple Loss(1): 0.6242\tClassification Loss: 2.3320\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tAll Loss: 6.8750\tTriple Loss(0): 2.1449\tClassification Loss: 2.5852\n",
      "Train Epoch: 1 [58560/110534 (53%)]\tAll Loss: 6.0188\tTriple Loss(0): 1.8324\tClassification Loss: 2.3540\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tAll Loss: 3.9976\tTriple Loss(1): 0.8488\tClassification Loss: 2.2999\n",
      "Train Epoch: 1 [59200/110534 (54%)]\tAll Loss: 4.1748\tTriple Loss(1): 0.7894\tClassification Loss: 2.5960\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tAll Loss: 3.9409\tTriple Loss(1): 0.7467\tClassification Loss: 2.4474\n",
      "Train Epoch: 1 [59840/110534 (54%)]\tAll Loss: 4.1466\tTriple Loss(1): 0.7673\tClassification Loss: 2.6120\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tAll Loss: 9.7092\tTriple Loss(0): 3.5342\tClassification Loss: 2.6408\n",
      "Train Epoch: 1 [60480/110534 (55%)]\tAll Loss: 4.5505\tTriple Loss(1): 0.9712\tClassification Loss: 2.6081\n",
      "\n",
      "Test set: Average loss: 2.7275, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tAll Loss: 4.0273\tTriple Loss(1): 0.6688\tClassification Loss: 2.6897\n",
      "Train Epoch: 1 [61120/110534 (55%)]\tAll Loss: 2.5612\tTriple Loss(0): 0.0000\tClassification Loss: 2.5612\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tAll Loss: 6.3996\tTriple Loss(0): 1.9380\tClassification Loss: 2.5237\n",
      "Train Epoch: 1 [61760/110534 (56%)]\tAll Loss: 3.9192\tTriple Loss(1): 0.5812\tClassification Loss: 2.7568\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tAll Loss: 4.1631\tTriple Loss(1): 0.8311\tClassification Loss: 2.5008\n",
      "Train Epoch: 1 [62400/110534 (56%)]\tAll Loss: 3.9093\tTriple Loss(1): 0.6345\tClassification Loss: 2.6403\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tAll Loss: 3.7834\tTriple Loss(1): 0.6801\tClassification Loss: 2.4232\n",
      "Train Epoch: 1 [63040/110534 (57%)]\tAll Loss: 7.7268\tTriple Loss(0): 2.5676\tClassification Loss: 2.5916\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tAll Loss: 3.9846\tTriple Loss(1): 0.7991\tClassification Loss: 2.3863\n",
      "Train Epoch: 1 [63680/110534 (58%)]\tAll Loss: 6.2936\tTriple Loss(0): 2.0286\tClassification Loss: 2.2364\n",
      "\n",
      "Test set: Average loss: 2.6635, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tAll Loss: 3.9434\tTriple Loss(1): 0.7060\tClassification Loss: 2.5315\n",
      "Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2000.pth.tar\n",
      "Train Epoch: 1 [64320/110534 (58%)]\tAll Loss: 4.6048\tTriple Loss(0): 1.0081\tClassification Loss: 2.5886\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tAll Loss: 3.8410\tTriple Loss(1): 0.7692\tClassification Loss: 2.3027\n",
      "Train Epoch: 1 [64960/110534 (59%)]\tAll Loss: 4.0947\tTriple Loss(1): 0.7699\tClassification Loss: 2.5548\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tAll Loss: 3.8123\tTriple Loss(1): 0.6383\tClassification Loss: 2.5357\n",
      "Train Epoch: 1 [65600/110534 (59%)]\tAll Loss: 6.1475\tTriple Loss(0): 1.8844\tClassification Loss: 2.3786\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tAll Loss: 4.0878\tTriple Loss(1): 0.8280\tClassification Loss: 2.4319\n",
      "Train Epoch: 1 [66240/110534 (60%)]\tAll Loss: 3.9971\tTriple Loss(1): 0.8480\tClassification Loss: 2.3010\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tAll Loss: 4.1392\tTriple Loss(1): 0.9196\tClassification Loss: 2.3000\n",
      "Train Epoch: 1 [66880/110534 (60%)]\tAll Loss: 3.9952\tTriple Loss(1): 0.6688\tClassification Loss: 2.6576\n",
      "\n",
      "Test set: Average loss: 2.7002, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tAll Loss: 4.0636\tTriple Loss(1): 0.8800\tClassification Loss: 2.3036\n",
      "Train Epoch: 1 [67520/110534 (61%)]\tAll Loss: 4.3451\tTriple Loss(1): 0.8504\tClassification Loss: 2.6442\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tAll Loss: 3.9375\tTriple Loss(1): 0.6755\tClassification Loss: 2.5865\n",
      "Train Epoch: 1 [68160/110534 (62%)]\tAll Loss: 4.3661\tTriple Loss(1): 0.8514\tClassification Loss: 2.6634\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tAll Loss: 4.2320\tTriple Loss(1): 0.8666\tClassification Loss: 2.4987\n",
      "Train Epoch: 1 [68800/110534 (62%)]\tAll Loss: 3.7889\tTriple Loss(1): 0.7018\tClassification Loss: 2.3853\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tAll Loss: 4.2493\tTriple Loss(1): 0.8332\tClassification Loss: 2.5828\n",
      "Train Epoch: 1 [69440/110534 (63%)]\tAll Loss: 4.3882\tTriple Loss(1): 0.9122\tClassification Loss: 2.5638\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tAll Loss: 4.1909\tTriple Loss(1): 0.7531\tClassification Loss: 2.6846\n",
      "Train Epoch: 1 [70080/110534 (63%)]\tAll Loss: 3.6834\tTriple Loss(1): 0.7582\tClassification Loss: 2.1669\n",
      "\n",
      "Test set: Average loss: 2.6748, Accuracy: 244/960 (25%)\n",
      "\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tAll Loss: 3.7322\tTriple Loss(1): 0.8063\tClassification Loss: 2.1195\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/110534 (0%)]\tClassification Loss: 3.2830\r\n",
      "train.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.2866, Accuracy: 881/42368 (2%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/110534 (1%)]\tClassification Loss: 2.7224\r\n",
      "Train Epoch: 1 [1280/110534 (1%)]\tClassification Loss: 2.7177\r\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tClassification Loss: 2.3749\r\n",
      "Train Epoch: 1 [2560/110534 (2%)]\tClassification Loss: 2.5729\r\n",
      "Train Epoch: 1 [3200/110534 (3%)]\tClassification Loss: 2.3552\r\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tClassification Loss: 2.4252\r\n",
      "Train Epoch: 1 [4480/110534 (4%)]\tClassification Loss: 2.3285\r\n",
      "Train Epoch: 1 [5120/110534 (5%)]\tClassification Loss: 2.1731\r\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tClassification Loss: 2.2027\r\n",
      "Train Epoch: 1 [6400/110534 (6%)]\tClassification Loss: 2.1161\r\n",
      "Train Epoch: 1 [7040/110534 (6%)]\tClassification Loss: 2.1546\r\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tClassification Loss: 2.2371\r\n",
      "Train Epoch: 1 [8320/110534 (8%)]\tClassification Loss: 2.1920\r\n",
      "Train Epoch: 1 [8960/110534 (8%)]\tClassification Loss: 2.2503\r\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tClassification Loss: 1.9809\r\n",
      "Train Epoch: 1 [10240/110534 (9%)]\tClassification Loss: 2.0242\r\n",
      "Train Epoch: 1 [10880/110534 (10%)]\tClassification Loss: 1.9162\r\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tClassification Loss: 2.3605\r\n",
      "Train Epoch: 1 [12160/110534 (11%)]\tClassification Loss: 1.8797\r\n",
      "Train Epoch: 1 [12800/110534 (12%)]\tClassification Loss: 2.0401\r\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tClassification Loss: 1.9739\r\n",
      "Train Epoch: 1 [14080/110534 (13%)]\tClassification Loss: 1.8512\r\n",
      "Train Epoch: 1 [14720/110534 (13%)]\tClassification Loss: 2.0219\r\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tClassification Loss: 1.8937\r\n",
      "Train Epoch: 1 [16000/110534 (14%)]\tClassification Loss: 2.1112\r\n",
      "Train Epoch: 1 [16640/110534 (15%)]\tClassification Loss: 1.8821\r\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tClassification Loss: 1.8996\r\n",
      "Train Epoch: 1 [17920/110534 (16%)]\tClassification Loss: 1.9671\r\n",
      "Train Epoch: 1 [18560/110534 (17%)]\tClassification Loss: 2.0795\r\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tClassification Loss: 1.9520\r\n",
      "Train Epoch: 1 [19840/110534 (18%)]\tClassification Loss: 1.9012\r\n",
      "Train Epoch: 1 [20480/110534 (19%)]\tClassification Loss: 2.0401\r\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tClassification Loss: 2.1207\r\n",
      "Train Epoch: 1 [21760/110534 (20%)]\tClassification Loss: 1.6625\r\n",
      "Train Epoch: 1 [22400/110534 (20%)]\tClassification Loss: 1.9000\r\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tClassification Loss: 1.5602\r\n",
      "Train Epoch: 1 [23680/110534 (21%)]\tClassification Loss: 2.1209\r\n",
      "Train Epoch: 1 [24320/110534 (22%)]\tClassification Loss: 1.5847\r\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tClassification Loss: 1.7797\r\n",
      "Train Epoch: 1 [25600/110534 (23%)]\tClassification Loss: 1.5993\r\n",
      "Train Epoch: 1 [26240/110534 (24%)]\tClassification Loss: 1.7593\r\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tClassification Loss: 2.2115\r\n",
      "Train Epoch: 1 [27520/110534 (25%)]\tClassification Loss: 1.8452\r\n",
      "Train Epoch: 1 [28160/110534 (25%)]\tClassification Loss: 1.9894\r\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tClassification Loss: 2.0064\r\n",
      "Train Epoch: 1 [29440/110534 (27%)]\tClassification Loss: 1.8556\r\n",
      "Train Epoch: 1 [30080/110534 (27%)]\tClassification Loss: 1.9429\r\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tClassification Loss: 1.9002\r\n",
      "Train Epoch: 1 [31360/110534 (28%)]\tClassification Loss: 1.8466\r\n",
      "Train Epoch: 1 [32000/110534 (29%)]\tClassification Loss: 1.6756\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tClassification Loss: 1.7005\r\n",
      "Train Epoch: 1 [33280/110534 (30%)]\tClassification Loss: 1.8853\r\n",
      "Train Epoch: 1 [33920/110534 (31%)]\tClassification Loss: 1.8128\r\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tClassification Loss: 1.8939\r\n",
      "Train Epoch: 1 [35200/110534 (32%)]\tClassification Loss: 1.7203\r\n",
      "Train Epoch: 1 [35840/110534 (32%)]\tClassification Loss: 1.6295\r\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tClassification Loss: 1.7771\r\n",
      "Train Epoch: 1 [37120/110534 (34%)]\tClassification Loss: 1.9714\r\n",
      "Train Epoch: 1 [37760/110534 (34%)]\tClassification Loss: 2.2096\r\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tClassification Loss: 1.9548\r\n",
      "Train Epoch: 1 [39040/110534 (35%)]\tClassification Loss: 1.7486\r\n",
      "Train Epoch: 1 [39680/110534 (36%)]\tClassification Loss: 1.6933\r\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tClassification Loss: 1.9459\r\n",
      "Train Epoch: 1 [40960/110534 (37%)]\tClassification Loss: 1.6157\r\n",
      "Train Epoch: 1 [41600/110534 (38%)]\tClassification Loss: 1.7086\r\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tClassification Loss: 1.9263\r\n",
      "Train Epoch: 1 [42880/110534 (39%)]\tClassification Loss: 1.6883\r\n",
      "Train Epoch: 1 [43520/110534 (39%)]\tClassification Loss: 1.2982\r\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tClassification Loss: 1.6984\r\n",
      "Train Epoch: 1 [44800/110534 (41%)]\tClassification Loss: 1.8301\r\n",
      "Train Epoch: 1 [45440/110534 (41%)]\tClassification Loss: 1.7233\r\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 1 [46720/110534 (42%)]\tClassification Loss: 1.6845\r\n",
      "Train Epoch: 1 [47360/110534 (43%)]\tClassification Loss: 1.7421\r\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tClassification Loss: 1.7164\r\n",
      "Train Epoch: 1 [48640/110534 (44%)]\tClassification Loss: 1.5246\r\n",
      "Train Epoch: 1 [49280/110534 (45%)]\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tClassification Loss: 1.8754\r\n",
      "Train Epoch: 1 [50560/110534 (46%)]\tClassification Loss: 1.7875\r\n",
      "Train Epoch: 1 [51200/110534 (46%)]\tClassification Loss: 1.6970\r\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tClassification Loss: 1.7703\r\n",
      "Train Epoch: 1 [52480/110534 (47%)]\tClassification Loss: 1.7871\r\n",
      "Train Epoch: 1 [53120/110534 (48%)]\tClassification Loss: 1.5486\r\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tClassification Loss: 1.7035\r\n",
      "Train Epoch: 1 [54400/110534 (49%)]\tClassification Loss: 1.6885\r\n",
      "Train Epoch: 1 [55040/110534 (50%)]\tClassification Loss: 1.8707\r\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tClassification Loss: 1.8622\r\n",
      "Train Epoch: 1 [56320/110534 (51%)]\tClassification Loss: 1.6002\r\n",
      "Train Epoch: 1 [56960/110534 (52%)]\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tClassification Loss: 1.8526\r\n",
      "Train Epoch: 1 [58240/110534 (53%)]\tClassification Loss: 1.8116\r\n",
      "Train Epoch: 1 [58880/110534 (53%)]\tClassification Loss: 2.1013\r\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tClassification Loss: 1.5211\r\n",
      "Train Epoch: 1 [60160/110534 (54%)]\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 1 [60800/110534 (55%)]\tClassification Loss: 1.8971\r\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tClassification Loss: 1.8057\r\n",
      "Train Epoch: 1 [62080/110534 (56%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 1 [62720/110534 (57%)]\tClassification Loss: 1.9606\r\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tClassification Loss: 1.5219\r\n",
      "Train Epoch: 1 [64000/110534 (58%)]\tClassification Loss: 1.5380\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [64640/110534 (58%)]\tClassification Loss: 1.4068\r\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tClassification Loss: 1.7826\r\n",
      "Train Epoch: 1 [65920/110534 (60%)]\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 1 [66560/110534 (60%)]\tClassification Loss: 1.4876\r\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tClassification Loss: 1.5753\r\n",
      "Train Epoch: 1 [67840/110534 (61%)]\tClassification Loss: 1.9129\r\n",
      "Train Epoch: 1 [68480/110534 (62%)]\tClassification Loss: 1.7711\r\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tClassification Loss: 1.8383\r\n",
      "Train Epoch: 1 [69760/110534 (63%)]\tClassification Loss: 1.6934\r\n",
      "Train Epoch: 1 [70400/110534 (64%)]\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tClassification Loss: 1.9336\r\n",
      "Train Epoch: 1 [71680/110534 (65%)]\tClassification Loss: 1.7150\r\n",
      "Train Epoch: 1 [72320/110534 (65%)]\tClassification Loss: 1.6626\r\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tClassification Loss: 1.8084\r\n",
      "Train Epoch: 1 [73600/110534 (67%)]\tClassification Loss: 1.8502\r\n",
      "Train Epoch: 1 [74240/110534 (67%)]\tClassification Loss: 1.9545\r\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tClassification Loss: 1.4022\r\n",
      "Train Epoch: 1 [75520/110534 (68%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 1 [76160/110534 (69%)]\tClassification Loss: 1.4773\r\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tClassification Loss: 1.5353\r\n",
      "Train Epoch: 1 [77440/110534 (70%)]\tClassification Loss: 1.4291\r\n",
      "Train Epoch: 1 [78080/110534 (71%)]\tClassification Loss: 1.7648\r\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tClassification Loss: 1.6058\r\n",
      "Train Epoch: 1 [79360/110534 (72%)]\tClassification Loss: 1.4660\r\n",
      "Train Epoch: 1 [80000/110534 (72%)]\tClassification Loss: 1.5182\r\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tClassification Loss: 1.4907\r\n",
      "Train Epoch: 1 [81280/110534 (74%)]\tClassification Loss: 2.0744\r\n",
      "Train Epoch: 1 [81920/110534 (74%)]\tClassification Loss: 1.8504\r\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tClassification Loss: 2.0825\r\n",
      "Train Epoch: 1 [83200/110534 (75%)]\tClassification Loss: 1.6420\r\n",
      "Train Epoch: 1 [83840/110534 (76%)]\tClassification Loss: 2.0594\r\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 1 [85120/110534 (77%)]\tClassification Loss: 1.6581\r\n",
      "Train Epoch: 1 [85760/110534 (78%)]\tClassification Loss: 1.6371\r\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 1 [87040/110534 (79%)]\tClassification Loss: 1.5464\r\n",
      "Train Epoch: 1 [87680/110534 (79%)]\tClassification Loss: 1.4622\r\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tClassification Loss: 1.7205\r\n",
      "Train Epoch: 1 [88960/110534 (80%)]\tClassification Loss: 1.6689\r\n",
      "Train Epoch: 1 [89600/110534 (81%)]\tClassification Loss: 1.7918\r\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tClassification Loss: 1.7639\r\n",
      "Train Epoch: 1 [90880/110534 (82%)]\tClassification Loss: 1.6792\r\n",
      "Train Epoch: 1 [91520/110534 (83%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tClassification Loss: 1.4161\r\n",
      "Train Epoch: 1 [92800/110534 (84%)]\tClassification Loss: 1.5393\r\n",
      "Train Epoch: 1 [93440/110534 (85%)]\tClassification Loss: 1.8946\r\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 1 [94720/110534 (86%)]\tClassification Loss: 1.7000\r\n",
      "Train Epoch: 1 [95360/110534 (86%)]\tClassification Loss: 1.5858\r\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tClassification Loss: 1.6703\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [96640/110534 (87%)]\tClassification Loss: 1.4982\r\n",
      "Train Epoch: 1 [97280/110534 (88%)]\tClassification Loss: 1.2956\r\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tClassification Loss: 1.3405\r\n",
      "Train Epoch: 1 [98560/110534 (89%)]\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 1 [99200/110534 (90%)]\tClassification Loss: 1.7252\r\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tClassification Loss: 1.5546\r\n",
      "Train Epoch: 1 [100480/110534 (91%)]\tClassification Loss: 1.8996\r\n",
      "Train Epoch: 1 [101120/110534 (91%)]\tClassification Loss: 1.5926\r\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tClassification Loss: 1.6356\r\n",
      "Train Epoch: 1 [102400/110534 (93%)]\tClassification Loss: 1.5151\r\n",
      "Train Epoch: 1 [103040/110534 (93%)]\tClassification Loss: 1.6247\r\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tClassification Loss: 1.8755\r\n",
      "Train Epoch: 1 [104320/110534 (94%)]\tClassification Loss: 1.5259\r\n",
      "Train Epoch: 1 [104960/110534 (95%)]\tClassification Loss: 1.7966\r\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tClassification Loss: 1.6040\r\n",
      "Train Epoch: 1 [106240/110534 (96%)]\tClassification Loss: 1.3866\r\n",
      "Train Epoch: 1 [106880/110534 (97%)]\tClassification Loss: 1.9838\r\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tClassification Loss: 1.6532\r\n",
      "Train Epoch: 1 [108160/110534 (98%)]\tClassification Loss: 1.6593\r\n",
      "Train Epoch: 1 [108800/110534 (98%)]\tClassification Loss: 1.8018\r\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tClassification Loss: 1.6680\r\n",
      "Train Epoch: 1 [110080/110534 (100%)]\tClassification Loss: 1.5229\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/110534 (0%)]\tClassification Loss: 1.8750\r\n",
      "\r\n",
      "Test set: Average loss: 1.5445, Accuracy: 22083/42368 (52%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/110534 (1%)]\tClassification Loss: 1.4835\r\n",
      "Train Epoch: 2 [1280/110534 (1%)]\tClassification Loss: 2.1183\r\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tClassification Loss: 1.7282\r\n",
      "Train Epoch: 2 [2560/110534 (2%)]\tClassification Loss: 1.9031\r\n",
      "Train Epoch: 2 [3200/110534 (3%)]\tClassification Loss: 1.5386\r\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 2 [4480/110534 (4%)]\tClassification Loss: 1.6591\r\n",
      "Train Epoch: 2 [5120/110534 (5%)]\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tClassification Loss: 1.7117\r\n",
      "Train Epoch: 2 [6400/110534 (6%)]\tClassification Loss: 1.3224\r\n",
      "Train Epoch: 2 [7040/110534 (6%)]\tClassification Loss: 1.6015\r\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tClassification Loss: 1.7565\r\n",
      "Train Epoch: 2 [8320/110534 (8%)]\tClassification Loss: 1.7614\r\n",
      "Train Epoch: 2 [8960/110534 (8%)]\tClassification Loss: 1.9147\r\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tClassification Loss: 1.4126\r\n",
      "Train Epoch: 2 [10240/110534 (9%)]\tClassification Loss: 1.7070\r\n",
      "Train Epoch: 2 [10880/110534 (10%)]\tClassification Loss: 1.5839\r\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tClassification Loss: 1.9769\r\n",
      "Train Epoch: 2 [12160/110534 (11%)]\tClassification Loss: 1.5012\r\n",
      "Train Epoch: 2 [12800/110534 (12%)]\tClassification Loss: 1.5868\r\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tClassification Loss: 1.4642\r\n",
      "Train Epoch: 2 [14080/110534 (13%)]\tClassification Loss: 1.6337\r\n",
      "Train Epoch: 2 [14720/110534 (13%)]\tClassification Loss: 1.8021\r\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tClassification Loss: 1.5672\r\n",
      "Train Epoch: 2 [16000/110534 (14%)]\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 2 [16640/110534 (15%)]\tClassification Loss: 1.5980\r\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tClassification Loss: 1.7394\r\n",
      "Train Epoch: 2 [17920/110534 (16%)]\tClassification Loss: 1.7472\r\n",
      "Train Epoch: 2 [18560/110534 (17%)]\tClassification Loss: 1.7990\r\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tClassification Loss: 1.6011\r\n",
      "Train Epoch: 2 [19840/110534 (18%)]\tClassification Loss: 1.6455\r\n",
      "Train Epoch: 2 [20480/110534 (19%)]\tClassification Loss: 1.8375\r\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tClassification Loss: 1.6775\r\n",
      "Train Epoch: 2 [21760/110534 (20%)]\tClassification Loss: 1.3818\r\n",
      "Train Epoch: 2 [22400/110534 (20%)]\tClassification Loss: 1.7026\r\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tClassification Loss: 1.1058\r\n",
      "Train Epoch: 2 [23680/110534 (21%)]\tClassification Loss: 1.7887\r\n",
      "Train Epoch: 2 [24320/110534 (22%)]\tClassification Loss: 1.2859\r\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 2 [25600/110534 (23%)]\tClassification Loss: 1.5068\r\n",
      "Train Epoch: 2 [26240/110534 (24%)]\tClassification Loss: 1.5864\r\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tClassification Loss: 1.8634\r\n",
      "Train Epoch: 2 [27520/110534 (25%)]\tClassification Loss: 1.5427\r\n",
      "Train Epoch: 2 [28160/110534 (25%)]\tClassification Loss: 1.9200\r\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tClassification Loss: 1.9742\r\n",
      "Train Epoch: 2 [29440/110534 (27%)]\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 2 [30080/110534 (27%)]\tClassification Loss: 1.7920\r\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tClassification Loss: 1.5552\r\n",
      "Train Epoch: 2 [31360/110534 (28%)]\tClassification Loss: 1.6268\r\n",
      "Train Epoch: 2 [32000/110534 (29%)]\tClassification Loss: 1.5518\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_500.pth.tar\r\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tClassification Loss: 1.3488\r\n",
      "Train Epoch: 2 [33280/110534 (30%)]\tClassification Loss: 1.6518\r\n",
      "Train Epoch: 2 [33920/110534 (31%)]\tClassification Loss: 1.6350\r\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tClassification Loss: 1.6605\r\n",
      "Train Epoch: 2 [35200/110534 (32%)]\tClassification Loss: 1.6797\r\n",
      "Train Epoch: 2 [35840/110534 (32%)]\tClassification Loss: 1.3802\r\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tClassification Loss: 1.5710\r\n",
      "Train Epoch: 2 [37120/110534 (34%)]\tClassification Loss: 1.7778\r\n",
      "Train Epoch: 2 [37760/110534 (34%)]\tClassification Loss: 2.0085\r\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tClassification Loss: 1.7027\r\n",
      "Train Epoch: 2 [39040/110534 (35%)]\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 2 [39680/110534 (36%)]\tClassification Loss: 1.6150\r\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tClassification Loss: 1.8060\r\n",
      "Train Epoch: 2 [40960/110534 (37%)]\tClassification Loss: 1.4558\r\n",
      "Train Epoch: 2 [41600/110534 (38%)]\tClassification Loss: 1.5027\r\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tClassification Loss: 1.6026\r\n",
      "Train Epoch: 2 [42880/110534 (39%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 2 [43520/110534 (39%)]\tClassification Loss: 1.3493\r\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tClassification Loss: 1.4719\r\n",
      "Train Epoch: 2 [44800/110534 (41%)]\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 2 [45440/110534 (41%)]\tClassification Loss: 1.7248\r\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tClassification Loss: 1.3953\r\n",
      "Train Epoch: 2 [46720/110534 (42%)]\tClassification Loss: 1.5037\r\n",
      "Train Epoch: 2 [47360/110534 (43%)]\tClassification Loss: 1.5299\r\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tClassification Loss: 1.7093\r\n",
      "Train Epoch: 2 [48640/110534 (44%)]\tClassification Loss: 1.3959\r\n",
      "Train Epoch: 2 [49280/110534 (45%)]\tClassification Loss: 1.5755\r\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tClassification Loss: 1.8094\r\n",
      "Train Epoch: 2 [50560/110534 (46%)]\tClassification Loss: 1.5690\r\n",
      "Train Epoch: 2 [51200/110534 (46%)]\tClassification Loss: 1.3979\r\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tClassification Loss: 1.6206\r\n",
      "Train Epoch: 2 [52480/110534 (47%)]\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 2 [53120/110534 (48%)]\tClassification Loss: 1.4135\r\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tClassification Loss: 1.6985\r\n",
      "Train Epoch: 2 [54400/110534 (49%)]\tClassification Loss: 1.5548\r\n",
      "Train Epoch: 2 [55040/110534 (50%)]\tClassification Loss: 1.9448\r\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tClassification Loss: 1.6919\r\n",
      "Train Epoch: 2 [56320/110534 (51%)]\tClassification Loss: 1.5949\r\n",
      "Train Epoch: 2 [56960/110534 (52%)]\tClassification Loss: 1.6244\r\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tClassification Loss: 1.8393\r\n",
      "Train Epoch: 2 [58240/110534 (53%)]\tClassification Loss: 1.6792\r\n",
      "Train Epoch: 2 [58880/110534 (53%)]\tClassification Loss: 1.8839\r\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tClassification Loss: 1.3625\r\n",
      "Train Epoch: 2 [60160/110534 (54%)]\tClassification Loss: 1.6554\r\n",
      "Train Epoch: 2 [60800/110534 (55%)]\tClassification Loss: 1.7989\r\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tClassification Loss: 1.7864\r\n",
      "Train Epoch: 2 [62080/110534 (56%)]\tClassification Loss: 1.5952\r\n",
      "Train Epoch: 2 [62720/110534 (57%)]\tClassification Loss: 1.7226\r\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tClassification Loss: 1.4735\r\n",
      "Train Epoch: 2 [64000/110534 (58%)]\tClassification Loss: 1.5559\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1000.pth.tar\r\n",
      "Train Epoch: 2 [64640/110534 (58%)]\tClassification Loss: 1.3295\r\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tClassification Loss: 1.5663\r\n",
      "Train Epoch: 2 [65920/110534 (60%)]\tClassification Loss: 1.6064\r\n",
      "Train Epoch: 2 [66560/110534 (60%)]\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 2 [67840/110534 (61%)]\tClassification Loss: 1.9748\r\n",
      "Train Epoch: 2 [68480/110534 (62%)]\tClassification Loss: 1.7302\r\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tClassification Loss: 1.8534\r\n",
      "Train Epoch: 2 [69760/110534 (63%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 2 [70400/110534 (64%)]\tClassification Loss: 1.3202\r\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tClassification Loss: 1.7289\r\n",
      "Train Epoch: 2 [71680/110534 (65%)]\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 2 [72320/110534 (65%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tClassification Loss: 1.6902\r\n",
      "Train Epoch: 2 [73600/110534 (67%)]\tClassification Loss: 1.8841\r\n",
      "Train Epoch: 2 [74240/110534 (67%)]\tClassification Loss: 1.9036\r\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tClassification Loss: 1.3267\r\n",
      "Train Epoch: 2 [75520/110534 (68%)]\tClassification Loss: 1.5880\r\n",
      "Train Epoch: 2 [76160/110534 (69%)]\tClassification Loss: 1.4670\r\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tClassification Loss: 1.3185\r\n",
      "Train Epoch: 2 [77440/110534 (70%)]\tClassification Loss: 1.5026\r\n",
      "Train Epoch: 2 [78080/110534 (71%)]\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tClassification Loss: 1.4040\r\n",
      "Train Epoch: 2 [79360/110534 (72%)]\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 2 [80000/110534 (72%)]\tClassification Loss: 1.4437\r\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tClassification Loss: 1.4244\r\n",
      "Train Epoch: 2 [81280/110534 (74%)]\tClassification Loss: 1.9144\r\n",
      "Train Epoch: 2 [81920/110534 (74%)]\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tClassification Loss: 1.8580\r\n",
      "Train Epoch: 2 [83200/110534 (75%)]\tClassification Loss: 1.4726\r\n",
      "Train Epoch: 2 [83840/110534 (76%)]\tClassification Loss: 1.7308\r\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tClassification Loss: 1.7310\r\n",
      "Train Epoch: 2 [85120/110534 (77%)]\tClassification Loss: 1.5188\r\n",
      "Train Epoch: 2 [85760/110534 (78%)]\tClassification Loss: 1.5079\r\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tClassification Loss: 1.7041\r\n",
      "Train Epoch: 2 [87040/110534 (79%)]\tClassification Loss: 1.4377\r\n",
      "Train Epoch: 2 [87680/110534 (79%)]\tClassification Loss: 1.6201\r\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tClassification Loss: 1.6048\r\n",
      "Train Epoch: 2 [88960/110534 (80%)]\tClassification Loss: 1.6529\r\n",
      "Train Epoch: 2 [89600/110534 (81%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tClassification Loss: 1.8012\r\n",
      "Train Epoch: 2 [90880/110534 (82%)]\tClassification Loss: 1.7244\r\n",
      "Train Epoch: 2 [91520/110534 (83%)]\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tClassification Loss: 1.5007\r\n",
      "Train Epoch: 2 [92800/110534 (84%)]\tClassification Loss: 1.4762\r\n",
      "Train Epoch: 2 [93440/110534 (85%)]\tClassification Loss: 1.8461\r\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 2 [94720/110534 (86%)]\tClassification Loss: 1.7513\r\n",
      "Train Epoch: 2 [95360/110534 (86%)]\tClassification Loss: 1.4492\r\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tClassification Loss: 1.5178\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [96640/110534 (87%)]\tClassification Loss: 1.4214\r\n",
      "Train Epoch: 2 [97280/110534 (88%)]\tClassification Loss: 1.2933\r\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tClassification Loss: 1.2532\r\n",
      "Train Epoch: 2 [98560/110534 (89%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 2 [99200/110534 (90%)]\tClassification Loss: 1.6293\r\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tClassification Loss: 1.4489\r\n",
      "Train Epoch: 2 [100480/110534 (91%)]\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 2 [101120/110534 (91%)]\tClassification Loss: 1.4539\r\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tClassification Loss: 1.6583\r\n",
      "Train Epoch: 2 [102400/110534 (93%)]\tClassification Loss: 1.4439\r\n",
      "Train Epoch: 2 [103040/110534 (93%)]\tClassification Loss: 1.4787\r\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tClassification Loss: 1.7426\r\n",
      "Train Epoch: 2 [104320/110534 (94%)]\tClassification Loss: 1.4802\r\n",
      "Train Epoch: 2 [104960/110534 (95%)]\tClassification Loss: 1.6473\r\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tClassification Loss: 1.5256\r\n",
      "Train Epoch: 2 [106240/110534 (96%)]\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 2 [106880/110534 (97%)]\tClassification Loss: 1.8416\r\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tClassification Loss: 1.5935\r\n",
      "Train Epoch: 2 [108160/110534 (98%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 2 [108800/110534 (98%)]\tClassification Loss: 1.7322\r\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 2 [110080/110534 (100%)]\tClassification Loss: 1.4981\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/110534 (0%)]\tClassification Loss: 1.7783\r\n",
      "\r\n",
      "Test set: Average loss: 1.4826, Accuracy: 22805/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 3 [640/110534 (1%)]\tClassification Loss: 1.4126\r\n",
      "Train Epoch: 3 [1280/110534 (1%)]\tClassification Loss: 2.0144\r\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tClassification Loss: 1.6305\r\n",
      "Train Epoch: 3 [2560/110534 (2%)]\tClassification Loss: 1.7436\r\n",
      "Train Epoch: 3 [3200/110534 (3%)]\tClassification Loss: 1.6239\r\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tClassification Loss: 1.5878\r\n",
      "Train Epoch: 3 [4480/110534 (4%)]\tClassification Loss: 1.6438\r\n",
      "Train Epoch: 3 [5120/110534 (5%)]\tClassification Loss: 1.6489\r\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tClassification Loss: 1.6217\r\n",
      "Train Epoch: 3 [6400/110534 (6%)]\tClassification Loss: 1.2499\r\n",
      "Train Epoch: 3 [7040/110534 (6%)]\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tClassification Loss: 1.5664\r\n",
      "Train Epoch: 3 [8320/110534 (8%)]\tClassification Loss: 1.9313\r\n",
      "Train Epoch: 3 [8960/110534 (8%)]\tClassification Loss: 1.8951\r\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tClassification Loss: 1.3350\r\n",
      "Train Epoch: 3 [10240/110534 (9%)]\tClassification Loss: 1.5681\r\n",
      "Train Epoch: 3 [10880/110534 (10%)]\tClassification Loss: 1.5171\r\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tClassification Loss: 1.8718\r\n",
      "Train Epoch: 3 [12160/110534 (11%)]\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 3 [12800/110534 (12%)]\tClassification Loss: 1.5502\r\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tClassification Loss: 1.3935\r\n",
      "Train Epoch: 3 [14080/110534 (13%)]\tClassification Loss: 1.4830\r\n",
      "Train Epoch: 3 [14720/110534 (13%)]\tClassification Loss: 1.8650\r\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 3 [16000/110534 (14%)]\tClassification Loss: 1.6578\r\n",
      "Train Epoch: 3 [16640/110534 (15%)]\tClassification Loss: 1.6752\r\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tClassification Loss: 1.8692\r\n",
      "Train Epoch: 3 [17920/110534 (16%)]\tClassification Loss: 1.6820\r\n",
      "Train Epoch: 3 [18560/110534 (17%)]\tClassification Loss: 1.6906\r\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tClassification Loss: 1.3965\r\n",
      "Train Epoch: 3 [19840/110534 (18%)]\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 3 [20480/110534 (19%)]\tClassification Loss: 1.8084\r\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tClassification Loss: 1.8578\r\n",
      "Train Epoch: 3 [21760/110534 (20%)]\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 3 [22400/110534 (20%)]\tClassification Loss: 1.4300\r\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tClassification Loss: 1.3404\r\n",
      "Train Epoch: 3 [23680/110534 (21%)]\tClassification Loss: 1.8723\r\n",
      "Train Epoch: 3 [24320/110534 (22%)]\tClassification Loss: 1.1714\r\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tClassification Loss: 1.6228\r\n",
      "Train Epoch: 3 [25600/110534 (23%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 3 [26240/110534 (24%)]\tClassification Loss: 1.4813\r\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tClassification Loss: 1.8162\r\n",
      "Train Epoch: 3 [27520/110534 (25%)]\tClassification Loss: 1.4825\r\n",
      "Train Epoch: 3 [28160/110534 (25%)]\tClassification Loss: 1.7576\r\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tClassification Loss: 1.9627\r\n",
      "Train Epoch: 3 [29440/110534 (27%)]\tClassification Loss: 1.7313\r\n",
      "Train Epoch: 3 [30080/110534 (27%)]\tClassification Loss: 1.5509\r\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tClassification Loss: 1.4649\r\n",
      "Train Epoch: 3 [31360/110534 (28%)]\tClassification Loss: 1.6101\r\n",
      "Train Epoch: 3 [32000/110534 (29%)]\tClassification Loss: 1.4486\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_500.pth.tar\r\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tClassification Loss: 1.3051\r\n",
      "Train Epoch: 3 [33280/110534 (30%)]\tClassification Loss: 1.7434\r\n",
      "Train Epoch: 3 [33920/110534 (31%)]\tClassification Loss: 1.4927\r\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tClassification Loss: 1.6947\r\n",
      "Train Epoch: 3 [35200/110534 (32%)]\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 3 [35840/110534 (32%)]\tClassification Loss: 1.2117\r\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 3 [37120/110534 (34%)]\tClassification Loss: 1.8384\r\n",
      "Train Epoch: 3 [37760/110534 (34%)]\tClassification Loss: 1.9289\r\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tClassification Loss: 1.6768\r\n",
      "Train Epoch: 3 [39040/110534 (35%)]\tClassification Loss: 1.6498\r\n",
      "Train Epoch: 3 [39680/110534 (36%)]\tClassification Loss: 1.6368\r\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tClassification Loss: 1.7864\r\n",
      "Train Epoch: 3 [40960/110534 (37%)]\tClassification Loss: 1.4136\r\n",
      "Train Epoch: 3 [41600/110534 (38%)]\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 3 [42880/110534 (39%)]\tClassification Loss: 1.6627\r\n",
      "Train Epoch: 3 [43520/110534 (39%)]\tClassification Loss: 1.3258\r\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 3 [44800/110534 (41%)]\tClassification Loss: 1.5443\r\n",
      "Train Epoch: 3 [45440/110534 (41%)]\tClassification Loss: 1.6319\r\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 3 [46720/110534 (42%)]\tClassification Loss: 1.3451\r\n",
      "Train Epoch: 3 [47360/110534 (43%)]\tClassification Loss: 1.5326\r\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tClassification Loss: 1.5940\r\n",
      "Train Epoch: 3 [48640/110534 (44%)]\tClassification Loss: 1.3841\r\n",
      "Train Epoch: 3 [49280/110534 (45%)]\tClassification Loss: 1.6234\r\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tClassification Loss: 1.5907\r\n",
      "Train Epoch: 3 [50560/110534 (46%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 3 [51200/110534 (46%)]\tClassification Loss: 1.4053\r\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 3 [52480/110534 (47%)]\tClassification Loss: 1.6605\r\n",
      "Train Epoch: 3 [53120/110534 (48%)]\tClassification Loss: 1.2320\r\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 3 [54400/110534 (49%)]\tClassification Loss: 1.6154\r\n",
      "Train Epoch: 3 [55040/110534 (50%)]\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tClassification Loss: 1.8200\r\n",
      "Train Epoch: 3 [56320/110534 (51%)]\tClassification Loss: 1.5028\r\n",
      "Train Epoch: 3 [56960/110534 (52%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tClassification Loss: 1.5545\r\n",
      "Train Epoch: 3 [58240/110534 (53%)]\tClassification Loss: 1.5379\r\n",
      "Train Epoch: 3 [58880/110534 (53%)]\tClassification Loss: 1.8619\r\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tClassification Loss: 1.5194\r\n",
      "Train Epoch: 3 [60160/110534 (54%)]\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 3 [60800/110534 (55%)]\tClassification Loss: 1.6916\r\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tClassification Loss: 1.8493\r\n",
      "Train Epoch: 3 [62080/110534 (56%)]\tClassification Loss: 1.5547\r\n",
      "Train Epoch: 3 [62720/110534 (57%)]\tClassification Loss: 1.7472\r\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tClassification Loss: 1.3232\r\n",
      "Train Epoch: 3 [64000/110534 (58%)]\tClassification Loss: 1.4836\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1000.pth.tar\r\n",
      "Train Epoch: 3 [64640/110534 (58%)]\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tClassification Loss: 1.6893\r\n",
      "Train Epoch: 3 [65920/110534 (60%)]\tClassification Loss: 1.3458\r\n",
      "Train Epoch: 3 [66560/110534 (60%)]\tClassification Loss: 1.4073\r\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tClassification Loss: 1.3791\r\n",
      "Train Epoch: 3 [67840/110534 (61%)]\tClassification Loss: 1.8766\r\n",
      "Train Epoch: 3 [68480/110534 (62%)]\tClassification Loss: 1.6001\r\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tClassification Loss: 1.6841\r\n",
      "Train Epoch: 3 [69760/110534 (63%)]\tClassification Loss: 1.7550\r\n",
      "Train Epoch: 3 [70400/110534 (64%)]\tClassification Loss: 1.3245\r\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tClassification Loss: 1.8646\r\n",
      "Train Epoch: 3 [71680/110534 (65%)]\tClassification Loss: 1.6796\r\n",
      "Train Epoch: 3 [72320/110534 (65%)]\tClassification Loss: 1.7420\r\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tClassification Loss: 1.5809\r\n",
      "Train Epoch: 3 [73600/110534 (67%)]\tClassification Loss: 1.8801\r\n",
      "Train Epoch: 3 [74240/110534 (67%)]\tClassification Loss: 1.8616\r\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tClassification Loss: 1.2927\r\n",
      "Train Epoch: 3 [75520/110534 (68%)]\tClassification Loss: 1.4646\r\n",
      "Train Epoch: 3 [76160/110534 (69%)]\tClassification Loss: 1.2699\r\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tClassification Loss: 1.4668\r\n",
      "Train Epoch: 3 [77440/110534 (70%)]\tClassification Loss: 1.3310\r\n",
      "Train Epoch: 3 [78080/110534 (71%)]\tClassification Loss: 1.5974\r\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tClassification Loss: 1.3226\r\n",
      "Train Epoch: 3 [79360/110534 (72%)]\tClassification Loss: 1.3801\r\n",
      "Train Epoch: 3 [80000/110534 (72%)]\tClassification Loss: 1.5312\r\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tClassification Loss: 1.4124\r\n",
      "Train Epoch: 3 [81280/110534 (74%)]\tClassification Loss: 1.8707\r\n",
      "Train Epoch: 3 [81920/110534 (74%)]\tClassification Loss: 1.7304\r\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tClassification Loss: 1.8653\r\n",
      "Train Epoch: 3 [83200/110534 (75%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 3 [83840/110534 (76%)]\tClassification Loss: 1.7800\r\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tClassification Loss: 1.6620\r\n",
      "Train Epoch: 3 [85120/110534 (77%)]\tClassification Loss: 1.4942\r\n",
      "Train Epoch: 3 [85760/110534 (78%)]\tClassification Loss: 1.5319\r\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tClassification Loss: 1.4270\r\n",
      "Train Epoch: 3 [87040/110534 (79%)]\tClassification Loss: 1.4966\r\n",
      "Train Epoch: 3 [87680/110534 (79%)]\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 3 [88960/110534 (80%)]\tClassification Loss: 1.6533\r\n",
      "Train Epoch: 3 [89600/110534 (81%)]\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tClassification Loss: 1.7819\r\n",
      "Train Epoch: 3 [90880/110534 (82%)]\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 3 [91520/110534 (83%)]\tClassification Loss: 1.2595\r\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tClassification Loss: 1.4885\r\n",
      "Train Epoch: 3 [92800/110534 (84%)]\tClassification Loss: 1.4268\r\n",
      "Train Epoch: 3 [93440/110534 (85%)]\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tClassification Loss: 1.6444\r\n",
      "Train Epoch: 3 [94720/110534 (86%)]\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 3 [95360/110534 (86%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tClassification Loss: 1.3981\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [96640/110534 (87%)]\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 3 [97280/110534 (88%)]\tClassification Loss: 1.2532\r\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tClassification Loss: 1.1090\r\n",
      "Train Epoch: 3 [98560/110534 (89%)]\tClassification Loss: 1.4611\r\n",
      "Train Epoch: 3 [99200/110534 (90%)]\tClassification Loss: 1.5233\r\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 3 [100480/110534 (91%)]\tClassification Loss: 1.7375\r\n",
      "Train Epoch: 3 [101120/110534 (91%)]\tClassification Loss: 1.4033\r\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tClassification Loss: 1.5349\r\n",
      "Train Epoch: 3 [102400/110534 (93%)]\tClassification Loss: 1.3909\r\n",
      "Train Epoch: 3 [103040/110534 (93%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tClassification Loss: 1.7092\r\n",
      "Train Epoch: 3 [104320/110534 (94%)]\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 3 [104960/110534 (95%)]\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tClassification Loss: 1.5957\r\n",
      "Train Epoch: 3 [106240/110534 (96%)]\tClassification Loss: 1.2513\r\n",
      "Train Epoch: 3 [106880/110534 (97%)]\tClassification Loss: 1.7303\r\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tClassification Loss: 1.7171\r\n",
      "Train Epoch: 3 [108160/110534 (98%)]\tClassification Loss: 1.6144\r\n",
      "Train Epoch: 3 [108800/110534 (98%)]\tClassification Loss: 1.8093\r\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tClassification Loss: 1.6034\r\n",
      "Train Epoch: 3 [110080/110534 (100%)]\tClassification Loss: 1.4996\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/110534 (0%)]\tClassification Loss: 1.8033\r\n",
      "\r\n",
      "Test set: Average loss: 1.4567, Accuracy: 23067/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 4 [640/110534 (1%)]\tClassification Loss: 1.3701\r\n",
      "Train Epoch: 4 [1280/110534 (1%)]\tClassification Loss: 2.0687\r\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tClassification Loss: 1.4995\r\n",
      "Train Epoch: 4 [2560/110534 (2%)]\tClassification Loss: 1.7058\r\n",
      "Train Epoch: 4 [3200/110534 (3%)]\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tClassification Loss: 1.4350\r\n",
      "Train Epoch: 4 [4480/110534 (4%)]\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 4 [5120/110534 (5%)]\tClassification Loss: 1.5882\r\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tClassification Loss: 1.5446\r\n",
      "Train Epoch: 4 [6400/110534 (6%)]\tClassification Loss: 1.3458\r\n",
      "Train Epoch: 4 [7040/110534 (6%)]\tClassification Loss: 1.5296\r\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tClassification Loss: 1.6183\r\n",
      "Train Epoch: 4 [8320/110534 (8%)]\tClassification Loss: 1.6545\r\n",
      "Train Epoch: 4 [8960/110534 (8%)]\tClassification Loss: 1.9555\r\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tClassification Loss: 1.4630\r\n",
      "Train Epoch: 4 [10240/110534 (9%)]\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 4 [10880/110534 (10%)]\tClassification Loss: 1.4684\r\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tClassification Loss: 1.7819\r\n",
      "Train Epoch: 4 [12160/110534 (11%)]\tClassification Loss: 1.4989\r\n",
      "Train Epoch: 4 [12800/110534 (12%)]\tClassification Loss: 1.5970\r\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tClassification Loss: 1.4438\r\n",
      "Train Epoch: 4 [14080/110534 (13%)]\tClassification Loss: 1.5299\r\n",
      "Train Epoch: 4 [14720/110534 (13%)]\tClassification Loss: 1.6988\r\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 4 [16000/110534 (14%)]\tClassification Loss: 1.6556\r\n",
      "Train Epoch: 4 [16640/110534 (15%)]\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 4 [17920/110534 (16%)]\tClassification Loss: 1.5445\r\n",
      "Train Epoch: 4 [18560/110534 (17%)]\tClassification Loss: 1.7779\r\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tClassification Loss: 1.5713\r\n",
      "Train Epoch: 4 [19840/110534 (18%)]\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 4 [20480/110534 (19%)]\tClassification Loss: 1.8826\r\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tClassification Loss: 1.7937\r\n",
      "Train Epoch: 4 [21760/110534 (20%)]\tClassification Loss: 1.4983\r\n",
      "Train Epoch: 4 [22400/110534 (20%)]\tClassification Loss: 1.4882\r\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tClassification Loss: 1.2387\r\n",
      "Train Epoch: 4 [23680/110534 (21%)]\tClassification Loss: 1.7286\r\n",
      "Train Epoch: 4 [24320/110534 (22%)]\tClassification Loss: 1.2642\r\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tClassification Loss: 1.7945\r\n",
      "Train Epoch: 4 [25600/110534 (23%)]\tClassification Loss: 1.5229\r\n",
      "Train Epoch: 4 [26240/110534 (24%)]\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tClassification Loss: 1.7915\r\n",
      "Train Epoch: 4 [27520/110534 (25%)]\tClassification Loss: 1.4705\r\n",
      "Train Epoch: 4 [28160/110534 (25%)]\tClassification Loss: 1.7610\r\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tClassification Loss: 1.8141\r\n",
      "Train Epoch: 4 [29440/110534 (27%)]\tClassification Loss: 1.5419\r\n",
      "Train Epoch: 4 [30080/110534 (27%)]\tClassification Loss: 1.8309\r\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tClassification Loss: 1.4493\r\n",
      "Train Epoch: 4 [31360/110534 (28%)]\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 4 [32000/110534 (29%)]\tClassification Loss: 1.4578\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_500.pth.tar\r\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tClassification Loss: 1.3171\r\n",
      "Train Epoch: 4 [33280/110534 (30%)]\tClassification Loss: 1.5707\r\n",
      "Train Epoch: 4 [33920/110534 (31%)]\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tClassification Loss: 1.6298\r\n",
      "Train Epoch: 4 [35200/110534 (32%)]\tClassification Loss: 1.3909\r\n",
      "Train Epoch: 4 [35840/110534 (32%)]\tClassification Loss: 1.2572\r\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tClassification Loss: 1.3755\r\n",
      "Train Epoch: 4 [37120/110534 (34%)]\tClassification Loss: 1.7744\r\n",
      "Train Epoch: 4 [37760/110534 (34%)]\tClassification Loss: 1.9843\r\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tClassification Loss: 1.5322\r\n",
      "Train Epoch: 4 [39040/110534 (35%)]\tClassification Loss: 1.5182\r\n",
      "Train Epoch: 4 [39680/110534 (36%)]\tClassification Loss: 1.7117\r\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tClassification Loss: 1.8278\r\n",
      "Train Epoch: 4 [40960/110534 (37%)]\tClassification Loss: 1.3103\r\n",
      "Train Epoch: 4 [41600/110534 (38%)]\tClassification Loss: 1.4785\r\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tClassification Loss: 1.5785\r\n",
      "Train Epoch: 4 [42880/110534 (39%)]\tClassification Loss: 1.4345\r\n",
      "Train Epoch: 4 [43520/110534 (39%)]\tClassification Loss: 1.4852\r\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tClassification Loss: 1.5954\r\n",
      "Train Epoch: 4 [44800/110534 (41%)]\tClassification Loss: 1.6870\r\n",
      "Train Epoch: 4 [45440/110534 (41%)]\tClassification Loss: 1.5866\r\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tClassification Loss: 1.3353\r\n",
      "Train Epoch: 4 [46720/110534 (42%)]\tClassification Loss: 1.6425\r\n",
      "Train Epoch: 4 [47360/110534 (43%)]\tClassification Loss: 1.5075\r\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tClassification Loss: 1.5649\r\n",
      "Train Epoch: 4 [48640/110534 (44%)]\tClassification Loss: 1.4845\r\n",
      "Train Epoch: 4 [49280/110534 (45%)]\tClassification Loss: 1.4961\r\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tClassification Loss: 1.6487\r\n",
      "Train Epoch: 4 [50560/110534 (46%)]\tClassification Loss: 1.6408\r\n",
      "Train Epoch: 4 [51200/110534 (46%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 4 [52480/110534 (47%)]\tClassification Loss: 1.4571\r\n",
      "Train Epoch: 4 [53120/110534 (48%)]\tClassification Loss: 1.1982\r\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tClassification Loss: 1.4589\r\n",
      "Train Epoch: 4 [54400/110534 (49%)]\tClassification Loss: 1.5091\r\n",
      "Train Epoch: 4 [55040/110534 (50%)]\tClassification Loss: 1.6294\r\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tClassification Loss: 1.8001\r\n",
      "Train Epoch: 4 [56320/110534 (51%)]\tClassification Loss: 1.4456\r\n",
      "Train Epoch: 4 [56960/110534 (52%)]\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tClassification Loss: 1.6599\r\n",
      "Train Epoch: 4 [58240/110534 (53%)]\tClassification Loss: 1.4780\r\n",
      "Train Epoch: 4 [58880/110534 (53%)]\tClassification Loss: 1.6494\r\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 4 [60160/110534 (54%)]\tClassification Loss: 1.3485\r\n",
      "Train Epoch: 4 [60800/110534 (55%)]\tClassification Loss: 1.6512\r\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tClassification Loss: 1.6141\r\n",
      "Train Epoch: 4 [62080/110534 (56%)]\tClassification Loss: 1.5599\r\n",
      "Train Epoch: 4 [62720/110534 (57%)]\tClassification Loss: 1.8219\r\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tClassification Loss: 1.3495\r\n",
      "Train Epoch: 4 [64000/110534 (58%)]\tClassification Loss: 1.5131\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1000.pth.tar\r\n",
      "Train Epoch: 4 [64640/110534 (58%)]\tClassification Loss: 1.3224\r\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tClassification Loss: 1.6562\r\n",
      "Train Epoch: 4 [65920/110534 (60%)]\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 4 [66560/110534 (60%)]\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tClassification Loss: 1.3167\r\n",
      "Train Epoch: 4 [67840/110534 (61%)]\tClassification Loss: 1.7493\r\n",
      "Train Epoch: 4 [68480/110534 (62%)]\tClassification Loss: 1.7208\r\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 4 [69760/110534 (63%)]\tClassification Loss: 1.5884\r\n",
      "Train Epoch: 4 [70400/110534 (64%)]\tClassification Loss: 1.3976\r\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tClassification Loss: 1.7693\r\n",
      "Train Epoch: 4 [71680/110534 (65%)]\tClassification Loss: 1.4990\r\n",
      "Train Epoch: 4 [72320/110534 (65%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tClassification Loss: 1.5826\r\n",
      "Train Epoch: 4 [73600/110534 (67%)]\tClassification Loss: 1.7496\r\n",
      "Train Epoch: 4 [74240/110534 (67%)]\tClassification Loss: 1.8928\r\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tClassification Loss: 1.2363\r\n",
      "Train Epoch: 4 [75520/110534 (68%)]\tClassification Loss: 1.5735\r\n",
      "Train Epoch: 4 [76160/110534 (69%)]\tClassification Loss: 1.2566\r\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tClassification Loss: 1.3369\r\n",
      "Train Epoch: 4 [77440/110534 (70%)]\tClassification Loss: 1.3619\r\n",
      "Train Epoch: 4 [78080/110534 (71%)]\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tClassification Loss: 1.3283\r\n",
      "Train Epoch: 4 [79360/110534 (72%)]\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 4 [80000/110534 (72%)]\tClassification Loss: 1.5061\r\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tClassification Loss: 1.4536\r\n",
      "Train Epoch: 4 [81280/110534 (74%)]\tClassification Loss: 1.9273\r\n",
      "Train Epoch: 4 [81920/110534 (74%)]\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tClassification Loss: 1.9433\r\n",
      "Train Epoch: 4 [83200/110534 (75%)]\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 4 [83840/110534 (76%)]\tClassification Loss: 1.8978\r\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tClassification Loss: 1.7978\r\n",
      "Train Epoch: 4 [85120/110534 (77%)]\tClassification Loss: 1.5303\r\n",
      "Train Epoch: 4 [85760/110534 (78%)]\tClassification Loss: 1.5163\r\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tClassification Loss: 1.6261\r\n",
      "Train Epoch: 4 [87040/110534 (79%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 4 [87680/110534 (79%)]\tClassification Loss: 1.3934\r\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tClassification Loss: 1.6097\r\n",
      "Train Epoch: 4 [88960/110534 (80%)]\tClassification Loss: 1.7452\r\n",
      "Train Epoch: 4 [89600/110534 (81%)]\tClassification Loss: 1.7490\r\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tClassification Loss: 1.7627\r\n",
      "Train Epoch: 4 [90880/110534 (82%)]\tClassification Loss: 1.5346\r\n",
      "Train Epoch: 4 [91520/110534 (83%)]\tClassification Loss: 1.1871\r\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tClassification Loss: 1.3951\r\n",
      "Train Epoch: 4 [92800/110534 (84%)]\tClassification Loss: 1.3984\r\n",
      "Train Epoch: 4 [93440/110534 (85%)]\tClassification Loss: 1.8542\r\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 4 [94720/110534 (86%)]\tClassification Loss: 1.5250\r\n",
      "Train Epoch: 4 [95360/110534 (86%)]\tClassification Loss: 1.5064\r\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tClassification Loss: 1.5113\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [96640/110534 (87%)]\tClassification Loss: 1.4084\r\n",
      "Train Epoch: 4 [97280/110534 (88%)]\tClassification Loss: 1.2738\r\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tClassification Loss: 1.3117\r\n",
      "Train Epoch: 4 [98560/110534 (89%)]\tClassification Loss: 1.2697\r\n",
      "Train Epoch: 4 [99200/110534 (90%)]\tClassification Loss: 1.5688\r\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 4 [100480/110534 (91%)]\tClassification Loss: 1.6808\r\n",
      "Train Epoch: 4 [101120/110534 (91%)]\tClassification Loss: 1.5543\r\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tClassification Loss: 1.5131\r\n",
      "Train Epoch: 4 [102400/110534 (93%)]\tClassification Loss: 1.4303\r\n",
      "Train Epoch: 4 [103040/110534 (93%)]\tClassification Loss: 1.3942\r\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tClassification Loss: 1.7292\r\n",
      "Train Epoch: 4 [104320/110534 (94%)]\tClassification Loss: 1.4755\r\n",
      "Train Epoch: 4 [104960/110534 (95%)]\tClassification Loss: 1.4358\r\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tClassification Loss: 1.4192\r\n",
      "Train Epoch: 4 [106240/110534 (96%)]\tClassification Loss: 1.2318\r\n",
      "Train Epoch: 4 [106880/110534 (97%)]\tClassification Loss: 1.7122\r\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tClassification Loss: 1.6923\r\n",
      "Train Epoch: 4 [108160/110534 (98%)]\tClassification Loss: 1.6795\r\n",
      "Train Epoch: 4 [108800/110534 (98%)]\tClassification Loss: 1.6786\r\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tClassification Loss: 1.4471\r\n",
      "Train Epoch: 4 [110080/110534 (100%)]\tClassification Loss: 1.3902\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/110534 (0%)]\tClassification Loss: 1.8214\r\n",
      "\r\n",
      "Test set: Average loss: 1.4414, Accuracy: 23184/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 5 [640/110534 (1%)]\tClassification Loss: 1.4757\r\n",
      "Train Epoch: 5 [1280/110534 (1%)]\tClassification Loss: 2.0677\r\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tClassification Loss: 1.5913\r\n",
      "Train Epoch: 5 [2560/110534 (2%)]\tClassification Loss: 1.7521\r\n",
      "Train Epoch: 5 [3200/110534 (3%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 5 [4480/110534 (4%)]\tClassification Loss: 1.4583\r\n",
      "Train Epoch: 5 [5120/110534 (5%)]\tClassification Loss: 1.6779\r\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tClassification Loss: 1.5874\r\n",
      "Train Epoch: 5 [6400/110534 (6%)]\tClassification Loss: 1.3437\r\n",
      "Train Epoch: 5 [7040/110534 (6%)]\tClassification Loss: 1.4731\r\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tClassification Loss: 1.4330\r\n",
      "Train Epoch: 5 [8320/110534 (8%)]\tClassification Loss: 1.5649\r\n",
      "Train Epoch: 5 [8960/110534 (8%)]\tClassification Loss: 1.9053\r\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 5 [10240/110534 (9%)]\tClassification Loss: 1.6265\r\n",
      "Train Epoch: 5 [10880/110534 (10%)]\tClassification Loss: 1.5864\r\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tClassification Loss: 1.7487\r\n",
      "Train Epoch: 5 [12160/110534 (11%)]\tClassification Loss: 1.3043\r\n",
      "Train Epoch: 5 [12800/110534 (12%)]\tClassification Loss: 1.6998\r\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tClassification Loss: 1.5795\r\n",
      "Train Epoch: 5 [14080/110534 (13%)]\tClassification Loss: 1.6049\r\n",
      "Train Epoch: 5 [14720/110534 (13%)]\tClassification Loss: 1.5807\r\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tClassification Loss: 1.4907\r\n",
      "Train Epoch: 5 [16000/110534 (14%)]\tClassification Loss: 1.6198\r\n",
      "Train Epoch: 5 [16640/110534 (15%)]\tClassification Loss: 1.5162\r\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tClassification Loss: 1.6290\r\n",
      "Train Epoch: 5 [17920/110534 (16%)]\tClassification Loss: 1.6295\r\n",
      "Train Epoch: 5 [18560/110534 (17%)]\tClassification Loss: 1.6427\r\n",
      "Train Epoch: 5 [19200/110534 (17%)]\tClassification Loss: 1.5794\r\n",
      "Train Epoch: 5 [19840/110534 (18%)]\tClassification Loss: 1.5450\r\n",
      "Train Epoch: 5 [20480/110534 (19%)]\tClassification Loss: 1.7526\r\n",
      "Train Epoch: 5 [21120/110534 (19%)]\tClassification Loss: 1.6720\r\n",
      "Train Epoch: 5 [21760/110534 (20%)]\tClassification Loss: 1.3151\r\n",
      "Train Epoch: 5 [22400/110534 (20%)]\tClassification Loss: 1.5810\r\n",
      "Train Epoch: 5 [23040/110534 (21%)]\tClassification Loss: 1.2549\r\n",
      "Train Epoch: 5 [23680/110534 (21%)]\tClassification Loss: 1.7112\r\n",
      "Train Epoch: 5 [24320/110534 (22%)]\tClassification Loss: 1.1438\r\n",
      "Train Epoch: 5 [24960/110534 (23%)]\tClassification Loss: 1.8037\r\n",
      "Train Epoch: 5 [25600/110534 (23%)]\tClassification Loss: 1.3492\r\n",
      "Train Epoch: 5 [26240/110534 (24%)]\tClassification Loss: 1.4599\r\n",
      "Train Epoch: 5 [26880/110534 (24%)]\tClassification Loss: 1.7940\r\n",
      "Train Epoch: 5 [27520/110534 (25%)]\tClassification Loss: 1.3934\r\n",
      "Train Epoch: 5 [28160/110534 (25%)]\tClassification Loss: 1.8175\r\n",
      "Train Epoch: 5 [28800/110534 (26%)]\tClassification Loss: 1.8557\r\n",
      "Train Epoch: 5 [29440/110534 (27%)]\tClassification Loss: 1.7247\r\n",
      "Train Epoch: 5 [30080/110534 (27%)]\tClassification Loss: 1.8264\r\n",
      "Train Epoch: 5 [30720/110534 (28%)]\tClassification Loss: 1.3616\r\n",
      "Train Epoch: 5 [31360/110534 (28%)]\tClassification Loss: 1.7320\r\n",
      "Train Epoch: 5 [32000/110534 (29%)]\tClassification Loss: 1.4909\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_500.pth.tar\r\n",
      "Train Epoch: 5 [32640/110534 (30%)]\tClassification Loss: 1.3405\r\n",
      "Train Epoch: 5 [33280/110534 (30%)]\tClassification Loss: 1.5699\r\n",
      "Train Epoch: 5 [33920/110534 (31%)]\tClassification Loss: 1.5548\r\n",
      "Train Epoch: 5 [34560/110534 (31%)]\tClassification Loss: 1.8079\r\n",
      "Train Epoch: 5 [35200/110534 (32%)]\tClassification Loss: 1.5180\r\n",
      "Train Epoch: 5 [35840/110534 (32%)]\tClassification Loss: 1.3576\r\n",
      "Train Epoch: 5 [36480/110534 (33%)]\tClassification Loss: 1.5626\r\n",
      "Train Epoch: 5 [37120/110534 (34%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 5 [37760/110534 (34%)]\tClassification Loss: 1.8593\r\n",
      "Train Epoch: 5 [38400/110534 (35%)]\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 5 [39040/110534 (35%)]\tClassification Loss: 1.5608\r\n",
      "Train Epoch: 5 [39680/110534 (36%)]\tClassification Loss: 1.5841\r\n",
      "Train Epoch: 5 [40320/110534 (36%)]\tClassification Loss: 1.6166\r\n",
      "Train Epoch: 5 [40960/110534 (37%)]\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 5 [41600/110534 (38%)]\tClassification Loss: 1.5943\r\n",
      "Train Epoch: 5 [42240/110534 (38%)]\tClassification Loss: 1.3299\r\n",
      "Train Epoch: 5 [42880/110534 (39%)]\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 5 [43520/110534 (39%)]\tClassification Loss: 1.3080\r\n",
      "Train Epoch: 5 [44160/110534 (40%)]\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 5 [44800/110534 (41%)]\tClassification Loss: 1.6378\r\n",
      "Train Epoch: 5 [45440/110534 (41%)]\tClassification Loss: 1.6996\r\n",
      "Train Epoch: 5 [46080/110534 (42%)]\tClassification Loss: 1.5284\r\n",
      "Train Epoch: 5 [46720/110534 (42%)]\tClassification Loss: 1.3721\r\n",
      "Train Epoch: 5 [47360/110534 (43%)]\tClassification Loss: 1.5247\r\n",
      "Train Epoch: 5 [48000/110534 (43%)]\tClassification Loss: 1.6861\r\n",
      "Train Epoch: 5 [48640/110534 (44%)]\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 5 [49280/110534 (45%)]\tClassification Loss: 1.4464\r\n",
      "Train Epoch: 5 [49920/110534 (45%)]\tClassification Loss: 1.6950\r\n",
      "Train Epoch: 5 [50560/110534 (46%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 5 [51200/110534 (46%)]\tClassification Loss: 1.5093\r\n",
      "Train Epoch: 5 [51840/110534 (47%)]\tClassification Loss: 1.4687\r\n",
      "Train Epoch: 5 [52480/110534 (47%)]\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 5 [53120/110534 (48%)]\tClassification Loss: 1.1426\r\n",
      "Train Epoch: 5 [53760/110534 (49%)]\tClassification Loss: 1.6003\r\n",
      "Train Epoch: 5 [54400/110534 (49%)]\tClassification Loss: 1.5969\r\n",
      "Train Epoch: 5 [55040/110534 (50%)]\tClassification Loss: 1.7294\r\n",
      "Train Epoch: 5 [55680/110534 (50%)]\tClassification Loss: 1.5083\r\n",
      "Train Epoch: 5 [56320/110534 (51%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 5 [56960/110534 (52%)]\tClassification Loss: 1.5441\r\n",
      "Train Epoch: 5 [57600/110534 (52%)]\tClassification Loss: 1.6365\r\n",
      "Train Epoch: 5 [58240/110534 (53%)]\tClassification Loss: 1.5318\r\n",
      "Train Epoch: 5 [58880/110534 (53%)]\tClassification Loss: 1.6830\r\n",
      "Train Epoch: 5 [59520/110534 (54%)]\tClassification Loss: 1.3243\r\n",
      "Train Epoch: 5 [60160/110534 (54%)]\tClassification Loss: 1.4946\r\n",
      "Train Epoch: 5 [60800/110534 (55%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 5 [61440/110534 (56%)]\tClassification Loss: 1.6402\r\n",
      "Train Epoch: 5 [62080/110534 (56%)]\tClassification Loss: 1.6797\r\n",
      "Train Epoch: 5 [62720/110534 (57%)]\tClassification Loss: 1.6522\r\n",
      "Train Epoch: 5 [63360/110534 (57%)]\tClassification Loss: 1.3651\r\n",
      "Train Epoch: 5 [64000/110534 (58%)]\tClassification Loss: 1.5904\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1000.pth.tar\r\n",
      "Train Epoch: 5 [64640/110534 (58%)]\tClassification Loss: 1.1687\r\n",
      "Train Epoch: 5 [65280/110534 (59%)]\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 5 [65920/110534 (60%)]\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 5 [66560/110534 (60%)]\tClassification Loss: 1.1877\r\n",
      "Train Epoch: 5 [67200/110534 (61%)]\tClassification Loss: 1.3384\r\n",
      "Train Epoch: 5 [67840/110534 (61%)]\tClassification Loss: 1.9029\r\n",
      "Train Epoch: 5 [68480/110534 (62%)]\tClassification Loss: 1.6124\r\n",
      "Train Epoch: 5 [69120/110534 (63%)]\tClassification Loss: 1.6342\r\n",
      "Train Epoch: 5 [69760/110534 (63%)]\tClassification Loss: 1.5580\r\n",
      "Train Epoch: 5 [70400/110534 (64%)]\tClassification Loss: 1.3434\r\n",
      "Train Epoch: 5 [71040/110534 (64%)]\tClassification Loss: 1.8258\r\n",
      "Train Epoch: 5 [71680/110534 (65%)]\tClassification Loss: 1.5200\r\n",
      "Train Epoch: 5 [72320/110534 (65%)]\tClassification Loss: 1.5541\r\n",
      "Train Epoch: 5 [72960/110534 (66%)]\tClassification Loss: 1.7447\r\n",
      "Train Epoch: 5 [73600/110534 (67%)]\tClassification Loss: 1.8587\r\n",
      "Train Epoch: 5 [74240/110534 (67%)]\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 5 [74880/110534 (68%)]\tClassification Loss: 1.0861\r\n",
      "Train Epoch: 5 [75520/110534 (68%)]\tClassification Loss: 1.4943\r\n",
      "Train Epoch: 5 [76160/110534 (69%)]\tClassification Loss: 1.3158\r\n",
      "Train Epoch: 5 [76800/110534 (69%)]\tClassification Loss: 1.3196\r\n",
      "Train Epoch: 5 [77440/110534 (70%)]\tClassification Loss: 1.5104\r\n",
      "Train Epoch: 5 [78080/110534 (71%)]\tClassification Loss: 1.6496\r\n",
      "Train Epoch: 5 [78720/110534 (71%)]\tClassification Loss: 1.3899\r\n",
      "Train Epoch: 5 [79360/110534 (72%)]\tClassification Loss: 1.4013\r\n",
      "Train Epoch: 5 [80000/110534 (72%)]\tClassification Loss: 1.3945\r\n",
      "Train Epoch: 5 [80640/110534 (73%)]\tClassification Loss: 1.3421\r\n",
      "Train Epoch: 5 [81280/110534 (74%)]\tClassification Loss: 1.8597\r\n",
      "Train Epoch: 5 [81920/110534 (74%)]\tClassification Loss: 1.5920\r\n",
      "Train Epoch: 5 [82560/110534 (75%)]\tClassification Loss: 1.7564\r\n",
      "Train Epoch: 5 [83200/110534 (75%)]\tClassification Loss: 1.5463\r\n",
      "Train Epoch: 5 [83840/110534 (76%)]\tClassification Loss: 1.8054\r\n",
      "Train Epoch: 5 [84480/110534 (76%)]\tClassification Loss: 1.5992\r\n",
      "Train Epoch: 5 [85120/110534 (77%)]\tClassification Loss: 1.6560\r\n",
      "Train Epoch: 5 [85760/110534 (78%)]\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 5 [86400/110534 (78%)]\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 5 [87040/110534 (79%)]\tClassification Loss: 1.4299\r\n",
      "Train Epoch: 5 [87680/110534 (79%)]\tClassification Loss: 1.4022\r\n",
      "Train Epoch: 5 [88320/110534 (80%)]\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 5 [88960/110534 (80%)]\tClassification Loss: 1.5702\r\n",
      "Train Epoch: 5 [89600/110534 (81%)]\tClassification Loss: 1.7738\r\n",
      "Train Epoch: 5 [90240/110534 (82%)]\tClassification Loss: 1.8169\r\n",
      "Train Epoch: 5 [90880/110534 (82%)]\tClassification Loss: 1.6093\r\n",
      "Train Epoch: 5 [91520/110534 (83%)]\tClassification Loss: 1.2746\r\n",
      "Train Epoch: 5 [92160/110534 (83%)]\tClassification Loss: 1.3925\r\n",
      "Train Epoch: 5 [92800/110534 (84%)]\tClassification Loss: 1.3981\r\n",
      "Train Epoch: 5 [93440/110534 (85%)]\tClassification Loss: 1.7626\r\n",
      "Train Epoch: 5 [94080/110534 (85%)]\tClassification Loss: 1.6719\r\n",
      "Train Epoch: 5 [94720/110534 (86%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 5 [95360/110534 (86%)]\tClassification Loss: 1.4519\r\n",
      "Train Epoch: 5 [96000/110534 (87%)]\tClassification Loss: 1.5347\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [96640/110534 (87%)]\tClassification Loss: 1.3060\r\n",
      "Train Epoch: 5 [97280/110534 (88%)]\tClassification Loss: 1.2843\r\n",
      "Train Epoch: 5 [97920/110534 (89%)]\tClassification Loss: 1.2391\r\n",
      "Train Epoch: 5 [98560/110534 (89%)]\tClassification Loss: 1.3395\r\n",
      "Train Epoch: 5 [99200/110534 (90%)]\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 5 [99840/110534 (90%)]\tClassification Loss: 1.6165\r\n",
      "Train Epoch: 5 [100480/110534 (91%)]\tClassification Loss: 1.6179\r\n",
      "Train Epoch: 5 [101120/110534 (91%)]\tClassification Loss: 1.6072\r\n",
      "Train Epoch: 5 [101760/110534 (92%)]\tClassification Loss: 1.5407\r\n",
      "Train Epoch: 5 [102400/110534 (93%)]\tClassification Loss: 1.4532\r\n",
      "Train Epoch: 5 [103040/110534 (93%)]\tClassification Loss: 1.5366\r\n",
      "Train Epoch: 5 [103680/110534 (94%)]\tClassification Loss: 1.6526\r\n",
      "Train Epoch: 5 [104320/110534 (94%)]\tClassification Loss: 1.3449\r\n",
      "Train Epoch: 5 [104960/110534 (95%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 5 [105600/110534 (96%)]\tClassification Loss: 1.5816\r\n",
      "Train Epoch: 5 [106240/110534 (96%)]\tClassification Loss: 1.2997\r\n",
      "Train Epoch: 5 [106880/110534 (97%)]\tClassification Loss: 1.7168\r\n",
      "Train Epoch: 5 [107520/110534 (97%)]\tClassification Loss: 1.7498\r\n",
      "Train Epoch: 5 [108160/110534 (98%)]\tClassification Loss: 1.5371\r\n",
      "Train Epoch: 5 [108800/110534 (98%)]\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 5 [109440/110534 (99%)]\tClassification Loss: 1.3873\r\n",
      "Train Epoch: 5 [110080/110534 (100%)]\tClassification Loss: 1.4041\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/110534 (0%)]\tClassification Loss: 1.6926\r\n",
      "\r\n",
      "Test set: Average loss: 1.4298, Accuracy: 23226/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 6 [640/110534 (1%)]\tClassification Loss: 1.4857\r\n",
      "Train Epoch: 6 [1280/110534 (1%)]\tClassification Loss: 1.8599\r\n",
      "Train Epoch: 6 [1920/110534 (2%)]\tClassification Loss: 1.5703\r\n",
      "Train Epoch: 6 [2560/110534 (2%)]\tClassification Loss: 1.6948\r\n",
      "Train Epoch: 6 [3200/110534 (3%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 6 [3840/110534 (3%)]\tClassification Loss: 1.3953\r\n",
      "Train Epoch: 6 [4480/110534 (4%)]\tClassification Loss: 1.5268\r\n",
      "Train Epoch: 6 [5120/110534 (5%)]\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 6 [5760/110534 (5%)]\tClassification Loss: 1.5884\r\n",
      "Train Epoch: 6 [6400/110534 (6%)]\tClassification Loss: 1.3312\r\n",
      "Train Epoch: 6 [7040/110534 (6%)]\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 6 [7680/110534 (7%)]\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 6 [8320/110534 (8%)]\tClassification Loss: 1.8124\r\n",
      "Train Epoch: 6 [8960/110534 (8%)]\tClassification Loss: 1.7182\r\n",
      "Train Epoch: 6 [9600/110534 (9%)]\tClassification Loss: 1.4601\r\n",
      "Train Epoch: 6 [10240/110534 (9%)]\tClassification Loss: 1.5294\r\n",
      "Train Epoch: 6 [10880/110534 (10%)]\tClassification Loss: 1.4472\r\n",
      "Train Epoch: 6 [11520/110534 (10%)]\tClassification Loss: 1.7232\r\n",
      "Train Epoch: 6 [12160/110534 (11%)]\tClassification Loss: 1.4503\r\n",
      "Train Epoch: 6 [12800/110534 (12%)]\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 6 [13440/110534 (12%)]\tClassification Loss: 1.3317\r\n",
      "Train Epoch: 6 [14080/110534 (13%)]\tClassification Loss: 1.5045\r\n",
      "Train Epoch: 6 [14720/110534 (13%)]\tClassification Loss: 1.8011\r\n",
      "Train Epoch: 6 [15360/110534 (14%)]\tClassification Loss: 1.5408\r\n",
      "Train Epoch: 6 [16000/110534 (14%)]\tClassification Loss: 1.6783\r\n",
      "Train Epoch: 6 [16640/110534 (15%)]\tClassification Loss: 1.5699\r\n",
      "Train Epoch: 6 [17280/110534 (16%)]\tClassification Loss: 1.7264\r\n",
      "Train Epoch: 6 [17920/110534 (16%)]\tClassification Loss: 1.4601\r\n",
      "Train Epoch: 6 [18560/110534 (17%)]\tClassification Loss: 1.7541\r\n",
      "Train Epoch: 6 [19200/110534 (17%)]\tClassification Loss: 1.5842\r\n",
      "Train Epoch: 6 [19840/110534 (18%)]\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 6 [20480/110534 (19%)]\tClassification Loss: 1.6829\r\n",
      "Train Epoch: 6 [21120/110534 (19%)]\tClassification Loss: 1.7296\r\n",
      "Train Epoch: 6 [21760/110534 (20%)]\tClassification Loss: 1.4134\r\n",
      "Train Epoch: 6 [22400/110534 (20%)]\tClassification Loss: 1.4823\r\n",
      "Train Epoch: 6 [23040/110534 (21%)]\tClassification Loss: 1.1718\r\n",
      "Train Epoch: 6 [23680/110534 (21%)]\tClassification Loss: 1.6524\r\n",
      "Train Epoch: 6 [24320/110534 (22%)]\tClassification Loss: 1.3577\r\n",
      "Train Epoch: 6 [24960/110534 (23%)]\tClassification Loss: 1.6395\r\n",
      "Train Epoch: 6 [25600/110534 (23%)]\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 6 [26240/110534 (24%)]\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 6 [26880/110534 (24%)]\tClassification Loss: 1.9807\r\n",
      "Train Epoch: 6 [27520/110534 (25%)]\tClassification Loss: 1.3622\r\n",
      "Train Epoch: 6 [28160/110534 (25%)]\tClassification Loss: 1.7367\r\n",
      "Train Epoch: 6 [28800/110534 (26%)]\tClassification Loss: 1.9396\r\n",
      "Train Epoch: 6 [29440/110534 (27%)]\tClassification Loss: 1.7013\r\n",
      "Train Epoch: 6 [30080/110534 (27%)]\tClassification Loss: 1.5953\r\n",
      "Train Epoch: 6 [30720/110534 (28%)]\tClassification Loss: 1.3307\r\n",
      "Train Epoch: 6 [31360/110534 (28%)]\tClassification Loss: 1.6516\r\n",
      "Train Epoch: 6 [32000/110534 (29%)]\tClassification Loss: 1.4203\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_500.pth.tar\r\n",
      "Train Epoch: 6 [32640/110534 (30%)]\tClassification Loss: 1.4420\r\n",
      "Train Epoch: 6 [33280/110534 (30%)]\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 6 [33920/110534 (31%)]\tClassification Loss: 1.6473\r\n",
      "Train Epoch: 6 [34560/110534 (31%)]\tClassification Loss: 1.6646\r\n",
      "Train Epoch: 6 [35200/110534 (32%)]\tClassification Loss: 1.4482\r\n",
      "Train Epoch: 6 [35840/110534 (32%)]\tClassification Loss: 1.2418\r\n",
      "Train Epoch: 6 [36480/110534 (33%)]\tClassification Loss: 1.5590\r\n",
      "Train Epoch: 6 [37120/110534 (34%)]\tClassification Loss: 1.7747\r\n",
      "Train Epoch: 6 [37760/110534 (34%)]\tClassification Loss: 1.9257\r\n",
      "Train Epoch: 6 [38400/110534 (35%)]\tClassification Loss: 1.4278\r\n",
      "Train Epoch: 6 [39040/110534 (35%)]\tClassification Loss: 1.4766\r\n",
      "Train Epoch: 6 [39680/110534 (36%)]\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 6 [40320/110534 (36%)]\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 6 [40960/110534 (37%)]\tClassification Loss: 1.4114\r\n",
      "Train Epoch: 6 [41600/110534 (38%)]\tClassification Loss: 1.4516\r\n",
      "Train Epoch: 6 [42240/110534 (38%)]\tClassification Loss: 1.4236\r\n",
      "Train Epoch: 6 [42880/110534 (39%)]\tClassification Loss: 1.4522\r\n",
      "Train Epoch: 6 [43520/110534 (39%)]\tClassification Loss: 1.3231\r\n",
      "Train Epoch: 6 [44160/110534 (40%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 6 [44800/110534 (41%)]\tClassification Loss: 1.7022\r\n",
      "Train Epoch: 6 [45440/110534 (41%)]\tClassification Loss: 1.6462\r\n",
      "Train Epoch: 6 [46080/110534 (42%)]\tClassification Loss: 1.3216\r\n",
      "Train Epoch: 6 [46720/110534 (42%)]\tClassification Loss: 1.4477\r\n",
      "Train Epoch: 6 [47360/110534 (43%)]\tClassification Loss: 1.4462\r\n",
      "Train Epoch: 6 [48000/110534 (43%)]\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 6 [48640/110534 (44%)]\tClassification Loss: 1.2407\r\n",
      "Train Epoch: 6 [49280/110534 (45%)]\tClassification Loss: 1.6676\r\n",
      "Train Epoch: 6 [49920/110534 (45%)]\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 6 [50560/110534 (46%)]\tClassification Loss: 1.5227\r\n",
      "Train Epoch: 6 [51200/110534 (46%)]\tClassification Loss: 1.4423\r\n",
      "Train Epoch: 6 [51840/110534 (47%)]\tClassification Loss: 1.4704\r\n",
      "Train Epoch: 6 [52480/110534 (47%)]\tClassification Loss: 1.5241\r\n",
      "Train Epoch: 6 [53120/110534 (48%)]\tClassification Loss: 1.0315\r\n",
      "Train Epoch: 6 [53760/110534 (49%)]\tClassification Loss: 1.5856\r\n",
      "Train Epoch: 6 [54400/110534 (49%)]\tClassification Loss: 1.4353\r\n",
      "Train Epoch: 6 [55040/110534 (50%)]\tClassification Loss: 1.6585\r\n",
      "Train Epoch: 6 [55680/110534 (50%)]\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 6 [56320/110534 (51%)]\tClassification Loss: 1.5992\r\n",
      "Train Epoch: 6 [56960/110534 (52%)]\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 6 [57600/110534 (52%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 6 [58240/110534 (53%)]\tClassification Loss: 1.5278\r\n",
      "Train Epoch: 6 [58880/110534 (53%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 6 [59520/110534 (54%)]\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 6 [60160/110534 (54%)]\tClassification Loss: 1.3419\r\n",
      "Train Epoch: 6 [60800/110534 (55%)]\tClassification Loss: 1.4606\r\n",
      "Train Epoch: 6 [61440/110534 (56%)]\tClassification Loss: 1.5210\r\n",
      "Train Epoch: 6 [62080/110534 (56%)]\tClassification Loss: 1.4696\r\n",
      "Train Epoch: 6 [62720/110534 (57%)]\tClassification Loss: 1.6205\r\n",
      "Train Epoch: 6 [63360/110534 (57%)]\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 6 [64000/110534 (58%)]\tClassification Loss: 1.4624\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1000.pth.tar\r\n",
      "Train Epoch: 6 [64640/110534 (58%)]\tClassification Loss: 1.2302\r\n",
      "Train Epoch: 6 [65280/110534 (59%)]\tClassification Loss: 1.5983\r\n",
      "Train Epoch: 6 [65920/110534 (60%)]\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 6 [66560/110534 (60%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 6 [67200/110534 (61%)]\tClassification Loss: 1.2895\r\n",
      "Train Epoch: 6 [67840/110534 (61%)]\tClassification Loss: 1.8815\r\n",
      "Train Epoch: 6 [68480/110534 (62%)]\tClassification Loss: 1.5807\r\n",
      "Train Epoch: 6 [69120/110534 (63%)]\tClassification Loss: 1.7410\r\n",
      "Train Epoch: 6 [69760/110534 (63%)]\tClassification Loss: 1.5767\r\n",
      "Train Epoch: 6 [70400/110534 (64%)]\tClassification Loss: 1.2633\r\n",
      "Train Epoch: 6 [71040/110534 (64%)]\tClassification Loss: 1.8210\r\n",
      "Train Epoch: 6 [71680/110534 (65%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 6 [72320/110534 (65%)]\tClassification Loss: 1.3991\r\n",
      "Train Epoch: 6 [72960/110534 (66%)]\tClassification Loss: 1.7314\r\n",
      "Train Epoch: 6 [73600/110534 (67%)]\tClassification Loss: 1.7801\r\n",
      "Train Epoch: 6 [74240/110534 (67%)]\tClassification Loss: 1.8161\r\n",
      "Train Epoch: 6 [74880/110534 (68%)]\tClassification Loss: 1.1840\r\n",
      "Train Epoch: 6 [75520/110534 (68%)]\tClassification Loss: 1.4236\r\n",
      "Train Epoch: 6 [76160/110534 (69%)]\tClassification Loss: 1.2733\r\n",
      "Train Epoch: 6 [76800/110534 (69%)]\tClassification Loss: 1.3633\r\n",
      "Train Epoch: 6 [77440/110534 (70%)]\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 6 [78080/110534 (71%)]\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 6 [78720/110534 (71%)]\tClassification Loss: 1.3052\r\n",
      "Train Epoch: 6 [79360/110534 (72%)]\tClassification Loss: 1.2278\r\n",
      "Train Epoch: 6 [80000/110534 (72%)]\tClassification Loss: 1.2745\r\n",
      "Train Epoch: 6 [80640/110534 (73%)]\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 6 [81280/110534 (74%)]\tClassification Loss: 1.8180\r\n",
      "Train Epoch: 6 [81920/110534 (74%)]\tClassification Loss: 1.5216\r\n",
      "Train Epoch: 6 [82560/110534 (75%)]\tClassification Loss: 2.0221\r\n",
      "Train Epoch: 6 [83200/110534 (75%)]\tClassification Loss: 1.3945\r\n",
      "Train Epoch: 6 [83840/110534 (76%)]\tClassification Loss: 1.9321\r\n",
      "Train Epoch: 6 [84480/110534 (76%)]\tClassification Loss: 1.5134\r\n",
      "Train Epoch: 6 [85120/110534 (77%)]\tClassification Loss: 1.4156\r\n",
      "Train Epoch: 6 [85760/110534 (78%)]\tClassification Loss: 1.6721\r\n",
      "Train Epoch: 6 [86400/110534 (78%)]\tClassification Loss: 1.6241\r\n",
      "Train Epoch: 6 [87040/110534 (79%)]\tClassification Loss: 1.3544\r\n",
      "Train Epoch: 6 [87680/110534 (79%)]\tClassification Loss: 1.3589\r\n",
      "Train Epoch: 6 [88320/110534 (80%)]\tClassification Loss: 1.5311\r\n",
      "Train Epoch: 6 [88960/110534 (80%)]\tClassification Loss: 1.5879\r\n",
      "Train Epoch: 6 [89600/110534 (81%)]\tClassification Loss: 1.6618\r\n",
      "Train Epoch: 6 [90240/110534 (82%)]\tClassification Loss: 1.6835\r\n",
      "Train Epoch: 6 [90880/110534 (82%)]\tClassification Loss: 1.5481\r\n",
      "Train Epoch: 6 [91520/110534 (83%)]\tClassification Loss: 1.3475\r\n",
      "Train Epoch: 6 [92160/110534 (83%)]\tClassification Loss: 1.4192\r\n",
      "Train Epoch: 6 [92800/110534 (84%)]\tClassification Loss: 1.3309\r\n",
      "Train Epoch: 6 [93440/110534 (85%)]\tClassification Loss: 1.8734\r\n",
      "Train Epoch: 6 [94080/110534 (85%)]\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 6 [94720/110534 (86%)]\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 6 [95360/110534 (86%)]\tClassification Loss: 1.5181\r\n",
      "Train Epoch: 6 [96000/110534 (87%)]\tClassification Loss: 1.4016\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [96640/110534 (87%)]\tClassification Loss: 1.3996\r\n",
      "Train Epoch: 6 [97280/110534 (88%)]\tClassification Loss: 1.2206\r\n",
      "Train Epoch: 6 [97920/110534 (89%)]\tClassification Loss: 1.1895\r\n",
      "Train Epoch: 6 [98560/110534 (89%)]\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 6 [99200/110534 (90%)]\tClassification Loss: 1.5696\r\n",
      "Train Epoch: 6 [99840/110534 (90%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 6 [100480/110534 (91%)]\tClassification Loss: 1.7708\r\n",
      "Train Epoch: 6 [101120/110534 (91%)]\tClassification Loss: 1.4874\r\n",
      "Train Epoch: 6 [101760/110534 (92%)]\tClassification Loss: 1.6184\r\n",
      "Train Epoch: 6 [102400/110534 (93%)]\tClassification Loss: 1.4474\r\n",
      "Train Epoch: 6 [103040/110534 (93%)]\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 6 [103680/110534 (94%)]\tClassification Loss: 1.7197\r\n",
      "Train Epoch: 6 [104320/110534 (94%)]\tClassification Loss: 1.3700\r\n",
      "Train Epoch: 6 [104960/110534 (95%)]\tClassification Loss: 1.5825\r\n",
      "Train Epoch: 6 [105600/110534 (96%)]\tClassification Loss: 1.4211\r\n",
      "Train Epoch: 6 [106240/110534 (96%)]\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 6 [106880/110534 (97%)]\tClassification Loss: 1.8362\r\n",
      "Train Epoch: 6 [107520/110534 (97%)]\tClassification Loss: 1.7157\r\n",
      "Train Epoch: 6 [108160/110534 (98%)]\tClassification Loss: 1.5768\r\n",
      "Train Epoch: 6 [108800/110534 (98%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 6 [109440/110534 (99%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 6 [110080/110534 (100%)]\tClassification Loss: 1.4760\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_final.pth.tar\r\n",
      "Train Epoch: 7 [0/110534 (0%)]\tClassification Loss: 1.6988\r\n",
      "\r\n",
      "Test set: Average loss: 1.4270, Accuracy: 23215/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 7 [640/110534 (1%)]\tClassification Loss: 1.3679\r\n",
      "Train Epoch: 7 [1280/110534 (1%)]\tClassification Loss: 2.0166\r\n",
      "Train Epoch: 7 [1920/110534 (2%)]\tClassification Loss: 1.5229\r\n",
      "Train Epoch: 7 [2560/110534 (2%)]\tClassification Loss: 1.7130\r\n",
      "Train Epoch: 7 [3200/110534 (3%)]\tClassification Loss: 1.6047\r\n",
      "Train Epoch: 7 [3840/110534 (3%)]\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 7 [4480/110534 (4%)]\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 7 [5120/110534 (5%)]\tClassification Loss: 1.5030\r\n",
      "Train Epoch: 7 [5760/110534 (5%)]\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 7 [6400/110534 (6%)]\tClassification Loss: 1.1791\r\n",
      "Train Epoch: 7 [7040/110534 (6%)]\tClassification Loss: 1.5773\r\n",
      "Train Epoch: 7 [7680/110534 (7%)]\tClassification Loss: 1.6393\r\n",
      "Train Epoch: 7 [8320/110534 (8%)]\tClassification Loss: 1.8889\r\n",
      "Train Epoch: 7 [8960/110534 (8%)]\tClassification Loss: 1.7215\r\n",
      "Train Epoch: 7 [9600/110534 (9%)]\tClassification Loss: 1.3620\r\n",
      "Train Epoch: 7 [10240/110534 (9%)]\tClassification Loss: 1.5870\r\n",
      "Train Epoch: 7 [10880/110534 (10%)]\tClassification Loss: 1.3765\r\n",
      "Train Epoch: 7 [11520/110534 (10%)]\tClassification Loss: 1.5837\r\n",
      "Train Epoch: 7 [12160/110534 (11%)]\tClassification Loss: 1.3322\r\n",
      "Train Epoch: 7 [12800/110534 (12%)]\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 7 [13440/110534 (12%)]\tClassification Loss: 1.4794\r\n",
      "Train Epoch: 7 [14080/110534 (13%)]\tClassification Loss: 1.6200\r\n",
      "Train Epoch: 7 [14720/110534 (13%)]\tClassification Loss: 1.7314\r\n",
      "Train Epoch: 7 [15360/110534 (14%)]\tClassification Loss: 1.5811\r\n",
      "Train Epoch: 7 [16000/110534 (14%)]\tClassification Loss: 1.5575\r\n",
      "Train Epoch: 7 [16640/110534 (15%)]\tClassification Loss: 1.5034\r\n",
      "Train Epoch: 7 [17280/110534 (16%)]\tClassification Loss: 1.6603\r\n",
      "Train Epoch: 7 [17920/110534 (16%)]\tClassification Loss: 1.5524\r\n",
      "Train Epoch: 7 [18560/110534 (17%)]\tClassification Loss: 1.7594\r\n",
      "Train Epoch: 7 [19200/110534 (17%)]\tClassification Loss: 1.5442\r\n",
      "Train Epoch: 7 [19840/110534 (18%)]\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 7 [20480/110534 (19%)]\tClassification Loss: 1.7101\r\n",
      "Train Epoch: 7 [21120/110534 (19%)]\tClassification Loss: 1.8039\r\n",
      "Train Epoch: 7 [21760/110534 (20%)]\tClassification Loss: 1.3146\r\n",
      "Train Epoch: 7 [22400/110534 (20%)]\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 7 [23040/110534 (21%)]\tClassification Loss: 1.0775\r\n",
      "Train Epoch: 7 [23680/110534 (21%)]\tClassification Loss: 1.8014\r\n",
      "Train Epoch: 7 [24320/110534 (22%)]\tClassification Loss: 1.2680\r\n",
      "Train Epoch: 7 [24960/110534 (23%)]\tClassification Loss: 1.6511\r\n",
      "Train Epoch: 7 [25600/110534 (23%)]\tClassification Loss: 1.3859\r\n",
      "Train Epoch: 7 [26240/110534 (24%)]\tClassification Loss: 1.4718\r\n",
      "Train Epoch: 7 [26880/110534 (24%)]\tClassification Loss: 1.8127\r\n",
      "Train Epoch: 7 [27520/110534 (25%)]\tClassification Loss: 1.3832\r\n",
      "Train Epoch: 7 [28160/110534 (25%)]\tClassification Loss: 1.6567\r\n",
      "Train Epoch: 7 [28800/110534 (26%)]\tClassification Loss: 1.8640\r\n",
      "Train Epoch: 7 [29440/110534 (27%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 7 [30080/110534 (27%)]\tClassification Loss: 1.7655\r\n",
      "Train Epoch: 7 [30720/110534 (28%)]\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 7 [31360/110534 (28%)]\tClassification Loss: 1.6510\r\n",
      "Train Epoch: 7 [32000/110534 (29%)]\tClassification Loss: 1.4586\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_500.pth.tar\r\n",
      "Train Epoch: 7 [32640/110534 (30%)]\tClassification Loss: 1.2961\r\n",
      "Train Epoch: 7 [33280/110534 (30%)]\tClassification Loss: 1.4505\r\n",
      "Train Epoch: 7 [33920/110534 (31%)]\tClassification Loss: 1.6747\r\n",
      "Train Epoch: 7 [34560/110534 (31%)]\tClassification Loss: 1.5431\r\n",
      "Train Epoch: 7 [35200/110534 (32%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 7 [35840/110534 (32%)]\tClassification Loss: 1.2118\r\n",
      "Train Epoch: 7 [36480/110534 (33%)]\tClassification Loss: 1.4299\r\n",
      "Train Epoch: 7 [37120/110534 (34%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 7 [37760/110534 (34%)]\tClassification Loss: 1.8314\r\n",
      "Train Epoch: 7 [38400/110534 (35%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 7 [39040/110534 (35%)]\tClassification Loss: 1.5432\r\n",
      "Train Epoch: 7 [39680/110534 (36%)]\tClassification Loss: 1.6585\r\n",
      "Train Epoch: 7 [40320/110534 (36%)]\tClassification Loss: 1.6173\r\n",
      "Train Epoch: 7 [40960/110534 (37%)]\tClassification Loss: 1.3774\r\n",
      "Train Epoch: 7 [41600/110534 (38%)]\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 7 [42240/110534 (38%)]\tClassification Loss: 1.3914\r\n",
      "Train Epoch: 7 [42880/110534 (39%)]\tClassification Loss: 1.4796\r\n",
      "Train Epoch: 7 [43520/110534 (39%)]\tClassification Loss: 1.3453\r\n",
      "Train Epoch: 7 [44160/110534 (40%)]\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 7 [44800/110534 (41%)]\tClassification Loss: 1.6044\r\n",
      "Train Epoch: 7 [45440/110534 (41%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 7 [46080/110534 (42%)]\tClassification Loss: 1.3989\r\n",
      "Train Epoch: 7 [46720/110534 (42%)]\tClassification Loss: 1.4133\r\n",
      "Train Epoch: 7 [47360/110534 (43%)]\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 7 [48000/110534 (43%)]\tClassification Loss: 1.5164\r\n",
      "Train Epoch: 7 [48640/110534 (44%)]\tClassification Loss: 1.3614\r\n",
      "Train Epoch: 7 [49280/110534 (45%)]\tClassification Loss: 1.4744\r\n",
      "Train Epoch: 7 [49920/110534 (45%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 7 [50560/110534 (46%)]\tClassification Loss: 1.5488\r\n",
      "Train Epoch: 7 [51200/110534 (46%)]\tClassification Loss: 1.4132\r\n",
      "Train Epoch: 7 [51840/110534 (47%)]\tClassification Loss: 1.4113\r\n",
      "Train Epoch: 7 [52480/110534 (47%)]\tClassification Loss: 1.4964\r\n",
      "Train Epoch: 7 [53120/110534 (48%)]\tClassification Loss: 1.2427\r\n",
      "Train Epoch: 7 [53760/110534 (49%)]\tClassification Loss: 1.5960\r\n",
      "Train Epoch: 7 [54400/110534 (49%)]\tClassification Loss: 1.4441\r\n",
      "Train Epoch: 7 [55040/110534 (50%)]\tClassification Loss: 1.6981\r\n",
      "Train Epoch: 7 [55680/110534 (50%)]\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 7 [56320/110534 (51%)]\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 7 [56960/110534 (52%)]\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 7 [57600/110534 (52%)]\tClassification Loss: 1.6345\r\n",
      "Train Epoch: 7 [58240/110534 (53%)]\tClassification Loss: 1.4969\r\n",
      "Train Epoch: 7 [58880/110534 (53%)]\tClassification Loss: 1.5531\r\n",
      "Train Epoch: 7 [59520/110534 (54%)]\tClassification Loss: 1.3446\r\n",
      "Train Epoch: 7 [60160/110534 (54%)]\tClassification Loss: 1.5582\r\n",
      "Train Epoch: 7 [60800/110534 (55%)]\tClassification Loss: 1.7598\r\n",
      "Train Epoch: 7 [61440/110534 (56%)]\tClassification Loss: 1.4294\r\n",
      "Train Epoch: 7 [62080/110534 (56%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 7 [62720/110534 (57%)]\tClassification Loss: 1.7460\r\n",
      "Train Epoch: 7 [63360/110534 (57%)]\tClassification Loss: 1.2624\r\n",
      "Train Epoch: 7 [64000/110534 (58%)]\tClassification Loss: 1.4629\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1000.pth.tar\r\n",
      "Train Epoch: 7 [64640/110534 (58%)]\tClassification Loss: 1.2184\r\n",
      "Train Epoch: 7 [65280/110534 (59%)]\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 7 [65920/110534 (60%)]\tClassification Loss: 1.5557\r\n",
      "Train Epoch: 7 [66560/110534 (60%)]\tClassification Loss: 1.3135\r\n",
      "Train Epoch: 7 [67200/110534 (61%)]\tClassification Loss: 1.2630\r\n",
      "Train Epoch: 7 [67840/110534 (61%)]\tClassification Loss: 1.8535\r\n",
      "Train Epoch: 7 [68480/110534 (62%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 7 [69120/110534 (63%)]\tClassification Loss: 1.6771\r\n",
      "Train Epoch: 7 [69760/110534 (63%)]\tClassification Loss: 1.6461\r\n",
      "Train Epoch: 7 [70400/110534 (64%)]\tClassification Loss: 1.1965\r\n",
      "Train Epoch: 7 [71040/110534 (64%)]\tClassification Loss: 1.7799\r\n",
      "Train Epoch: 7 [71680/110534 (65%)]\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 7 [72320/110534 (65%)]\tClassification Loss: 1.5945\r\n",
      "Train Epoch: 7 [72960/110534 (66%)]\tClassification Loss: 1.5822\r\n",
      "Train Epoch: 7 [73600/110534 (67%)]\tClassification Loss: 1.8029\r\n",
      "Train Epoch: 7 [74240/110534 (67%)]\tClassification Loss: 1.8410\r\n",
      "Train Epoch: 7 [74880/110534 (68%)]\tClassification Loss: 1.3161\r\n",
      "Train Epoch: 7 [75520/110534 (68%)]\tClassification Loss: 1.4719\r\n",
      "Train Epoch: 7 [76160/110534 (69%)]\tClassification Loss: 1.4244\r\n",
      "Train Epoch: 7 [76800/110534 (69%)]\tClassification Loss: 1.3929\r\n",
      "Train Epoch: 7 [77440/110534 (70%)]\tClassification Loss: 1.3235\r\n",
      "Train Epoch: 7 [78080/110534 (71%)]\tClassification Loss: 1.4400\r\n",
      "Train Epoch: 7 [78720/110534 (71%)]\tClassification Loss: 1.5041\r\n",
      "Train Epoch: 7 [79360/110534 (72%)]\tClassification Loss: 1.3253\r\n",
      "Train Epoch: 7 [80000/110534 (72%)]\tClassification Loss: 1.3107\r\n",
      "Train Epoch: 7 [80640/110534 (73%)]\tClassification Loss: 1.4970\r\n",
      "Train Epoch: 7 [81280/110534 (74%)]\tClassification Loss: 1.8687\r\n",
      "Train Epoch: 7 [81920/110534 (74%)]\tClassification Loss: 1.5664\r\n",
      "Train Epoch: 7 [82560/110534 (75%)]\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 7 [83200/110534 (75%)]\tClassification Loss: 1.5228\r\n",
      "Train Epoch: 7 [83840/110534 (76%)]\tClassification Loss: 2.0034\r\n",
      "Train Epoch: 7 [84480/110534 (76%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 7 [85120/110534 (77%)]\tClassification Loss: 1.6907\r\n",
      "Train Epoch: 7 [85760/110534 (78%)]\tClassification Loss: 1.5947\r\n",
      "Train Epoch: 7 [86400/110534 (78%)]\tClassification Loss: 1.6115\r\n",
      "Train Epoch: 7 [87040/110534 (79%)]\tClassification Loss: 1.4216\r\n",
      "Train Epoch: 7 [87680/110534 (79%)]\tClassification Loss: 1.4357\r\n",
      "Train Epoch: 7 [88320/110534 (80%)]\tClassification Loss: 1.5804\r\n",
      "Train Epoch: 7 [88960/110534 (80%)]\tClassification Loss: 1.5756\r\n",
      "Train Epoch: 7 [89600/110534 (81%)]\tClassification Loss: 1.7133\r\n",
      "Train Epoch: 7 [90240/110534 (82%)]\tClassification Loss: 1.8376\r\n",
      "Train Epoch: 7 [90880/110534 (82%)]\tClassification Loss: 1.5922\r\n",
      "Train Epoch: 7 [91520/110534 (83%)]\tClassification Loss: 1.2403\r\n",
      "Train Epoch: 7 [92160/110534 (83%)]\tClassification Loss: 1.5330\r\n",
      "Train Epoch: 7 [92800/110534 (84%)]\tClassification Loss: 1.5464\r\n",
      "Train Epoch: 7 [93440/110534 (85%)]\tClassification Loss: 1.6815\r\n",
      "Train Epoch: 7 [94080/110534 (85%)]\tClassification Loss: 1.7303\r\n",
      "Train Epoch: 7 [94720/110534 (86%)]\tClassification Loss: 1.3765\r\n",
      "Train Epoch: 7 [95360/110534 (86%)]\tClassification Loss: 1.4172\r\n",
      "Train Epoch: 7 [96000/110534 (87%)]\tClassification Loss: 1.4775\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1500.pth.tar\r\n",
      "Train Epoch: 7 [96640/110534 (87%)]\tClassification Loss: 1.3837\r\n",
      "Train Epoch: 7 [97280/110534 (88%)]\tClassification Loss: 1.2498\r\n",
      "Train Epoch: 7 [97920/110534 (89%)]\tClassification Loss: 1.2079\r\n",
      "Train Epoch: 7 [98560/110534 (89%)]\tClassification Loss: 1.5092\r\n",
      "Train Epoch: 7 [99200/110534 (90%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 7 [99840/110534 (90%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 7 [100480/110534 (91%)]\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 7 [101120/110534 (91%)]\tClassification Loss: 1.4286\r\n",
      "Train Epoch: 7 [101760/110534 (92%)]\tClassification Loss: 1.5649\r\n",
      "Train Epoch: 7 [102400/110534 (93%)]\tClassification Loss: 1.4746\r\n",
      "Train Epoch: 7 [103040/110534 (93%)]\tClassification Loss: 1.3857\r\n",
      "Train Epoch: 7 [103680/110534 (94%)]\tClassification Loss: 1.6402\r\n",
      "Train Epoch: 7 [104320/110534 (94%)]\tClassification Loss: 1.4183\r\n",
      "Train Epoch: 7 [104960/110534 (95%)]\tClassification Loss: 1.5395\r\n",
      "Train Epoch: 7 [105600/110534 (96%)]\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 7 [106240/110534 (96%)]\tClassification Loss: 1.1975\r\n",
      "Train Epoch: 7 [106880/110534 (97%)]\tClassification Loss: 1.7517\r\n",
      "Train Epoch: 7 [107520/110534 (97%)]\tClassification Loss: 1.5548\r\n",
      "Train Epoch: 7 [108160/110534 (98%)]\tClassification Loss: 1.4925\r\n",
      "Train Epoch: 7 [108800/110534 (98%)]\tClassification Loss: 1.6179\r\n",
      "Train Epoch: 7 [109440/110534 (99%)]\tClassification Loss: 1.3916\r\n",
      "Train Epoch: 7 [110080/110534 (100%)]\tClassification Loss: 1.4416\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_final.pth.tar\r\n",
      "Train Epoch: 8 [0/110534 (0%)]\tClassification Loss: 1.6107\r\n",
      "\r\n",
      "Test set: Average loss: 1.4198, Accuracy: 23271/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 8 [640/110534 (1%)]\tClassification Loss: 1.2308\r\n",
      "Train Epoch: 8 [1280/110534 (1%)]\tClassification Loss: 1.9450\r\n",
      "Train Epoch: 8 [1920/110534 (2%)]\tClassification Loss: 1.6056\r\n",
      "Train Epoch: 8 [2560/110534 (2%)]\tClassification Loss: 1.8124\r\n",
      "Train Epoch: 8 [3200/110534 (3%)]\tClassification Loss: 1.4078\r\n",
      "Train Epoch: 8 [3840/110534 (3%)]\tClassification Loss: 1.4137\r\n",
      "Train Epoch: 8 [4480/110534 (4%)]\tClassification Loss: 1.5633\r\n",
      "Train Epoch: 8 [5120/110534 (5%)]\tClassification Loss: 1.6646\r\n",
      "Train Epoch: 8 [5760/110534 (5%)]\tClassification Loss: 1.4611\r\n",
      "Train Epoch: 8 [6400/110534 (6%)]\tClassification Loss: 1.1662\r\n",
      "Train Epoch: 8 [7040/110534 (6%)]\tClassification Loss: 1.5272\r\n",
      "Train Epoch: 8 [7680/110534 (7%)]\tClassification Loss: 1.5357\r\n",
      "Train Epoch: 8 [8320/110534 (8%)]\tClassification Loss: 1.8003\r\n",
      "Train Epoch: 8 [8960/110534 (8%)]\tClassification Loss: 1.7961\r\n",
      "Train Epoch: 8 [9600/110534 (9%)]\tClassification Loss: 1.4286\r\n",
      "Train Epoch: 8 [10240/110534 (9%)]\tClassification Loss: 1.4893\r\n",
      "Train Epoch: 8 [10880/110534 (10%)]\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 8 [11520/110534 (10%)]\tClassification Loss: 1.7178\r\n",
      "Train Epoch: 8 [12160/110534 (11%)]\tClassification Loss: 1.3171\r\n",
      "Train Epoch: 8 [12800/110534 (12%)]\tClassification Loss: 1.6094\r\n",
      "Train Epoch: 8 [13440/110534 (12%)]\tClassification Loss: 1.4138\r\n",
      "Train Epoch: 8 [14080/110534 (13%)]\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 8 [14720/110534 (13%)]\tClassification Loss: 1.7552\r\n",
      "Train Epoch: 8 [15360/110534 (14%)]\tClassification Loss: 1.4108\r\n",
      "Train Epoch: 8 [16000/110534 (14%)]\tClassification Loss: 1.6020\r\n",
      "Train Epoch: 8 [16640/110534 (15%)]\tClassification Loss: 1.4780\r\n",
      "Train Epoch: 8 [17280/110534 (16%)]\tClassification Loss: 1.6220\r\n",
      "Train Epoch: 8 [17920/110534 (16%)]\tClassification Loss: 1.7406\r\n",
      "Train Epoch: 8 [18560/110534 (17%)]\tClassification Loss: 1.6832\r\n",
      "Train Epoch: 8 [19200/110534 (17%)]\tClassification Loss: 1.5786\r\n",
      "Train Epoch: 8 [19840/110534 (18%)]\tClassification Loss: 1.5106\r\n",
      "Train Epoch: 8 [20480/110534 (19%)]\tClassification Loss: 1.5875\r\n",
      "Train Epoch: 8 [21120/110534 (19%)]\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 8 [21760/110534 (20%)]\tClassification Loss: 1.2822\r\n",
      "Train Epoch: 8 [22400/110534 (20%)]\tClassification Loss: 1.4630\r\n",
      "Train Epoch: 8 [23040/110534 (21%)]\tClassification Loss: 1.1124\r\n",
      "Train Epoch: 8 [23680/110534 (21%)]\tClassification Loss: 1.7869\r\n",
      "Train Epoch: 8 [24320/110534 (22%)]\tClassification Loss: 1.1166\r\n",
      "Train Epoch: 8 [24960/110534 (23%)]\tClassification Loss: 1.8123\r\n",
      "Train Epoch: 8 [25600/110534 (23%)]\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 8 [26240/110534 (24%)]\tClassification Loss: 1.5912\r\n",
      "Train Epoch: 8 [26880/110534 (24%)]\tClassification Loss: 1.8368\r\n",
      "Train Epoch: 8 [27520/110534 (25%)]\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 8 [28160/110534 (25%)]\tClassification Loss: 1.8016\r\n",
      "Train Epoch: 8 [28800/110534 (26%)]\tClassification Loss: 1.8173\r\n",
      "Train Epoch: 8 [29440/110534 (27%)]\tClassification Loss: 1.5388\r\n",
      "Train Epoch: 8 [30080/110534 (27%)]\tClassification Loss: 1.8110\r\n",
      "Train Epoch: 8 [30720/110534 (28%)]\tClassification Loss: 1.3995\r\n",
      "Train Epoch: 8 [31360/110534 (28%)]\tClassification Loss: 1.5613\r\n",
      "Train Epoch: 8 [32000/110534 (29%)]\tClassification Loss: 1.5403\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_500.pth.tar\r\n",
      "Train Epoch: 8 [32640/110534 (30%)]\tClassification Loss: 1.3145\r\n",
      "Train Epoch: 8 [33280/110534 (30%)]\tClassification Loss: 1.4774\r\n",
      "Train Epoch: 8 [33920/110534 (31%)]\tClassification Loss: 1.5058\r\n",
      "Train Epoch: 8 [34560/110534 (31%)]\tClassification Loss: 1.7639\r\n",
      "Train Epoch: 8 [35200/110534 (32%)]\tClassification Loss: 1.5765\r\n",
      "Train Epoch: 8 [35840/110534 (32%)]\tClassification Loss: 1.2775\r\n",
      "Train Epoch: 8 [36480/110534 (33%)]\tClassification Loss: 1.4960\r\n",
      "Train Epoch: 8 [37120/110534 (34%)]\tClassification Loss: 1.6631\r\n",
      "Train Epoch: 8 [37760/110534 (34%)]\tClassification Loss: 1.9771\r\n",
      "Train Epoch: 8 [38400/110534 (35%)]\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 8 [39040/110534 (35%)]\tClassification Loss: 1.5828\r\n",
      "Train Epoch: 8 [39680/110534 (36%)]\tClassification Loss: 1.6082\r\n",
      "Train Epoch: 8 [40320/110534 (36%)]\tClassification Loss: 1.5264\r\n",
      "Train Epoch: 8 [40960/110534 (37%)]\tClassification Loss: 1.3256\r\n",
      "Train Epoch: 8 [41600/110534 (38%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 8 [42240/110534 (38%)]\tClassification Loss: 1.2819\r\n",
      "Train Epoch: 8 [42880/110534 (39%)]\tClassification Loss: 1.4395\r\n",
      "Train Epoch: 8 [43520/110534 (39%)]\tClassification Loss: 1.3344\r\n",
      "Train Epoch: 8 [44160/110534 (40%)]\tClassification Loss: 1.4895\r\n",
      "Train Epoch: 8 [44800/110534 (41%)]\tClassification Loss: 1.5487\r\n",
      "Train Epoch: 8 [45440/110534 (41%)]\tClassification Loss: 1.3878\r\n",
      "Train Epoch: 8 [46080/110534 (42%)]\tClassification Loss: 1.3299\r\n",
      "Train Epoch: 8 [46720/110534 (42%)]\tClassification Loss: 1.4909\r\n",
      "Train Epoch: 8 [47360/110534 (43%)]\tClassification Loss: 1.5047\r\n",
      "Train Epoch: 8 [48000/110534 (43%)]\tClassification Loss: 1.4620\r\n",
      "Train Epoch: 8 [48640/110534 (44%)]\tClassification Loss: 1.2658\r\n",
      "Train Epoch: 8 [49280/110534 (45%)]\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 8 [49920/110534 (45%)]\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 8 [50560/110534 (46%)]\tClassification Loss: 1.6300\r\n",
      "Train Epoch: 8 [51200/110534 (46%)]\tClassification Loss: 1.3810\r\n",
      "Train Epoch: 8 [51840/110534 (47%)]\tClassification Loss: 1.4030\r\n",
      "Train Epoch: 8 [52480/110534 (47%)]\tClassification Loss: 1.4042\r\n",
      "Train Epoch: 8 [53120/110534 (48%)]\tClassification Loss: 1.1493\r\n",
      "Train Epoch: 8 [53760/110534 (49%)]\tClassification Loss: 1.5596\r\n",
      "Train Epoch: 8 [54400/110534 (49%)]\tClassification Loss: 1.5226\r\n",
      "Train Epoch: 8 [55040/110534 (50%)]\tClassification Loss: 1.7335\r\n",
      "Train Epoch: 8 [55680/110534 (50%)]\tClassification Loss: 1.5434\r\n",
      "Train Epoch: 8 [56320/110534 (51%)]\tClassification Loss: 1.6558\r\n",
      "Train Epoch: 8 [56960/110534 (52%)]\tClassification Loss: 1.3948\r\n",
      "Train Epoch: 8 [57600/110534 (52%)]\tClassification Loss: 1.5423\r\n",
      "Train Epoch: 8 [58240/110534 (53%)]\tClassification Loss: 1.6476\r\n",
      "Train Epoch: 8 [58880/110534 (53%)]\tClassification Loss: 1.7182\r\n",
      "Train Epoch: 8 [59520/110534 (54%)]\tClassification Loss: 1.4019\r\n",
      "Train Epoch: 8 [60160/110534 (54%)]\tClassification Loss: 1.3282\r\n",
      "Train Epoch: 8 [60800/110534 (55%)]\tClassification Loss: 1.6597\r\n",
      "Train Epoch: 8 [61440/110534 (56%)]\tClassification Loss: 1.4492\r\n",
      "Train Epoch: 8 [62080/110534 (56%)]\tClassification Loss: 1.3494\r\n",
      "Train Epoch: 8 [62720/110534 (57%)]\tClassification Loss: 1.7043\r\n",
      "Train Epoch: 8 [63360/110534 (57%)]\tClassification Loss: 1.3320\r\n",
      "Train Epoch: 8 [64000/110534 (58%)]\tClassification Loss: 1.5327\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1000.pth.tar\r\n",
      "Train Epoch: 8 [64640/110534 (58%)]\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 8 [65280/110534 (59%)]\tClassification Loss: 1.5748\r\n",
      "Train Epoch: 8 [65920/110534 (60%)]\tClassification Loss: 1.3855\r\n",
      "Train Epoch: 8 [66560/110534 (60%)]\tClassification Loss: 1.3501\r\n",
      "Train Epoch: 8 [67200/110534 (61%)]\tClassification Loss: 1.4241\r\n",
      "Train Epoch: 8 [67840/110534 (61%)]\tClassification Loss: 1.8135\r\n",
      "Train Epoch: 8 [68480/110534 (62%)]\tClassification Loss: 1.6334\r\n",
      "Train Epoch: 8 [69120/110534 (63%)]\tClassification Loss: 1.7409\r\n",
      "Train Epoch: 8 [69760/110534 (63%)]\tClassification Loss: 1.5222\r\n",
      "Train Epoch: 8 [70400/110534 (64%)]\tClassification Loss: 1.2695\r\n",
      "Train Epoch: 8 [71040/110534 (64%)]\tClassification Loss: 1.8550\r\n",
      "Train Epoch: 8 [71680/110534 (65%)]\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 8 [72320/110534 (65%)]\tClassification Loss: 1.5346\r\n",
      "Train Epoch: 8 [72960/110534 (66%)]\tClassification Loss: 1.6621\r\n",
      "Train Epoch: 8 [73600/110534 (67%)]\tClassification Loss: 1.7277\r\n",
      "Train Epoch: 8 [74240/110534 (67%)]\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 8 [74880/110534 (68%)]\tClassification Loss: 1.1167\r\n",
      "Train Epoch: 8 [75520/110534 (68%)]\tClassification Loss: 1.5448\r\n",
      "Train Epoch: 8 [76160/110534 (69%)]\tClassification Loss: 1.2894\r\n",
      "Train Epoch: 8 [76800/110534 (69%)]\tClassification Loss: 1.4218\r\n",
      "Train Epoch: 8 [77440/110534 (70%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 8 [78080/110534 (71%)]\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 8 [78720/110534 (71%)]\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 8 [79360/110534 (72%)]\tClassification Loss: 1.4038\r\n",
      "Train Epoch: 8 [80000/110534 (72%)]\tClassification Loss: 1.4125\r\n",
      "Train Epoch: 8 [80640/110534 (73%)]\tClassification Loss: 1.4142\r\n",
      "Train Epoch: 8 [81280/110534 (74%)]\tClassification Loss: 1.8700\r\n",
      "Train Epoch: 8 [81920/110534 (74%)]\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 8 [82560/110534 (75%)]\tClassification Loss: 1.8780\r\n",
      "Train Epoch: 8 [83200/110534 (75%)]\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 8 [83840/110534 (76%)]\tClassification Loss: 1.7661\r\n",
      "Train Epoch: 8 [84480/110534 (76%)]\tClassification Loss: 1.5016\r\n",
      "Train Epoch: 8 [85120/110534 (77%)]\tClassification Loss: 1.3826\r\n",
      "Train Epoch: 8 [85760/110534 (78%)]\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 8 [86400/110534 (78%)]\tClassification Loss: 1.6804\r\n",
      "Train Epoch: 8 [87040/110534 (79%)]\tClassification Loss: 1.5096\r\n",
      "Train Epoch: 8 [87680/110534 (79%)]\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 8 [88320/110534 (80%)]\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 8 [88960/110534 (80%)]\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 8 [89600/110534 (81%)]\tClassification Loss: 1.6888\r\n",
      "Train Epoch: 8 [90240/110534 (82%)]\tClassification Loss: 1.8003\r\n",
      "Train Epoch: 8 [90880/110534 (82%)]\tClassification Loss: 1.7462\r\n",
      "Train Epoch: 8 [91520/110534 (83%)]\tClassification Loss: 1.1715\r\n",
      "Train Epoch: 8 [92160/110534 (83%)]\tClassification Loss: 1.3745\r\n",
      "Train Epoch: 8 [92800/110534 (84%)]\tClassification Loss: 1.4701\r\n",
      "Train Epoch: 8 [93440/110534 (85%)]\tClassification Loss: 1.6515\r\n",
      "Train Epoch: 8 [94080/110534 (85%)]\tClassification Loss: 1.6709\r\n",
      "Train Epoch: 8 [94720/110534 (86%)]\tClassification Loss: 1.4870\r\n",
      "Train Epoch: 8 [95360/110534 (86%)]\tClassification Loss: 1.3612\r\n",
      "Train Epoch: 8 [96000/110534 (87%)]\tClassification Loss: 1.4157\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1500.pth.tar\r\n",
      "Train Epoch: 8 [96640/110534 (87%)]\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 8 [97280/110534 (88%)]\tClassification Loss: 1.3355\r\n",
      "Train Epoch: 8 [97920/110534 (89%)]\tClassification Loss: 1.2566\r\n",
      "Train Epoch: 8 [98560/110534 (89%)]\tClassification Loss: 1.4512\r\n",
      "Train Epoch: 8 [99200/110534 (90%)]\tClassification Loss: 1.5832\r\n",
      "Train Epoch: 8 [99840/110534 (90%)]\tClassification Loss: 1.5771\r\n",
      "Train Epoch: 8 [100480/110534 (91%)]\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 8 [101120/110534 (91%)]\tClassification Loss: 1.5332\r\n",
      "Train Epoch: 8 [101760/110534 (92%)]\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 8 [102400/110534 (93%)]\tClassification Loss: 1.4247\r\n",
      "Train Epoch: 8 [103040/110534 (93%)]\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 8 [103680/110534 (94%)]\tClassification Loss: 1.6627\r\n",
      "Train Epoch: 8 [104320/110534 (94%)]\tClassification Loss: 1.3667\r\n",
      "Train Epoch: 8 [104960/110534 (95%)]\tClassification Loss: 1.4700\r\n",
      "Train Epoch: 8 [105600/110534 (96%)]\tClassification Loss: 1.5558\r\n",
      "Train Epoch: 8 [106240/110534 (96%)]\tClassification Loss: 1.2339\r\n",
      "Train Epoch: 8 [106880/110534 (97%)]\tClassification Loss: 1.7520\r\n",
      "Train Epoch: 8 [107520/110534 (97%)]\tClassification Loss: 1.7223\r\n",
      "Train Epoch: 8 [108160/110534 (98%)]\tClassification Loss: 1.5020\r\n",
      "Train Epoch: 8 [108800/110534 (98%)]\tClassification Loss: 1.7151\r\n",
      "Train Epoch: 8 [109440/110534 (99%)]\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 8 [110080/110534 (100%)]\tClassification Loss: 1.4230\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_final.pth.tar\r\n",
      "Train Epoch: 9 [0/110534 (0%)]\tClassification Loss: 1.6065\r\n",
      "\r\n",
      "Test set: Average loss: 1.4244, Accuracy: 23225/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 9 [640/110534 (1%)]\tClassification Loss: 1.2743\r\n",
      "Train Epoch: 9 [1280/110534 (1%)]\tClassification Loss: 2.0521\r\n",
      "Train Epoch: 9 [1920/110534 (2%)]\tClassification Loss: 1.5424\r\n",
      "Train Epoch: 9 [2560/110534 (2%)]\tClassification Loss: 1.7168\r\n",
      "Train Epoch: 9 [3200/110534 (3%)]\tClassification Loss: 1.4778\r\n",
      "Train Epoch: 9 [3840/110534 (3%)]\tClassification Loss: 1.3217\r\n",
      "Train Epoch: 9 [4480/110534 (4%)]\tClassification Loss: 1.5421\r\n",
      "Train Epoch: 9 [5120/110534 (5%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 9 [5760/110534 (5%)]\tClassification Loss: 1.5188\r\n",
      "Train Epoch: 9 [6400/110534 (6%)]\tClassification Loss: 1.3413\r\n",
      "Train Epoch: 9 [7040/110534 (6%)]\tClassification Loss: 1.5065\r\n",
      "Train Epoch: 9 [7680/110534 (7%)]\tClassification Loss: 1.6659\r\n",
      "Train Epoch: 9 [8320/110534 (8%)]\tClassification Loss: 1.8136\r\n",
      "Train Epoch: 9 [8960/110534 (8%)]\tClassification Loss: 1.8475\r\n",
      "Train Epoch: 9 [9600/110534 (9%)]\tClassification Loss: 1.4290\r\n",
      "Train Epoch: 9 [10240/110534 (9%)]\tClassification Loss: 1.5455\r\n",
      "Train Epoch: 9 [10880/110534 (10%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 9 [11520/110534 (10%)]\tClassification Loss: 1.7535\r\n",
      "Train Epoch: 9 [12160/110534 (11%)]\tClassification Loss: 1.4338\r\n",
      "Train Epoch: 9 [12800/110534 (12%)]\tClassification Loss: 1.7494\r\n",
      "Train Epoch: 9 [13440/110534 (12%)]\tClassification Loss: 1.4395\r\n",
      "Train Epoch: 9 [14080/110534 (13%)]\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 9 [14720/110534 (13%)]\tClassification Loss: 1.6889\r\n",
      "Train Epoch: 9 [15360/110534 (14%)]\tClassification Loss: 1.5258\r\n",
      "Train Epoch: 9 [16000/110534 (14%)]\tClassification Loss: 1.7348\r\n",
      "Train Epoch: 9 [16640/110534 (15%)]\tClassification Loss: 1.4692\r\n",
      "Train Epoch: 9 [17280/110534 (16%)]\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 9 [17920/110534 (16%)]\tClassification Loss: 1.7515\r\n",
      "Train Epoch: 9 [18560/110534 (17%)]\tClassification Loss: 1.6313\r\n",
      "Train Epoch: 9 [19200/110534 (17%)]\tClassification Loss: 1.7075\r\n",
      "Train Epoch: 9 [19840/110534 (18%)]\tClassification Loss: 1.5136\r\n",
      "Train Epoch: 9 [20480/110534 (19%)]\tClassification Loss: 1.8075\r\n",
      "Train Epoch: 9 [21120/110534 (19%)]\tClassification Loss: 1.7562\r\n",
      "Train Epoch: 9 [21760/110534 (20%)]\tClassification Loss: 1.4494\r\n",
      "Train Epoch: 9 [22400/110534 (20%)]\tClassification Loss: 1.4964\r\n",
      "Train Epoch: 9 [23040/110534 (21%)]\tClassification Loss: 1.1316\r\n",
      "Train Epoch: 9 [23680/110534 (21%)]\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 9 [24320/110534 (22%)]\tClassification Loss: 1.1067\r\n",
      "Train Epoch: 9 [24960/110534 (23%)]\tClassification Loss: 1.6991\r\n",
      "Train Epoch: 9 [25600/110534 (23%)]\tClassification Loss: 1.3119\r\n",
      "Train Epoch: 9 [26240/110534 (24%)]\tClassification Loss: 1.4969\r\n",
      "Train Epoch: 9 [26880/110534 (24%)]\tClassification Loss: 1.7426\r\n",
      "Train Epoch: 9 [27520/110534 (25%)]\tClassification Loss: 1.3866\r\n",
      "Train Epoch: 9 [28160/110534 (25%)]\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 9 [28800/110534 (26%)]\tClassification Loss: 1.8511\r\n",
      "Train Epoch: 9 [29440/110534 (27%)]\tClassification Loss: 1.5707\r\n",
      "Train Epoch: 9 [30080/110534 (27%)]\tClassification Loss: 1.8826\r\n",
      "Train Epoch: 9 [30720/110534 (28%)]\tClassification Loss: 1.4693\r\n",
      "Train Epoch: 9 [31360/110534 (28%)]\tClassification Loss: 1.6612\r\n",
      "Train Epoch: 9 [32000/110534 (29%)]\tClassification Loss: 1.4956\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_500.pth.tar\r\n",
      "Train Epoch: 9 [32640/110534 (30%)]\tClassification Loss: 1.2890\r\n",
      "Train Epoch: 9 [33280/110534 (30%)]\tClassification Loss: 1.5108\r\n",
      "Train Epoch: 9 [33920/110534 (31%)]\tClassification Loss: 1.5326\r\n",
      "Train Epoch: 9 [34560/110534 (31%)]\tClassification Loss: 1.8336\r\n",
      "Train Epoch: 9 [35200/110534 (32%)]\tClassification Loss: 1.4101\r\n",
      "Train Epoch: 9 [35840/110534 (32%)]\tClassification Loss: 1.2623\r\n",
      "Train Epoch: 9 [36480/110534 (33%)]\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 9 [37120/110534 (34%)]\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 9 [37760/110534 (34%)]\tClassification Loss: 1.7527\r\n",
      "Train Epoch: 9 [38400/110534 (35%)]\tClassification Loss: 1.5826\r\n",
      "Train Epoch: 9 [39040/110534 (35%)]\tClassification Loss: 1.4790\r\n",
      "Train Epoch: 9 [39680/110534 (36%)]\tClassification Loss: 1.6990\r\n",
      "Train Epoch: 9 [40320/110534 (36%)]\tClassification Loss: 1.6813\r\n",
      "Train Epoch: 9 [40960/110534 (37%)]\tClassification Loss: 1.2634\r\n",
      "Train Epoch: 9 [41600/110534 (38%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 9 [42240/110534 (38%)]\tClassification Loss: 1.4242\r\n",
      "Train Epoch: 9 [42880/110534 (39%)]\tClassification Loss: 1.4873\r\n",
      "Train Epoch: 9 [43520/110534 (39%)]\tClassification Loss: 1.2945\r\n",
      "Train Epoch: 9 [44160/110534 (40%)]\tClassification Loss: 1.3979\r\n",
      "Train Epoch: 9 [44800/110534 (41%)]\tClassification Loss: 1.4886\r\n",
      "Train Epoch: 9 [45440/110534 (41%)]\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 9 [46080/110534 (42%)]\tClassification Loss: 1.2807\r\n",
      "Train Epoch: 9 [46720/110534 (42%)]\tClassification Loss: 1.5961\r\n",
      "Train Epoch: 9 [47360/110534 (43%)]\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 9 [48000/110534 (43%)]\tClassification Loss: 1.6298\r\n",
      "Train Epoch: 9 [48640/110534 (44%)]\tClassification Loss: 1.4189\r\n",
      "Train Epoch: 9 [49280/110534 (45%)]\tClassification Loss: 1.6011\r\n",
      "Train Epoch: 9 [49920/110534 (45%)]\tClassification Loss: 1.7428\r\n",
      "Train Epoch: 9 [50560/110534 (46%)]\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 9 [51200/110534 (46%)]\tClassification Loss: 1.5031\r\n",
      "Train Epoch: 9 [51840/110534 (47%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 9 [52480/110534 (47%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 9 [53120/110534 (48%)]\tClassification Loss: 1.1890\r\n",
      "Train Epoch: 9 [53760/110534 (49%)]\tClassification Loss: 1.5920\r\n",
      "Train Epoch: 9 [54400/110534 (49%)]\tClassification Loss: 1.5936\r\n",
      "Train Epoch: 9 [55040/110534 (50%)]\tClassification Loss: 1.7364\r\n",
      "Train Epoch: 9 [55680/110534 (50%)]\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 9 [56320/110534 (51%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 9 [56960/110534 (52%)]\tClassification Loss: 1.4704\r\n",
      "Train Epoch: 9 [57600/110534 (52%)]\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 9 [58240/110534 (53%)]\tClassification Loss: 1.5553\r\n",
      "Train Epoch: 9 [58880/110534 (53%)]\tClassification Loss: 1.7723\r\n",
      "Train Epoch: 9 [59520/110534 (54%)]\tClassification Loss: 1.4402\r\n",
      "Train Epoch: 9 [60160/110534 (54%)]\tClassification Loss: 1.5807\r\n",
      "Train Epoch: 9 [60800/110534 (55%)]\tClassification Loss: 1.5797\r\n",
      "Train Epoch: 9 [61440/110534 (56%)]\tClassification Loss: 1.5583\r\n",
      "Train Epoch: 9 [62080/110534 (56%)]\tClassification Loss: 1.5005\r\n",
      "Train Epoch: 9 [62720/110534 (57%)]\tClassification Loss: 1.7893\r\n",
      "Train Epoch: 9 [63360/110534 (57%)]\tClassification Loss: 1.2388\r\n",
      "Train Epoch: 9 [64000/110534 (58%)]\tClassification Loss: 1.6121\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1000.pth.tar\r\n",
      "Train Epoch: 9 [64640/110534 (58%)]\tClassification Loss: 1.0682\r\n",
      "Train Epoch: 9 [65280/110534 (59%)]\tClassification Loss: 1.4663\r\n",
      "Train Epoch: 9 [65920/110534 (60%)]\tClassification Loss: 1.4527\r\n",
      "Train Epoch: 9 [66560/110534 (60%)]\tClassification Loss: 1.2795\r\n",
      "Train Epoch: 9 [67200/110534 (61%)]\tClassification Loss: 1.3485\r\n",
      "Train Epoch: 9 [67840/110534 (61%)]\tClassification Loss: 1.8357\r\n",
      "Train Epoch: 9 [68480/110534 (62%)]\tClassification Loss: 1.6202\r\n",
      "Train Epoch: 9 [69120/110534 (63%)]\tClassification Loss: 1.6523\r\n",
      "Train Epoch: 9 [69760/110534 (63%)]\tClassification Loss: 1.6384\r\n",
      "Train Epoch: 9 [70400/110534 (64%)]\tClassification Loss: 1.1380\r\n",
      "Train Epoch: 9 [71040/110534 (64%)]\tClassification Loss: 1.8303\r\n",
      "Train Epoch: 9 [71680/110534 (65%)]\tClassification Loss: 1.4771\r\n",
      "Train Epoch: 9 [72320/110534 (65%)]\tClassification Loss: 1.6388\r\n",
      "Train Epoch: 9 [72960/110534 (66%)]\tClassification Loss: 1.6221\r\n",
      "Train Epoch: 9 [73600/110534 (67%)]\tClassification Loss: 1.7911\r\n",
      "Train Epoch: 9 [74240/110534 (67%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 9 [74880/110534 (68%)]\tClassification Loss: 1.2161\r\n",
      "Train Epoch: 9 [75520/110534 (68%)]\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 9 [76160/110534 (69%)]\tClassification Loss: 1.3167\r\n",
      "Train Epoch: 9 [76800/110534 (69%)]\tClassification Loss: 1.2786\r\n",
      "Train Epoch: 9 [77440/110534 (70%)]\tClassification Loss: 1.4871\r\n",
      "Train Epoch: 9 [78080/110534 (71%)]\tClassification Loss: 1.3520\r\n",
      "Train Epoch: 9 [78720/110534 (71%)]\tClassification Loss: 1.3487\r\n",
      "Train Epoch: 9 [79360/110534 (72%)]\tClassification Loss: 1.4122\r\n",
      "Train Epoch: 9 [80000/110534 (72%)]\tClassification Loss: 1.4845\r\n",
      "Train Epoch: 9 [80640/110534 (73%)]\tClassification Loss: 1.3902\r\n",
      "Train Epoch: 9 [81280/110534 (74%)]\tClassification Loss: 1.8435\r\n",
      "Train Epoch: 9 [81920/110534 (74%)]\tClassification Loss: 1.4744\r\n",
      "Train Epoch: 9 [82560/110534 (75%)]\tClassification Loss: 1.8104\r\n",
      "Train Epoch: 9 [83200/110534 (75%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 9 [83840/110534 (76%)]\tClassification Loss: 1.8918\r\n",
      "Train Epoch: 9 [84480/110534 (76%)]\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 9 [85120/110534 (77%)]\tClassification Loss: 1.3430\r\n",
      "Train Epoch: 9 [85760/110534 (78%)]\tClassification Loss: 1.6784\r\n",
      "Train Epoch: 9 [86400/110534 (78%)]\tClassification Loss: 1.6209\r\n",
      "Train Epoch: 9 [87040/110534 (79%)]\tClassification Loss: 1.4734\r\n",
      "Train Epoch: 9 [87680/110534 (79%)]\tClassification Loss: 1.3652\r\n",
      "Train Epoch: 9 [88320/110534 (80%)]\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 9 [88960/110534 (80%)]\tClassification Loss: 1.6454\r\n",
      "Train Epoch: 9 [89600/110534 (81%)]\tClassification Loss: 1.7950\r\n",
      "Train Epoch: 9 [90240/110534 (82%)]\tClassification Loss: 1.9066\r\n",
      "Train Epoch: 9 [90880/110534 (82%)]\tClassification Loss: 1.5685\r\n",
      "Train Epoch: 9 [91520/110534 (83%)]\tClassification Loss: 1.2874\r\n",
      "Train Epoch: 9 [92160/110534 (83%)]\tClassification Loss: 1.3995\r\n",
      "Train Epoch: 9 [92800/110534 (84%)]\tClassification Loss: 1.4631\r\n",
      "Train Epoch: 9 [93440/110534 (85%)]\tClassification Loss: 1.6814\r\n",
      "Train Epoch: 9 [94080/110534 (85%)]\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 9 [94720/110534 (86%)]\tClassification Loss: 1.3713\r\n",
      "Train Epoch: 9 [95360/110534 (86%)]\tClassification Loss: 1.3544\r\n",
      "Train Epoch: 9 [96000/110534 (87%)]\tClassification Loss: 1.3402\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1500.pth.tar\r\n",
      "Train Epoch: 9 [96640/110534 (87%)]\tClassification Loss: 1.4281\r\n",
      "Train Epoch: 9 [97280/110534 (88%)]\tClassification Loss: 1.2246\r\n",
      "Train Epoch: 9 [97920/110534 (89%)]\tClassification Loss: 1.1602\r\n",
      "Train Epoch: 9 [98560/110534 (89%)]\tClassification Loss: 1.5374\r\n",
      "Train Epoch: 9 [99200/110534 (90%)]\tClassification Loss: 1.6025\r\n",
      "Train Epoch: 9 [99840/110534 (90%)]\tClassification Loss: 1.5543\r\n",
      "Train Epoch: 9 [100480/110534 (91%)]\tClassification Loss: 1.6570\r\n",
      "Train Epoch: 9 [101120/110534 (91%)]\tClassification Loss: 1.4446\r\n",
      "Train Epoch: 9 [101760/110534 (92%)]\tClassification Loss: 1.5817\r\n",
      "Train Epoch: 9 [102400/110534 (93%)]\tClassification Loss: 1.3148\r\n",
      "Train Epoch: 9 [103040/110534 (93%)]\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 9 [103680/110534 (94%)]\tClassification Loss: 1.7466\r\n",
      "Train Epoch: 9 [104320/110534 (94%)]\tClassification Loss: 1.3955\r\n",
      "Train Epoch: 9 [104960/110534 (95%)]\tClassification Loss: 1.4372\r\n",
      "Train Epoch: 9 [105600/110534 (96%)]\tClassification Loss: 1.4732\r\n",
      "Train Epoch: 9 [106240/110534 (96%)]\tClassification Loss: 1.2440\r\n",
      "Train Epoch: 9 [106880/110534 (97%)]\tClassification Loss: 1.7100\r\n",
      "Train Epoch: 9 [107520/110534 (97%)]\tClassification Loss: 1.4416\r\n",
      "Train Epoch: 9 [108160/110534 (98%)]\tClassification Loss: 1.6324\r\n",
      "Train Epoch: 9 [108800/110534 (98%)]\tClassification Loss: 1.6718\r\n",
      "Train Epoch: 9 [109440/110534 (99%)]\tClassification Loss: 1.3911\r\n",
      "Train Epoch: 9 [110080/110534 (100%)]\tClassification Loss: 1.4637\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_final.pth.tar\r\n",
      "Train Epoch: 10 [0/110534 (0%)]\tClassification Loss: 1.5631\r\n",
      "\r\n",
      "Test set: Average loss: 1.4198, Accuracy: 23207/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 10 [640/110534 (1%)]\tClassification Loss: 1.4471\r\n",
      "Train Epoch: 10 [1280/110534 (1%)]\tClassification Loss: 1.8677\r\n",
      "Train Epoch: 10 [1920/110534 (2%)]\tClassification Loss: 1.6444\r\n",
      "Train Epoch: 10 [2560/110534 (2%)]\tClassification Loss: 1.7606\r\n",
      "Train Epoch: 10 [3200/110534 (3%)]\tClassification Loss: 1.7067\r\n",
      "Train Epoch: 10 [3840/110534 (3%)]\tClassification Loss: 1.3999\r\n",
      "Train Epoch: 10 [4480/110534 (4%)]\tClassification Loss: 1.5163\r\n",
      "Train Epoch: 10 [5120/110534 (5%)]\tClassification Loss: 1.7455\r\n",
      "Train Epoch: 10 [5760/110534 (5%)]\tClassification Loss: 1.6171\r\n",
      "Train Epoch: 10 [6400/110534 (6%)]\tClassification Loss: 1.0950\r\n",
      "Train Epoch: 10 [7040/110534 (6%)]\tClassification Loss: 1.4616\r\n",
      "Train Epoch: 10 [7680/110534 (7%)]\tClassification Loss: 1.5909\r\n",
      "Train Epoch: 10 [8320/110534 (8%)]\tClassification Loss: 1.7520\r\n",
      "Train Epoch: 10 [8960/110534 (8%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 10 [9600/110534 (9%)]\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 10 [10240/110534 (9%)]\tClassification Loss: 1.5348\r\n",
      "Train Epoch: 10 [10880/110534 (10%)]\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 10 [11520/110534 (10%)]\tClassification Loss: 1.6700\r\n",
      "Train Epoch: 10 [12160/110534 (11%)]\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 10 [12800/110534 (12%)]\tClassification Loss: 1.6045\r\n",
      "Train Epoch: 10 [13440/110534 (12%)]\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 10 [14080/110534 (13%)]\tClassification Loss: 1.4832\r\n",
      "Train Epoch: 10 [14720/110534 (13%)]\tClassification Loss: 1.6873\r\n",
      "Train Epoch: 10 [15360/110534 (14%)]\tClassification Loss: 1.5242\r\n",
      "Train Epoch: 10 [16000/110534 (14%)]\tClassification Loss: 1.8053\r\n",
      "Train Epoch: 10 [16640/110534 (15%)]\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 10 [17280/110534 (16%)]\tClassification Loss: 1.4380\r\n",
      "Train Epoch: 10 [17920/110534 (16%)]\tClassification Loss: 1.6091\r\n",
      "Train Epoch: 10 [18560/110534 (17%)]\tClassification Loss: 1.7910\r\n",
      "Train Epoch: 10 [19200/110534 (17%)]\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 10 [19840/110534 (18%)]\tClassification Loss: 1.4825\r\n",
      "Train Epoch: 10 [20480/110534 (19%)]\tClassification Loss: 1.6197\r\n",
      "Train Epoch: 10 [21120/110534 (19%)]\tClassification Loss: 1.6576\r\n",
      "Train Epoch: 10 [21760/110534 (20%)]\tClassification Loss: 1.4078\r\n",
      "Train Epoch: 10 [22400/110534 (20%)]\tClassification Loss: 1.6399\r\n",
      "Train Epoch: 10 [23040/110534 (21%)]\tClassification Loss: 1.0742\r\n",
      "Train Epoch: 10 [23680/110534 (21%)]\tClassification Loss: 1.5129\r\n",
      "Train Epoch: 10 [24320/110534 (22%)]\tClassification Loss: 1.2492\r\n",
      "Train Epoch: 10 [24960/110534 (23%)]\tClassification Loss: 1.7786\r\n",
      "Train Epoch: 10 [25600/110534 (23%)]\tClassification Loss: 1.2680\r\n",
      "Train Epoch: 10 [26240/110534 (24%)]\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 10 [26880/110534 (24%)]\tClassification Loss: 1.8365\r\n",
      "Train Epoch: 10 [27520/110534 (25%)]\tClassification Loss: 1.4744\r\n",
      "Train Epoch: 10 [28160/110534 (25%)]\tClassification Loss: 1.7487\r\n",
      "Train Epoch: 10 [28800/110534 (26%)]\tClassification Loss: 1.9155\r\n",
      "Train Epoch: 10 [29440/110534 (27%)]\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 10 [30080/110534 (27%)]\tClassification Loss: 1.6919\r\n",
      "Train Epoch: 10 [30720/110534 (28%)]\tClassification Loss: 1.3392\r\n",
      "Train Epoch: 10 [31360/110534 (28%)]\tClassification Loss: 1.5958\r\n",
      "Train Epoch: 10 [32000/110534 (29%)]\tClassification Loss: 1.4692\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_500.pth.tar\r\n",
      "Train Epoch: 10 [32640/110534 (30%)]\tClassification Loss: 1.3451\r\n",
      "Train Epoch: 10 [33280/110534 (30%)]\tClassification Loss: 1.5026\r\n",
      "Train Epoch: 10 [33920/110534 (31%)]\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 10 [34560/110534 (31%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 10 [35200/110534 (32%)]\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 10 [35840/110534 (32%)]\tClassification Loss: 1.1466\r\n",
      "Train Epoch: 10 [36480/110534 (33%)]\tClassification Loss: 1.3738\r\n",
      "Train Epoch: 10 [37120/110534 (34%)]\tClassification Loss: 1.4526\r\n",
      "Train Epoch: 10 [37760/110534 (34%)]\tClassification Loss: 1.8711\r\n",
      "Train Epoch: 10 [38400/110534 (35%)]\tClassification Loss: 1.3506\r\n",
      "Train Epoch: 10 [39040/110534 (35%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 10 [39680/110534 (36%)]\tClassification Loss: 1.6718\r\n",
      "Train Epoch: 10 [40320/110534 (36%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 10 [40960/110534 (37%)]\tClassification Loss: 1.4577\r\n",
      "Train Epoch: 10 [41600/110534 (38%)]\tClassification Loss: 1.5521\r\n",
      "Train Epoch: 10 [42240/110534 (38%)]\tClassification Loss: 1.4509\r\n",
      "Train Epoch: 10 [42880/110534 (39%)]\tClassification Loss: 1.4278\r\n",
      "Train Epoch: 10 [43520/110534 (39%)]\tClassification Loss: 1.4298\r\n",
      "Train Epoch: 10 [44160/110534 (40%)]\tClassification Loss: 1.5252\r\n",
      "Train Epoch: 10 [44800/110534 (41%)]\tClassification Loss: 1.6072\r\n",
      "Train Epoch: 10 [45440/110534 (41%)]\tClassification Loss: 1.6512\r\n",
      "Train Epoch: 10 [46080/110534 (42%)]\tClassification Loss: 1.3623\r\n",
      "Train Epoch: 10 [46720/110534 (42%)]\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 10 [47360/110534 (43%)]\tClassification Loss: 1.5590\r\n",
      "Train Epoch: 10 [48000/110534 (43%)]\tClassification Loss: 1.7000\r\n",
      "Train Epoch: 10 [48640/110534 (44%)]\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 10 [49280/110534 (45%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 10 [49920/110534 (45%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 10 [50560/110534 (46%)]\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 10 [51200/110534 (46%)]\tClassification Loss: 1.4758\r\n",
      "Train Epoch: 10 [51840/110534 (47%)]\tClassification Loss: 1.5801\r\n",
      "Train Epoch: 10 [52480/110534 (47%)]\tClassification Loss: 1.4728\r\n",
      "Train Epoch: 10 [53120/110534 (48%)]\tClassification Loss: 1.1158\r\n",
      "Train Epoch: 10 [53760/110534 (49%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 10 [54400/110534 (49%)]\tClassification Loss: 1.5178\r\n",
      "Train Epoch: 10 [55040/110534 (50%)]\tClassification Loss: 1.6891\r\n",
      "Train Epoch: 10 [55680/110534 (50%)]\tClassification Loss: 1.5820\r\n",
      "Train Epoch: 10 [56320/110534 (51%)]\tClassification Loss: 1.4866\r\n",
      "Train Epoch: 10 [56960/110534 (52%)]\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 10 [57600/110534 (52%)]\tClassification Loss: 1.6529\r\n",
      "Train Epoch: 10 [58240/110534 (53%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 10 [58880/110534 (53%)]\tClassification Loss: 1.7479\r\n",
      "Train Epoch: 10 [59520/110534 (54%)]\tClassification Loss: 1.2689\r\n",
      "Train Epoch: 10 [60160/110534 (54%)]\tClassification Loss: 1.3937\r\n",
      "Train Epoch: 10 [60800/110534 (55%)]\tClassification Loss: 1.7538\r\n",
      "Train Epoch: 10 [61440/110534 (56%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 10 [62080/110534 (56%)]\tClassification Loss: 1.4158\r\n",
      "Train Epoch: 10 [62720/110534 (57%)]\tClassification Loss: 1.7379\r\n",
      "Train Epoch: 10 [63360/110534 (57%)]\tClassification Loss: 1.1356\r\n",
      "Train Epoch: 10 [64000/110534 (58%)]\tClassification Loss: 1.5844\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1000.pth.tar\r\n",
      "Train Epoch: 10 [64640/110534 (58%)]\tClassification Loss: 1.1809\r\n",
      "Train Epoch: 10 [65280/110534 (59%)]\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 10 [65920/110534 (60%)]\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 10 [66560/110534 (60%)]\tClassification Loss: 1.4119\r\n",
      "Train Epoch: 10 [67200/110534 (61%)]\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 10 [67840/110534 (61%)]\tClassification Loss: 1.8978\r\n",
      "Train Epoch: 10 [68480/110534 (62%)]\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 10 [69120/110534 (63%)]\tClassification Loss: 1.6910\r\n",
      "Train Epoch: 10 [69760/110534 (63%)]\tClassification Loss: 1.6503\r\n",
      "Train Epoch: 10 [70400/110534 (64%)]\tClassification Loss: 1.2928\r\n",
      "Train Epoch: 10 [71040/110534 (64%)]\tClassification Loss: 1.7801\r\n",
      "Train Epoch: 10 [71680/110534 (65%)]\tClassification Loss: 1.5303\r\n",
      "Train Epoch: 10 [72320/110534 (65%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 10 [72960/110534 (66%)]\tClassification Loss: 1.6676\r\n",
      "Train Epoch: 10 [73600/110534 (67%)]\tClassification Loss: 1.8558\r\n",
      "Train Epoch: 10 [74240/110534 (67%)]\tClassification Loss: 1.7429\r\n",
      "Train Epoch: 10 [74880/110534 (68%)]\tClassification Loss: 1.3449\r\n",
      "Train Epoch: 10 [75520/110534 (68%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 10 [76160/110534 (69%)]\tClassification Loss: 1.4190\r\n",
      "Train Epoch: 10 [76800/110534 (69%)]\tClassification Loss: 1.3735\r\n",
      "Train Epoch: 10 [77440/110534 (70%)]\tClassification Loss: 1.5075\r\n",
      "Train Epoch: 10 [78080/110534 (71%)]\tClassification Loss: 1.4020\r\n",
      "Train Epoch: 10 [78720/110534 (71%)]\tClassification Loss: 1.3394\r\n",
      "Train Epoch: 10 [79360/110534 (72%)]\tClassification Loss: 1.2740\r\n",
      "Train Epoch: 10 [80000/110534 (72%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 10 [80640/110534 (73%)]\tClassification Loss: 1.3746\r\n",
      "Train Epoch: 10 [81280/110534 (74%)]\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 10 [81920/110534 (74%)]\tClassification Loss: 1.5167\r\n",
      "Train Epoch: 10 [82560/110534 (75%)]\tClassification Loss: 1.7650\r\n",
      "Train Epoch: 10 [83200/110534 (75%)]\tClassification Loss: 1.5284\r\n",
      "Train Epoch: 10 [83840/110534 (76%)]\tClassification Loss: 1.9477\r\n",
      "Train Epoch: 10 [84480/110534 (76%)]\tClassification Loss: 1.6657\r\n",
      "Train Epoch: 10 [85120/110534 (77%)]\tClassification Loss: 1.3833\r\n",
      "Train Epoch: 10 [85760/110534 (78%)]\tClassification Loss: 1.5285\r\n",
      "Train Epoch: 10 [86400/110534 (78%)]\tClassification Loss: 1.5799\r\n",
      "Train Epoch: 10 [87040/110534 (79%)]\tClassification Loss: 1.4791\r\n",
      "Train Epoch: 10 [87680/110534 (79%)]\tClassification Loss: 1.4394\r\n",
      "Train Epoch: 10 [88320/110534 (80%)]\tClassification Loss: 1.6000\r\n",
      "Train Epoch: 10 [88960/110534 (80%)]\tClassification Loss: 1.6307\r\n",
      "Train Epoch: 10 [89600/110534 (81%)]\tClassification Loss: 1.7364\r\n",
      "Train Epoch: 10 [90240/110534 (82%)]\tClassification Loss: 1.7441\r\n",
      "Train Epoch: 10 [90880/110534 (82%)]\tClassification Loss: 1.6572\r\n",
      "Train Epoch: 10 [91520/110534 (83%)]\tClassification Loss: 1.2386\r\n",
      "Train Epoch: 10 [92160/110534 (83%)]\tClassification Loss: 1.3542\r\n",
      "Train Epoch: 10 [92800/110534 (84%)]\tClassification Loss: 1.4937\r\n",
      "Train Epoch: 10 [93440/110534 (85%)]\tClassification Loss: 1.6960\r\n",
      "Train Epoch: 10 [94080/110534 (85%)]\tClassification Loss: 1.6755\r\n",
      "Train Epoch: 10 [94720/110534 (86%)]\tClassification Loss: 1.4013\r\n",
      "Train Epoch: 10 [95360/110534 (86%)]\tClassification Loss: 1.6103\r\n",
      "Train Epoch: 10 [96000/110534 (87%)]\tClassification Loss: 1.5072\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1500.pth.tar\r\n",
      "Train Epoch: 10 [96640/110534 (87%)]\tClassification Loss: 1.3830\r\n",
      "Train Epoch: 10 [97280/110534 (88%)]\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 10 [97920/110534 (89%)]\tClassification Loss: 1.1668\r\n",
      "Train Epoch: 10 [98560/110534 (89%)]\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 10 [99200/110534 (90%)]\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 10 [99840/110534 (90%)]\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 10 [100480/110534 (91%)]\tClassification Loss: 1.6545\r\n",
      "Train Epoch: 10 [101120/110534 (91%)]\tClassification Loss: 1.5388\r\n",
      "Train Epoch: 10 [101760/110534 (92%)]\tClassification Loss: 1.4965\r\n",
      "Train Epoch: 10 [102400/110534 (93%)]\tClassification Loss: 1.5158\r\n",
      "Train Epoch: 10 [103040/110534 (93%)]\tClassification Loss: 1.4995\r\n",
      "Train Epoch: 10 [103680/110534 (94%)]\tClassification Loss: 1.6944\r\n",
      "Train Epoch: 10 [104320/110534 (94%)]\tClassification Loss: 1.2434\r\n",
      "Train Epoch: 10 [104960/110534 (95%)]\tClassification Loss: 1.3884\r\n",
      "Train Epoch: 10 [105600/110534 (96%)]\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 10 [106240/110534 (96%)]\tClassification Loss: 1.4046\r\n",
      "Train Epoch: 10 [106880/110534 (97%)]\tClassification Loss: 1.7305\r\n",
      "Train Epoch: 10 [107520/110534 (97%)]\tClassification Loss: 1.7363\r\n",
      "Train Epoch: 10 [108160/110534 (98%)]\tClassification Loss: 1.4756\r\n",
      "Train Epoch: 10 [108800/110534 (98%)]\tClassification Loss: 1.6258\r\n",
      "Train Epoch: 10 [109440/110534 (99%)]\tClassification Loss: 1.4770\r\n",
      "Train Epoch: 10 [110080/110534 (100%)]\tClassification Loss: 1.4643\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_final.pth.tar\r\n",
      "Train Epoch: 11 [0/110534 (0%)]\tClassification Loss: 1.5511\r\n",
      "\r\n",
      "Test set: Average loss: 1.4156, Accuracy: 23262/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 11 [640/110534 (1%)]\tClassification Loss: 1.3239\r\n",
      "Train Epoch: 11 [1280/110534 (1%)]\tClassification Loss: 2.0091\r\n",
      "Train Epoch: 11 [1920/110534 (2%)]\tClassification Loss: 1.6261\r\n",
      "Train Epoch: 11 [2560/110534 (2%)]\tClassification Loss: 1.7683\r\n",
      "Train Epoch: 11 [3200/110534 (3%)]\tClassification Loss: 1.5032\r\n",
      "Train Epoch: 11 [3840/110534 (3%)]\tClassification Loss: 1.4544\r\n",
      "Train Epoch: 11 [4480/110534 (4%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 11 [5120/110534 (5%)]\tClassification Loss: 1.6576\r\n",
      "Train Epoch: 11 [5760/110534 (5%)]\tClassification Loss: 1.4692\r\n",
      "Train Epoch: 11 [6400/110534 (6%)]\tClassification Loss: 1.2019\r\n",
      "Train Epoch: 11 [7040/110534 (6%)]\tClassification Loss: 1.4813\r\n",
      "Train Epoch: 11 [7680/110534 (7%)]\tClassification Loss: 1.6831\r\n",
      "Train Epoch: 11 [8320/110534 (8%)]\tClassification Loss: 1.8637\r\n",
      "Train Epoch: 11 [8960/110534 (8%)]\tClassification Loss: 1.7141\r\n",
      "Train Epoch: 11 [9600/110534 (9%)]\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 11 [10240/110534 (9%)]\tClassification Loss: 1.5472\r\n",
      "Train Epoch: 11 [10880/110534 (10%)]\tClassification Loss: 1.3915\r\n",
      "Train Epoch: 11 [11520/110534 (10%)]\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 11 [12160/110534 (11%)]\tClassification Loss: 1.5081\r\n",
      "Train Epoch: 11 [12800/110534 (12%)]\tClassification Loss: 1.5516\r\n",
      "Train Epoch: 11 [13440/110534 (12%)]\tClassification Loss: 1.5283\r\n",
      "Train Epoch: 11 [14080/110534 (13%)]\tClassification Loss: 1.5324\r\n",
      "Train Epoch: 11 [14720/110534 (13%)]\tClassification Loss: 1.7908\r\n",
      "Train Epoch: 11 [15360/110534 (14%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 11 [16000/110534 (14%)]\tClassification Loss: 1.5558\r\n",
      "Train Epoch: 11 [16640/110534 (15%)]\tClassification Loss: 1.5548\r\n",
      "Train Epoch: 11 [17280/110534 (16%)]\tClassification Loss: 1.7140\r\n",
      "Train Epoch: 11 [17920/110534 (16%)]\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 11 [18560/110534 (17%)]\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 11 [19200/110534 (17%)]\tClassification Loss: 1.6065\r\n",
      "Train Epoch: 11 [19840/110534 (18%)]\tClassification Loss: 1.5478\r\n",
      "Train Epoch: 11 [20480/110534 (19%)]\tClassification Loss: 1.7443\r\n",
      "Train Epoch: 11 [21120/110534 (19%)]\tClassification Loss: 1.5401\r\n",
      "Train Epoch: 11 [21760/110534 (20%)]\tClassification Loss: 1.4607\r\n",
      "Train Epoch: 11 [22400/110534 (20%)]\tClassification Loss: 1.4740\r\n",
      "Train Epoch: 11 [23040/110534 (21%)]\tClassification Loss: 1.1079\r\n",
      "Train Epoch: 11 [23680/110534 (21%)]\tClassification Loss: 1.7346\r\n",
      "Train Epoch: 11 [24320/110534 (22%)]\tClassification Loss: 1.1779\r\n",
      "Train Epoch: 11 [24960/110534 (23%)]\tClassification Loss: 1.6633\r\n",
      "Train Epoch: 11 [25600/110534 (23%)]\tClassification Loss: 1.3355\r\n",
      "Train Epoch: 11 [26240/110534 (24%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 11 [26880/110534 (24%)]\tClassification Loss: 1.7327\r\n",
      "Train Epoch: 11 [27520/110534 (25%)]\tClassification Loss: 1.3528\r\n",
      "Train Epoch: 11 [28160/110534 (25%)]\tClassification Loss: 1.8330\r\n",
      "Train Epoch: 11 [28800/110534 (26%)]\tClassification Loss: 1.8022\r\n",
      "Train Epoch: 11 [29440/110534 (27%)]\tClassification Loss: 1.5378\r\n",
      "Train Epoch: 11 [30080/110534 (27%)]\tClassification Loss: 1.8528\r\n",
      "Train Epoch: 11 [30720/110534 (28%)]\tClassification Loss: 1.5008\r\n",
      "Train Epoch: 11 [31360/110534 (28%)]\tClassification Loss: 1.5978\r\n",
      "Train Epoch: 11 [32000/110534 (29%)]\tClassification Loss: 1.6073\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_500.pth.tar\r\n",
      "Train Epoch: 11 [32640/110534 (30%)]\tClassification Loss: 1.3243\r\n",
      "Train Epoch: 11 [33280/110534 (30%)]\tClassification Loss: 1.5834\r\n",
      "Train Epoch: 11 [33920/110534 (31%)]\tClassification Loss: 1.4658\r\n",
      "Train Epoch: 11 [34560/110534 (31%)]\tClassification Loss: 1.6864\r\n",
      "Train Epoch: 11 [35200/110534 (32%)]\tClassification Loss: 1.3213\r\n",
      "Train Epoch: 11 [35840/110534 (32%)]\tClassification Loss: 1.2605\r\n",
      "Train Epoch: 11 [36480/110534 (33%)]\tClassification Loss: 1.4081\r\n",
      "Train Epoch: 11 [37120/110534 (34%)]\tClassification Loss: 1.6213\r\n",
      "Train Epoch: 11 [37760/110534 (34%)]\tClassification Loss: 1.9465\r\n",
      "Train Epoch: 11 [38400/110534 (35%)]\tClassification Loss: 1.5000\r\n",
      "Train Epoch: 11 [39040/110534 (35%)]\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 11 [39680/110534 (36%)]\tClassification Loss: 1.5513\r\n",
      "Train Epoch: 11 [40320/110534 (36%)]\tClassification Loss: 1.5476\r\n",
      "Train Epoch: 11 [40960/110534 (37%)]\tClassification Loss: 1.2751\r\n",
      "Train Epoch: 11 [41600/110534 (38%)]\tClassification Loss: 1.4255\r\n",
      "Train Epoch: 11 [42240/110534 (38%)]\tClassification Loss: 1.3651\r\n",
      "Train Epoch: 11 [42880/110534 (39%)]\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 11 [43520/110534 (39%)]\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 11 [44160/110534 (40%)]\tClassification Loss: 1.6021\r\n",
      "Train Epoch: 11 [44800/110534 (41%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 11 [45440/110534 (41%)]\tClassification Loss: 1.5327\r\n",
      "Train Epoch: 11 [46080/110534 (42%)]\tClassification Loss: 1.3901\r\n",
      "Train Epoch: 11 [46720/110534 (42%)]\tClassification Loss: 1.4683\r\n",
      "Train Epoch: 11 [47360/110534 (43%)]\tClassification Loss: 1.5566\r\n",
      "Train Epoch: 11 [48000/110534 (43%)]\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 11 [48640/110534 (44%)]\tClassification Loss: 1.3818\r\n",
      "Train Epoch: 11 [49280/110534 (45%)]\tClassification Loss: 1.5434\r\n",
      "Train Epoch: 11 [49920/110534 (45%)]\tClassification Loss: 1.6457\r\n",
      "Train Epoch: 11 [50560/110534 (46%)]\tClassification Loss: 1.5321\r\n",
      "Train Epoch: 11 [51200/110534 (46%)]\tClassification Loss: 1.5050\r\n",
      "Train Epoch: 11 [51840/110534 (47%)]\tClassification Loss: 1.4789\r\n",
      "Train Epoch: 11 [52480/110534 (47%)]\tClassification Loss: 1.4359\r\n",
      "Train Epoch: 11 [53120/110534 (48%)]\tClassification Loss: 1.1505\r\n",
      "Train Epoch: 11 [53760/110534 (49%)]\tClassification Loss: 1.6150\r\n",
      "Train Epoch: 11 [54400/110534 (49%)]\tClassification Loss: 1.4920\r\n",
      "Train Epoch: 11 [55040/110534 (50%)]\tClassification Loss: 1.6920\r\n",
      "Train Epoch: 11 [55680/110534 (50%)]\tClassification Loss: 1.6043\r\n",
      "Train Epoch: 11 [56320/110534 (51%)]\tClassification Loss: 1.4796\r\n",
      "Train Epoch: 11 [56960/110534 (52%)]\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 11 [57600/110534 (52%)]\tClassification Loss: 1.6960\r\n",
      "Train Epoch: 11 [58240/110534 (53%)]\tClassification Loss: 1.5748\r\n",
      "Train Epoch: 11 [58880/110534 (53%)]\tClassification Loss: 1.7636\r\n",
      "Train Epoch: 11 [59520/110534 (54%)]\tClassification Loss: 1.4150\r\n",
      "Train Epoch: 11 [60160/110534 (54%)]\tClassification Loss: 1.4201\r\n",
      "Train Epoch: 11 [60800/110534 (55%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 11 [61440/110534 (56%)]\tClassification Loss: 1.5345\r\n",
      "Train Epoch: 11 [62080/110534 (56%)]\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 11 [62720/110534 (57%)]\tClassification Loss: 1.6831\r\n",
      "Train Epoch: 11 [63360/110534 (57%)]\tClassification Loss: 1.3350\r\n",
      "Train Epoch: 11 [64000/110534 (58%)]\tClassification Loss: 1.4935\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_1000.pth.tar\r\n",
      "Train Epoch: 11 [64640/110534 (58%)]\tClassification Loss: 1.3571\r\n",
      "Train Epoch: 11 [65280/110534 (59%)]\tClassification Loss: 1.5576\r\n",
      "Train Epoch: 11 [65920/110534 (60%)]\tClassification Loss: 1.5362\r\n",
      "Train Epoch: 11 [66560/110534 (60%)]\tClassification Loss: 1.3687\r\n",
      "Train Epoch: 11 [67200/110534 (61%)]\tClassification Loss: 1.2970\r\n",
      "Train Epoch: 11 [67840/110534 (61%)]\tClassification Loss: 1.9044\r\n",
      "Train Epoch: 11 [68480/110534 (62%)]\tClassification Loss: 1.6260\r\n",
      "Train Epoch: 11 [69120/110534 (63%)]\tClassification Loss: 1.7059\r\n",
      "Train Epoch: 11 [69760/110534 (63%)]\tClassification Loss: 1.8084\r\n",
      "Train Epoch: 11 [70400/110534 (64%)]\tClassification Loss: 1.1782\r\n",
      "Train Epoch: 11 [71040/110534 (64%)]\tClassification Loss: 1.9740\r\n",
      "Train Epoch: 11 [71680/110534 (65%)]\tClassification Loss: 1.4394\r\n",
      "Train Epoch: 11 [72320/110534 (65%)]\tClassification Loss: 1.6754\r\n",
      "Train Epoch: 11 [72960/110534 (66%)]\tClassification Loss: 1.7292\r\n",
      "Train Epoch: 11 [73600/110534 (67%)]\tClassification Loss: 1.8334\r\n",
      "Train Epoch: 11 [74240/110534 (67%)]\tClassification Loss: 1.7359\r\n",
      "Train Epoch: 11 [74880/110534 (68%)]\tClassification Loss: 1.3086\r\n",
      "Train Epoch: 11 [75520/110534 (68%)]\tClassification Loss: 1.5242\r\n",
      "Train Epoch: 11 [76160/110534 (69%)]\tClassification Loss: 1.4073\r\n",
      "Train Epoch: 11 [76800/110534 (69%)]\tClassification Loss: 1.3172\r\n",
      "Train Epoch: 11 [77440/110534 (70%)]\tClassification Loss: 1.3043\r\n",
      "Train Epoch: 11 [78080/110534 (71%)]\tClassification Loss: 1.4757\r\n",
      "Train Epoch: 11 [78720/110534 (71%)]\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 11 [79360/110534 (72%)]\tClassification Loss: 1.5059\r\n",
      "Train Epoch: 11 [80000/110534 (72%)]\tClassification Loss: 1.4919\r\n",
      "Train Epoch: 11 [80640/110534 (73%)]\tClassification Loss: 1.4308\r\n",
      "Train Epoch: 11 [81280/110534 (74%)]\tClassification Loss: 1.6016\r\n",
      "Train Epoch: 11 [81920/110534 (74%)]\tClassification Loss: 1.5726\r\n",
      "Train Epoch: 11 [82560/110534 (75%)]\tClassification Loss: 1.8289\r\n",
      "Train Epoch: 11 [83200/110534 (75%)]\tClassification Loss: 1.5496\r\n",
      "Train Epoch: 11 [83840/110534 (76%)]\tClassification Loss: 1.7728\r\n",
      "Train Epoch: 11 [84480/110534 (76%)]\tClassification Loss: 1.6484\r\n",
      "Train Epoch: 11 [85120/110534 (77%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 11 [85760/110534 (78%)]\tClassification Loss: 1.5037\r\n",
      "Train Epoch: 11 [86400/110534 (78%)]\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 11 [87040/110534 (79%)]\tClassification Loss: 1.5202\r\n",
      "Train Epoch: 11 [87680/110534 (79%)]\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 11 [88320/110534 (80%)]\tClassification Loss: 1.4754\r\n",
      "Train Epoch: 11 [88960/110534 (80%)]\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 11 [89600/110534 (81%)]\tClassification Loss: 1.8409\r\n",
      "Train Epoch: 11 [90240/110534 (82%)]\tClassification Loss: 1.7317\r\n",
      "Train Epoch: 11 [90880/110534 (82%)]\tClassification Loss: 1.5537\r\n",
      "Train Epoch: 11 [91520/110534 (83%)]\tClassification Loss: 1.2525\r\n",
      "Train Epoch: 11 [92160/110534 (83%)]\tClassification Loss: 1.3008\r\n",
      "Train Epoch: 11 [92800/110534 (84%)]\tClassification Loss: 1.3153\r\n",
      "Train Epoch: 11 [93440/110534 (85%)]\tClassification Loss: 1.7262\r\n",
      "Train Epoch: 11 [94080/110534 (85%)]\tClassification Loss: 1.5758\r\n",
      "Train Epoch: 11 [94720/110534 (86%)]\tClassification Loss: 1.4488\r\n",
      "Train Epoch: 11 [95360/110534 (86%)]\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 11 [96000/110534 (87%)]\tClassification Loss: 1.3771\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_1500.pth.tar\r\n",
      "Train Epoch: 11 [96640/110534 (87%)]\tClassification Loss: 1.3645\r\n",
      "Train Epoch: 11 [97280/110534 (88%)]\tClassification Loss: 1.2747\r\n",
      "Train Epoch: 11 [97920/110534 (89%)]\tClassification Loss: 1.1036\r\n",
      "Train Epoch: 11 [98560/110534 (89%)]\tClassification Loss: 1.4790\r\n",
      "Train Epoch: 11 [99200/110534 (90%)]\tClassification Loss: 1.5306\r\n",
      "Train Epoch: 11 [99840/110534 (90%)]\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 11 [100480/110534 (91%)]\tClassification Loss: 1.5752\r\n",
      "Train Epoch: 11 [101120/110534 (91%)]\tClassification Loss: 1.2981\r\n",
      "Train Epoch: 11 [101760/110534 (92%)]\tClassification Loss: 1.5773\r\n",
      "Train Epoch: 11 [102400/110534 (93%)]\tClassification Loss: 1.4125\r\n",
      "Train Epoch: 11 [103040/110534 (93%)]\tClassification Loss: 1.3419\r\n",
      "Train Epoch: 11 [103680/110534 (94%)]\tClassification Loss: 1.6814\r\n",
      "Train Epoch: 11 [104320/110534 (94%)]\tClassification Loss: 1.2885\r\n",
      "Train Epoch: 11 [104960/110534 (95%)]\tClassification Loss: 1.3849\r\n",
      "Train Epoch: 11 [105600/110534 (96%)]\tClassification Loss: 1.4134\r\n",
      "Train Epoch: 11 [106240/110534 (96%)]\tClassification Loss: 1.3807\r\n",
      "Train Epoch: 11 [106880/110534 (97%)]\tClassification Loss: 1.6946\r\n",
      "Train Epoch: 11 [107520/110534 (97%)]\tClassification Loss: 1.5994\r\n",
      "Train Epoch: 11 [108160/110534 (98%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 11 [108800/110534 (98%)]\tClassification Loss: 1.7778\r\n",
      "Train Epoch: 11 [109440/110534 (99%)]\tClassification Loss: 1.3648\r\n",
      "Train Epoch: 11 [110080/110534 (100%)]\tClassification Loss: 1.5201\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_final.pth.tar\r\n",
      "Train Epoch: 12 [0/110534 (0%)]\tClassification Loss: 1.4947\r\n",
      "\r\n",
      "Test set: Average loss: 1.4165, Accuracy: 23236/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 12 [640/110534 (1%)]\tClassification Loss: 1.3897\r\n",
      "Train Epoch: 12 [1280/110534 (1%)]\tClassification Loss: 1.9104\r\n",
      "Train Epoch: 12 [1920/110534 (2%)]\tClassification Loss: 1.4662\r\n",
      "Train Epoch: 12 [2560/110534 (2%)]\tClassification Loss: 1.8491\r\n",
      "Train Epoch: 12 [3200/110534 (3%)]\tClassification Loss: 1.5786\r\n",
      "Train Epoch: 12 [3840/110534 (3%)]\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 12 [4480/110534 (4%)]\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 12 [5120/110534 (5%)]\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 12 [5760/110534 (5%)]\tClassification Loss: 1.5555\r\n",
      "Train Epoch: 12 [6400/110534 (6%)]\tClassification Loss: 1.2386\r\n",
      "Train Epoch: 12 [7040/110534 (6%)]\tClassification Loss: 1.3900\r\n",
      "Train Epoch: 12 [7680/110534 (7%)]\tClassification Loss: 1.5105\r\n",
      "Train Epoch: 12 [8320/110534 (8%)]\tClassification Loss: 1.6315\r\n",
      "Train Epoch: 12 [8960/110534 (8%)]\tClassification Loss: 1.6478\r\n",
      "Train Epoch: 12 [9600/110534 (9%)]\tClassification Loss: 1.4959\r\n",
      "Train Epoch: 12 [10240/110534 (9%)]\tClassification Loss: 1.4070\r\n",
      "Train Epoch: 12 [10880/110534 (10%)]\tClassification Loss: 1.4624\r\n",
      "Train Epoch: 12 [11520/110534 (10%)]\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 12 [12160/110534 (11%)]\tClassification Loss: 1.4647\r\n",
      "Train Epoch: 12 [12800/110534 (12%)]\tClassification Loss: 1.7307\r\n",
      "Train Epoch: 12 [13440/110534 (12%)]\tClassification Loss: 1.4871\r\n",
      "Train Epoch: 12 [14080/110534 (13%)]\tClassification Loss: 1.6242\r\n",
      "Train Epoch: 12 [14720/110534 (13%)]\tClassification Loss: 1.8058\r\n",
      "Train Epoch: 12 [15360/110534 (14%)]\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 12 [16000/110534 (14%)]\tClassification Loss: 1.6473\r\n",
      "Train Epoch: 12 [16640/110534 (15%)]\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 12 [17280/110534 (16%)]\tClassification Loss: 1.6814\r\n",
      "Train Epoch: 12 [17920/110534 (16%)]\tClassification Loss: 1.6849\r\n",
      "Train Epoch: 12 [18560/110534 (17%)]\tClassification Loss: 1.6764\r\n",
      "Train Epoch: 12 [19200/110534 (17%)]\tClassification Loss: 1.5948\r\n",
      "Train Epoch: 12 [19840/110534 (18%)]\tClassification Loss: 1.3559\r\n",
      "Train Epoch: 12 [20480/110534 (19%)]\tClassification Loss: 1.6637\r\n",
      "Train Epoch: 12 [21120/110534 (19%)]\tClassification Loss: 1.6223\r\n",
      "Train Epoch: 12 [21760/110534 (20%)]\tClassification Loss: 1.3926\r\n",
      "Train Epoch: 12 [22400/110534 (20%)]\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 12 [23040/110534 (21%)]\tClassification Loss: 1.0827\r\n",
      "Train Epoch: 12 [23680/110534 (21%)]\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 12 [24320/110534 (22%)]\tClassification Loss: 1.3051\r\n",
      "Train Epoch: 12 [24960/110534 (23%)]\tClassification Loss: 1.7739\r\n",
      "Train Epoch: 12 [25600/110534 (23%)]\tClassification Loss: 1.3056\r\n",
      "Train Epoch: 12 [26240/110534 (24%)]\tClassification Loss: 1.5105\r\n",
      "Train Epoch: 12 [26880/110534 (24%)]\tClassification Loss: 1.6965\r\n",
      "Train Epoch: 12 [27520/110534 (25%)]\tClassification Loss: 1.4502\r\n",
      "Train Epoch: 12 [28160/110534 (25%)]\tClassification Loss: 1.7946\r\n",
      "Train Epoch: 12 [28800/110534 (26%)]\tClassification Loss: 2.0113\r\n",
      "Train Epoch: 12 [29440/110534 (27%)]\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 12 [30080/110534 (27%)]\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 12 [30720/110534 (28%)]\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 12 [31360/110534 (28%)]\tClassification Loss: 1.6043\r\n",
      "Train Epoch: 12 [32000/110534 (29%)]\tClassification Loss: 1.5183\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_500.pth.tar\r\n",
      "Train Epoch: 12 [32640/110534 (30%)]\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 12 [33280/110534 (30%)]\tClassification Loss: 1.4683\r\n",
      "Train Epoch: 12 [33920/110534 (31%)]\tClassification Loss: 1.6189\r\n",
      "Train Epoch: 12 [34560/110534 (31%)]\tClassification Loss: 1.8133\r\n",
      "Train Epoch: 12 [35200/110534 (32%)]\tClassification Loss: 1.4181\r\n",
      "Train Epoch: 12 [35840/110534 (32%)]\tClassification Loss: 1.1302\r\n",
      "Train Epoch: 12 [36480/110534 (33%)]\tClassification Loss: 1.4135\r\n",
      "Train Epoch: 12 [37120/110534 (34%)]\tClassification Loss: 1.5312\r\n",
      "Train Epoch: 12 [37760/110534 (34%)]\tClassification Loss: 2.0688\r\n",
      "Train Epoch: 12 [38400/110534 (35%)]\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 12 [39040/110534 (35%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 12 [39680/110534 (36%)]\tClassification Loss: 1.5915\r\n",
      "Train Epoch: 12 [40320/110534 (36%)]\tClassification Loss: 1.6765\r\n",
      "Train Epoch: 12 [40960/110534 (37%)]\tClassification Loss: 1.2954\r\n",
      "Train Epoch: 12 [41600/110534 (38%)]\tClassification Loss: 1.4931\r\n",
      "Train Epoch: 12 [42240/110534 (38%)]\tClassification Loss: 1.4001\r\n",
      "Train Epoch: 12 [42880/110534 (39%)]\tClassification Loss: 1.3625\r\n",
      "Train Epoch: 12 [43520/110534 (39%)]\tClassification Loss: 1.3450\r\n",
      "Train Epoch: 12 [44160/110534 (40%)]\tClassification Loss: 1.5677\r\n",
      "Train Epoch: 12 [44800/110534 (41%)]\tClassification Loss: 1.5768\r\n",
      "Train Epoch: 12 [45440/110534 (41%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 12 [46080/110534 (42%)]\tClassification Loss: 1.2216\r\n",
      "Train Epoch: 12 [46720/110534 (42%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 12 [47360/110534 (43%)]\tClassification Loss: 1.5198\r\n",
      "Train Epoch: 12 [48000/110534 (43%)]\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 12 [48640/110534 (44%)]\tClassification Loss: 1.4176\r\n",
      "Train Epoch: 12 [49280/110534 (45%)]\tClassification Loss: 1.7309\r\n",
      "Train Epoch: 12 [49920/110534 (45%)]\tClassification Loss: 1.5556\r\n",
      "Train Epoch: 12 [50560/110534 (46%)]\tClassification Loss: 1.8154\r\n",
      "Train Epoch: 12 [51200/110534 (46%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 12 [51840/110534 (47%)]\tClassification Loss: 1.4863\r\n",
      "Train Epoch: 12 [52480/110534 (47%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 12 [53120/110534 (48%)]\tClassification Loss: 1.0785\r\n",
      "Train Epoch: 12 [53760/110534 (49%)]\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 12 [54400/110534 (49%)]\tClassification Loss: 1.3964\r\n",
      "Train Epoch: 12 [55040/110534 (50%)]\tClassification Loss: 1.7712\r\n",
      "Train Epoch: 12 [55680/110534 (50%)]\tClassification Loss: 1.7428\r\n",
      "Train Epoch: 12 [56320/110534 (51%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 12 [56960/110534 (52%)]\tClassification Loss: 1.4520\r\n",
      "Train Epoch: 12 [57600/110534 (52%)]\tClassification Loss: 1.5796\r\n",
      "Train Epoch: 12 [58240/110534 (53%)]\tClassification Loss: 1.5034\r\n",
      "Train Epoch: 12 [58880/110534 (53%)]\tClassification Loss: 1.8011\r\n",
      "Train Epoch: 12 [59520/110534 (54%)]\tClassification Loss: 1.3860\r\n",
      "Train Epoch: 12 [60160/110534 (54%)]\tClassification Loss: 1.4064\r\n",
      "Train Epoch: 12 [60800/110534 (55%)]\tClassification Loss: 1.5516\r\n",
      "Train Epoch: 12 [61440/110534 (56%)]\tClassification Loss: 1.5163\r\n",
      "Train Epoch: 12 [62080/110534 (56%)]\tClassification Loss: 1.6056\r\n",
      "Train Epoch: 12 [62720/110534 (57%)]\tClassification Loss: 1.6530\r\n",
      "Train Epoch: 12 [63360/110534 (57%)]\tClassification Loss: 1.4836\r\n",
      "Train Epoch: 12 [64000/110534 (58%)]\tClassification Loss: 1.5337\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_1000.pth.tar\r\n",
      "Train Epoch: 12 [64640/110534 (58%)]\tClassification Loss: 1.2170\r\n",
      "Train Epoch: 12 [65280/110534 (59%)]\tClassification Loss: 1.5315\r\n",
      "Train Epoch: 12 [65920/110534 (60%)]\tClassification Loss: 1.3840\r\n",
      "Train Epoch: 12 [66560/110534 (60%)]\tClassification Loss: 1.1850\r\n",
      "Train Epoch: 12 [67200/110534 (61%)]\tClassification Loss: 1.2777\r\n",
      "Train Epoch: 12 [67840/110534 (61%)]\tClassification Loss: 1.8550\r\n",
      "Train Epoch: 12 [68480/110534 (62%)]\tClassification Loss: 1.4044\r\n",
      "Train Epoch: 12 [69120/110534 (63%)]\tClassification Loss: 1.6848\r\n",
      "Train Epoch: 12 [69760/110534 (63%)]\tClassification Loss: 1.7481\r\n",
      "Train Epoch: 12 [70400/110534 (64%)]\tClassification Loss: 1.2407\r\n",
      "Train Epoch: 12 [71040/110534 (64%)]\tClassification Loss: 1.8991\r\n",
      "Train Epoch: 12 [71680/110534 (65%)]\tClassification Loss: 1.5764\r\n",
      "Train Epoch: 12 [72320/110534 (65%)]\tClassification Loss: 1.6846\r\n",
      "Train Epoch: 12 [72960/110534 (66%)]\tClassification Loss: 1.7783\r\n",
      "Train Epoch: 12 [73600/110534 (67%)]\tClassification Loss: 1.7972\r\n",
      "Train Epoch: 12 [74240/110534 (67%)]\tClassification Loss: 1.7454\r\n",
      "Train Epoch: 12 [74880/110534 (68%)]\tClassification Loss: 1.2088\r\n",
      "Train Epoch: 12 [75520/110534 (68%)]\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 12 [76160/110534 (69%)]\tClassification Loss: 1.2960\r\n",
      "Train Epoch: 12 [76800/110534 (69%)]\tClassification Loss: 1.2538\r\n",
      "Train Epoch: 12 [77440/110534 (70%)]\tClassification Loss: 1.3296\r\n",
      "Train Epoch: 12 [78080/110534 (71%)]\tClassification Loss: 1.3804\r\n",
      "Train Epoch: 12 [78720/110534 (71%)]\tClassification Loss: 1.4748\r\n",
      "Train Epoch: 12 [79360/110534 (72%)]\tClassification Loss: 1.4260\r\n",
      "Train Epoch: 12 [80000/110534 (72%)]\tClassification Loss: 1.3597\r\n",
      "Train Epoch: 12 [80640/110534 (73%)]\tClassification Loss: 1.3982\r\n",
      "Train Epoch: 12 [81280/110534 (74%)]\tClassification Loss: 1.8394\r\n",
      "Train Epoch: 12 [81920/110534 (74%)]\tClassification Loss: 1.5903\r\n",
      "Train Epoch: 12 [82560/110534 (75%)]\tClassification Loss: 1.7190\r\n",
      "Train Epoch: 12 [83200/110534 (75%)]\tClassification Loss: 1.5346\r\n",
      "Train Epoch: 12 [83840/110534 (76%)]\tClassification Loss: 1.7767\r\n",
      "Train Epoch: 12 [84480/110534 (76%)]\tClassification Loss: 1.6927\r\n",
      "Train Epoch: 12 [85120/110534 (77%)]\tClassification Loss: 1.6627\r\n",
      "Train Epoch: 12 [85760/110534 (78%)]\tClassification Loss: 1.4986\r\n",
      "Train Epoch: 12 [86400/110534 (78%)]\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 12 [87040/110534 (79%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 12 [87680/110534 (79%)]\tClassification Loss: 1.3970\r\n",
      "Train Epoch: 12 [88320/110534 (80%)]\tClassification Loss: 1.5138\r\n",
      "Train Epoch: 12 [88960/110534 (80%)]\tClassification Loss: 1.5504\r\n",
      "Train Epoch: 12 [89600/110534 (81%)]\tClassification Loss: 1.8084\r\n",
      "Train Epoch: 12 [90240/110534 (82%)]\tClassification Loss: 1.6934\r\n",
      "Train Epoch: 12 [90880/110534 (82%)]\tClassification Loss: 1.6679\r\n",
      "Train Epoch: 12 [91520/110534 (83%)]\tClassification Loss: 1.2567\r\n",
      "Train Epoch: 12 [92160/110534 (83%)]\tClassification Loss: 1.3570\r\n",
      "Train Epoch: 12 [92800/110534 (84%)]\tClassification Loss: 1.3349\r\n",
      "Train Epoch: 12 [93440/110534 (85%)]\tClassification Loss: 1.8555\r\n",
      "Train Epoch: 12 [94080/110534 (85%)]\tClassification Loss: 1.6022\r\n",
      "Train Epoch: 12 [94720/110534 (86%)]\tClassification Loss: 1.5464\r\n",
      "Train Epoch: 12 [95360/110534 (86%)]\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 12 [96000/110534 (87%)]\tClassification Loss: 1.5468\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_1500.pth.tar\r\n",
      "Train Epoch: 12 [96640/110534 (87%)]\tClassification Loss: 1.3684\r\n",
      "Train Epoch: 12 [97280/110534 (88%)]\tClassification Loss: 1.2272\r\n",
      "Train Epoch: 12 [97920/110534 (89%)]\tClassification Loss: 1.1159\r\n",
      "Train Epoch: 12 [98560/110534 (89%)]\tClassification Loss: 1.4767\r\n",
      "Train Epoch: 12 [99200/110534 (90%)]\tClassification Loss: 1.6437\r\n",
      "Train Epoch: 12 [99840/110534 (90%)]\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 12 [100480/110534 (91%)]\tClassification Loss: 1.7834\r\n",
      "Train Epoch: 12 [101120/110534 (91%)]\tClassification Loss: 1.6656\r\n",
      "Train Epoch: 12 [101760/110534 (92%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 12 [102400/110534 (93%)]\tClassification Loss: 1.3919\r\n",
      "Train Epoch: 12 [103040/110534 (93%)]\tClassification Loss: 1.6432\r\n",
      "Train Epoch: 12 [103680/110534 (94%)]\tClassification Loss: 1.6489\r\n",
      "Train Epoch: 12 [104320/110534 (94%)]\tClassification Loss: 1.3498\r\n",
      "Train Epoch: 12 [104960/110534 (95%)]\tClassification Loss: 1.4568\r\n",
      "Train Epoch: 12 [105600/110534 (96%)]\tClassification Loss: 1.5806\r\n",
      "Train Epoch: 12 [106240/110534 (96%)]\tClassification Loss: 1.2806\r\n",
      "Train Epoch: 12 [106880/110534 (97%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 12 [107520/110534 (97%)]\tClassification Loss: 1.6631\r\n",
      "Train Epoch: 12 [108160/110534 (98%)]\tClassification Loss: 1.5611\r\n",
      "Train Epoch: 12 [108800/110534 (98%)]\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 12 [109440/110534 (99%)]\tClassification Loss: 1.4785\r\n",
      "Train Epoch: 12 [110080/110534 (100%)]\tClassification Loss: 1.3992\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_final.pth.tar\r\n",
      "Train Epoch: 13 [0/110534 (0%)]\tClassification Loss: 1.5211\r\n",
      "\r\n",
      "Test set: Average loss: 1.4218, Accuracy: 23119/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 13 [640/110534 (1%)]\tClassification Loss: 1.2997\r\n",
      "Train Epoch: 13 [1280/110534 (1%)]\tClassification Loss: 1.9248\r\n",
      "Train Epoch: 13 [1920/110534 (2%)]\tClassification Loss: 1.5523\r\n",
      "Train Epoch: 13 [2560/110534 (2%)]\tClassification Loss: 1.7741\r\n",
      "Train Epoch: 13 [3200/110534 (3%)]\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 13 [3840/110534 (3%)]\tClassification Loss: 1.4689\r\n",
      "Train Epoch: 13 [4480/110534 (4%)]\tClassification Loss: 1.5800\r\n",
      "Train Epoch: 13 [5120/110534 (5%)]\tClassification Loss: 1.7812\r\n",
      "Train Epoch: 13 [5760/110534 (5%)]\tClassification Loss: 1.6223\r\n",
      "Train Epoch: 13 [6400/110534 (6%)]\tClassification Loss: 1.1606\r\n",
      "Train Epoch: 13 [7040/110534 (6%)]\tClassification Loss: 1.4260\r\n",
      "Train Epoch: 13 [7680/110534 (7%)]\tClassification Loss: 1.7481\r\n",
      "Train Epoch: 13 [8320/110534 (8%)]\tClassification Loss: 1.8723\r\n",
      "Train Epoch: 13 [8960/110534 (8%)]\tClassification Loss: 1.8080\r\n",
      "Train Epoch: 13 [9600/110534 (9%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 13 [10240/110534 (9%)]\tClassification Loss: 1.5057\r\n",
      "Train Epoch: 13 [10880/110534 (10%)]\tClassification Loss: 1.4241\r\n",
      "Train Epoch: 13 [11520/110534 (10%)]\tClassification Loss: 1.6066\r\n",
      "Train Epoch: 13 [12160/110534 (11%)]\tClassification Loss: 1.3708\r\n",
      "Train Epoch: 13 [12800/110534 (12%)]\tClassification Loss: 1.7525\r\n",
      "Train Epoch: 13 [13440/110534 (12%)]\tClassification Loss: 1.4431\r\n",
      "Train Epoch: 13 [14080/110534 (13%)]\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 13 [14720/110534 (13%)]\tClassification Loss: 1.8395\r\n",
      "Train Epoch: 13 [15360/110534 (14%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 13 [16000/110534 (14%)]\tClassification Loss: 1.7331\r\n",
      "Train Epoch: 13 [16640/110534 (15%)]\tClassification Loss: 1.6236\r\n",
      "Train Epoch: 13 [17280/110534 (16%)]\tClassification Loss: 1.8044\r\n",
      "Train Epoch: 13 [17920/110534 (16%)]\tClassification Loss: 1.6951\r\n",
      "Train Epoch: 13 [18560/110534 (17%)]\tClassification Loss: 1.7075\r\n",
      "Train Epoch: 13 [19200/110534 (17%)]\tClassification Loss: 1.7016\r\n",
      "Train Epoch: 13 [19840/110534 (18%)]\tClassification Loss: 1.5825\r\n",
      "Train Epoch: 13 [20480/110534 (19%)]\tClassification Loss: 1.7251\r\n",
      "Train Epoch: 13 [21120/110534 (19%)]\tClassification Loss: 1.8176\r\n",
      "Train Epoch: 13 [21760/110534 (20%)]\tClassification Loss: 1.3534\r\n",
      "Train Epoch: 13 [22400/110534 (20%)]\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 13 [23040/110534 (21%)]\tClassification Loss: 1.0291\r\n",
      "Train Epoch: 13 [23680/110534 (21%)]\tClassification Loss: 1.5032\r\n",
      "Train Epoch: 13 [24320/110534 (22%)]\tClassification Loss: 1.3101\r\n",
      "Train Epoch: 13 [24960/110534 (23%)]\tClassification Loss: 1.7127\r\n",
      "Train Epoch: 13 [25600/110534 (23%)]\tClassification Loss: 1.1853\r\n",
      "Train Epoch: 13 [26240/110534 (24%)]\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 13 [26880/110534 (24%)]\tClassification Loss: 1.7948\r\n",
      "Train Epoch: 13 [27520/110534 (25%)]\tClassification Loss: 1.3786\r\n",
      "Train Epoch: 13 [28160/110534 (25%)]\tClassification Loss: 1.7527\r\n",
      "Train Epoch: 13 [28800/110534 (26%)]\tClassification Loss: 1.7690\r\n",
      "Train Epoch: 13 [29440/110534 (27%)]\tClassification Loss: 1.6881\r\n",
      "Train Epoch: 13 [30080/110534 (27%)]\tClassification Loss: 1.6925\r\n",
      "Train Epoch: 13 [30720/110534 (28%)]\tClassification Loss: 1.3010\r\n",
      "Train Epoch: 13 [31360/110534 (28%)]\tClassification Loss: 1.5628\r\n",
      "Train Epoch: 13 [32000/110534 (29%)]\tClassification Loss: 1.5905\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_500.pth.tar\r\n",
      "Train Epoch: 13 [32640/110534 (30%)]\tClassification Loss: 1.3694\r\n",
      "Train Epoch: 13 [33280/110534 (30%)]\tClassification Loss: 1.5045\r\n",
      "Train Epoch: 13 [33920/110534 (31%)]\tClassification Loss: 1.5048\r\n",
      "Train Epoch: 13 [34560/110534 (31%)]\tClassification Loss: 1.7251\r\n",
      "Train Epoch: 13 [35200/110534 (32%)]\tClassification Loss: 1.4546\r\n",
      "Train Epoch: 13 [35840/110534 (32%)]\tClassification Loss: 1.2469\r\n",
      "Train Epoch: 13 [36480/110534 (33%)]\tClassification Loss: 1.3288\r\n",
      "Train Epoch: 13 [37120/110534 (34%)]\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 13 [37760/110534 (34%)]\tClassification Loss: 1.9407\r\n",
      "Train Epoch: 13 [38400/110534 (35%)]\tClassification Loss: 1.3992\r\n",
      "Train Epoch: 13 [39040/110534 (35%)]\tClassification Loss: 1.4046\r\n",
      "Train Epoch: 13 [39680/110534 (36%)]\tClassification Loss: 1.4818\r\n",
      "Train Epoch: 13 [40320/110534 (36%)]\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 13 [40960/110534 (37%)]\tClassification Loss: 1.3318\r\n",
      "Train Epoch: 13 [41600/110534 (38%)]\tClassification Loss: 1.3866\r\n",
      "Train Epoch: 13 [42240/110534 (38%)]\tClassification Loss: 1.4558\r\n",
      "Train Epoch: 13 [42880/110534 (39%)]\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 13 [43520/110534 (39%)]\tClassification Loss: 1.2298\r\n",
      "Train Epoch: 13 [44160/110534 (40%)]\tClassification Loss: 1.4369\r\n",
      "Train Epoch: 13 [44800/110534 (41%)]\tClassification Loss: 1.7163\r\n",
      "Train Epoch: 13 [45440/110534 (41%)]\tClassification Loss: 1.6021\r\n",
      "Train Epoch: 13 [46080/110534 (42%)]\tClassification Loss: 1.3972\r\n",
      "Train Epoch: 13 [46720/110534 (42%)]\tClassification Loss: 1.5264\r\n",
      "Train Epoch: 13 [47360/110534 (43%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 13 [48000/110534 (43%)]\tClassification Loss: 1.6291\r\n",
      "Train Epoch: 13 [48640/110534 (44%)]\tClassification Loss: 1.3619\r\n",
      "Train Epoch: 13 [49280/110534 (45%)]\tClassification Loss: 1.4087\r\n",
      "Train Epoch: 13 [49920/110534 (45%)]\tClassification Loss: 1.5213\r\n",
      "Train Epoch: 13 [50560/110534 (46%)]\tClassification Loss: 1.5175\r\n",
      "Train Epoch: 13 [51200/110534 (46%)]\tClassification Loss: 1.3005\r\n",
      "Train Epoch: 13 [51840/110534 (47%)]\tClassification Loss: 1.5126\r\n",
      "Train Epoch: 13 [52480/110534 (47%)]\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 13 [53120/110534 (48%)]\tClassification Loss: 1.1396\r\n",
      "Train Epoch: 13 [53760/110534 (49%)]\tClassification Loss: 1.6506\r\n",
      "Train Epoch: 13 [54400/110534 (49%)]\tClassification Loss: 1.3980\r\n",
      "Train Epoch: 13 [55040/110534 (50%)]\tClassification Loss: 1.6338\r\n",
      "Train Epoch: 13 [55680/110534 (50%)]\tClassification Loss: 1.6480\r\n",
      "Train Epoch: 13 [56320/110534 (51%)]\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 13 [56960/110534 (52%)]\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 13 [57600/110534 (52%)]\tClassification Loss: 1.6083\r\n",
      "Train Epoch: 13 [58240/110534 (53%)]\tClassification Loss: 1.4370\r\n",
      "Train Epoch: 13 [58880/110534 (53%)]\tClassification Loss: 1.7925\r\n",
      "Train Epoch: 13 [59520/110534 (54%)]\tClassification Loss: 1.2764\r\n",
      "Train Epoch: 13 [60160/110534 (54%)]\tClassification Loss: 1.3481\r\n",
      "Train Epoch: 13 [60800/110534 (55%)]\tClassification Loss: 1.6078\r\n",
      "Train Epoch: 13 [61440/110534 (56%)]\tClassification Loss: 1.5051\r\n",
      "Train Epoch: 13 [62080/110534 (56%)]\tClassification Loss: 1.6154\r\n",
      "Train Epoch: 13 [62720/110534 (57%)]\tClassification Loss: 1.6393\r\n",
      "Train Epoch: 13 [63360/110534 (57%)]\tClassification Loss: 1.2901\r\n",
      "Train Epoch: 13 [64000/110534 (58%)]\tClassification Loss: 1.5652\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_1000.pth.tar\r\n",
      "Train Epoch: 13 [64640/110534 (58%)]\tClassification Loss: 1.1788\r\n",
      "Train Epoch: 13 [65280/110534 (59%)]\tClassification Loss: 1.5508\r\n",
      "Train Epoch: 13 [65920/110534 (60%)]\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 13 [66560/110534 (60%)]\tClassification Loss: 1.3554\r\n",
      "Train Epoch: 13 [67200/110534 (61%)]\tClassification Loss: 1.3280\r\n",
      "Train Epoch: 13 [67840/110534 (61%)]\tClassification Loss: 1.8610\r\n",
      "Train Epoch: 13 [68480/110534 (62%)]\tClassification Loss: 1.6037\r\n",
      "Train Epoch: 13 [69120/110534 (63%)]\tClassification Loss: 1.6846\r\n",
      "Train Epoch: 13 [69760/110534 (63%)]\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 13 [70400/110534 (64%)]\tClassification Loss: 1.3750\r\n",
      "Train Epoch: 13 [71040/110534 (64%)]\tClassification Loss: 1.7246\r\n",
      "Train Epoch: 13 [71680/110534 (65%)]\tClassification Loss: 1.6192\r\n",
      "Train Epoch: 13 [72320/110534 (65%)]\tClassification Loss: 1.6454\r\n",
      "Train Epoch: 13 [72960/110534 (66%)]\tClassification Loss: 1.6658\r\n",
      "Train Epoch: 13 [73600/110534 (67%)]\tClassification Loss: 1.8651\r\n",
      "Train Epoch: 13 [74240/110534 (67%)]\tClassification Loss: 1.7052\r\n",
      "Train Epoch: 13 [74880/110534 (68%)]\tClassification Loss: 1.1557\r\n",
      "Train Epoch: 13 [75520/110534 (68%)]\tClassification Loss: 1.4863\r\n",
      "Train Epoch: 13 [76160/110534 (69%)]\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 13 [76800/110534 (69%)]\tClassification Loss: 1.3465\r\n",
      "Train Epoch: 13 [77440/110534 (70%)]\tClassification Loss: 1.4621\r\n",
      "Train Epoch: 13 [78080/110534 (71%)]\tClassification Loss: 1.4307\r\n",
      "Train Epoch: 13 [78720/110534 (71%)]\tClassification Loss: 1.6323\r\n",
      "Train Epoch: 13 [79360/110534 (72%)]\tClassification Loss: 1.4145\r\n",
      "Train Epoch: 13 [80000/110534 (72%)]\tClassification Loss: 1.5502\r\n",
      "Train Epoch: 13 [80640/110534 (73%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 13 [81280/110534 (74%)]\tClassification Loss: 1.7788\r\n",
      "Train Epoch: 13 [81920/110534 (74%)]\tClassification Loss: 1.3938\r\n",
      "Train Epoch: 13 [82560/110534 (75%)]\tClassification Loss: 1.8634\r\n",
      "Train Epoch: 13 [83200/110534 (75%)]\tClassification Loss: 1.6160\r\n",
      "Train Epoch: 13 [83840/110534 (76%)]\tClassification Loss: 1.7860\r\n",
      "Train Epoch: 13 [84480/110534 (76%)]\tClassification Loss: 1.7481\r\n",
      "Train Epoch: 13 [85120/110534 (77%)]\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 13 [85760/110534 (78%)]\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 13 [86400/110534 (78%)]\tClassification Loss: 1.7428\r\n",
      "Train Epoch: 13 [87040/110534 (79%)]\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 13 [87680/110534 (79%)]\tClassification Loss: 1.2991\r\n",
      "Train Epoch: 13 [88320/110534 (80%)]\tClassification Loss: 1.5931\r\n",
      "Train Epoch: 13 [88960/110534 (80%)]\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 13 [89600/110534 (81%)]\tClassification Loss: 1.5898\r\n",
      "Train Epoch: 13 [90240/110534 (82%)]\tClassification Loss: 1.8127\r\n",
      "Train Epoch: 13 [90880/110534 (82%)]\tClassification Loss: 1.5303\r\n",
      "Train Epoch: 13 [91520/110534 (83%)]\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 13 [92160/110534 (83%)]\tClassification Loss: 1.3273\r\n",
      "Train Epoch: 13 [92800/110534 (84%)]\tClassification Loss: 1.3887\r\n",
      "Train Epoch: 13 [93440/110534 (85%)]\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 13 [94080/110534 (85%)]\tClassification Loss: 1.4434\r\n",
      "Train Epoch: 13 [94720/110534 (86%)]\tClassification Loss: 1.3838\r\n",
      "Train Epoch: 13 [95360/110534 (86%)]\tClassification Loss: 1.3537\r\n",
      "Train Epoch: 13 [96000/110534 (87%)]\tClassification Loss: 1.4100\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_1500.pth.tar\r\n",
      "Train Epoch: 13 [96640/110534 (87%)]\tClassification Loss: 1.3468\r\n",
      "Train Epoch: 13 [97280/110534 (88%)]\tClassification Loss: 1.2479\r\n",
      "Train Epoch: 13 [97920/110534 (89%)]\tClassification Loss: 1.1385\r\n",
      "Train Epoch: 13 [98560/110534 (89%)]\tClassification Loss: 1.4067\r\n",
      "Train Epoch: 13 [99200/110534 (90%)]\tClassification Loss: 1.6521\r\n",
      "Train Epoch: 13 [99840/110534 (90%)]\tClassification Loss: 1.3885\r\n",
      "Train Epoch: 13 [100480/110534 (91%)]\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 13 [101120/110534 (91%)]\tClassification Loss: 1.4808\r\n",
      "Train Epoch: 13 [101760/110534 (92%)]\tClassification Loss: 1.5816\r\n",
      "Train Epoch: 13 [102400/110534 (93%)]\tClassification Loss: 1.4151\r\n",
      "Train Epoch: 13 [103040/110534 (93%)]\tClassification Loss: 1.6018\r\n",
      "Train Epoch: 13 [103680/110534 (94%)]\tClassification Loss: 1.7210\r\n",
      "Train Epoch: 13 [104320/110534 (94%)]\tClassification Loss: 1.4427\r\n",
      "Train Epoch: 13 [104960/110534 (95%)]\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 13 [105600/110534 (96%)]\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 13 [106240/110534 (96%)]\tClassification Loss: 1.3064\r\n",
      "Train Epoch: 13 [106880/110534 (97%)]\tClassification Loss: 1.8112\r\n",
      "Train Epoch: 13 [107520/110534 (97%)]\tClassification Loss: 1.7532\r\n",
      "Train Epoch: 13 [108160/110534 (98%)]\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 13 [108800/110534 (98%)]\tClassification Loss: 1.6336\r\n",
      "Train Epoch: 13 [109440/110534 (99%)]\tClassification Loss: 1.4919\r\n",
      "Train Epoch: 13 [110080/110534 (100%)]\tClassification Loss: 1.3806\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_final.pth.tar\r\n",
      "Train Epoch: 14 [0/110534 (0%)]\tClassification Loss: 1.6896\r\n",
      "\r\n",
      "Test set: Average loss: 1.4217, Accuracy: 23169/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 14 [640/110534 (1%)]\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 14 [1280/110534 (1%)]\tClassification Loss: 2.0333\r\n",
      "Train Epoch: 14 [1920/110534 (2%)]\tClassification Loss: 1.4491\r\n",
      "Train Epoch: 14 [2560/110534 (2%)]\tClassification Loss: 1.6964\r\n",
      "Train Epoch: 14 [3200/110534 (3%)]\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 14 [3840/110534 (3%)]\tClassification Loss: 1.4595\r\n",
      "Train Epoch: 14 [4480/110534 (4%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 14 [5120/110534 (5%)]\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 14 [5760/110534 (5%)]\tClassification Loss: 1.5497\r\n",
      "Train Epoch: 14 [6400/110534 (6%)]\tClassification Loss: 1.3480\r\n",
      "Train Epoch: 14 [7040/110534 (6%)]\tClassification Loss: 1.4937\r\n",
      "Train Epoch: 14 [7680/110534 (7%)]\tClassification Loss: 1.6143\r\n",
      "Train Epoch: 14 [8320/110534 (8%)]\tClassification Loss: 1.7814\r\n",
      "Train Epoch: 14 [8960/110534 (8%)]\tClassification Loss: 1.8033\r\n",
      "Train Epoch: 14 [9600/110534 (9%)]\tClassification Loss: 1.4639\r\n",
      "Train Epoch: 14 [10240/110534 (9%)]\tClassification Loss: 1.4982\r\n",
      "Train Epoch: 14 [10880/110534 (10%)]\tClassification Loss: 1.5247\r\n",
      "Train Epoch: 14 [11520/110534 (10%)]\tClassification Loss: 1.6255\r\n",
      "Train Epoch: 14 [12160/110534 (11%)]\tClassification Loss: 1.3444\r\n",
      "Train Epoch: 14 [12800/110534 (12%)]\tClassification Loss: 1.7707\r\n",
      "Train Epoch: 14 [13440/110534 (12%)]\tClassification Loss: 1.3871\r\n",
      "Train Epoch: 14 [14080/110534 (13%)]\tClassification Loss: 1.5423\r\n",
      "Train Epoch: 14 [14720/110534 (13%)]\tClassification Loss: 1.7886\r\n",
      "Train Epoch: 14 [15360/110534 (14%)]\tClassification Loss: 1.4703\r\n",
      "Train Epoch: 14 [16000/110534 (14%)]\tClassification Loss: 1.6320\r\n",
      "Train Epoch: 14 [16640/110534 (15%)]\tClassification Loss: 1.5132\r\n",
      "Train Epoch: 14 [17280/110534 (16%)]\tClassification Loss: 1.7147\r\n",
      "Train Epoch: 14 [17920/110534 (16%)]\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 14 [18560/110534 (17%)]\tClassification Loss: 1.6745\r\n",
      "Train Epoch: 14 [19200/110534 (17%)]\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 14 [19840/110534 (18%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 14 [20480/110534 (19%)]\tClassification Loss: 1.8798\r\n",
      "Train Epoch: 14 [21120/110534 (19%)]\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 14 [21760/110534 (20%)]\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 14 [22400/110534 (20%)]\tClassification Loss: 1.7295\r\n",
      "Train Epoch: 14 [23040/110534 (21%)]\tClassification Loss: 1.1819\r\n",
      "Train Epoch: 14 [23680/110534 (21%)]\tClassification Loss: 1.6549\r\n",
      "Train Epoch: 14 [24320/110534 (22%)]\tClassification Loss: 1.2779\r\n",
      "Train Epoch: 14 [24960/110534 (23%)]\tClassification Loss: 1.6264\r\n",
      "Train Epoch: 14 [25600/110534 (23%)]\tClassification Loss: 1.2370\r\n",
      "Train Epoch: 14 [26240/110534 (24%)]\tClassification Loss: 1.4115\r\n",
      "Train Epoch: 14 [26880/110534 (24%)]\tClassification Loss: 1.6643\r\n",
      "Train Epoch: 14 [27520/110534 (25%)]\tClassification Loss: 1.5560\r\n",
      "Train Epoch: 14 [28160/110534 (25%)]\tClassification Loss: 1.6815\r\n",
      "Train Epoch: 14 [28800/110534 (26%)]\tClassification Loss: 1.7307\r\n",
      "Train Epoch: 14 [29440/110534 (27%)]\tClassification Loss: 1.7065\r\n",
      "Train Epoch: 14 [30080/110534 (27%)]\tClassification Loss: 1.9540\r\n",
      "Train Epoch: 14 [30720/110534 (28%)]\tClassification Loss: 1.4816\r\n",
      "Train Epoch: 14 [31360/110534 (28%)]\tClassification Loss: 1.5549\r\n",
      "Train Epoch: 14 [32000/110534 (29%)]\tClassification Loss: 1.3854\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_500.pth.tar\r\n",
      "Train Epoch: 14 [32640/110534 (30%)]\tClassification Loss: 1.2664\r\n",
      "Train Epoch: 14 [33280/110534 (30%)]\tClassification Loss: 1.6588\r\n",
      "Train Epoch: 14 [33920/110534 (31%)]\tClassification Loss: 1.5228\r\n",
      "Train Epoch: 14 [34560/110534 (31%)]\tClassification Loss: 1.6741\r\n",
      "Train Epoch: 14 [35200/110534 (32%)]\tClassification Loss: 1.3267\r\n",
      "Train Epoch: 14 [35840/110534 (32%)]\tClassification Loss: 1.2349\r\n",
      "Train Epoch: 14 [36480/110534 (33%)]\tClassification Loss: 1.4032\r\n",
      "Train Epoch: 14 [37120/110534 (34%)]\tClassification Loss: 1.6054\r\n",
      "Train Epoch: 14 [37760/110534 (34%)]\tClassification Loss: 1.9360\r\n",
      "Train Epoch: 14 [38400/110534 (35%)]\tClassification Loss: 1.4892\r\n",
      "Train Epoch: 14 [39040/110534 (35%)]\tClassification Loss: 1.5272\r\n",
      "Train Epoch: 14 [39680/110534 (36%)]\tClassification Loss: 1.5230\r\n",
      "Train Epoch: 14 [40320/110534 (36%)]\tClassification Loss: 1.5969\r\n",
      "Train Epoch: 14 [40960/110534 (37%)]\tClassification Loss: 1.4092\r\n",
      "Train Epoch: 14 [41600/110534 (38%)]\tClassification Loss: 1.3926\r\n",
      "Train Epoch: 14 [42240/110534 (38%)]\tClassification Loss: 1.4939\r\n",
      "Train Epoch: 14 [42880/110534 (39%)]\tClassification Loss: 1.4334\r\n",
      "Train Epoch: 14 [43520/110534 (39%)]\tClassification Loss: 1.2842\r\n",
      "Train Epoch: 14 [44160/110534 (40%)]\tClassification Loss: 1.6079\r\n",
      "Train Epoch: 14 [44800/110534 (41%)]\tClassification Loss: 1.5897\r\n",
      "Train Epoch: 14 [45440/110534 (41%)]\tClassification Loss: 1.4798\r\n",
      "Train Epoch: 14 [46080/110534 (42%)]\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 14 [46720/110534 (42%)]\tClassification Loss: 1.5002\r\n",
      "Train Epoch: 14 [47360/110534 (43%)]\tClassification Loss: 1.5154\r\n",
      "Train Epoch: 14 [48000/110534 (43%)]\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 14 [48640/110534 (44%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 14 [49280/110534 (45%)]\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 14 [49920/110534 (45%)]\tClassification Loss: 1.4832\r\n",
      "Train Epoch: 14 [50560/110534 (46%)]\tClassification Loss: 1.6209\r\n",
      "Train Epoch: 14 [51200/110534 (46%)]\tClassification Loss: 1.3119\r\n",
      "Train Epoch: 14 [51840/110534 (47%)]\tClassification Loss: 1.3813\r\n",
      "Train Epoch: 14 [52480/110534 (47%)]\tClassification Loss: 1.4779\r\n",
      "Train Epoch: 14 [53120/110534 (48%)]\tClassification Loss: 1.3294\r\n",
      "Train Epoch: 14 [53760/110534 (49%)]\tClassification Loss: 1.4642\r\n",
      "Train Epoch: 14 [54400/110534 (49%)]\tClassification Loss: 1.4472\r\n",
      "Train Epoch: 14 [55040/110534 (50%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 14 [55680/110534 (50%)]\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 14 [56320/110534 (51%)]\tClassification Loss: 1.5056\r\n",
      "Train Epoch: 14 [56960/110534 (52%)]\tClassification Loss: 1.5555\r\n",
      "Train Epoch: 14 [57600/110534 (52%)]\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 14 [58240/110534 (53%)]\tClassification Loss: 1.5049\r\n",
      "Train Epoch: 14 [58880/110534 (53%)]\tClassification Loss: 1.8352\r\n",
      "Train Epoch: 14 [59520/110534 (54%)]\tClassification Loss: 1.2715\r\n",
      "Train Epoch: 14 [60160/110534 (54%)]\tClassification Loss: 1.5453\r\n",
      "Train Epoch: 14 [60800/110534 (55%)]\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 14 [61440/110534 (56%)]\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 14 [62080/110534 (56%)]\tClassification Loss: 1.4656\r\n",
      "Train Epoch: 14 [62720/110534 (57%)]\tClassification Loss: 1.8024\r\n",
      "Train Epoch: 14 [63360/110534 (57%)]\tClassification Loss: 1.1956\r\n",
      "Train Epoch: 14 [64000/110534 (58%)]\tClassification Loss: 1.5654\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_1000.pth.tar\r\n",
      "Train Epoch: 14 [64640/110534 (58%)]\tClassification Loss: 1.1660\r\n",
      "Train Epoch: 14 [65280/110534 (59%)]\tClassification Loss: 1.5173\r\n",
      "Train Epoch: 14 [65920/110534 (60%)]\tClassification Loss: 1.4326\r\n",
      "Train Epoch: 14 [66560/110534 (60%)]\tClassification Loss: 1.3414\r\n",
      "Train Epoch: 14 [67200/110534 (61%)]\tClassification Loss: 1.2620\r\n",
      "Train Epoch: 14 [67840/110534 (61%)]\tClassification Loss: 1.7971\r\n",
      "Train Epoch: 14 [68480/110534 (62%)]\tClassification Loss: 1.6248\r\n",
      "Train Epoch: 14 [69120/110534 (63%)]\tClassification Loss: 1.8479\r\n",
      "Train Epoch: 14 [69760/110534 (63%)]\tClassification Loss: 1.6040\r\n",
      "Train Epoch: 14 [70400/110534 (64%)]\tClassification Loss: 1.1381\r\n",
      "Train Epoch: 14 [71040/110534 (64%)]\tClassification Loss: 1.8640\r\n",
      "Train Epoch: 14 [71680/110534 (65%)]\tClassification Loss: 1.4831\r\n",
      "Train Epoch: 14 [72320/110534 (65%)]\tClassification Loss: 1.7240\r\n",
      "Train Epoch: 14 [72960/110534 (66%)]\tClassification Loss: 1.9070\r\n",
      "Train Epoch: 14 [73600/110534 (67%)]\tClassification Loss: 1.6580\r\n",
      "Train Epoch: 14 [74240/110534 (67%)]\tClassification Loss: 1.7785\r\n",
      "Train Epoch: 14 [74880/110534 (68%)]\tClassification Loss: 1.1403\r\n",
      "Train Epoch: 14 [75520/110534 (68%)]\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 14 [76160/110534 (69%)]\tClassification Loss: 1.2698\r\n",
      "Train Epoch: 14 [76800/110534 (69%)]\tClassification Loss: 1.3862\r\n",
      "Train Epoch: 14 [77440/110534 (70%)]\tClassification Loss: 1.3588\r\n",
      "Train Epoch: 14 [78080/110534 (71%)]\tClassification Loss: 1.5473\r\n",
      "Train Epoch: 14 [78720/110534 (71%)]\tClassification Loss: 1.4161\r\n",
      "Train Epoch: 14 [79360/110534 (72%)]\tClassification Loss: 1.3483\r\n",
      "Train Epoch: 14 [80000/110534 (72%)]\tClassification Loss: 1.3812\r\n",
      "Train Epoch: 14 [80640/110534 (73%)]\tClassification Loss: 1.4097\r\n",
      "Train Epoch: 14 [81280/110534 (74%)]\tClassification Loss: 1.6823\r\n",
      "Train Epoch: 14 [81920/110534 (74%)]\tClassification Loss: 1.5657\r\n",
      "Train Epoch: 14 [82560/110534 (75%)]\tClassification Loss: 1.6113\r\n",
      "Train Epoch: 14 [83200/110534 (75%)]\tClassification Loss: 1.5540\r\n",
      "Train Epoch: 14 [83840/110534 (76%)]\tClassification Loss: 2.1069\r\n",
      "Train Epoch: 14 [84480/110534 (76%)]\tClassification Loss: 1.5538\r\n",
      "Train Epoch: 14 [85120/110534 (77%)]\tClassification Loss: 1.3672\r\n",
      "Train Epoch: 14 [85760/110534 (78%)]\tClassification Loss: 1.4669\r\n",
      "Train Epoch: 14 [86400/110534 (78%)]\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 14 [87040/110534 (79%)]\tClassification Loss: 1.5567\r\n",
      "Train Epoch: 14 [87680/110534 (79%)]\tClassification Loss: 1.3569\r\n",
      "Train Epoch: 14 [88320/110534 (80%)]\tClassification Loss: 1.5370\r\n",
      "Train Epoch: 14 [88960/110534 (80%)]\tClassification Loss: 1.6810\r\n",
      "Train Epoch: 14 [89600/110534 (81%)]\tClassification Loss: 1.7052\r\n",
      "Train Epoch: 14 [90240/110534 (82%)]\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 14 [90880/110534 (82%)]\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 14 [91520/110534 (83%)]\tClassification Loss: 1.3779\r\n",
      "Train Epoch: 14 [92160/110534 (83%)]\tClassification Loss: 1.3200\r\n",
      "Train Epoch: 14 [92800/110534 (84%)]\tClassification Loss: 1.3859\r\n",
      "Train Epoch: 14 [93440/110534 (85%)]\tClassification Loss: 1.6118\r\n",
      "Train Epoch: 14 [94080/110534 (85%)]\tClassification Loss: 1.4152\r\n",
      "Train Epoch: 14 [94720/110534 (86%)]\tClassification Loss: 1.2969\r\n",
      "Train Epoch: 14 [95360/110534 (86%)]\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 14 [96000/110534 (87%)]\tClassification Loss: 1.4040\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_1500.pth.tar\r\n",
      "Train Epoch: 14 [96640/110534 (87%)]\tClassification Loss: 1.3429\r\n",
      "Train Epoch: 14 [97280/110534 (88%)]\tClassification Loss: 1.2242\r\n",
      "Train Epoch: 14 [97920/110534 (89%)]\tClassification Loss: 1.1998\r\n",
      "Train Epoch: 14 [98560/110534 (89%)]\tClassification Loss: 1.5435\r\n",
      "Train Epoch: 14 [99200/110534 (90%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 14 [99840/110534 (90%)]\tClassification Loss: 1.4970\r\n",
      "Train Epoch: 14 [100480/110534 (91%)]\tClassification Loss: 1.7501\r\n",
      "Train Epoch: 14 [101120/110534 (91%)]\tClassification Loss: 1.5104\r\n",
      "Train Epoch: 14 [101760/110534 (92%)]\tClassification Loss: 1.5040\r\n",
      "Train Epoch: 14 [102400/110534 (93%)]\tClassification Loss: 1.5843\r\n",
      "Train Epoch: 14 [103040/110534 (93%)]\tClassification Loss: 1.6380\r\n",
      "Train Epoch: 14 [103680/110534 (94%)]\tClassification Loss: 1.7251\r\n",
      "Train Epoch: 14 [104320/110534 (94%)]\tClassification Loss: 1.4852\r\n",
      "Train Epoch: 14 [104960/110534 (95%)]\tClassification Loss: 1.4719\r\n",
      "Train Epoch: 14 [105600/110534 (96%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 14 [106240/110534 (96%)]\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 14 [106880/110534 (97%)]\tClassification Loss: 1.8377\r\n",
      "Train Epoch: 14 [107520/110534 (97%)]\tClassification Loss: 1.4428\r\n",
      "Train Epoch: 14 [108160/110534 (98%)]\tClassification Loss: 1.5396\r\n",
      "Train Epoch: 14 [108800/110534 (98%)]\tClassification Loss: 1.6695\r\n",
      "Train Epoch: 14 [109440/110534 (99%)]\tClassification Loss: 1.4344\r\n",
      "Train Epoch: 14 [110080/110534 (100%)]\tClassification Loss: 1.5101\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_final.pth.tar\r\n",
      "Train Epoch: 15 [0/110534 (0%)]\tClassification Loss: 1.5813\r\n",
      "\r\n",
      "Test set: Average loss: 1.4169, Accuracy: 23188/42368 (55%)\r\n",
      "\r\n",
      "Train Epoch: 15 [640/110534 (1%)]\tClassification Loss: 1.2973\r\n",
      "Train Epoch: 15 [1280/110534 (1%)]\tClassification Loss: 1.9754\r\n",
      "Train Epoch: 15 [1920/110534 (2%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 15 [2560/110534 (2%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 15 [3200/110534 (3%)]\tClassification Loss: 1.3911\r\n",
      "Train Epoch: 15 [3840/110534 (3%)]\tClassification Loss: 1.5039\r\n",
      "Train Epoch: 15 [4480/110534 (4%)]\tClassification Loss: 1.5162\r\n",
      "Train Epoch: 15 [5120/110534 (5%)]\tClassification Loss: 1.5714\r\n",
      "Train Epoch: 15 [5760/110534 (5%)]\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 15 [6400/110534 (6%)]\tClassification Loss: 1.2808\r\n",
      "Train Epoch: 15 [7040/110534 (6%)]\tClassification Loss: 1.5263\r\n",
      "Train Epoch: 15 [7680/110534 (7%)]\tClassification Loss: 1.5075\r\n",
      "Train Epoch: 15 [8320/110534 (8%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 15 [8960/110534 (8%)]\tClassification Loss: 1.8931\r\n",
      "Train Epoch: 15 [9600/110534 (9%)]\tClassification Loss: 1.5255\r\n",
      "Train Epoch: 15 [10240/110534 (9%)]\tClassification Loss: 1.4774\r\n",
      "Train Epoch: 15 [10880/110534 (10%)]\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 15 [11520/110534 (10%)]\tClassification Loss: 1.5867\r\n",
      "Train Epoch: 15 [12160/110534 (11%)]\tClassification Loss: 1.4125\r\n",
      "Train Epoch: 15 [12800/110534 (12%)]\tClassification Loss: 1.5799\r\n",
      "Train Epoch: 15 [13440/110534 (12%)]\tClassification Loss: 1.3841\r\n",
      "Train Epoch: 15 [14080/110534 (13%)]\tClassification Loss: 1.6345\r\n",
      "Train Epoch: 15 [14720/110534 (13%)]\tClassification Loss: 1.7724\r\n",
      "Train Epoch: 15 [15360/110534 (14%)]\tClassification Loss: 1.5378\r\n",
      "Train Epoch: 15 [16000/110534 (14%)]\tClassification Loss: 1.7733\r\n",
      "Train Epoch: 15 [16640/110534 (15%)]\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 15 [17280/110534 (16%)]\tClassification Loss: 1.5846\r\n",
      "Train Epoch: 15 [17920/110534 (16%)]\tClassification Loss: 1.5086\r\n",
      "Train Epoch: 15 [18560/110534 (17%)]\tClassification Loss: 1.8942\r\n",
      "Train Epoch: 15 [19200/110534 (17%)]\tClassification Loss: 1.5711\r\n",
      "Train Epoch: 15 [19840/110534 (18%)]\tClassification Loss: 1.5894\r\n",
      "Train Epoch: 15 [20480/110534 (19%)]\tClassification Loss: 1.8709\r\n",
      "Train Epoch: 15 [21120/110534 (19%)]\tClassification Loss: 1.6889\r\n",
      "Train Epoch: 15 [21760/110534 (20%)]\tClassification Loss: 1.5091\r\n",
      "Train Epoch: 15 [22400/110534 (20%)]\tClassification Loss: 1.5141\r\n",
      "Train Epoch: 15 [23040/110534 (21%)]\tClassification Loss: 1.1098\r\n",
      "Train Epoch: 15 [23680/110534 (21%)]\tClassification Loss: 1.5452\r\n",
      "Train Epoch: 15 [24320/110534 (22%)]\tClassification Loss: 1.2927\r\n",
      "Train Epoch: 15 [24960/110534 (23%)]\tClassification Loss: 1.6793\r\n",
      "Train Epoch: 15 [25600/110534 (23%)]\tClassification Loss: 1.2292\r\n",
      "Train Epoch: 15 [26240/110534 (24%)]\tClassification Loss: 1.5142\r\n",
      "Train Epoch: 15 [26880/110534 (24%)]\tClassification Loss: 1.7403\r\n",
      "Train Epoch: 15 [27520/110534 (25%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 15 [28160/110534 (25%)]\tClassification Loss: 1.6044\r\n",
      "Train Epoch: 15 [28800/110534 (26%)]\tClassification Loss: 1.7844\r\n",
      "Train Epoch: 15 [29440/110534 (27%)]\tClassification Loss: 1.7407\r\n",
      "Train Epoch: 15 [30080/110534 (27%)]\tClassification Loss: 1.7758\r\n",
      "Train Epoch: 15 [30720/110534 (28%)]\tClassification Loss: 1.3586\r\n",
      "Train Epoch: 15 [31360/110534 (28%)]\tClassification Loss: 1.6886\r\n",
      "Train Epoch: 15 [32000/110534 (29%)]\tClassification Loss: 1.6256\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_500.pth.tar\r\n",
      "Train Epoch: 15 [32640/110534 (30%)]\tClassification Loss: 1.3548\r\n",
      "Train Epoch: 15 [33280/110534 (30%)]\tClassification Loss: 1.4971\r\n",
      "Train Epoch: 15 [33920/110534 (31%)]\tClassification Loss: 1.6259\r\n",
      "Train Epoch: 15 [34560/110534 (31%)]\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 15 [35200/110534 (32%)]\tClassification Loss: 1.5470\r\n",
      "Train Epoch: 15 [35840/110534 (32%)]\tClassification Loss: 1.3396\r\n",
      "Train Epoch: 15 [36480/110534 (33%)]\tClassification Loss: 1.4831\r\n",
      "Train Epoch: 15 [37120/110534 (34%)]\tClassification Loss: 1.6698\r\n",
      "Train Epoch: 15 [37760/110534 (34%)]\tClassification Loss: 1.8528\r\n",
      "Train Epoch: 15 [38400/110534 (35%)]\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 15 [39040/110534 (35%)]\tClassification Loss: 1.3274\r\n",
      "Train Epoch: 15 [39680/110534 (36%)]\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 15 [40320/110534 (36%)]\tClassification Loss: 1.5824\r\n",
      "Train Epoch: 15 [40960/110534 (37%)]\tClassification Loss: 1.3005\r\n",
      "Train Epoch: 15 [41600/110534 (38%)]\tClassification Loss: 1.3328\r\n",
      "Train Epoch: 15 [42240/110534 (38%)]\tClassification Loss: 1.4190\r\n",
      "Train Epoch: 15 [42880/110534 (39%)]\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 15 [43520/110534 (39%)]\tClassification Loss: 1.3814\r\n",
      "Train Epoch: 15 [44160/110534 (40%)]\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 15 [44800/110534 (41%)]\tClassification Loss: 1.6559\r\n",
      "Train Epoch: 15 [45440/110534 (41%)]\tClassification Loss: 1.6671\r\n",
      "Train Epoch: 15 [46080/110534 (42%)]\tClassification Loss: 1.3833\r\n",
      "Train Epoch: 15 [46720/110534 (42%)]\tClassification Loss: 1.5741\r\n",
      "Train Epoch: 15 [47360/110534 (43%)]\tClassification Loss: 1.5454\r\n",
      "Train Epoch: 15 [48000/110534 (43%)]\tClassification Loss: 1.5953\r\n",
      "Train Epoch: 15 [48640/110534 (44%)]\tClassification Loss: 1.4438\r\n",
      "Train Epoch: 15 [49280/110534 (45%)]\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 15 [49920/110534 (45%)]\tClassification Loss: 1.5153\r\n",
      "Train Epoch: 15 [50560/110534 (46%)]\tClassification Loss: 1.5330\r\n",
      "Train Epoch: 15 [51200/110534 (46%)]\tClassification Loss: 1.3164\r\n",
      "Train Epoch: 15 [51840/110534 (47%)]\tClassification Loss: 1.6241\r\n",
      "Train Epoch: 15 [52480/110534 (47%)]\tClassification Loss: 1.6276\r\n",
      "Train Epoch: 15 [53120/110534 (48%)]\tClassification Loss: 1.1405\r\n",
      "Train Epoch: 15 [53760/110534 (49%)]\tClassification Loss: 1.4822\r\n",
      "Train Epoch: 15 [54400/110534 (49%)]\tClassification Loss: 1.4951\r\n",
      "Train Epoch: 15 [55040/110534 (50%)]\tClassification Loss: 1.6720\r\n",
      "Train Epoch: 15 [55680/110534 (50%)]\tClassification Loss: 1.5977\r\n",
      "Train Epoch: 15 [56320/110534 (51%)]\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 15 [56960/110534 (52%)]\tClassification Loss: 1.6129\r\n",
      "Train Epoch: 15 [57600/110534 (52%)]\tClassification Loss: 1.6374\r\n",
      "Train Epoch: 15 [58240/110534 (53%)]\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 15 [58880/110534 (53%)]\tClassification Loss: 1.8081\r\n",
      "Train Epoch: 15 [59520/110534 (54%)]\tClassification Loss: 1.4930\r\n",
      "Train Epoch: 15 [60160/110534 (54%)]\tClassification Loss: 1.4549\r\n",
      "Train Epoch: 15 [60800/110534 (55%)]\tClassification Loss: 1.4397\r\n",
      "Train Epoch: 15 [61440/110534 (56%)]\tClassification Loss: 1.5047\r\n",
      "Train Epoch: 15 [62080/110534 (56%)]\tClassification Loss: 1.5523\r\n",
      "Train Epoch: 15 [62720/110534 (57%)]\tClassification Loss: 1.6617\r\n",
      "Train Epoch: 15 [63360/110534 (57%)]\tClassification Loss: 1.2429\r\n",
      "Train Epoch: 15 [64000/110534 (58%)]\tClassification Loss: 1.6040\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_1000.pth.tar\r\n",
      "Train Epoch: 15 [64640/110534 (58%)]\tClassification Loss: 1.2406\r\n",
      "Train Epoch: 15 [65280/110534 (59%)]\tClassification Loss: 1.5231\r\n",
      "Train Epoch: 15 [65920/110534 (60%)]\tClassification Loss: 1.6471\r\n",
      "Train Epoch: 15 [66560/110534 (60%)]\tClassification Loss: 1.1916\r\n",
      "Train Epoch: 15 [67200/110534 (61%)]\tClassification Loss: 1.2652\r\n",
      "Train Epoch: 15 [67840/110534 (61%)]\tClassification Loss: 1.7994\r\n",
      "Train Epoch: 15 [68480/110534 (62%)]\tClassification Loss: 1.7459\r\n",
      "Train Epoch: 15 [69120/110534 (63%)]\tClassification Loss: 1.7886\r\n",
      "Train Epoch: 15 [69760/110534 (63%)]\tClassification Loss: 1.6232\r\n",
      "Train Epoch: 15 [70400/110534 (64%)]\tClassification Loss: 1.2702\r\n",
      "Train Epoch: 15 [71040/110534 (64%)]\tClassification Loss: 1.8782\r\n",
      "Train Epoch: 15 [71680/110534 (65%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 15 [72320/110534 (65%)]\tClassification Loss: 1.5642\r\n",
      "Train Epoch: 15 [72960/110534 (66%)]\tClassification Loss: 1.7947\r\n",
      "Train Epoch: 15 [73600/110534 (67%)]\tClassification Loss: 1.7537\r\n",
      "Train Epoch: 15 [74240/110534 (67%)]\tClassification Loss: 1.6908\r\n",
      "Train Epoch: 15 [74880/110534 (68%)]\tClassification Loss: 1.1733\r\n",
      "Train Epoch: 15 [75520/110534 (68%)]\tClassification Loss: 1.5748\r\n",
      "Train Epoch: 15 [76160/110534 (69%)]\tClassification Loss: 1.3712\r\n",
      "Train Epoch: 15 [76800/110534 (69%)]\tClassification Loss: 1.3395\r\n",
      "Train Epoch: 15 [77440/110534 (70%)]\tClassification Loss: 1.5154\r\n",
      "Train Epoch: 15 [78080/110534 (71%)]\tClassification Loss: 1.5167\r\n",
      "Train Epoch: 15 [78720/110534 (71%)]\tClassification Loss: 1.3486\r\n",
      "Train Epoch: 15 [79360/110534 (72%)]\tClassification Loss: 1.3869\r\n",
      "Train Epoch: 15 [80000/110534 (72%)]\tClassification Loss: 1.3667\r\n",
      "Train Epoch: 15 [80640/110534 (73%)]\tClassification Loss: 1.4985\r\n",
      "Train Epoch: 15 [81280/110534 (74%)]\tClassification Loss: 1.8106\r\n",
      "Train Epoch: 15 [81920/110534 (74%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 15 [82560/110534 (75%)]\tClassification Loss: 1.7208\r\n",
      "Train Epoch: 15 [83200/110534 (75%)]\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 15 [83840/110534 (76%)]\tClassification Loss: 1.8134\r\n",
      "Train Epoch: 15 [84480/110534 (76%)]\tClassification Loss: 1.7318\r\n",
      "Train Epoch: 15 [85120/110534 (77%)]\tClassification Loss: 1.4189\r\n",
      "Train Epoch: 15 [85760/110534 (78%)]\tClassification Loss: 1.4367\r\n",
      "Train Epoch: 15 [86400/110534 (78%)]\tClassification Loss: 1.6567\r\n",
      "Train Epoch: 15 [87040/110534 (79%)]\tClassification Loss: 1.5117\r\n",
      "Train Epoch: 15 [87680/110534 (79%)]\tClassification Loss: 1.3864\r\n",
      "Train Epoch: 15 [88320/110534 (80%)]\tClassification Loss: 1.4675\r\n",
      "Train Epoch: 15 [88960/110534 (80%)]\tClassification Loss: 1.6615\r\n",
      "Train Epoch: 15 [89600/110534 (81%)]\tClassification Loss: 1.7716\r\n",
      "Train Epoch: 15 [90240/110534 (82%)]\tClassification Loss: 1.7801\r\n",
      "Train Epoch: 15 [90880/110534 (82%)]\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 15 [91520/110534 (83%)]\tClassification Loss: 1.3287\r\n",
      "Train Epoch: 15 [92160/110534 (83%)]\tClassification Loss: 1.3519\r\n",
      "Train Epoch: 15 [92800/110534 (84%)]\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 15 [93440/110534 (85%)]\tClassification Loss: 1.8062\r\n",
      "Train Epoch: 15 [94080/110534 (85%)]\tClassification Loss: 1.5708\r\n",
      "Train Epoch: 15 [94720/110534 (86%)]\tClassification Loss: 1.3451\r\n",
      "Train Epoch: 15 [95360/110534 (86%)]\tClassification Loss: 1.3823\r\n",
      "Train Epoch: 15 [96000/110534 (87%)]\tClassification Loss: 1.4027\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_1500.pth.tar\r\n",
      "Train Epoch: 15 [96640/110534 (87%)]\tClassification Loss: 1.3554\r\n",
      "Train Epoch: 15 [97280/110534 (88%)]\tClassification Loss: 1.3284\r\n",
      "Train Epoch: 15 [97920/110534 (89%)]\tClassification Loss: 1.1401\r\n",
      "Train Epoch: 15 [98560/110534 (89%)]\tClassification Loss: 1.5107\r\n",
      "Train Epoch: 15 [99200/110534 (90%)]\tClassification Loss: 1.6113\r\n",
      "Train Epoch: 15 [99840/110534 (90%)]\tClassification Loss: 1.4692\r\n",
      "Train Epoch: 15 [100480/110534 (91%)]\tClassification Loss: 1.6207\r\n",
      "Train Epoch: 15 [101120/110534 (91%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 15 [101760/110534 (92%)]\tClassification Loss: 1.4883\r\n",
      "Train Epoch: 15 [102400/110534 (93%)]\tClassification Loss: 1.5387\r\n",
      "Train Epoch: 15 [103040/110534 (93%)]\tClassification Loss: 1.3340\r\n",
      "Train Epoch: 15 [103680/110534 (94%)]\tClassification Loss: 1.6283\r\n",
      "Train Epoch: 15 [104320/110534 (94%)]\tClassification Loss: 1.4743\r\n",
      "Train Epoch: 15 [104960/110534 (95%)]\tClassification Loss: 1.5138\r\n",
      "Train Epoch: 15 [105600/110534 (96%)]\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 15 [106240/110534 (96%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 15 [106880/110534 (97%)]\tClassification Loss: 1.6767\r\n",
      "Train Epoch: 15 [107520/110534 (97%)]\tClassification Loss: 1.4824\r\n",
      "Train Epoch: 15 [108160/110534 (98%)]\tClassification Loss: 1.5694\r\n",
      "Train Epoch: 15 [108800/110534 (98%)]\tClassification Loss: 1.5652\r\n",
      "Train Epoch: 15 [109440/110534 (99%)]\tClassification Loss: 1.4279\r\n",
      "Train Epoch: 15 [110080/110534 (100%)]\tClassification Loss: 1.3903\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_final.pth.tar\r\n",
      "Train Epoch: 16 [0/110534 (0%)]\tClassification Loss: 1.5277\r\n",
      "\r\n",
      "Test set: Average loss: 1.4268, Accuracy: 23060/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 16 [640/110534 (1%)]\tClassification Loss: 1.3355\r\n",
      "Train Epoch: 16 [1280/110534 (1%)]\tClassification Loss: 1.9720\r\n",
      "Train Epoch: 16 [1920/110534 (2%)]\tClassification Loss: 1.6913\r\n",
      "Train Epoch: 16 [2560/110534 (2%)]\tClassification Loss: 1.7980\r\n",
      "Train Epoch: 16 [3200/110534 (3%)]\tClassification Loss: 1.5528\r\n",
      "Train Epoch: 16 [3840/110534 (3%)]\tClassification Loss: 1.3851\r\n",
      "Train Epoch: 16 [4480/110534 (4%)]\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 16 [5120/110534 (5%)]\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 16 [5760/110534 (5%)]\tClassification Loss: 1.5335\r\n",
      "Train Epoch: 16 [6400/110534 (6%)]\tClassification Loss: 1.3758\r\n",
      "Train Epoch: 16 [7040/110534 (6%)]\tClassification Loss: 1.5280\r\n",
      "Train Epoch: 16 [7680/110534 (7%)]\tClassification Loss: 1.6822\r\n",
      "Train Epoch: 16 [8320/110534 (8%)]\tClassification Loss: 1.8177\r\n",
      "Train Epoch: 16 [8960/110534 (8%)]\tClassification Loss: 1.6796\r\n",
      "Train Epoch: 16 [9600/110534 (9%)]\tClassification Loss: 1.3938\r\n",
      "Train Epoch: 16 [10240/110534 (9%)]\tClassification Loss: 1.7351\r\n",
      "Train Epoch: 16 [10880/110534 (10%)]\tClassification Loss: 1.3884\r\n",
      "Train Epoch: 16 [11520/110534 (10%)]\tClassification Loss: 1.6174\r\n",
      "Train Epoch: 16 [12160/110534 (11%)]\tClassification Loss: 1.3784\r\n",
      "Train Epoch: 16 [12800/110534 (12%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 16 [13440/110534 (12%)]\tClassification Loss: 1.5846\r\n",
      "Train Epoch: 16 [14080/110534 (13%)]\tClassification Loss: 1.6834\r\n",
      "Train Epoch: 16 [14720/110534 (13%)]\tClassification Loss: 1.8115\r\n",
      "Train Epoch: 16 [15360/110534 (14%)]\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 16 [16000/110534 (14%)]\tClassification Loss: 1.4488\r\n",
      "Train Epoch: 16 [16640/110534 (15%)]\tClassification Loss: 1.5738\r\n",
      "Train Epoch: 16 [17280/110534 (16%)]\tClassification Loss: 1.6727\r\n",
      "Train Epoch: 16 [17920/110534 (16%)]\tClassification Loss: 1.6412\r\n",
      "Train Epoch: 16 [18560/110534 (17%)]\tClassification Loss: 1.6838\r\n",
      "Train Epoch: 16 [19200/110534 (17%)]\tClassification Loss: 1.6524\r\n",
      "Train Epoch: 16 [19840/110534 (18%)]\tClassification Loss: 1.5798\r\n",
      "Train Epoch: 16 [20480/110534 (19%)]\tClassification Loss: 1.7413\r\n",
      "Train Epoch: 16 [21120/110534 (19%)]\tClassification Loss: 1.6450\r\n",
      "Train Epoch: 16 [21760/110534 (20%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 16 [22400/110534 (20%)]\tClassification Loss: 1.5115\r\n",
      "Train Epoch: 16 [23040/110534 (21%)]\tClassification Loss: 1.0544\r\n",
      "Train Epoch: 16 [23680/110534 (21%)]\tClassification Loss: 1.6190\r\n",
      "Train Epoch: 16 [24320/110534 (22%)]\tClassification Loss: 1.3143\r\n",
      "Train Epoch: 16 [24960/110534 (23%)]\tClassification Loss: 1.9102\r\n",
      "Train Epoch: 16 [25600/110534 (23%)]\tClassification Loss: 1.2619\r\n",
      "Train Epoch: 16 [26240/110534 (24%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 16 [26880/110534 (24%)]\tClassification Loss: 1.6145\r\n",
      "Train Epoch: 16 [27520/110534 (25%)]\tClassification Loss: 1.3796\r\n",
      "Train Epoch: 16 [28160/110534 (25%)]\tClassification Loss: 1.7782\r\n",
      "Train Epoch: 16 [28800/110534 (26%)]\tClassification Loss: 1.7823\r\n",
      "Train Epoch: 16 [29440/110534 (27%)]\tClassification Loss: 1.6053\r\n",
      "Train Epoch: 16 [30080/110534 (27%)]\tClassification Loss: 1.8129\r\n",
      "Train Epoch: 16 [30720/110534 (28%)]\tClassification Loss: 1.3798\r\n",
      "Train Epoch: 16 [31360/110534 (28%)]\tClassification Loss: 1.7156\r\n",
      "Train Epoch: 16 [32000/110534 (29%)]\tClassification Loss: 1.4889\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_500.pth.tar\r\n",
      "Train Epoch: 16 [32640/110534 (30%)]\tClassification Loss: 1.3078\r\n",
      "Train Epoch: 16 [33280/110534 (30%)]\tClassification Loss: 1.5253\r\n",
      "Train Epoch: 16 [33920/110534 (31%)]\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 16 [34560/110534 (31%)]\tClassification Loss: 1.6705\r\n",
      "Train Epoch: 16 [35200/110534 (32%)]\tClassification Loss: 1.4168\r\n",
      "Train Epoch: 16 [35840/110534 (32%)]\tClassification Loss: 1.2247\r\n",
      "Train Epoch: 16 [36480/110534 (33%)]\tClassification Loss: 1.4264\r\n",
      "Train Epoch: 16 [37120/110534 (34%)]\tClassification Loss: 1.5894\r\n",
      "Train Epoch: 16 [37760/110534 (34%)]\tClassification Loss: 1.8459\r\n",
      "Train Epoch: 16 [38400/110534 (35%)]\tClassification Loss: 1.3354\r\n",
      "Train Epoch: 16 [39040/110534 (35%)]\tClassification Loss: 1.5653\r\n",
      "Train Epoch: 16 [39680/110534 (36%)]\tClassification Loss: 1.5706\r\n",
      "Train Epoch: 16 [40320/110534 (36%)]\tClassification Loss: 1.3740\r\n",
      "Train Epoch: 16 [40960/110534 (37%)]\tClassification Loss: 1.3856\r\n",
      "Train Epoch: 16 [41600/110534 (38%)]\tClassification Loss: 1.4039\r\n",
      "Train Epoch: 16 [42240/110534 (38%)]\tClassification Loss: 1.3880\r\n",
      "Train Epoch: 16 [42880/110534 (39%)]\tClassification Loss: 1.4994\r\n",
      "Train Epoch: 16 [43520/110534 (39%)]\tClassification Loss: 1.3187\r\n",
      "Train Epoch: 16 [44160/110534 (40%)]\tClassification Loss: 1.4724\r\n",
      "Train Epoch: 16 [44800/110534 (41%)]\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 16 [45440/110534 (41%)]\tClassification Loss: 1.5580\r\n",
      "Train Epoch: 16 [46080/110534 (42%)]\tClassification Loss: 1.4044\r\n",
      "Train Epoch: 16 [46720/110534 (42%)]\tClassification Loss: 1.4302\r\n",
      "Train Epoch: 16 [47360/110534 (43%)]\tClassification Loss: 1.4466\r\n",
      "Train Epoch: 16 [48000/110534 (43%)]\tClassification Loss: 1.6601\r\n",
      "Train Epoch: 16 [48640/110534 (44%)]\tClassification Loss: 1.4574\r\n",
      "Train Epoch: 16 [49280/110534 (45%)]\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 16 [49920/110534 (45%)]\tClassification Loss: 1.5318\r\n",
      "Train Epoch: 16 [50560/110534 (46%)]\tClassification Loss: 1.4305\r\n",
      "Train Epoch: 16 [51200/110534 (46%)]\tClassification Loss: 1.3912\r\n",
      "Train Epoch: 16 [51840/110534 (47%)]\tClassification Loss: 1.5996\r\n",
      "Train Epoch: 16 [52480/110534 (47%)]\tClassification Loss: 1.4943\r\n",
      "Train Epoch: 16 [53120/110534 (48%)]\tClassification Loss: 1.1533\r\n",
      "Train Epoch: 16 [53760/110534 (49%)]\tClassification Loss: 1.6751\r\n",
      "Train Epoch: 16 [54400/110534 (49%)]\tClassification Loss: 1.5054\r\n",
      "Train Epoch: 16 [55040/110534 (50%)]\tClassification Loss: 1.5791\r\n",
      "Train Epoch: 16 [55680/110534 (50%)]\tClassification Loss: 1.5534\r\n",
      "Train Epoch: 16 [56320/110534 (51%)]\tClassification Loss: 1.5217\r\n",
      "Train Epoch: 16 [56960/110534 (52%)]\tClassification Loss: 1.4979\r\n",
      "Train Epoch: 16 [57600/110534 (52%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 16 [58240/110534 (53%)]\tClassification Loss: 1.5964\r\n",
      "Train Epoch: 16 [58880/110534 (53%)]\tClassification Loss: 1.8497\r\n",
      "Train Epoch: 16 [59520/110534 (54%)]\tClassification Loss: 1.3625\r\n",
      "Train Epoch: 16 [60160/110534 (54%)]\tClassification Loss: 1.4176\r\n",
      "Train Epoch: 16 [60800/110534 (55%)]\tClassification Loss: 1.5619\r\n",
      "Train Epoch: 16 [61440/110534 (56%)]\tClassification Loss: 1.4262\r\n",
      "Train Epoch: 16 [62080/110534 (56%)]\tClassification Loss: 1.5945\r\n",
      "Train Epoch: 16 [62720/110534 (57%)]\tClassification Loss: 1.6880\r\n",
      "Train Epoch: 16 [63360/110534 (57%)]\tClassification Loss: 1.3561\r\n",
      "Train Epoch: 16 [64000/110534 (58%)]\tClassification Loss: 1.6655\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_1000.pth.tar\r\n",
      "Train Epoch: 16 [64640/110534 (58%)]\tClassification Loss: 1.1052\r\n",
      "Train Epoch: 16 [65280/110534 (59%)]\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 16 [65920/110534 (60%)]\tClassification Loss: 1.5615\r\n",
      "Train Epoch: 16 [66560/110534 (60%)]\tClassification Loss: 1.4756\r\n",
      "Train Epoch: 16 [67200/110534 (61%)]\tClassification Loss: 1.3070\r\n",
      "Train Epoch: 16 [67840/110534 (61%)]\tClassification Loss: 1.8427\r\n",
      "Train Epoch: 16 [68480/110534 (62%)]\tClassification Loss: 1.5697\r\n",
      "Train Epoch: 16 [69120/110534 (63%)]\tClassification Loss: 1.8104\r\n",
      "Train Epoch: 16 [69760/110534 (63%)]\tClassification Loss: 1.5485\r\n",
      "Train Epoch: 16 [70400/110534 (64%)]\tClassification Loss: 1.1773\r\n",
      "Train Epoch: 16 [71040/110534 (64%)]\tClassification Loss: 1.8841\r\n",
      "Train Epoch: 16 [71680/110534 (65%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 16 [72320/110534 (65%)]\tClassification Loss: 1.7375\r\n",
      "Train Epoch: 16 [72960/110534 (66%)]\tClassification Loss: 1.7499\r\n",
      "Train Epoch: 16 [73600/110534 (67%)]\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 16 [74240/110534 (67%)]\tClassification Loss: 1.7353\r\n",
      "Train Epoch: 16 [74880/110534 (68%)]\tClassification Loss: 1.1501\r\n",
      "Train Epoch: 16 [75520/110534 (68%)]\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 16 [76160/110534 (69%)]\tClassification Loss: 1.4247\r\n",
      "Train Epoch: 16 [76800/110534 (69%)]\tClassification Loss: 1.2943\r\n",
      "Train Epoch: 16 [77440/110534 (70%)]\tClassification Loss: 1.2720\r\n",
      "Train Epoch: 16 [78080/110534 (71%)]\tClassification Loss: 1.3455\r\n",
      "Train Epoch: 16 [78720/110534 (71%)]\tClassification Loss: 1.4871\r\n",
      "Train Epoch: 16 [79360/110534 (72%)]\tClassification Loss: 1.3712\r\n",
      "Train Epoch: 16 [80000/110534 (72%)]\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 16 [80640/110534 (73%)]\tClassification Loss: 1.4672\r\n",
      "Train Epoch: 16 [81280/110534 (74%)]\tClassification Loss: 1.8329\r\n",
      "Train Epoch: 16 [81920/110534 (74%)]\tClassification Loss: 1.5496\r\n",
      "Train Epoch: 16 [82560/110534 (75%)]\tClassification Loss: 1.8017\r\n",
      "Train Epoch: 16 [83200/110534 (75%)]\tClassification Loss: 1.4081\r\n",
      "Train Epoch: 16 [83840/110534 (76%)]\tClassification Loss: 1.8643\r\n",
      "Train Epoch: 16 [84480/110534 (76%)]\tClassification Loss: 1.6730\r\n",
      "Train Epoch: 16 [85120/110534 (77%)]\tClassification Loss: 1.3841\r\n",
      "Train Epoch: 16 [85760/110534 (78%)]\tClassification Loss: 1.5310\r\n",
      "Train Epoch: 16 [86400/110534 (78%)]\tClassification Loss: 1.8081\r\n",
      "Train Epoch: 16 [87040/110534 (79%)]\tClassification Loss: 1.5018\r\n",
      "Train Epoch: 16 [87680/110534 (79%)]\tClassification Loss: 1.3226\r\n",
      "Train Epoch: 16 [88320/110534 (80%)]\tClassification Loss: 1.5161\r\n",
      "Train Epoch: 16 [88960/110534 (80%)]\tClassification Loss: 1.6956\r\n",
      "Train Epoch: 16 [89600/110534 (81%)]\tClassification Loss: 1.7979\r\n",
      "Train Epoch: 16 [90240/110534 (82%)]\tClassification Loss: 1.7883\r\n",
      "Train Epoch: 16 [90880/110534 (82%)]\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 16 [91520/110534 (83%)]\tClassification Loss: 1.3456\r\n",
      "Train Epoch: 16 [92160/110534 (83%)]\tClassification Loss: 1.4451\r\n",
      "Train Epoch: 16 [92800/110534 (84%)]\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 16 [93440/110534 (85%)]\tClassification Loss: 1.8917\r\n",
      "Train Epoch: 16 [94080/110534 (85%)]\tClassification Loss: 1.5631\r\n",
      "Train Epoch: 16 [94720/110534 (86%)]\tClassification Loss: 1.2983\r\n",
      "Train Epoch: 16 [95360/110534 (86%)]\tClassification Loss: 1.5562\r\n",
      "Train Epoch: 16 [96000/110534 (87%)]\tClassification Loss: 1.5018\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_1500.pth.tar\r\n",
      "Train Epoch: 16 [96640/110534 (87%)]\tClassification Loss: 1.5115\r\n",
      "Train Epoch: 16 [97280/110534 (88%)]\tClassification Loss: 1.3417\r\n",
      "Train Epoch: 16 [97920/110534 (89%)]\tClassification Loss: 1.2161\r\n",
      "Train Epoch: 16 [98560/110534 (89%)]\tClassification Loss: 1.3239\r\n",
      "Train Epoch: 16 [99200/110534 (90%)]\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 16 [99840/110534 (90%)]\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 16 [100480/110534 (91%)]\tClassification Loss: 1.6487\r\n",
      "Train Epoch: 16 [101120/110534 (91%)]\tClassification Loss: 1.4691\r\n",
      "Train Epoch: 16 [101760/110534 (92%)]\tClassification Loss: 1.5798\r\n",
      "Train Epoch: 16 [102400/110534 (93%)]\tClassification Loss: 1.5530\r\n",
      "Train Epoch: 16 [103040/110534 (93%)]\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 16 [103680/110534 (94%)]\tClassification Loss: 1.6842\r\n",
      "Train Epoch: 16 [104320/110534 (94%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 16 [104960/110534 (95%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 16 [105600/110534 (96%)]\tClassification Loss: 1.4074\r\n",
      "Train Epoch: 16 [106240/110534 (96%)]\tClassification Loss: 1.1591\r\n",
      "Train Epoch: 16 [106880/110534 (97%)]\tClassification Loss: 1.7020\r\n",
      "Train Epoch: 16 [107520/110534 (97%)]\tClassification Loss: 1.6387\r\n",
      "Train Epoch: 16 [108160/110534 (98%)]\tClassification Loss: 1.6291\r\n",
      "Train Epoch: 16 [108800/110534 (98%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 16 [109440/110534 (99%)]\tClassification Loss: 1.3910\r\n",
      "Train Epoch: 16 [110080/110534 (100%)]\tClassification Loss: 1.4076\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_final.pth.tar\r\n",
      "Train Epoch: 17 [0/110534 (0%)]\tClassification Loss: 1.4829\r\n",
      "\r\n",
      "Test set: Average loss: 1.4254, Accuracy: 23039/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 17 [640/110534 (1%)]\tClassification Loss: 1.3808\r\n",
      "Train Epoch: 17 [1280/110534 (1%)]\tClassification Loss: 1.8631\r\n",
      "Train Epoch: 17 [1920/110534 (2%)]\tClassification Loss: 1.6445\r\n",
      "Train Epoch: 17 [2560/110534 (2%)]\tClassification Loss: 1.7646\r\n",
      "Train Epoch: 17 [3200/110534 (3%)]\tClassification Loss: 1.4729\r\n",
      "Train Epoch: 17 [3840/110534 (3%)]\tClassification Loss: 1.3545\r\n",
      "Train Epoch: 17 [4480/110534 (4%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 17 [5120/110534 (5%)]\tClassification Loss: 1.6500\r\n",
      "Train Epoch: 17 [5760/110534 (5%)]\tClassification Loss: 1.4691\r\n",
      "Train Epoch: 17 [6400/110534 (6%)]\tClassification Loss: 1.3817\r\n",
      "Train Epoch: 17 [7040/110534 (6%)]\tClassification Loss: 1.5413\r\n",
      "Train Epoch: 17 [7680/110534 (7%)]\tClassification Loss: 1.4142\r\n",
      "Train Epoch: 17 [8320/110534 (8%)]\tClassification Loss: 1.6497\r\n",
      "Train Epoch: 17 [8960/110534 (8%)]\tClassification Loss: 1.6508\r\n",
      "Train Epoch: 17 [9600/110534 (9%)]\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 17 [10240/110534 (9%)]\tClassification Loss: 1.5548\r\n",
      "Train Epoch: 17 [10880/110534 (10%)]\tClassification Loss: 1.4910\r\n",
      "Train Epoch: 17 [11520/110534 (10%)]\tClassification Loss: 1.6772\r\n",
      "Train Epoch: 17 [12160/110534 (11%)]\tClassification Loss: 1.3528\r\n",
      "Train Epoch: 17 [12800/110534 (12%)]\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 17 [13440/110534 (12%)]\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 17 [14080/110534 (13%)]\tClassification Loss: 1.7550\r\n",
      "Train Epoch: 17 [14720/110534 (13%)]\tClassification Loss: 1.7335\r\n",
      "Train Epoch: 17 [15360/110534 (14%)]\tClassification Loss: 1.6230\r\n",
      "Train Epoch: 17 [16000/110534 (14%)]\tClassification Loss: 1.6412\r\n",
      "Train Epoch: 17 [16640/110534 (15%)]\tClassification Loss: 1.4861\r\n",
      "Train Epoch: 17 [17280/110534 (16%)]\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 17 [17920/110534 (16%)]\tClassification Loss: 1.6600\r\n",
      "Train Epoch: 17 [18560/110534 (17%)]\tClassification Loss: 1.8568\r\n",
      "Train Epoch: 17 [19200/110534 (17%)]\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 17 [19840/110534 (18%)]\tClassification Loss: 1.6288\r\n",
      "Train Epoch: 17 [20480/110534 (19%)]\tClassification Loss: 1.9357\r\n",
      "Train Epoch: 17 [21120/110534 (19%)]\tClassification Loss: 1.6306\r\n",
      "Train Epoch: 17 [21760/110534 (20%)]\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 17 [22400/110534 (20%)]\tClassification Loss: 1.5611\r\n",
      "Train Epoch: 17 [23040/110534 (21%)]\tClassification Loss: 1.1655\r\n",
      "Train Epoch: 17 [23680/110534 (21%)]\tClassification Loss: 1.6929\r\n",
      "Train Epoch: 17 [24320/110534 (22%)]\tClassification Loss: 1.2473\r\n",
      "Train Epoch: 17 [24960/110534 (23%)]\tClassification Loss: 1.7565\r\n",
      "Train Epoch: 17 [25600/110534 (23%)]\tClassification Loss: 1.3748\r\n",
      "Train Epoch: 17 [26240/110534 (24%)]\tClassification Loss: 1.4814\r\n",
      "Train Epoch: 17 [26880/110534 (24%)]\tClassification Loss: 1.7949\r\n",
      "Train Epoch: 17 [27520/110534 (25%)]\tClassification Loss: 1.4970\r\n",
      "Train Epoch: 17 [28160/110534 (25%)]\tClassification Loss: 1.7299\r\n",
      "Train Epoch: 17 [28800/110534 (26%)]\tClassification Loss: 1.9264\r\n",
      "Train Epoch: 17 [29440/110534 (27%)]\tClassification Loss: 1.7265\r\n",
      "Train Epoch: 17 [30080/110534 (27%)]\tClassification Loss: 1.9056\r\n",
      "Train Epoch: 17 [30720/110534 (28%)]\tClassification Loss: 1.6041\r\n",
      "Train Epoch: 17 [31360/110534 (28%)]\tClassification Loss: 1.5070\r\n",
      "Train Epoch: 17 [32000/110534 (29%)]\tClassification Loss: 1.4099\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_500.pth.tar\r\n",
      "Train Epoch: 17 [32640/110534 (30%)]\tClassification Loss: 1.3967\r\n",
      "Train Epoch: 17 [33280/110534 (30%)]\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 17 [33920/110534 (31%)]\tClassification Loss: 1.6763\r\n",
      "Train Epoch: 17 [34560/110534 (31%)]\tClassification Loss: 1.6438\r\n",
      "Train Epoch: 17 [35200/110534 (32%)]\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 17 [35840/110534 (32%)]\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 17 [36480/110534 (33%)]\tClassification Loss: 1.5381\r\n",
      "Train Epoch: 17 [37120/110534 (34%)]\tClassification Loss: 1.4969\r\n",
      "Train Epoch: 17 [37760/110534 (34%)]\tClassification Loss: 1.9300\r\n",
      "Train Epoch: 17 [38400/110534 (35%)]\tClassification Loss: 1.5258\r\n",
      "Train Epoch: 17 [39040/110534 (35%)]\tClassification Loss: 1.4096\r\n",
      "Train Epoch: 17 [39680/110534 (36%)]\tClassification Loss: 1.5243\r\n",
      "Train Epoch: 17 [40320/110534 (36%)]\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 17 [40960/110534 (37%)]\tClassification Loss: 1.3249\r\n",
      "Train Epoch: 17 [41600/110534 (38%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 17 [42240/110534 (38%)]\tClassification Loss: 1.5596\r\n",
      "Train Epoch: 17 [42880/110534 (39%)]\tClassification Loss: 1.4849\r\n",
      "Train Epoch: 17 [43520/110534 (39%)]\tClassification Loss: 1.4148\r\n",
      "Train Epoch: 17 [44160/110534 (40%)]\tClassification Loss: 1.5868\r\n",
      "Train Epoch: 17 [44800/110534 (41%)]\tClassification Loss: 1.7915\r\n",
      "Train Epoch: 17 [45440/110534 (41%)]\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 17 [46080/110534 (42%)]\tClassification Loss: 1.3594\r\n",
      "Train Epoch: 17 [46720/110534 (42%)]\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 17 [47360/110534 (43%)]\tClassification Loss: 1.3702\r\n",
      "Train Epoch: 17 [48000/110534 (43%)]\tClassification Loss: 1.5659\r\n",
      "Train Epoch: 17 [48640/110534 (44%)]\tClassification Loss: 1.4715\r\n",
      "Train Epoch: 17 [49280/110534 (45%)]\tClassification Loss: 1.5989\r\n",
      "Train Epoch: 17 [49920/110534 (45%)]\tClassification Loss: 1.7679\r\n",
      "Train Epoch: 17 [50560/110534 (46%)]\tClassification Loss: 1.6452\r\n",
      "Train Epoch: 17 [51200/110534 (46%)]\tClassification Loss: 1.4463\r\n",
      "Train Epoch: 17 [51840/110534 (47%)]\tClassification Loss: 1.5510\r\n",
      "Train Epoch: 17 [52480/110534 (47%)]\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 17 [53120/110534 (48%)]\tClassification Loss: 1.0976\r\n",
      "Train Epoch: 17 [53760/110534 (49%)]\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 17 [54400/110534 (49%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 17 [55040/110534 (50%)]\tClassification Loss: 1.6889\r\n",
      "Train Epoch: 17 [55680/110534 (50%)]\tClassification Loss: 1.7003\r\n",
      "Train Epoch: 17 [56320/110534 (51%)]\tClassification Loss: 1.4314\r\n",
      "Train Epoch: 17 [56960/110534 (52%)]\tClassification Loss: 1.4394\r\n",
      "Train Epoch: 17 [57600/110534 (52%)]\tClassification Loss: 1.7841\r\n",
      "Train Epoch: 17 [58240/110534 (53%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 17 [58880/110534 (53%)]\tClassification Loss: 1.7394\r\n",
      "Train Epoch: 17 [59520/110534 (54%)]\tClassification Loss: 1.3840\r\n",
      "Train Epoch: 17 [60160/110534 (54%)]\tClassification Loss: 1.4409\r\n",
      "Train Epoch: 17 [60800/110534 (55%)]\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 17 [61440/110534 (56%)]\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 17 [62080/110534 (56%)]\tClassification Loss: 1.4855\r\n",
      "Train Epoch: 17 [62720/110534 (57%)]\tClassification Loss: 1.7079\r\n",
      "Train Epoch: 17 [63360/110534 (57%)]\tClassification Loss: 1.2898\r\n",
      "Train Epoch: 17 [64000/110534 (58%)]\tClassification Loss: 1.5239\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_1000.pth.tar\r\n",
      "Train Epoch: 17 [64640/110534 (58%)]\tClassification Loss: 1.1992\r\n",
      "Train Epoch: 17 [65280/110534 (59%)]\tClassification Loss: 1.4508\r\n",
      "Train Epoch: 17 [65920/110534 (60%)]\tClassification Loss: 1.5102\r\n",
      "Train Epoch: 17 [66560/110534 (60%)]\tClassification Loss: 1.4080\r\n",
      "Train Epoch: 17 [67200/110534 (61%)]\tClassification Loss: 1.2517\r\n",
      "Train Epoch: 17 [67840/110534 (61%)]\tClassification Loss: 1.8872\r\n",
      "Train Epoch: 17 [68480/110534 (62%)]\tClassification Loss: 1.5177\r\n",
      "Train Epoch: 17 [69120/110534 (63%)]\tClassification Loss: 1.7372\r\n",
      "Train Epoch: 17 [69760/110534 (63%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 17 [70400/110534 (64%)]\tClassification Loss: 1.3723\r\n",
      "Train Epoch: 17 [71040/110534 (64%)]\tClassification Loss: 1.7883\r\n",
      "Train Epoch: 17 [71680/110534 (65%)]\tClassification Loss: 1.4581\r\n",
      "Train Epoch: 17 [72320/110534 (65%)]\tClassification Loss: 1.5286\r\n",
      "Train Epoch: 17 [72960/110534 (66%)]\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 17 [73600/110534 (67%)]\tClassification Loss: 1.7128\r\n",
      "Train Epoch: 17 [74240/110534 (67%)]\tClassification Loss: 1.6025\r\n",
      "Train Epoch: 17 [74880/110534 (68%)]\tClassification Loss: 1.2006\r\n",
      "Train Epoch: 17 [75520/110534 (68%)]\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 17 [76160/110534 (69%)]\tClassification Loss: 1.3813\r\n",
      "Train Epoch: 17 [76800/110534 (69%)]\tClassification Loss: 1.2678\r\n",
      "Train Epoch: 17 [77440/110534 (70%)]\tClassification Loss: 1.4682\r\n",
      "Train Epoch: 17 [78080/110534 (71%)]\tClassification Loss: 1.3479\r\n",
      "Train Epoch: 17 [78720/110534 (71%)]\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 17 [79360/110534 (72%)]\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 17 [80000/110534 (72%)]\tClassification Loss: 1.3237\r\n",
      "Train Epoch: 17 [80640/110534 (73%)]\tClassification Loss: 1.3605\r\n",
      "Train Epoch: 17 [81280/110534 (74%)]\tClassification Loss: 1.7566\r\n",
      "Train Epoch: 17 [81920/110534 (74%)]\tClassification Loss: 1.4194\r\n",
      "Train Epoch: 17 [82560/110534 (75%)]\tClassification Loss: 1.7606\r\n",
      "Train Epoch: 17 [83200/110534 (75%)]\tClassification Loss: 1.3709\r\n",
      "Train Epoch: 17 [83840/110534 (76%)]\tClassification Loss: 1.9032\r\n",
      "Train Epoch: 17 [84480/110534 (76%)]\tClassification Loss: 1.5605\r\n",
      "Train Epoch: 17 [85120/110534 (77%)]\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 17 [85760/110534 (78%)]\tClassification Loss: 1.5963\r\n",
      "Train Epoch: 17 [86400/110534 (78%)]\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 17 [87040/110534 (79%)]\tClassification Loss: 1.4863\r\n",
      "Train Epoch: 17 [87680/110534 (79%)]\tClassification Loss: 1.1935\r\n",
      "Train Epoch: 17 [88320/110534 (80%)]\tClassification Loss: 1.6224\r\n",
      "Train Epoch: 17 [88960/110534 (80%)]\tClassification Loss: 1.6499\r\n",
      "Train Epoch: 17 [89600/110534 (81%)]\tClassification Loss: 1.7542\r\n",
      "Train Epoch: 17 [90240/110534 (82%)]\tClassification Loss: 1.6719\r\n",
      "Train Epoch: 17 [90880/110534 (82%)]\tClassification Loss: 1.5563\r\n",
      "Train Epoch: 17 [91520/110534 (83%)]\tClassification Loss: 1.2003\r\n",
      "Train Epoch: 17 [92160/110534 (83%)]\tClassification Loss: 1.4314\r\n",
      "Train Epoch: 17 [92800/110534 (84%)]\tClassification Loss: 1.4414\r\n",
      "Train Epoch: 17 [93440/110534 (85%)]\tClassification Loss: 1.8028\r\n",
      "Train Epoch: 17 [94080/110534 (85%)]\tClassification Loss: 1.6458\r\n",
      "Train Epoch: 17 [94720/110534 (86%)]\tClassification Loss: 1.3584\r\n",
      "Train Epoch: 17 [95360/110534 (86%)]\tClassification Loss: 1.3738\r\n",
      "Train Epoch: 17 [96000/110534 (87%)]\tClassification Loss: 1.5293\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_1500.pth.tar\r\n",
      "Train Epoch: 17 [96640/110534 (87%)]\tClassification Loss: 1.2777\r\n",
      "Train Epoch: 17 [97280/110534 (88%)]\tClassification Loss: 1.0589\r\n",
      "Train Epoch: 17 [97920/110534 (89%)]\tClassification Loss: 1.2859\r\n",
      "Train Epoch: 17 [98560/110534 (89%)]\tClassification Loss: 1.3498\r\n",
      "Train Epoch: 17 [99200/110534 (90%)]\tClassification Loss: 1.7388\r\n",
      "Train Epoch: 17 [99840/110534 (90%)]\tClassification Loss: 1.3424\r\n",
      "Train Epoch: 17 [100480/110534 (91%)]\tClassification Loss: 1.8931\r\n",
      "Train Epoch: 17 [101120/110534 (91%)]\tClassification Loss: 1.4152\r\n",
      "Train Epoch: 17 [101760/110534 (92%)]\tClassification Loss: 1.5706\r\n",
      "Train Epoch: 17 [102400/110534 (93%)]\tClassification Loss: 1.4780\r\n",
      "Train Epoch: 17 [103040/110534 (93%)]\tClassification Loss: 1.4560\r\n",
      "Train Epoch: 17 [103680/110534 (94%)]\tClassification Loss: 1.6939\r\n",
      "Train Epoch: 17 [104320/110534 (94%)]\tClassification Loss: 1.5972\r\n",
      "Train Epoch: 17 [104960/110534 (95%)]\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 17 [105600/110534 (96%)]\tClassification Loss: 1.3098\r\n",
      "Train Epoch: 17 [106240/110534 (96%)]\tClassification Loss: 1.1327\r\n",
      "Train Epoch: 17 [106880/110534 (97%)]\tClassification Loss: 1.5994\r\n",
      "Train Epoch: 17 [107520/110534 (97%)]\tClassification Loss: 1.6919\r\n",
      "Train Epoch: 17 [108160/110534 (98%)]\tClassification Loss: 1.5957\r\n",
      "Train Epoch: 17 [108800/110534 (98%)]\tClassification Loss: 1.7936\r\n",
      "Train Epoch: 17 [109440/110534 (99%)]\tClassification Loss: 1.4738\r\n",
      "Train Epoch: 17 [110080/110534 (100%)]\tClassification Loss: 1.4043\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_final.pth.tar\r\n",
      "Train Epoch: 18 [0/110534 (0%)]\tClassification Loss: 1.4566\r\n",
      "\r\n",
      "Test set: Average loss: 1.4268, Accuracy: 22972/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 18 [640/110534 (1%)]\tClassification Loss: 1.3683\r\n",
      "Train Epoch: 18 [1280/110534 (1%)]\tClassification Loss: 1.8444\r\n",
      "Train Epoch: 18 [1920/110534 (2%)]\tClassification Loss: 1.5703\r\n",
      "Train Epoch: 18 [2560/110534 (2%)]\tClassification Loss: 1.8496\r\n",
      "Train Epoch: 18 [3200/110534 (3%)]\tClassification Loss: 1.4703\r\n",
      "Train Epoch: 18 [3840/110534 (3%)]\tClassification Loss: 1.4272\r\n",
      "Train Epoch: 18 [4480/110534 (4%)]\tClassification Loss: 1.5818\r\n",
      "Train Epoch: 18 [5120/110534 (5%)]\tClassification Loss: 1.5956\r\n",
      "Train Epoch: 18 [5760/110534 (5%)]\tClassification Loss: 1.5061\r\n",
      "Train Epoch: 18 [6400/110534 (6%)]\tClassification Loss: 1.2827\r\n",
      "Train Epoch: 18 [7040/110534 (6%)]\tClassification Loss: 1.4053\r\n",
      "Train Epoch: 18 [7680/110534 (7%)]\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 18 [8320/110534 (8%)]\tClassification Loss: 1.7012\r\n",
      "Train Epoch: 18 [8960/110534 (8%)]\tClassification Loss: 1.6453\r\n",
      "Train Epoch: 18 [9600/110534 (9%)]\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 18 [10240/110534 (9%)]\tClassification Loss: 1.5220\r\n",
      "Train Epoch: 18 [10880/110534 (10%)]\tClassification Loss: 1.6569\r\n",
      "Train Epoch: 18 [11520/110534 (10%)]\tClassification Loss: 1.6096\r\n",
      "Train Epoch: 18 [12160/110534 (11%)]\tClassification Loss: 1.3467\r\n",
      "Train Epoch: 18 [12800/110534 (12%)]\tClassification Loss: 1.6867\r\n",
      "Train Epoch: 18 [13440/110534 (12%)]\tClassification Loss: 1.4950\r\n",
      "Train Epoch: 18 [14080/110534 (13%)]\tClassification Loss: 1.5827\r\n",
      "Train Epoch: 18 [14720/110534 (13%)]\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 18 [15360/110534 (14%)]\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 18 [16000/110534 (14%)]\tClassification Loss: 1.6569\r\n",
      "Train Epoch: 18 [16640/110534 (15%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 18 [17280/110534 (16%)]\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 18 [17920/110534 (16%)]\tClassification Loss: 1.5592\r\n",
      "Train Epoch: 18 [18560/110534 (17%)]\tClassification Loss: 1.7354\r\n",
      "Train Epoch: 18 [19200/110534 (17%)]\tClassification Loss: 1.6269\r\n",
      "Train Epoch: 18 [19840/110534 (18%)]\tClassification Loss: 1.5741\r\n",
      "Train Epoch: 18 [20480/110534 (19%)]\tClassification Loss: 1.7756\r\n",
      "Train Epoch: 18 [21120/110534 (19%)]\tClassification Loss: 1.6949\r\n",
      "Train Epoch: 18 [21760/110534 (20%)]\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 18 [22400/110534 (20%)]\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 18 [23040/110534 (21%)]\tClassification Loss: 1.1336\r\n",
      "Train Epoch: 18 [23680/110534 (21%)]\tClassification Loss: 1.6494\r\n",
      "Train Epoch: 18 [24320/110534 (22%)]\tClassification Loss: 1.2561\r\n",
      "Train Epoch: 18 [24960/110534 (23%)]\tClassification Loss: 1.8035\r\n",
      "Train Epoch: 18 [25600/110534 (23%)]\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 18 [26240/110534 (24%)]\tClassification Loss: 1.3282\r\n",
      "Train Epoch: 18 [26880/110534 (24%)]\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 18 [27520/110534 (25%)]\tClassification Loss: 1.3185\r\n",
      "Train Epoch: 18 [28160/110534 (25%)]\tClassification Loss: 1.8512\r\n",
      "Train Epoch: 18 [28800/110534 (26%)]\tClassification Loss: 1.9734\r\n",
      "Train Epoch: 18 [29440/110534 (27%)]\tClassification Loss: 1.6292\r\n",
      "Train Epoch: 18 [30080/110534 (27%)]\tClassification Loss: 1.7859\r\n",
      "Train Epoch: 18 [30720/110534 (28%)]\tClassification Loss: 1.2818\r\n",
      "Train Epoch: 18 [31360/110534 (28%)]\tClassification Loss: 1.6586\r\n",
      "Train Epoch: 18 [32000/110534 (29%)]\tClassification Loss: 1.4115\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_500.pth.tar\r\n",
      "Train Epoch: 18 [32640/110534 (30%)]\tClassification Loss: 1.4249\r\n",
      "Train Epoch: 18 [33280/110534 (30%)]\tClassification Loss: 1.6212\r\n",
      "Train Epoch: 18 [33920/110534 (31%)]\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 18 [34560/110534 (31%)]\tClassification Loss: 1.6841\r\n",
      "Train Epoch: 18 [35200/110534 (32%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 18 [35840/110534 (32%)]\tClassification Loss: 1.1472\r\n",
      "Train Epoch: 18 [36480/110534 (33%)]\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 18 [37120/110534 (34%)]\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 18 [37760/110534 (34%)]\tClassification Loss: 1.8734\r\n",
      "Train Epoch: 18 [38400/110534 (35%)]\tClassification Loss: 1.5655\r\n",
      "Train Epoch: 18 [39040/110534 (35%)]\tClassification Loss: 1.4102\r\n",
      "Train Epoch: 18 [39680/110534 (36%)]\tClassification Loss: 1.6447\r\n",
      "Train Epoch: 18 [40320/110534 (36%)]\tClassification Loss: 1.4748\r\n",
      "Train Epoch: 18 [40960/110534 (37%)]\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 18 [41600/110534 (38%)]\tClassification Loss: 1.3954\r\n",
      "Train Epoch: 18 [42240/110534 (38%)]\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 18 [42880/110534 (39%)]\tClassification Loss: 1.3827\r\n",
      "Train Epoch: 18 [43520/110534 (39%)]\tClassification Loss: 1.3045\r\n",
      "Train Epoch: 18 [44160/110534 (40%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 18 [44800/110534 (41%)]\tClassification Loss: 1.5316\r\n",
      "Train Epoch: 18 [45440/110534 (41%)]\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 18 [46080/110534 (42%)]\tClassification Loss: 1.3432\r\n",
      "Train Epoch: 18 [46720/110534 (42%)]\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 18 [47360/110534 (43%)]\tClassification Loss: 1.4145\r\n",
      "Train Epoch: 18 [48000/110534 (43%)]\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 18 [48640/110534 (44%)]\tClassification Loss: 1.3344\r\n",
      "Train Epoch: 18 [49280/110534 (45%)]\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 18 [49920/110534 (45%)]\tClassification Loss: 1.6642\r\n",
      "Train Epoch: 18 [50560/110534 (46%)]\tClassification Loss: 1.6332\r\n",
      "Train Epoch: 18 [51200/110534 (46%)]\tClassification Loss: 1.4096\r\n",
      "Train Epoch: 18 [51840/110534 (47%)]\tClassification Loss: 1.5044\r\n",
      "Train Epoch: 18 [52480/110534 (47%)]\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 18 [53120/110534 (48%)]\tClassification Loss: 1.2142\r\n",
      "Train Epoch: 18 [53760/110534 (49%)]\tClassification Loss: 1.5642\r\n",
      "Train Epoch: 18 [54400/110534 (49%)]\tClassification Loss: 1.2896\r\n",
      "Train Epoch: 18 [55040/110534 (50%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 18 [55680/110534 (50%)]\tClassification Loss: 1.7444\r\n",
      "Train Epoch: 18 [56320/110534 (51%)]\tClassification Loss: 1.3977\r\n",
      "Train Epoch: 18 [56960/110534 (52%)]\tClassification Loss: 1.4739\r\n",
      "Train Epoch: 18 [57600/110534 (52%)]\tClassification Loss: 1.7443\r\n",
      "Train Epoch: 18 [58240/110534 (53%)]\tClassification Loss: 1.5547\r\n",
      "Train Epoch: 18 [58880/110534 (53%)]\tClassification Loss: 1.7068\r\n",
      "Train Epoch: 18 [59520/110534 (54%)]\tClassification Loss: 1.5874\r\n",
      "Train Epoch: 18 [60160/110534 (54%)]\tClassification Loss: 1.6220\r\n",
      "Train Epoch: 18 [60800/110534 (55%)]\tClassification Loss: 1.5484\r\n",
      "Train Epoch: 18 [61440/110534 (56%)]\tClassification Loss: 1.4466\r\n",
      "Train Epoch: 18 [62080/110534 (56%)]\tClassification Loss: 1.5201\r\n",
      "Train Epoch: 18 [62720/110534 (57%)]\tClassification Loss: 1.6307\r\n",
      "Train Epoch: 18 [63360/110534 (57%)]\tClassification Loss: 1.2672\r\n",
      "Train Epoch: 18 [64000/110534 (58%)]\tClassification Loss: 1.5494\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_1000.pth.tar\r\n",
      "Train Epoch: 18 [64640/110534 (58%)]\tClassification Loss: 1.1619\r\n",
      "Train Epoch: 18 [65280/110534 (59%)]\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 18 [65920/110534 (60%)]\tClassification Loss: 1.6336\r\n",
      "Train Epoch: 18 [66560/110534 (60%)]\tClassification Loss: 1.3253\r\n",
      "Train Epoch: 18 [67200/110534 (61%)]\tClassification Loss: 1.4866\r\n",
      "Train Epoch: 18 [67840/110534 (61%)]\tClassification Loss: 1.8643\r\n",
      "Train Epoch: 18 [68480/110534 (62%)]\tClassification Loss: 1.5976\r\n",
      "Train Epoch: 18 [69120/110534 (63%)]\tClassification Loss: 1.6912\r\n",
      "Train Epoch: 18 [69760/110534 (63%)]\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 18 [70400/110534 (64%)]\tClassification Loss: 1.1738\r\n",
      "Train Epoch: 18 [71040/110534 (64%)]\tClassification Loss: 1.9285\r\n",
      "Train Epoch: 18 [71680/110534 (65%)]\tClassification Loss: 1.5479\r\n",
      "Train Epoch: 18 [72320/110534 (65%)]\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 18 [72960/110534 (66%)]\tClassification Loss: 1.6588\r\n",
      "Train Epoch: 18 [73600/110534 (67%)]\tClassification Loss: 1.8037\r\n",
      "Train Epoch: 18 [74240/110534 (67%)]\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 18 [74880/110534 (68%)]\tClassification Loss: 1.2398\r\n",
      "Train Epoch: 18 [75520/110534 (68%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 18 [76160/110534 (69%)]\tClassification Loss: 1.3028\r\n",
      "Train Epoch: 18 [76800/110534 (69%)]\tClassification Loss: 1.3206\r\n",
      "Train Epoch: 18 [77440/110534 (70%)]\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 18 [78080/110534 (71%)]\tClassification Loss: 1.5018\r\n",
      "Train Epoch: 18 [78720/110534 (71%)]\tClassification Loss: 1.5011\r\n",
      "Train Epoch: 18 [79360/110534 (72%)]\tClassification Loss: 1.4912\r\n",
      "Train Epoch: 18 [80000/110534 (72%)]\tClassification Loss: 1.3822\r\n",
      "Train Epoch: 18 [80640/110534 (73%)]\tClassification Loss: 1.5406\r\n",
      "Train Epoch: 18 [81280/110534 (74%)]\tClassification Loss: 1.7996\r\n",
      "Train Epoch: 18 [81920/110534 (74%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 18 [82560/110534 (75%)]\tClassification Loss: 1.7294\r\n",
      "Train Epoch: 18 [83200/110534 (75%)]\tClassification Loss: 1.4851\r\n",
      "Train Epoch: 18 [83840/110534 (76%)]\tClassification Loss: 1.8305\r\n",
      "Train Epoch: 18 [84480/110534 (76%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 18 [85120/110534 (77%)]\tClassification Loss: 1.4686\r\n",
      "Train Epoch: 18 [85760/110534 (78%)]\tClassification Loss: 1.4922\r\n",
      "Train Epoch: 18 [86400/110534 (78%)]\tClassification Loss: 1.6469\r\n",
      "Train Epoch: 18 [87040/110534 (79%)]\tClassification Loss: 1.4409\r\n",
      "Train Epoch: 18 [87680/110534 (79%)]\tClassification Loss: 1.3075\r\n",
      "Train Epoch: 18 [88320/110534 (80%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 18 [88960/110534 (80%)]\tClassification Loss: 1.7025\r\n",
      "Train Epoch: 18 [89600/110534 (81%)]\tClassification Loss: 1.7896\r\n",
      "Train Epoch: 18 [90240/110534 (82%)]\tClassification Loss: 1.8553\r\n",
      "Train Epoch: 18 [90880/110534 (82%)]\tClassification Loss: 1.5443\r\n",
      "Train Epoch: 18 [91520/110534 (83%)]\tClassification Loss: 1.2791\r\n",
      "Train Epoch: 18 [92160/110534 (83%)]\tClassification Loss: 1.2992\r\n",
      "Train Epoch: 18 [92800/110534 (84%)]\tClassification Loss: 1.4232\r\n",
      "Train Epoch: 18 [93440/110534 (85%)]\tClassification Loss: 1.7795\r\n",
      "Train Epoch: 18 [94080/110534 (85%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 18 [94720/110534 (86%)]\tClassification Loss: 1.6068\r\n",
      "Train Epoch: 18 [95360/110534 (86%)]\tClassification Loss: 1.4495\r\n",
      "Train Epoch: 18 [96000/110534 (87%)]\tClassification Loss: 1.3823\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_1500.pth.tar\r\n",
      "Train Epoch: 18 [96640/110534 (87%)]\tClassification Loss: 1.2644\r\n",
      "Train Epoch: 18 [97280/110534 (88%)]\tClassification Loss: 1.2823\r\n",
      "Train Epoch: 18 [97920/110534 (89%)]\tClassification Loss: 1.1485\r\n",
      "Train Epoch: 18 [98560/110534 (89%)]\tClassification Loss: 1.3423\r\n",
      "Train Epoch: 18 [99200/110534 (90%)]\tClassification Loss: 1.6149\r\n",
      "Train Epoch: 18 [99840/110534 (90%)]\tClassification Loss: 1.5016\r\n",
      "Train Epoch: 18 [100480/110534 (91%)]\tClassification Loss: 1.7274\r\n",
      "Train Epoch: 18 [101120/110534 (91%)]\tClassification Loss: 1.4072\r\n",
      "Train Epoch: 18 [101760/110534 (92%)]\tClassification Loss: 1.5309\r\n",
      "Train Epoch: 18 [102400/110534 (93%)]\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 18 [103040/110534 (93%)]\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 18 [103680/110534 (94%)]\tClassification Loss: 1.8338\r\n",
      "Train Epoch: 18 [104320/110534 (94%)]\tClassification Loss: 1.3259\r\n",
      "Train Epoch: 18 [104960/110534 (95%)]\tClassification Loss: 1.3988\r\n",
      "Train Epoch: 18 [105600/110534 (96%)]\tClassification Loss: 1.4199\r\n",
      "Train Epoch: 18 [106240/110534 (96%)]\tClassification Loss: 1.3462\r\n",
      "Train Epoch: 18 [106880/110534 (97%)]\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 18 [107520/110534 (97%)]\tClassification Loss: 1.6630\r\n",
      "Train Epoch: 18 [108160/110534 (98%)]\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 18 [108800/110534 (98%)]\tClassification Loss: 1.7121\r\n",
      "Train Epoch: 18 [109440/110534 (99%)]\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 18 [110080/110534 (100%)]\tClassification Loss: 1.5305\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_final.pth.tar\r\n",
      "Train Epoch: 19 [0/110534 (0%)]\tClassification Loss: 1.4706\r\n",
      "\r\n",
      "Test set: Average loss: 1.4298, Accuracy: 22912/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 19 [640/110534 (1%)]\tClassification Loss: 1.3148\r\n",
      "Train Epoch: 19 [1280/110534 (1%)]\tClassification Loss: 2.0969\r\n",
      "Train Epoch: 19 [1920/110534 (2%)]\tClassification Loss: 1.6947\r\n",
      "Train Epoch: 19 [2560/110534 (2%)]\tClassification Loss: 1.7502\r\n",
      "Train Epoch: 19 [3200/110534 (3%)]\tClassification Loss: 1.4302\r\n",
      "Train Epoch: 19 [3840/110534 (3%)]\tClassification Loss: 1.5268\r\n",
      "Train Epoch: 19 [4480/110534 (4%)]\tClassification Loss: 1.4274\r\n",
      "Train Epoch: 19 [5120/110534 (5%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 19 [5760/110534 (5%)]\tClassification Loss: 1.6972\r\n",
      "Train Epoch: 19 [6400/110534 (6%)]\tClassification Loss: 1.2594\r\n",
      "Train Epoch: 19 [7040/110534 (6%)]\tClassification Loss: 1.4998\r\n",
      "Train Epoch: 19 [7680/110534 (7%)]\tClassification Loss: 1.4187\r\n",
      "Train Epoch: 19 [8320/110534 (8%)]\tClassification Loss: 1.8228\r\n",
      "Train Epoch: 19 [8960/110534 (8%)]\tClassification Loss: 1.6831\r\n",
      "Train Epoch: 19 [9600/110534 (9%)]\tClassification Loss: 1.4329\r\n",
      "Train Epoch: 19 [10240/110534 (9%)]\tClassification Loss: 1.4760\r\n",
      "Train Epoch: 19 [10880/110534 (10%)]\tClassification Loss: 1.6194\r\n",
      "Train Epoch: 19 [11520/110534 (10%)]\tClassification Loss: 1.5840\r\n",
      "Train Epoch: 19 [12160/110534 (11%)]\tClassification Loss: 1.2299\r\n",
      "Train Epoch: 19 [12800/110534 (12%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 19 [13440/110534 (12%)]\tClassification Loss: 1.4037\r\n",
      "Train Epoch: 19 [14080/110534 (13%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 19 [14720/110534 (13%)]\tClassification Loss: 1.7083\r\n",
      "Train Epoch: 19 [15360/110534 (14%)]\tClassification Loss: 1.5861\r\n",
      "Train Epoch: 19 [16000/110534 (14%)]\tClassification Loss: 1.5306\r\n",
      "Train Epoch: 19 [16640/110534 (15%)]\tClassification Loss: 1.4473\r\n",
      "Train Epoch: 19 [17280/110534 (16%)]\tClassification Loss: 1.5863\r\n",
      "Train Epoch: 19 [17920/110534 (16%)]\tClassification Loss: 1.4192\r\n",
      "Train Epoch: 19 [18560/110534 (17%)]\tClassification Loss: 1.9347\r\n",
      "Train Epoch: 19 [19200/110534 (17%)]\tClassification Loss: 1.6583\r\n",
      "Train Epoch: 19 [19840/110534 (18%)]\tClassification Loss: 1.6002\r\n",
      "Train Epoch: 19 [20480/110534 (19%)]\tClassification Loss: 1.7562\r\n",
      "Train Epoch: 19 [21120/110534 (19%)]\tClassification Loss: 1.7902\r\n",
      "Train Epoch: 19 [21760/110534 (20%)]\tClassification Loss: 1.3536\r\n",
      "Train Epoch: 19 [22400/110534 (20%)]\tClassification Loss: 1.6576\r\n",
      "Train Epoch: 19 [23040/110534 (21%)]\tClassification Loss: 1.0069\r\n",
      "Train Epoch: 19 [23680/110534 (21%)]\tClassification Loss: 1.5050\r\n",
      "Train Epoch: 19 [24320/110534 (22%)]\tClassification Loss: 1.2268\r\n",
      "Train Epoch: 19 [24960/110534 (23%)]\tClassification Loss: 1.7857\r\n",
      "Train Epoch: 19 [25600/110534 (23%)]\tClassification Loss: 1.3096\r\n",
      "Train Epoch: 19 [26240/110534 (24%)]\tClassification Loss: 1.2822\r\n",
      "Train Epoch: 19 [26880/110534 (24%)]\tClassification Loss: 1.7020\r\n",
      "Train Epoch: 19 [27520/110534 (25%)]\tClassification Loss: 1.5010\r\n",
      "Train Epoch: 19 [28160/110534 (25%)]\tClassification Loss: 1.8022\r\n",
      "Train Epoch: 19 [28800/110534 (26%)]\tClassification Loss: 1.6839\r\n",
      "Train Epoch: 19 [29440/110534 (27%)]\tClassification Loss: 1.6698\r\n",
      "Train Epoch: 19 [30080/110534 (27%)]\tClassification Loss: 1.8718\r\n",
      "Train Epoch: 19 [30720/110534 (28%)]\tClassification Loss: 1.4146\r\n",
      "Train Epoch: 19 [31360/110534 (28%)]\tClassification Loss: 1.6491\r\n",
      "Train Epoch: 19 [32000/110534 (29%)]\tClassification Loss: 1.4964\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_500.pth.tar\r\n",
      "Train Epoch: 19 [32640/110534 (30%)]\tClassification Loss: 1.4085\r\n",
      "Train Epoch: 19 [33280/110534 (30%)]\tClassification Loss: 1.4818\r\n",
      "Train Epoch: 19 [33920/110534 (31%)]\tClassification Loss: 1.5882\r\n",
      "Train Epoch: 19 [34560/110534 (31%)]\tClassification Loss: 1.6741\r\n",
      "Train Epoch: 19 [35200/110534 (32%)]\tClassification Loss: 1.3599\r\n",
      "Train Epoch: 19 [35840/110534 (32%)]\tClassification Loss: 1.1092\r\n",
      "Train Epoch: 19 [36480/110534 (33%)]\tClassification Loss: 1.3546\r\n",
      "Train Epoch: 19 [37120/110534 (34%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 19 [37760/110534 (34%)]\tClassification Loss: 1.8496\r\n",
      "Train Epoch: 19 [38400/110534 (35%)]\tClassification Loss: 1.3329\r\n",
      "Train Epoch: 19 [39040/110534 (35%)]\tClassification Loss: 1.4294\r\n",
      "Train Epoch: 19 [39680/110534 (36%)]\tClassification Loss: 1.6049\r\n",
      "Train Epoch: 19 [40320/110534 (36%)]\tClassification Loss: 1.5582\r\n",
      "Train Epoch: 19 [40960/110534 (37%)]\tClassification Loss: 1.5224\r\n",
      "Train Epoch: 19 [41600/110534 (38%)]\tClassification Loss: 1.5282\r\n",
      "Train Epoch: 19 [42240/110534 (38%)]\tClassification Loss: 1.4171\r\n",
      "Train Epoch: 19 [42880/110534 (39%)]\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 19 [43520/110534 (39%)]\tClassification Loss: 1.2766\r\n",
      "Train Epoch: 19 [44160/110534 (40%)]\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 19 [44800/110534 (41%)]\tClassification Loss: 1.5082\r\n",
      "Train Epoch: 19 [45440/110534 (41%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 19 [46080/110534 (42%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 19 [46720/110534 (42%)]\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 19 [47360/110534 (43%)]\tClassification Loss: 1.4005\r\n",
      "Train Epoch: 19 [48000/110534 (43%)]\tClassification Loss: 1.6894\r\n",
      "Train Epoch: 19 [48640/110534 (44%)]\tClassification Loss: 1.3990\r\n",
      "Train Epoch: 19 [49280/110534 (45%)]\tClassification Loss: 1.5591\r\n",
      "Train Epoch: 19 [49920/110534 (45%)]\tClassification Loss: 1.5585\r\n",
      "Train Epoch: 19 [50560/110534 (46%)]\tClassification Loss: 1.5421\r\n",
      "Train Epoch: 19 [51200/110534 (46%)]\tClassification Loss: 1.3513\r\n",
      "Train Epoch: 19 [51840/110534 (47%)]\tClassification Loss: 1.4852\r\n",
      "Train Epoch: 19 [52480/110534 (47%)]\tClassification Loss: 1.5281\r\n",
      "Train Epoch: 19 [53120/110534 (48%)]\tClassification Loss: 1.1580\r\n",
      "Train Epoch: 19 [53760/110534 (49%)]\tClassification Loss: 1.6163\r\n",
      "Train Epoch: 19 [54400/110534 (49%)]\tClassification Loss: 1.3465\r\n",
      "Train Epoch: 19 [55040/110534 (50%)]\tClassification Loss: 1.7502\r\n",
      "Train Epoch: 19 [55680/110534 (50%)]\tClassification Loss: 1.6104\r\n",
      "Train Epoch: 19 [56320/110534 (51%)]\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 19 [56960/110534 (52%)]\tClassification Loss: 1.4470\r\n",
      "Train Epoch: 19 [57600/110534 (52%)]\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 19 [58240/110534 (53%)]\tClassification Loss: 1.5634\r\n",
      "Train Epoch: 19 [58880/110534 (53%)]\tClassification Loss: 1.7379\r\n",
      "Train Epoch: 19 [59520/110534 (54%)]\tClassification Loss: 1.1762\r\n",
      "Train Epoch: 19 [60160/110534 (54%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 19 [60800/110534 (55%)]\tClassification Loss: 1.5360\r\n",
      "Train Epoch: 19 [61440/110534 (56%)]\tClassification Loss: 1.4363\r\n",
      "Train Epoch: 19 [62080/110534 (56%)]\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 19 [62720/110534 (57%)]\tClassification Loss: 1.6030\r\n",
      "Train Epoch: 19 [63360/110534 (57%)]\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 19 [64000/110534 (58%)]\tClassification Loss: 1.4892\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_1000.pth.tar\r\n",
      "Train Epoch: 19 [64640/110534 (58%)]\tClassification Loss: 1.1776\r\n",
      "Train Epoch: 19 [65280/110534 (59%)]\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 19 [65920/110534 (60%)]\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 19 [66560/110534 (60%)]\tClassification Loss: 1.3198\r\n",
      "Train Epoch: 19 [67200/110534 (61%)]\tClassification Loss: 1.3123\r\n",
      "Train Epoch: 19 [67840/110534 (61%)]\tClassification Loss: 1.7771\r\n",
      "Train Epoch: 19 [68480/110534 (62%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 19 [69120/110534 (63%)]\tClassification Loss: 1.8821\r\n",
      "Train Epoch: 19 [69760/110534 (63%)]\tClassification Loss: 1.6791\r\n",
      "Train Epoch: 19 [70400/110534 (64%)]\tClassification Loss: 1.2494\r\n",
      "Train Epoch: 19 [71040/110534 (64%)]\tClassification Loss: 1.8803\r\n",
      "Train Epoch: 19 [71680/110534 (65%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 19 [72320/110534 (65%)]\tClassification Loss: 1.6532\r\n",
      "Train Epoch: 19 [72960/110534 (66%)]\tClassification Loss: 1.6451\r\n",
      "Train Epoch: 19 [73600/110534 (67%)]\tClassification Loss: 1.8824\r\n",
      "Train Epoch: 19 [74240/110534 (67%)]\tClassification Loss: 1.6143\r\n",
      "Train Epoch: 19 [74880/110534 (68%)]\tClassification Loss: 1.1644\r\n",
      "Train Epoch: 19 [75520/110534 (68%)]\tClassification Loss: 1.6014\r\n",
      "Train Epoch: 19 [76160/110534 (69%)]\tClassification Loss: 1.3114\r\n",
      "Train Epoch: 19 [76800/110534 (69%)]\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 19 [77440/110534 (70%)]\tClassification Loss: 1.4698\r\n",
      "Train Epoch: 19 [78080/110534 (71%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 19 [78720/110534 (71%)]\tClassification Loss: 1.4933\r\n",
      "Train Epoch: 19 [79360/110534 (72%)]\tClassification Loss: 1.4911\r\n",
      "Train Epoch: 19 [80000/110534 (72%)]\tClassification Loss: 1.4744\r\n",
      "Train Epoch: 19 [80640/110534 (73%)]\tClassification Loss: 1.3974\r\n",
      "Train Epoch: 19 [81280/110534 (74%)]\tClassification Loss: 1.8253\r\n",
      "Train Epoch: 19 [81920/110534 (74%)]\tClassification Loss: 1.5784\r\n",
      "Train Epoch: 19 [82560/110534 (75%)]\tClassification Loss: 1.7896\r\n",
      "Train Epoch: 19 [83200/110534 (75%)]\tClassification Loss: 1.3671\r\n",
      "Train Epoch: 19 [83840/110534 (76%)]\tClassification Loss: 1.7817\r\n",
      "Train Epoch: 19 [84480/110534 (76%)]\tClassification Loss: 1.6435\r\n",
      "Train Epoch: 19 [85120/110534 (77%)]\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 19 [85760/110534 (78%)]\tClassification Loss: 1.4466\r\n",
      "Train Epoch: 19 [86400/110534 (78%)]\tClassification Loss: 1.6113\r\n",
      "Train Epoch: 19 [87040/110534 (79%)]\tClassification Loss: 1.5022\r\n",
      "Train Epoch: 19 [87680/110534 (79%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 19 [88320/110534 (80%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 19 [88960/110534 (80%)]\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 19 [89600/110534 (81%)]\tClassification Loss: 1.6393\r\n",
      "Train Epoch: 19 [90240/110534 (82%)]\tClassification Loss: 1.7989\r\n",
      "Train Epoch: 19 [90880/110534 (82%)]\tClassification Loss: 1.6752\r\n",
      "Train Epoch: 19 [91520/110534 (83%)]\tClassification Loss: 1.1759\r\n",
      "Train Epoch: 19 [92160/110534 (83%)]\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 19 [92800/110534 (84%)]\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 19 [93440/110534 (85%)]\tClassification Loss: 1.8677\r\n",
      "Train Epoch: 19 [94080/110534 (85%)]\tClassification Loss: 1.5799\r\n",
      "Train Epoch: 19 [94720/110534 (86%)]\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 19 [95360/110534 (86%)]\tClassification Loss: 1.4476\r\n",
      "Train Epoch: 19 [96000/110534 (87%)]\tClassification Loss: 1.5003\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_1500.pth.tar\r\n",
      "Train Epoch: 19 [96640/110534 (87%)]\tClassification Loss: 1.3610\r\n",
      "Train Epoch: 19 [97280/110534 (88%)]\tClassification Loss: 1.2112\r\n",
      "Train Epoch: 19 [97920/110534 (89%)]\tClassification Loss: 1.1747\r\n",
      "Train Epoch: 19 [98560/110534 (89%)]\tClassification Loss: 1.4498\r\n",
      "Train Epoch: 19 [99200/110534 (90%)]\tClassification Loss: 1.8460\r\n",
      "Train Epoch: 19 [99840/110534 (90%)]\tClassification Loss: 1.5138\r\n",
      "Train Epoch: 19 [100480/110534 (91%)]\tClassification Loss: 1.7110\r\n",
      "Train Epoch: 19 [101120/110534 (91%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 19 [101760/110534 (92%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 19 [102400/110534 (93%)]\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 19 [103040/110534 (93%)]\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 19 [103680/110534 (94%)]\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 19 [104320/110534 (94%)]\tClassification Loss: 1.3565\r\n",
      "Train Epoch: 19 [104960/110534 (95%)]\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 19 [105600/110534 (96%)]\tClassification Loss: 1.5960\r\n",
      "Train Epoch: 19 [106240/110534 (96%)]\tClassification Loss: 1.1406\r\n",
      "Train Epoch: 19 [106880/110534 (97%)]\tClassification Loss: 1.6861\r\n",
      "Train Epoch: 19 [107520/110534 (97%)]\tClassification Loss: 1.5594\r\n",
      "Train Epoch: 19 [108160/110534 (98%)]\tClassification Loss: 1.5690\r\n",
      "Train Epoch: 19 [108800/110534 (98%)]\tClassification Loss: 1.5959\r\n",
      "Train Epoch: 19 [109440/110534 (99%)]\tClassification Loss: 1.4915\r\n",
      "Train Epoch: 19 [110080/110534 (100%)]\tClassification Loss: 1.5930\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_final.pth.tar\r\n",
      "Train Epoch: 20 [0/110534 (0%)]\tClassification Loss: 1.4882\r\n",
      "\r\n",
      "Test set: Average loss: 1.4220, Accuracy: 23043/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 20 [640/110534 (1%)]\tClassification Loss: 1.3792\r\n",
      "Train Epoch: 20 [1280/110534 (1%)]\tClassification Loss: 1.8748\r\n",
      "Train Epoch: 20 [1920/110534 (2%)]\tClassification Loss: 1.6038\r\n",
      "Train Epoch: 20 [2560/110534 (2%)]\tClassification Loss: 1.7997\r\n",
      "Train Epoch: 20 [3200/110534 (3%)]\tClassification Loss: 1.4316\r\n",
      "Train Epoch: 20 [3840/110534 (3%)]\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 20 [4480/110534 (4%)]\tClassification Loss: 1.3902\r\n",
      "Train Epoch: 20 [5120/110534 (5%)]\tClassification Loss: 1.5872\r\n",
      "Train Epoch: 20 [5760/110534 (5%)]\tClassification Loss: 1.5211\r\n",
      "Train Epoch: 20 [6400/110534 (6%)]\tClassification Loss: 1.3569\r\n",
      "Train Epoch: 20 [7040/110534 (6%)]\tClassification Loss: 1.4462\r\n",
      "Train Epoch: 20 [7680/110534 (7%)]\tClassification Loss: 1.4750\r\n",
      "Train Epoch: 20 [8320/110534 (8%)]\tClassification Loss: 1.7105\r\n",
      "Train Epoch: 20 [8960/110534 (8%)]\tClassification Loss: 1.8518\r\n",
      "Train Epoch: 20 [9600/110534 (9%)]\tClassification Loss: 1.4348\r\n",
      "Train Epoch: 20 [10240/110534 (9%)]\tClassification Loss: 1.4595\r\n",
      "Train Epoch: 20 [10880/110534 (10%)]\tClassification Loss: 1.5867\r\n",
      "Train Epoch: 20 [11520/110534 (10%)]\tClassification Loss: 1.6560\r\n",
      "Train Epoch: 20 [12160/110534 (11%)]\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 20 [12800/110534 (12%)]\tClassification Loss: 1.5962\r\n",
      "Train Epoch: 20 [13440/110534 (12%)]\tClassification Loss: 1.4357\r\n",
      "Train Epoch: 20 [14080/110534 (13%)]\tClassification Loss: 1.6029\r\n",
      "Train Epoch: 20 [14720/110534 (13%)]\tClassification Loss: 1.7707\r\n",
      "Train Epoch: 20 [15360/110534 (14%)]\tClassification Loss: 1.4739\r\n",
      "Train Epoch: 20 [16000/110534 (14%)]\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 20 [16640/110534 (15%)]\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 20 [17280/110534 (16%)]\tClassification Loss: 1.6112\r\n",
      "Train Epoch: 20 [17920/110534 (16%)]\tClassification Loss: 1.4815\r\n",
      "Train Epoch: 20 [18560/110534 (17%)]\tClassification Loss: 1.7265\r\n",
      "Train Epoch: 20 [19200/110534 (17%)]\tClassification Loss: 1.6803\r\n",
      "Train Epoch: 20 [19840/110534 (18%)]\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 20 [20480/110534 (19%)]\tClassification Loss: 1.7332\r\n",
      "Train Epoch: 20 [21120/110534 (19%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 20 [21760/110534 (20%)]\tClassification Loss: 1.2576\r\n",
      "Train Epoch: 20 [22400/110534 (20%)]\tClassification Loss: 1.5302\r\n",
      "Train Epoch: 20 [23040/110534 (21%)]\tClassification Loss: 1.1846\r\n",
      "Train Epoch: 20 [23680/110534 (21%)]\tClassification Loss: 1.6475\r\n",
      "Train Epoch: 20 [24320/110534 (22%)]\tClassification Loss: 1.2902\r\n",
      "Train Epoch: 20 [24960/110534 (23%)]\tClassification Loss: 1.7996\r\n",
      "Train Epoch: 20 [25600/110534 (23%)]\tClassification Loss: 1.4789\r\n",
      "Train Epoch: 20 [26240/110534 (24%)]\tClassification Loss: 1.4160\r\n",
      "Train Epoch: 20 [26880/110534 (24%)]\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 20 [27520/110534 (25%)]\tClassification Loss: 1.4576\r\n",
      "Train Epoch: 20 [28160/110534 (25%)]\tClassification Loss: 1.8350\r\n",
      "Train Epoch: 20 [28800/110534 (26%)]\tClassification Loss: 1.8053\r\n",
      "Train Epoch: 20 [29440/110534 (27%)]\tClassification Loss: 1.6579\r\n",
      "Train Epoch: 20 [30080/110534 (27%)]\tClassification Loss: 1.7073\r\n",
      "Train Epoch: 20 [30720/110534 (28%)]\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 20 [31360/110534 (28%)]\tClassification Loss: 1.7150\r\n",
      "Train Epoch: 20 [32000/110534 (29%)]\tClassification Loss: 1.4508\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_500.pth.tar\r\n",
      "Train Epoch: 20 [32640/110534 (30%)]\tClassification Loss: 1.3684\r\n",
      "Train Epoch: 20 [33280/110534 (30%)]\tClassification Loss: 1.5541\r\n",
      "Train Epoch: 20 [33920/110534 (31%)]\tClassification Loss: 1.6146\r\n",
      "Train Epoch: 20 [34560/110534 (31%)]\tClassification Loss: 1.8061\r\n",
      "Train Epoch: 20 [35200/110534 (32%)]\tClassification Loss: 1.5505\r\n",
      "Train Epoch: 20 [35840/110534 (32%)]\tClassification Loss: 1.2525\r\n",
      "Train Epoch: 20 [36480/110534 (33%)]\tClassification Loss: 1.5199\r\n",
      "Train Epoch: 20 [37120/110534 (34%)]\tClassification Loss: 1.5504\r\n",
      "Train Epoch: 20 [37760/110534 (34%)]\tClassification Loss: 1.8624\r\n",
      "Train Epoch: 20 [38400/110534 (35%)]\tClassification Loss: 1.4360\r\n",
      "Train Epoch: 20 [39040/110534 (35%)]\tClassification Loss: 1.4023\r\n",
      "Train Epoch: 20 [39680/110534 (36%)]\tClassification Loss: 1.5633\r\n",
      "Train Epoch: 20 [40320/110534 (36%)]\tClassification Loss: 1.4791\r\n",
      "Train Epoch: 20 [40960/110534 (37%)]\tClassification Loss: 1.3233\r\n",
      "Train Epoch: 20 [41600/110534 (38%)]\tClassification Loss: 1.2986\r\n",
      "Train Epoch: 20 [42240/110534 (38%)]\tClassification Loss: 1.4880\r\n",
      "Train Epoch: 20 [42880/110534 (39%)]\tClassification Loss: 1.4287\r\n",
      "Train Epoch: 20 [43520/110534 (39%)]\tClassification Loss: 1.4413\r\n",
      "Train Epoch: 20 [44160/110534 (40%)]\tClassification Loss: 1.5374\r\n",
      "Train Epoch: 20 [44800/110534 (41%)]\tClassification Loss: 1.6881\r\n",
      "Train Epoch: 20 [45440/110534 (41%)]\tClassification Loss: 1.5883\r\n",
      "Train Epoch: 20 [46080/110534 (42%)]\tClassification Loss: 1.3561\r\n",
      "Train Epoch: 20 [46720/110534 (42%)]\tClassification Loss: 1.5496\r\n",
      "Train Epoch: 20 [47360/110534 (43%)]\tClassification Loss: 1.5099\r\n",
      "Train Epoch: 20 [48000/110534 (43%)]\tClassification Loss: 1.6164\r\n",
      "Train Epoch: 20 [48640/110534 (44%)]\tClassification Loss: 1.4247\r\n",
      "Train Epoch: 20 [49280/110534 (45%)]\tClassification Loss: 1.5172\r\n",
      "Train Epoch: 20 [49920/110534 (45%)]\tClassification Loss: 1.5366\r\n",
      "Train Epoch: 20 [50560/110534 (46%)]\tClassification Loss: 1.5974\r\n",
      "Train Epoch: 20 [51200/110534 (46%)]\tClassification Loss: 1.3393\r\n",
      "Train Epoch: 20 [51840/110534 (47%)]\tClassification Loss: 1.4225\r\n",
      "Train Epoch: 20 [52480/110534 (47%)]\tClassification Loss: 1.5088\r\n",
      "Train Epoch: 20 [53120/110534 (48%)]\tClassification Loss: 1.2178\r\n",
      "Train Epoch: 20 [53760/110534 (49%)]\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 20 [54400/110534 (49%)]\tClassification Loss: 1.5126\r\n",
      "Train Epoch: 20 [55040/110534 (50%)]\tClassification Loss: 1.6919\r\n",
      "Train Epoch: 20 [55680/110534 (50%)]\tClassification Loss: 1.6484\r\n",
      "Train Epoch: 20 [56320/110534 (51%)]\tClassification Loss: 1.5034\r\n",
      "Train Epoch: 20 [56960/110534 (52%)]\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 20 [57600/110534 (52%)]\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 20 [58240/110534 (53%)]\tClassification Loss: 1.5538\r\n",
      "Train Epoch: 20 [58880/110534 (53%)]\tClassification Loss: 1.6754\r\n",
      "Train Epoch: 20 [59520/110534 (54%)]\tClassification Loss: 1.3221\r\n",
      "Train Epoch: 20 [60160/110534 (54%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 20 [60800/110534 (55%)]\tClassification Loss: 1.6327\r\n",
      "Train Epoch: 20 [61440/110534 (56%)]\tClassification Loss: 1.7077\r\n",
      "Train Epoch: 20 [62080/110534 (56%)]\tClassification Loss: 1.4650\r\n",
      "Train Epoch: 20 [62720/110534 (57%)]\tClassification Loss: 1.6723\r\n",
      "Train Epoch: 20 [63360/110534 (57%)]\tClassification Loss: 1.3415\r\n",
      "Train Epoch: 20 [64000/110534 (58%)]\tClassification Loss: 1.6607\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_1000.pth.tar\r\n",
      "Train Epoch: 20 [64640/110534 (58%)]\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 20 [65280/110534 (59%)]\tClassification Loss: 1.4396\r\n",
      "Train Epoch: 20 [65920/110534 (60%)]\tClassification Loss: 1.3771\r\n",
      "Train Epoch: 20 [66560/110534 (60%)]\tClassification Loss: 1.3819\r\n",
      "Train Epoch: 20 [67200/110534 (61%)]\tClassification Loss: 1.3898\r\n",
      "Train Epoch: 20 [67840/110534 (61%)]\tClassification Loss: 1.6678\r\n",
      "Train Epoch: 20 [68480/110534 (62%)]\tClassification Loss: 1.6359\r\n",
      "Train Epoch: 20 [69120/110534 (63%)]\tClassification Loss: 1.7368\r\n",
      "Train Epoch: 20 [69760/110534 (63%)]\tClassification Loss: 1.6647\r\n",
      "Train Epoch: 20 [70400/110534 (64%)]\tClassification Loss: 1.2303\r\n",
      "Train Epoch: 20 [71040/110534 (64%)]\tClassification Loss: 1.9954\r\n",
      "Train Epoch: 20 [71680/110534 (65%)]\tClassification Loss: 1.4836\r\n",
      "Train Epoch: 20 [72320/110534 (65%)]\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 20 [72960/110534 (66%)]\tClassification Loss: 1.8336\r\n",
      "Train Epoch: 20 [73600/110534 (67%)]\tClassification Loss: 1.7831\r\n",
      "Train Epoch: 20 [74240/110534 (67%)]\tClassification Loss: 1.7162\r\n",
      "Train Epoch: 20 [74880/110534 (68%)]\tClassification Loss: 1.1842\r\n",
      "Train Epoch: 20 [75520/110534 (68%)]\tClassification Loss: 1.5941\r\n",
      "Train Epoch: 20 [76160/110534 (69%)]\tClassification Loss: 1.4056\r\n",
      "Train Epoch: 20 [76800/110534 (69%)]\tClassification Loss: 1.4052\r\n",
      "Train Epoch: 20 [77440/110534 (70%)]\tClassification Loss: 1.4617\r\n",
      "Train Epoch: 20 [78080/110534 (71%)]\tClassification Loss: 1.4839\r\n",
      "Train Epoch: 20 [78720/110534 (71%)]\tClassification Loss: 1.5610\r\n",
      "Train Epoch: 20 [79360/110534 (72%)]\tClassification Loss: 1.4452\r\n",
      "Train Epoch: 20 [80000/110534 (72%)]\tClassification Loss: 1.3108\r\n",
      "Train Epoch: 20 [80640/110534 (73%)]\tClassification Loss: 1.4115\r\n",
      "Train Epoch: 20 [81280/110534 (74%)]\tClassification Loss: 1.7669\r\n",
      "Train Epoch: 20 [81920/110534 (74%)]\tClassification Loss: 1.4037\r\n",
      "Train Epoch: 20 [82560/110534 (75%)]\tClassification Loss: 1.6127\r\n",
      "Train Epoch: 20 [83200/110534 (75%)]\tClassification Loss: 1.5053\r\n",
      "Train Epoch: 20 [83840/110534 (76%)]\tClassification Loss: 1.8781\r\n",
      "Train Epoch: 20 [84480/110534 (76%)]\tClassification Loss: 1.8374\r\n",
      "Train Epoch: 20 [85120/110534 (77%)]\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 20 [85760/110534 (78%)]\tClassification Loss: 1.4378\r\n",
      "Train Epoch: 20 [86400/110534 (78%)]\tClassification Loss: 1.6662\r\n",
      "Train Epoch: 20 [87040/110534 (79%)]\tClassification Loss: 1.4936\r\n",
      "Train Epoch: 20 [87680/110534 (79%)]\tClassification Loss: 1.3239\r\n",
      "Train Epoch: 20 [88320/110534 (80%)]\tClassification Loss: 1.6452\r\n",
      "Train Epoch: 20 [88960/110534 (80%)]\tClassification Loss: 1.6563\r\n",
      "Train Epoch: 20 [89600/110534 (81%)]\tClassification Loss: 1.8731\r\n",
      "Train Epoch: 20 [90240/110534 (82%)]\tClassification Loss: 1.7428\r\n",
      "Train Epoch: 20 [90880/110534 (82%)]\tClassification Loss: 1.6617\r\n",
      "Train Epoch: 20 [91520/110534 (83%)]\tClassification Loss: 1.3081\r\n",
      "Train Epoch: 20 [92160/110534 (83%)]\tClassification Loss: 1.5089\r\n",
      "Train Epoch: 20 [92800/110534 (84%)]\tClassification Loss: 1.2848\r\n",
      "Train Epoch: 20 [93440/110534 (85%)]\tClassification Loss: 1.7783\r\n",
      "Train Epoch: 20 [94080/110534 (85%)]\tClassification Loss: 1.6538\r\n",
      "Train Epoch: 20 [94720/110534 (86%)]\tClassification Loss: 1.3806\r\n",
      "Train Epoch: 20 [95360/110534 (86%)]\tClassification Loss: 1.3801\r\n",
      "Train Epoch: 20 [96000/110534 (87%)]\tClassification Loss: 1.5613\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_1500.pth.tar\r\n",
      "Train Epoch: 20 [96640/110534 (87%)]\tClassification Loss: 1.3095\r\n",
      "Train Epoch: 20 [97280/110534 (88%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 20 [97920/110534 (89%)]\tClassification Loss: 1.2173\r\n",
      "Train Epoch: 20 [98560/110534 (89%)]\tClassification Loss: 1.4249\r\n",
      "Train Epoch: 20 [99200/110534 (90%)]\tClassification Loss: 1.7073\r\n",
      "Train Epoch: 20 [99840/110534 (90%)]\tClassification Loss: 1.4456\r\n",
      "Train Epoch: 20 [100480/110534 (91%)]\tClassification Loss: 1.6827\r\n",
      "Train Epoch: 20 [101120/110534 (91%)]\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 20 [101760/110534 (92%)]\tClassification Loss: 1.4792\r\n",
      "Train Epoch: 20 [102400/110534 (93%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 20 [103040/110534 (93%)]\tClassification Loss: 1.2962\r\n",
      "Train Epoch: 20 [103680/110534 (94%)]\tClassification Loss: 1.7821\r\n",
      "Train Epoch: 20 [104320/110534 (94%)]\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 20 [104960/110534 (95%)]\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 20 [105600/110534 (96%)]\tClassification Loss: 1.5246\r\n",
      "Train Epoch: 20 [106240/110534 (96%)]\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 20 [106880/110534 (97%)]\tClassification Loss: 1.5416\r\n",
      "Train Epoch: 20 [107520/110534 (97%)]\tClassification Loss: 1.8962\r\n",
      "Train Epoch: 20 [108160/110534 (98%)]\tClassification Loss: 1.5534\r\n",
      "Train Epoch: 20 [108800/110534 (98%)]\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 20 [109440/110534 (99%)]\tClassification Loss: 1.3343\r\n",
      "Train Epoch: 20 [110080/110534 (100%)]\tClassification Loss: 1.4341\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_final.pth.tar\r\n",
      "Train Epoch: 21 [0/110534 (0%)]\tClassification Loss: 1.3823\r\n",
      "\r\n",
      "Test set: Average loss: 1.4271, Accuracy: 22986/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 21 [640/110534 (1%)]\tClassification Loss: 1.2976\r\n",
      "Train Epoch: 21 [1280/110534 (1%)]\tClassification Loss: 1.9553\r\n",
      "Train Epoch: 21 [1920/110534 (2%)]\tClassification Loss: 1.6809\r\n",
      "Train Epoch: 21 [2560/110534 (2%)]\tClassification Loss: 1.6802\r\n",
      "Train Epoch: 21 [3200/110534 (3%)]\tClassification Loss: 1.4357\r\n",
      "Train Epoch: 21 [3840/110534 (3%)]\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 21 [4480/110534 (4%)]\tClassification Loss: 1.5372\r\n",
      "Train Epoch: 21 [5120/110534 (5%)]\tClassification Loss: 1.5846\r\n",
      "Train Epoch: 21 [5760/110534 (5%)]\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 21 [6400/110534 (6%)]\tClassification Loss: 1.3663\r\n",
      "Train Epoch: 21 [7040/110534 (6%)]\tClassification Loss: 1.6529\r\n",
      "Train Epoch: 21 [7680/110534 (7%)]\tClassification Loss: 1.5290\r\n",
      "Train Epoch: 21 [8320/110534 (8%)]\tClassification Loss: 1.7104\r\n",
      "Train Epoch: 21 [8960/110534 (8%)]\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 21 [9600/110534 (9%)]\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 21 [10240/110534 (9%)]\tClassification Loss: 1.4544\r\n",
      "Train Epoch: 21 [10880/110534 (10%)]\tClassification Loss: 1.4554\r\n",
      "Train Epoch: 21 [11520/110534 (10%)]\tClassification Loss: 1.5539\r\n",
      "Train Epoch: 21 [12160/110534 (11%)]\tClassification Loss: 1.3889\r\n",
      "Train Epoch: 21 [12800/110534 (12%)]\tClassification Loss: 1.5593\r\n",
      "Train Epoch: 21 [13440/110534 (12%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 21 [14080/110534 (13%)]\tClassification Loss: 1.4781\r\n",
      "Train Epoch: 21 [14720/110534 (13%)]\tClassification Loss: 1.8002\r\n",
      "Train Epoch: 21 [15360/110534 (14%)]\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 21 [16000/110534 (14%)]\tClassification Loss: 1.4820\r\n",
      "Train Epoch: 21 [16640/110534 (15%)]\tClassification Loss: 1.4869\r\n",
      "Train Epoch: 21 [17280/110534 (16%)]\tClassification Loss: 1.6353\r\n",
      "Train Epoch: 21 [17920/110534 (16%)]\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 21 [18560/110534 (17%)]\tClassification Loss: 1.7637\r\n",
      "Train Epoch: 21 [19200/110534 (17%)]\tClassification Loss: 1.5839\r\n",
      "Train Epoch: 21 [19840/110534 (18%)]\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 21 [20480/110534 (19%)]\tClassification Loss: 1.8294\r\n",
      "Train Epoch: 21 [21120/110534 (19%)]\tClassification Loss: 1.8082\r\n",
      "Train Epoch: 21 [21760/110534 (20%)]\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 21 [22400/110534 (20%)]\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 21 [23040/110534 (21%)]\tClassification Loss: 1.0813\r\n",
      "Train Epoch: 21 [23680/110534 (21%)]\tClassification Loss: 1.7259\r\n",
      "Train Epoch: 21 [24320/110534 (22%)]\tClassification Loss: 1.1275\r\n",
      "Train Epoch: 21 [24960/110534 (23%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 21 [25600/110534 (23%)]\tClassification Loss: 1.1944\r\n",
      "Train Epoch: 21 [26240/110534 (24%)]\tClassification Loss: 1.4201\r\n",
      "Train Epoch: 21 [26880/110534 (24%)]\tClassification Loss: 1.6390\r\n",
      "Train Epoch: 21 [27520/110534 (25%)]\tClassification Loss: 1.4380\r\n",
      "Train Epoch: 21 [28160/110534 (25%)]\tClassification Loss: 1.8048\r\n",
      "Train Epoch: 21 [28800/110534 (26%)]\tClassification Loss: 1.8135\r\n",
      "Train Epoch: 21 [29440/110534 (27%)]\tClassification Loss: 1.8601\r\n",
      "Train Epoch: 21 [30080/110534 (27%)]\tClassification Loss: 1.6715\r\n",
      "Train Epoch: 21 [30720/110534 (28%)]\tClassification Loss: 1.4438\r\n",
      "Train Epoch: 21 [31360/110534 (28%)]\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 21 [32000/110534 (29%)]\tClassification Loss: 1.4872\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_500.pth.tar\r\n",
      "Train Epoch: 21 [32640/110534 (30%)]\tClassification Loss: 1.3314\r\n",
      "Train Epoch: 21 [33280/110534 (30%)]\tClassification Loss: 1.3937\r\n",
      "Train Epoch: 21 [33920/110534 (31%)]\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 21 [34560/110534 (31%)]\tClassification Loss: 1.7724\r\n",
      "Train Epoch: 21 [35200/110534 (32%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 21 [35840/110534 (32%)]\tClassification Loss: 1.1395\r\n",
      "Train Epoch: 21 [36480/110534 (33%)]\tClassification Loss: 1.3984\r\n",
      "Train Epoch: 21 [37120/110534 (34%)]\tClassification Loss: 1.6192\r\n",
      "Train Epoch: 21 [37760/110534 (34%)]\tClassification Loss: 1.7801\r\n",
      "Train Epoch: 21 [38400/110534 (35%)]\tClassification Loss: 1.4618\r\n",
      "Train Epoch: 21 [39040/110534 (35%)]\tClassification Loss: 1.4200\r\n",
      "Train Epoch: 21 [39680/110534 (36%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 21 [40320/110534 (36%)]\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 21 [40960/110534 (37%)]\tClassification Loss: 1.2934\r\n",
      "Train Epoch: 21 [41600/110534 (38%)]\tClassification Loss: 1.3631\r\n",
      "Train Epoch: 21 [42240/110534 (38%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 21 [42880/110534 (39%)]\tClassification Loss: 1.4400\r\n",
      "Train Epoch: 21 [43520/110534 (39%)]\tClassification Loss: 1.2746\r\n",
      "Train Epoch: 21 [44160/110534 (40%)]\tClassification Loss: 1.4952\r\n",
      "Train Epoch: 21 [44800/110534 (41%)]\tClassification Loss: 1.5471\r\n",
      "Train Epoch: 21 [45440/110534 (41%)]\tClassification Loss: 1.6065\r\n",
      "Train Epoch: 21 [46080/110534 (42%)]\tClassification Loss: 1.4844\r\n",
      "Train Epoch: 21 [46720/110534 (42%)]\tClassification Loss: 1.4563\r\n",
      "Train Epoch: 21 [47360/110534 (43%)]\tClassification Loss: 1.4103\r\n",
      "Train Epoch: 21 [48000/110534 (43%)]\tClassification Loss: 1.6339\r\n",
      "Train Epoch: 21 [48640/110534 (44%)]\tClassification Loss: 1.3457\r\n",
      "Train Epoch: 21 [49280/110534 (45%)]\tClassification Loss: 1.5061\r\n",
      "Train Epoch: 21 [49920/110534 (45%)]\tClassification Loss: 1.7691\r\n",
      "Train Epoch: 21 [50560/110534 (46%)]\tClassification Loss: 1.5572\r\n",
      "Train Epoch: 21 [51200/110534 (46%)]\tClassification Loss: 1.4352\r\n",
      "Train Epoch: 21 [51840/110534 (47%)]\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 21 [52480/110534 (47%)]\tClassification Loss: 1.5673\r\n",
      "Train Epoch: 21 [53120/110534 (48%)]\tClassification Loss: 1.2074\r\n",
      "Train Epoch: 21 [53760/110534 (49%)]\tClassification Loss: 1.6488\r\n",
      "Train Epoch: 21 [54400/110534 (49%)]\tClassification Loss: 1.3725\r\n",
      "Train Epoch: 21 [55040/110534 (50%)]\tClassification Loss: 1.7783\r\n",
      "Train Epoch: 21 [55680/110534 (50%)]\tClassification Loss: 1.7123\r\n",
      "Train Epoch: 21 [56320/110534 (51%)]\tClassification Loss: 1.4780\r\n",
      "Train Epoch: 21 [56960/110534 (52%)]\tClassification Loss: 1.6144\r\n",
      "Train Epoch: 21 [57600/110534 (52%)]\tClassification Loss: 1.6463\r\n",
      "Train Epoch: 21 [58240/110534 (53%)]\tClassification Loss: 1.7483\r\n",
      "Train Epoch: 21 [58880/110534 (53%)]\tClassification Loss: 1.6713\r\n",
      "Train Epoch: 21 [59520/110534 (54%)]\tClassification Loss: 1.3958\r\n",
      "Train Epoch: 21 [60160/110534 (54%)]\tClassification Loss: 1.3405\r\n",
      "Train Epoch: 21 [60800/110534 (55%)]\tClassification Loss: 1.4565\r\n",
      "Train Epoch: 21 [61440/110534 (56%)]\tClassification Loss: 1.5213\r\n",
      "Train Epoch: 21 [62080/110534 (56%)]\tClassification Loss: 1.5226\r\n",
      "Train Epoch: 21 [62720/110534 (57%)]\tClassification Loss: 1.6706\r\n",
      "Train Epoch: 21 [63360/110534 (57%)]\tClassification Loss: 1.2849\r\n",
      "Train Epoch: 21 [64000/110534 (58%)]\tClassification Loss: 1.4697\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_1000.pth.tar\r\n",
      "Train Epoch: 21 [64640/110534 (58%)]\tClassification Loss: 1.2625\r\n",
      "Train Epoch: 21 [65280/110534 (59%)]\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 21 [65920/110534 (60%)]\tClassification Loss: 1.4486\r\n",
      "Train Epoch: 21 [66560/110534 (60%)]\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 21 [67200/110534 (61%)]\tClassification Loss: 1.2159\r\n",
      "Train Epoch: 21 [67840/110534 (61%)]\tClassification Loss: 1.8606\r\n",
      "Train Epoch: 21 [68480/110534 (62%)]\tClassification Loss: 1.7556\r\n",
      "Train Epoch: 21 [69120/110534 (63%)]\tClassification Loss: 1.7758\r\n",
      "Train Epoch: 21 [69760/110534 (63%)]\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 21 [70400/110534 (64%)]\tClassification Loss: 1.1847\r\n",
      "Train Epoch: 21 [71040/110534 (64%)]\tClassification Loss: 1.7689\r\n",
      "Train Epoch: 21 [71680/110534 (65%)]\tClassification Loss: 1.4588\r\n",
      "Train Epoch: 21 [72320/110534 (65%)]\tClassification Loss: 1.6517\r\n",
      "Train Epoch: 21 [72960/110534 (66%)]\tClassification Loss: 1.8494\r\n",
      "Train Epoch: 21 [73600/110534 (67%)]\tClassification Loss: 1.7295\r\n",
      "Train Epoch: 21 [74240/110534 (67%)]\tClassification Loss: 1.7127\r\n",
      "Train Epoch: 21 [74880/110534 (68%)]\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 21 [75520/110534 (68%)]\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 21 [76160/110534 (69%)]\tClassification Loss: 1.2915\r\n",
      "Train Epoch: 21 [76800/110534 (69%)]\tClassification Loss: 1.4461\r\n",
      "Train Epoch: 21 [77440/110534 (70%)]\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 21 [78080/110534 (71%)]\tClassification Loss: 1.4231\r\n",
      "Train Epoch: 21 [78720/110534 (71%)]\tClassification Loss: 1.3786\r\n",
      "Train Epoch: 21 [79360/110534 (72%)]\tClassification Loss: 1.4048\r\n",
      "Train Epoch: 21 [80000/110534 (72%)]\tClassification Loss: 1.4130\r\n",
      "Train Epoch: 21 [80640/110534 (73%)]\tClassification Loss: 1.4036\r\n",
      "Train Epoch: 21 [81280/110534 (74%)]\tClassification Loss: 1.9153\r\n",
      "Train Epoch: 21 [81920/110534 (74%)]\tClassification Loss: 1.5048\r\n",
      "Train Epoch: 21 [82560/110534 (75%)]\tClassification Loss: 1.6445\r\n",
      "Train Epoch: 21 [83200/110534 (75%)]\tClassification Loss: 1.6130\r\n",
      "Train Epoch: 21 [83840/110534 (76%)]\tClassification Loss: 1.8470\r\n",
      "Train Epoch: 21 [84480/110534 (76%)]\tClassification Loss: 1.8147\r\n",
      "Train Epoch: 21 [85120/110534 (77%)]\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 21 [85760/110534 (78%)]\tClassification Loss: 1.5553\r\n",
      "Train Epoch: 21 [86400/110534 (78%)]\tClassification Loss: 1.7760\r\n",
      "Train Epoch: 21 [87040/110534 (79%)]\tClassification Loss: 1.5777\r\n",
      "Train Epoch: 21 [87680/110534 (79%)]\tClassification Loss: 1.3771\r\n",
      "Train Epoch: 21 [88320/110534 (80%)]\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 21 [88960/110534 (80%)]\tClassification Loss: 1.7783\r\n",
      "Train Epoch: 21 [89600/110534 (81%)]\tClassification Loss: 1.6472\r\n",
      "Train Epoch: 21 [90240/110534 (82%)]\tClassification Loss: 1.7781\r\n",
      "Train Epoch: 21 [90880/110534 (82%)]\tClassification Loss: 1.6244\r\n",
      "Train Epoch: 21 [91520/110534 (83%)]\tClassification Loss: 1.2760\r\n",
      "Train Epoch: 21 [92160/110534 (83%)]\tClassification Loss: 1.3498\r\n",
      "Train Epoch: 21 [92800/110534 (84%)]\tClassification Loss: 1.4299\r\n",
      "Train Epoch: 21 [93440/110534 (85%)]\tClassification Loss: 1.6201\r\n",
      "Train Epoch: 21 [94080/110534 (85%)]\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 21 [94720/110534 (86%)]\tClassification Loss: 1.3788\r\n",
      "Train Epoch: 21 [95360/110534 (86%)]\tClassification Loss: 1.3998\r\n",
      "Train Epoch: 21 [96000/110534 (87%)]\tClassification Loss: 1.4588\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_1500.pth.tar\r\n",
      "Train Epoch: 21 [96640/110534 (87%)]\tClassification Loss: 1.2580\r\n",
      "Train Epoch: 21 [97280/110534 (88%)]\tClassification Loss: 1.3256\r\n",
      "Train Epoch: 21 [97920/110534 (89%)]\tClassification Loss: 1.2110\r\n",
      "Train Epoch: 21 [98560/110534 (89%)]\tClassification Loss: 1.5622\r\n",
      "Train Epoch: 21 [99200/110534 (90%)]\tClassification Loss: 1.5747\r\n",
      "Train Epoch: 21 [99840/110534 (90%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 21 [100480/110534 (91%)]\tClassification Loss: 1.6653\r\n",
      "Train Epoch: 21 [101120/110534 (91%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 21 [101760/110534 (92%)]\tClassification Loss: 1.3935\r\n",
      "Train Epoch: 21 [102400/110534 (93%)]\tClassification Loss: 1.3243\r\n",
      "Train Epoch: 21 [103040/110534 (93%)]\tClassification Loss: 1.4810\r\n",
      "Train Epoch: 21 [103680/110534 (94%)]\tClassification Loss: 1.7063\r\n",
      "Train Epoch: 21 [104320/110534 (94%)]\tClassification Loss: 1.5306\r\n",
      "Train Epoch: 21 [104960/110534 (95%)]\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 21 [105600/110534 (96%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 21 [106240/110534 (96%)]\tClassification Loss: 1.4126\r\n",
      "Train Epoch: 21 [106880/110534 (97%)]\tClassification Loss: 1.7579\r\n",
      "Train Epoch: 21 [107520/110534 (97%)]\tClassification Loss: 1.6843\r\n",
      "Train Epoch: 21 [108160/110534 (98%)]\tClassification Loss: 1.5728\r\n",
      "Train Epoch: 21 [108800/110534 (98%)]\tClassification Loss: 1.5970\r\n",
      "Train Epoch: 21 [109440/110534 (99%)]\tClassification Loss: 1.4570\r\n",
      "Train Epoch: 21 [110080/110534 (100%)]\tClassification Loss: 1.4620\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_final.pth.tar\r\n",
      "Train Epoch: 22 [0/110534 (0%)]\tClassification Loss: 1.4710\r\n",
      "\r\n",
      "Test set: Average loss: 1.4299, Accuracy: 22871/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 22 [640/110534 (1%)]\tClassification Loss: 1.1688\r\n",
      "Train Epoch: 22 [1280/110534 (1%)]\tClassification Loss: 1.8990\r\n",
      "Train Epoch: 22 [1920/110534 (2%)]\tClassification Loss: 1.6981\r\n",
      "Train Epoch: 22 [2560/110534 (2%)]\tClassification Loss: 1.7629\r\n",
      "Train Epoch: 22 [3200/110534 (3%)]\tClassification Loss: 1.3989\r\n",
      "Train Epoch: 22 [3840/110534 (3%)]\tClassification Loss: 1.2548\r\n",
      "Train Epoch: 22 [4480/110534 (4%)]\tClassification Loss: 1.6074\r\n",
      "Train Epoch: 22 [5120/110534 (5%)]\tClassification Loss: 1.5783\r\n",
      "Train Epoch: 22 [5760/110534 (5%)]\tClassification Loss: 1.5568\r\n",
      "Train Epoch: 22 [6400/110534 (6%)]\tClassification Loss: 1.2076\r\n",
      "Train Epoch: 22 [7040/110534 (6%)]\tClassification Loss: 1.5374\r\n",
      "Train Epoch: 22 [7680/110534 (7%)]\tClassification Loss: 1.6843\r\n",
      "Train Epoch: 22 [8320/110534 (8%)]\tClassification Loss: 1.7524\r\n",
      "Train Epoch: 22 [8960/110534 (8%)]\tClassification Loss: 1.7631\r\n",
      "Train Epoch: 22 [9600/110534 (9%)]\tClassification Loss: 1.5077\r\n",
      "Train Epoch: 22 [10240/110534 (9%)]\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 22 [10880/110534 (10%)]\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 22 [11520/110534 (10%)]\tClassification Loss: 1.6193\r\n",
      "Train Epoch: 22 [12160/110534 (11%)]\tClassification Loss: 1.4529\r\n",
      "Train Epoch: 22 [12800/110534 (12%)]\tClassification Loss: 1.7037\r\n",
      "Train Epoch: 22 [13440/110534 (12%)]\tClassification Loss: 1.3642\r\n",
      "Train Epoch: 22 [14080/110534 (13%)]\tClassification Loss: 1.6307\r\n",
      "Train Epoch: 22 [14720/110534 (13%)]\tClassification Loss: 1.7275\r\n",
      "Train Epoch: 22 [15360/110534 (14%)]\tClassification Loss: 1.6247\r\n",
      "Train Epoch: 22 [16000/110534 (14%)]\tClassification Loss: 1.6881\r\n",
      "Train Epoch: 22 [16640/110534 (15%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 22 [17280/110534 (16%)]\tClassification Loss: 1.7055\r\n",
      "Train Epoch: 22 [17920/110534 (16%)]\tClassification Loss: 1.4149\r\n",
      "Train Epoch: 22 [18560/110534 (17%)]\tClassification Loss: 1.7211\r\n",
      "Train Epoch: 22 [19200/110534 (17%)]\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 22 [19840/110534 (18%)]\tClassification Loss: 1.5929\r\n",
      "Train Epoch: 22 [20480/110534 (19%)]\tClassification Loss: 1.8085\r\n",
      "Train Epoch: 22 [21120/110534 (19%)]\tClassification Loss: 1.7776\r\n",
      "Train Epoch: 22 [21760/110534 (20%)]\tClassification Loss: 1.2827\r\n",
      "Train Epoch: 22 [22400/110534 (20%)]\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 22 [23040/110534 (21%)]\tClassification Loss: 1.0599\r\n",
      "Train Epoch: 22 [23680/110534 (21%)]\tClassification Loss: 1.7555\r\n",
      "Train Epoch: 22 [24320/110534 (22%)]\tClassification Loss: 1.3209\r\n",
      "Train Epoch: 22 [24960/110534 (23%)]\tClassification Loss: 1.6102\r\n",
      "Train Epoch: 22 [25600/110534 (23%)]\tClassification Loss: 1.2540\r\n",
      "Train Epoch: 22 [26240/110534 (24%)]\tClassification Loss: 1.4861\r\n",
      "Train Epoch: 22 [26880/110534 (24%)]\tClassification Loss: 1.7927\r\n",
      "Train Epoch: 22 [27520/110534 (25%)]\tClassification Loss: 1.4519\r\n",
      "Train Epoch: 22 [28160/110534 (25%)]\tClassification Loss: 1.7753\r\n",
      "Train Epoch: 22 [28800/110534 (26%)]\tClassification Loss: 1.8506\r\n",
      "Train Epoch: 22 [29440/110534 (27%)]\tClassification Loss: 1.7270\r\n",
      "Train Epoch: 22 [30080/110534 (27%)]\tClassification Loss: 1.7779\r\n",
      "Train Epoch: 22 [30720/110534 (28%)]\tClassification Loss: 1.4894\r\n",
      "Train Epoch: 22 [31360/110534 (28%)]\tClassification Loss: 1.5304\r\n",
      "Train Epoch: 22 [32000/110534 (29%)]\tClassification Loss: 1.5675\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_500.pth.tar\r\n",
      "Train Epoch: 22 [32640/110534 (30%)]\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 22 [33280/110534 (30%)]\tClassification Loss: 1.4183\r\n",
      "Train Epoch: 22 [33920/110534 (31%)]\tClassification Loss: 1.6054\r\n",
      "Train Epoch: 22 [34560/110534 (31%)]\tClassification Loss: 1.7340\r\n",
      "Train Epoch: 22 [35200/110534 (32%)]\tClassification Loss: 1.3700\r\n",
      "Train Epoch: 22 [35840/110534 (32%)]\tClassification Loss: 1.2401\r\n",
      "Train Epoch: 22 [36480/110534 (33%)]\tClassification Loss: 1.4718\r\n",
      "Train Epoch: 22 [37120/110534 (34%)]\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 22 [37760/110534 (34%)]\tClassification Loss: 1.7534\r\n",
      "Train Epoch: 22 [38400/110534 (35%)]\tClassification Loss: 1.4987\r\n",
      "Train Epoch: 22 [39040/110534 (35%)]\tClassification Loss: 1.4006\r\n",
      "Train Epoch: 22 [39680/110534 (36%)]\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 22 [40320/110534 (36%)]\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 22 [40960/110534 (37%)]\tClassification Loss: 1.2086\r\n",
      "Train Epoch: 22 [41600/110534 (38%)]\tClassification Loss: 1.4831\r\n",
      "Train Epoch: 22 [42240/110534 (38%)]\tClassification Loss: 1.3479\r\n",
      "Train Epoch: 22 [42880/110534 (39%)]\tClassification Loss: 1.4438\r\n",
      "Train Epoch: 22 [43520/110534 (39%)]\tClassification Loss: 1.2331\r\n",
      "Train Epoch: 22 [44160/110534 (40%)]\tClassification Loss: 1.6480\r\n",
      "Train Epoch: 22 [44800/110534 (41%)]\tClassification Loss: 1.6614\r\n",
      "Train Epoch: 22 [45440/110534 (41%)]\tClassification Loss: 1.6513\r\n",
      "Train Epoch: 22 [46080/110534 (42%)]\tClassification Loss: 1.3766\r\n",
      "Train Epoch: 22 [46720/110534 (42%)]\tClassification Loss: 1.5113\r\n",
      "Train Epoch: 22 [47360/110534 (43%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 22 [48000/110534 (43%)]\tClassification Loss: 1.6296\r\n",
      "Train Epoch: 22 [48640/110534 (44%)]\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 22 [49280/110534 (45%)]\tClassification Loss: 1.6769\r\n",
      "Train Epoch: 22 [49920/110534 (45%)]\tClassification Loss: 1.6198\r\n",
      "Train Epoch: 22 [50560/110534 (46%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 22 [51200/110534 (46%)]\tClassification Loss: 1.2860\r\n",
      "Train Epoch: 22 [51840/110534 (47%)]\tClassification Loss: 1.5733\r\n",
      "Train Epoch: 22 [52480/110534 (47%)]\tClassification Loss: 1.6007\r\n",
      "Train Epoch: 22 [53120/110534 (48%)]\tClassification Loss: 1.2029\r\n",
      "Train Epoch: 22 [53760/110534 (49%)]\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 22 [54400/110534 (49%)]\tClassification Loss: 1.4098\r\n",
      "Train Epoch: 22 [55040/110534 (50%)]\tClassification Loss: 1.6494\r\n",
      "Train Epoch: 22 [55680/110534 (50%)]\tClassification Loss: 1.7984\r\n",
      "Train Epoch: 22 [56320/110534 (51%)]\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 22 [56960/110534 (52%)]\tClassification Loss: 1.5090\r\n",
      "Train Epoch: 22 [57600/110534 (52%)]\tClassification Loss: 1.7474\r\n",
      "Train Epoch: 22 [58240/110534 (53%)]\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 22 [58880/110534 (53%)]\tClassification Loss: 1.7234\r\n",
      "Train Epoch: 22 [59520/110534 (54%)]\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 22 [60160/110534 (54%)]\tClassification Loss: 1.4772\r\n",
      "Train Epoch: 22 [60800/110534 (55%)]\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 22 [61440/110534 (56%)]\tClassification Loss: 1.3554\r\n",
      "Train Epoch: 22 [62080/110534 (56%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 22 [62720/110534 (57%)]\tClassification Loss: 1.7427\r\n",
      "Train Epoch: 22 [63360/110534 (57%)]\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 22 [64000/110534 (58%)]\tClassification Loss: 1.5604\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_1000.pth.tar\r\n",
      "Train Epoch: 22 [64640/110534 (58%)]\tClassification Loss: 1.1722\r\n",
      "Train Epoch: 22 [65280/110534 (59%)]\tClassification Loss: 1.5775\r\n",
      "Train Epoch: 22 [65920/110534 (60%)]\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 22 [66560/110534 (60%)]\tClassification Loss: 1.4072\r\n",
      "Train Epoch: 22 [67200/110534 (61%)]\tClassification Loss: 1.3887\r\n",
      "Train Epoch: 22 [67840/110534 (61%)]\tClassification Loss: 1.8931\r\n",
      "Train Epoch: 22 [68480/110534 (62%)]\tClassification Loss: 1.6119\r\n",
      "Train Epoch: 22 [69120/110534 (63%)]\tClassification Loss: 1.7449\r\n",
      "Train Epoch: 22 [69760/110534 (63%)]\tClassification Loss: 1.6642\r\n",
      "Train Epoch: 22 [70400/110534 (64%)]\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 22 [71040/110534 (64%)]\tClassification Loss: 1.8677\r\n",
      "Train Epoch: 22 [71680/110534 (65%)]\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 22 [72320/110534 (65%)]\tClassification Loss: 1.6493\r\n",
      "Train Epoch: 22 [72960/110534 (66%)]\tClassification Loss: 1.7948\r\n",
      "Train Epoch: 22 [73600/110534 (67%)]\tClassification Loss: 1.7759\r\n",
      "Train Epoch: 22 [74240/110534 (67%)]\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 22 [74880/110534 (68%)]\tClassification Loss: 1.2097\r\n",
      "Train Epoch: 22 [75520/110534 (68%)]\tClassification Loss: 1.5932\r\n",
      "Train Epoch: 22 [76160/110534 (69%)]\tClassification Loss: 1.3616\r\n",
      "Train Epoch: 22 [76800/110534 (69%)]\tClassification Loss: 1.2472\r\n",
      "Train Epoch: 22 [77440/110534 (70%)]\tClassification Loss: 1.5487\r\n",
      "Train Epoch: 22 [78080/110534 (71%)]\tClassification Loss: 1.3292\r\n",
      "Train Epoch: 22 [78720/110534 (71%)]\tClassification Loss: 1.5260\r\n",
      "Train Epoch: 22 [79360/110534 (72%)]\tClassification Loss: 1.4757\r\n",
      "Train Epoch: 22 [80000/110534 (72%)]\tClassification Loss: 1.3499\r\n",
      "Train Epoch: 22 [80640/110534 (73%)]\tClassification Loss: 1.5637\r\n",
      "Train Epoch: 22 [81280/110534 (74%)]\tClassification Loss: 1.8148\r\n",
      "Train Epoch: 22 [81920/110534 (74%)]\tClassification Loss: 1.4881\r\n",
      "Train Epoch: 22 [82560/110534 (75%)]\tClassification Loss: 1.8248\r\n",
      "Train Epoch: 22 [83200/110534 (75%)]\tClassification Loss: 1.3861\r\n",
      "Train Epoch: 22 [83840/110534 (76%)]\tClassification Loss: 1.7738\r\n",
      "Train Epoch: 22 [84480/110534 (76%)]\tClassification Loss: 1.6398\r\n",
      "Train Epoch: 22 [85120/110534 (77%)]\tClassification Loss: 1.5750\r\n",
      "Train Epoch: 22 [85760/110534 (78%)]\tClassification Loss: 1.4549\r\n",
      "Train Epoch: 22 [86400/110534 (78%)]\tClassification Loss: 1.5751\r\n",
      "Train Epoch: 22 [87040/110534 (79%)]\tClassification Loss: 1.4742\r\n",
      "Train Epoch: 22 [87680/110534 (79%)]\tClassification Loss: 1.5373\r\n",
      "Train Epoch: 22 [88320/110534 (80%)]\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 22 [88960/110534 (80%)]\tClassification Loss: 1.6183\r\n",
      "Train Epoch: 22 [89600/110534 (81%)]\tClassification Loss: 1.7458\r\n",
      "Train Epoch: 22 [90240/110534 (82%)]\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 22 [90880/110534 (82%)]\tClassification Loss: 1.5380\r\n",
      "Train Epoch: 22 [91520/110534 (83%)]\tClassification Loss: 1.2694\r\n",
      "Train Epoch: 22 [92160/110534 (83%)]\tClassification Loss: 1.4554\r\n",
      "Train Epoch: 22 [92800/110534 (84%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 22 [93440/110534 (85%)]\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 22 [94080/110534 (85%)]\tClassification Loss: 1.5218\r\n",
      "Train Epoch: 22 [94720/110534 (86%)]\tClassification Loss: 1.4634\r\n",
      "Train Epoch: 22 [95360/110534 (86%)]\tClassification Loss: 1.4168\r\n",
      "Train Epoch: 22 [96000/110534 (87%)]\tClassification Loss: 1.4550\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_1500.pth.tar\r\n",
      "Train Epoch: 22 [96640/110534 (87%)]\tClassification Loss: 1.3640\r\n",
      "Train Epoch: 22 [97280/110534 (88%)]\tClassification Loss: 1.2299\r\n",
      "Train Epoch: 22 [97920/110534 (89%)]\tClassification Loss: 1.1707\r\n",
      "Train Epoch: 22 [98560/110534 (89%)]\tClassification Loss: 1.4219\r\n",
      "Train Epoch: 22 [99200/110534 (90%)]\tClassification Loss: 1.8033\r\n",
      "Train Epoch: 22 [99840/110534 (90%)]\tClassification Loss: 1.4548\r\n",
      "Train Epoch: 22 [100480/110534 (91%)]\tClassification Loss: 1.7001\r\n",
      "Train Epoch: 22 [101120/110534 (91%)]\tClassification Loss: 1.4295\r\n",
      "Train Epoch: 22 [101760/110534 (92%)]\tClassification Loss: 1.5987\r\n",
      "Train Epoch: 22 [102400/110534 (93%)]\tClassification Loss: 1.5339\r\n",
      "Train Epoch: 22 [103040/110534 (93%)]\tClassification Loss: 1.3158\r\n",
      "Train Epoch: 22 [103680/110534 (94%)]\tClassification Loss: 1.6858\r\n",
      "Train Epoch: 22 [104320/110534 (94%)]\tClassification Loss: 1.4948\r\n",
      "Train Epoch: 22 [104960/110534 (95%)]\tClassification Loss: 1.5806\r\n",
      "Train Epoch: 22 [105600/110534 (96%)]\tClassification Loss: 1.5175\r\n",
      "Train Epoch: 22 [106240/110534 (96%)]\tClassification Loss: 1.3055\r\n",
      "Train Epoch: 22 [106880/110534 (97%)]\tClassification Loss: 1.7414\r\n",
      "Train Epoch: 22 [107520/110534 (97%)]\tClassification Loss: 1.6099\r\n",
      "Train Epoch: 22 [108160/110534 (98%)]\tClassification Loss: 1.4434\r\n",
      "Train Epoch: 22 [108800/110534 (98%)]\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 22 [109440/110534 (99%)]\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 22 [110080/110534 (100%)]\tClassification Loss: 1.4966\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_final.pth.tar\r\n",
      "Train Epoch: 23 [0/110534 (0%)]\tClassification Loss: 1.4103\r\n",
      "\r\n",
      "Test set: Average loss: 1.4281, Accuracy: 22897/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 23 [640/110534 (1%)]\tClassification Loss: 1.2702\r\n",
      "Train Epoch: 23 [1280/110534 (1%)]\tClassification Loss: 2.0365\r\n",
      "Train Epoch: 23 [1920/110534 (2%)]\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 23 [2560/110534 (2%)]\tClassification Loss: 1.7337\r\n",
      "Train Epoch: 23 [3200/110534 (3%)]\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 23 [3840/110534 (3%)]\tClassification Loss: 1.4678\r\n",
      "Train Epoch: 23 [4480/110534 (4%)]\tClassification Loss: 1.6139\r\n",
      "Train Epoch: 23 [5120/110534 (5%)]\tClassification Loss: 1.5270\r\n",
      "Train Epoch: 23 [5760/110534 (5%)]\tClassification Loss: 1.6286\r\n",
      "Train Epoch: 23 [6400/110534 (6%)]\tClassification Loss: 1.3068\r\n",
      "Train Epoch: 23 [7040/110534 (6%)]\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 23 [7680/110534 (7%)]\tClassification Loss: 1.4772\r\n",
      "Train Epoch: 23 [8320/110534 (8%)]\tClassification Loss: 1.7137\r\n",
      "Train Epoch: 23 [8960/110534 (8%)]\tClassification Loss: 1.7597\r\n",
      "Train Epoch: 23 [9600/110534 (9%)]\tClassification Loss: 1.4815\r\n",
      "Train Epoch: 23 [10240/110534 (9%)]\tClassification Loss: 1.5266\r\n",
      "Train Epoch: 23 [10880/110534 (10%)]\tClassification Loss: 1.5412\r\n",
      "Train Epoch: 23 [11520/110534 (10%)]\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 23 [12160/110534 (11%)]\tClassification Loss: 1.4017\r\n",
      "Train Epoch: 23 [12800/110534 (12%)]\tClassification Loss: 1.5517\r\n",
      "Train Epoch: 23 [13440/110534 (12%)]\tClassification Loss: 1.3897\r\n",
      "Train Epoch: 23 [14080/110534 (13%)]\tClassification Loss: 1.6310\r\n",
      "Train Epoch: 23 [14720/110534 (13%)]\tClassification Loss: 1.7330\r\n",
      "Train Epoch: 23 [15360/110534 (14%)]\tClassification Loss: 1.5308\r\n",
      "Train Epoch: 23 [16000/110534 (14%)]\tClassification Loss: 1.6986\r\n",
      "Train Epoch: 23 [16640/110534 (15%)]\tClassification Loss: 1.4323\r\n",
      "Train Epoch: 23 [17280/110534 (16%)]\tClassification Loss: 1.6258\r\n",
      "Train Epoch: 23 [17920/110534 (16%)]\tClassification Loss: 1.5302\r\n",
      "Train Epoch: 23 [18560/110534 (17%)]\tClassification Loss: 1.7012\r\n",
      "Train Epoch: 23 [19200/110534 (17%)]\tClassification Loss: 1.7826\r\n",
      "Train Epoch: 23 [19840/110534 (18%)]\tClassification Loss: 1.5941\r\n",
      "Train Epoch: 23 [20480/110534 (19%)]\tClassification Loss: 1.7975\r\n",
      "Train Epoch: 23 [21120/110534 (19%)]\tClassification Loss: 1.7665\r\n",
      "Train Epoch: 23 [21760/110534 (20%)]\tClassification Loss: 1.5066\r\n",
      "Train Epoch: 23 [22400/110534 (20%)]\tClassification Loss: 1.4352\r\n",
      "Train Epoch: 23 [23040/110534 (21%)]\tClassification Loss: 1.1967\r\n",
      "Train Epoch: 23 [23680/110534 (21%)]\tClassification Loss: 1.5493\r\n",
      "Train Epoch: 23 [24320/110534 (22%)]\tClassification Loss: 1.2428\r\n",
      "Train Epoch: 23 [24960/110534 (23%)]\tClassification Loss: 1.8128\r\n",
      "Train Epoch: 23 [25600/110534 (23%)]\tClassification Loss: 1.3099\r\n",
      "Train Epoch: 23 [26240/110534 (24%)]\tClassification Loss: 1.3893\r\n",
      "Train Epoch: 23 [26880/110534 (24%)]\tClassification Loss: 1.6989\r\n",
      "Train Epoch: 23 [27520/110534 (25%)]\tClassification Loss: 1.3549\r\n",
      "Train Epoch: 23 [28160/110534 (25%)]\tClassification Loss: 1.7229\r\n",
      "Train Epoch: 23 [28800/110534 (26%)]\tClassification Loss: 1.8378\r\n",
      "Train Epoch: 23 [29440/110534 (27%)]\tClassification Loss: 1.5360\r\n",
      "Train Epoch: 23 [30080/110534 (27%)]\tClassification Loss: 1.8736\r\n",
      "Train Epoch: 23 [30720/110534 (28%)]\tClassification Loss: 1.5195\r\n",
      "Train Epoch: 23 [31360/110534 (28%)]\tClassification Loss: 1.6113\r\n",
      "Train Epoch: 23 [32000/110534 (29%)]\tClassification Loss: 1.5105\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_500.pth.tar\r\n",
      "Train Epoch: 23 [32640/110534 (30%)]\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 23 [33280/110534 (30%)]\tClassification Loss: 1.4663\r\n",
      "Train Epoch: 23 [33920/110534 (31%)]\tClassification Loss: 1.5315\r\n",
      "Train Epoch: 23 [34560/110534 (31%)]\tClassification Loss: 1.8691\r\n",
      "Train Epoch: 23 [35200/110534 (32%)]\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 23 [35840/110534 (32%)]\tClassification Loss: 1.2068\r\n",
      "Train Epoch: 23 [36480/110534 (33%)]\tClassification Loss: 1.4150\r\n",
      "Train Epoch: 23 [37120/110534 (34%)]\tClassification Loss: 1.6618\r\n",
      "Train Epoch: 23 [37760/110534 (34%)]\tClassification Loss: 1.7826\r\n",
      "Train Epoch: 23 [38400/110534 (35%)]\tClassification Loss: 1.3776\r\n",
      "Train Epoch: 23 [39040/110534 (35%)]\tClassification Loss: 1.4287\r\n",
      "Train Epoch: 23 [39680/110534 (36%)]\tClassification Loss: 1.6837\r\n",
      "Train Epoch: 23 [40320/110534 (36%)]\tClassification Loss: 1.4768\r\n",
      "Train Epoch: 23 [40960/110534 (37%)]\tClassification Loss: 1.4957\r\n",
      "Train Epoch: 23 [41600/110534 (38%)]\tClassification Loss: 1.3419\r\n",
      "Train Epoch: 23 [42240/110534 (38%)]\tClassification Loss: 1.4239\r\n",
      "Train Epoch: 23 [42880/110534 (39%)]\tClassification Loss: 1.4735\r\n",
      "Train Epoch: 23 [43520/110534 (39%)]\tClassification Loss: 1.2074\r\n",
      "Train Epoch: 23 [44160/110534 (40%)]\tClassification Loss: 1.5619\r\n",
      "Train Epoch: 23 [44800/110534 (41%)]\tClassification Loss: 1.6980\r\n",
      "Train Epoch: 23 [45440/110534 (41%)]\tClassification Loss: 1.5599\r\n",
      "Train Epoch: 23 [46080/110534 (42%)]\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 23 [46720/110534 (42%)]\tClassification Loss: 1.4411\r\n",
      "Train Epoch: 23 [47360/110534 (43%)]\tClassification Loss: 1.4256\r\n",
      "Train Epoch: 23 [48000/110534 (43%)]\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 23 [48640/110534 (44%)]\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 23 [49280/110534 (45%)]\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 23 [49920/110534 (45%)]\tClassification Loss: 1.6924\r\n",
      "Train Epoch: 23 [50560/110534 (46%)]\tClassification Loss: 1.4893\r\n",
      "Train Epoch: 23 [51200/110534 (46%)]\tClassification Loss: 1.4086\r\n",
      "Train Epoch: 23 [51840/110534 (47%)]\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 23 [52480/110534 (47%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 23 [53120/110534 (48%)]\tClassification Loss: 1.1356\r\n",
      "Train Epoch: 23 [53760/110534 (49%)]\tClassification Loss: 1.6146\r\n",
      "Train Epoch: 23 [54400/110534 (49%)]\tClassification Loss: 1.5626\r\n",
      "Train Epoch: 23 [55040/110534 (50%)]\tClassification Loss: 1.7551\r\n",
      "Train Epoch: 23 [55680/110534 (50%)]\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 23 [56320/110534 (51%)]\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 23 [56960/110534 (52%)]\tClassification Loss: 1.6121\r\n",
      "Train Epoch: 23 [57600/110534 (52%)]\tClassification Loss: 1.8566\r\n",
      "Train Epoch: 23 [58240/110534 (53%)]\tClassification Loss: 1.5659\r\n",
      "Train Epoch: 23 [58880/110534 (53%)]\tClassification Loss: 1.7906\r\n",
      "Train Epoch: 23 [59520/110534 (54%)]\tClassification Loss: 1.4474\r\n",
      "Train Epoch: 23 [60160/110534 (54%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 23 [60800/110534 (55%)]\tClassification Loss: 1.5467\r\n",
      "Train Epoch: 23 [61440/110534 (56%)]\tClassification Loss: 1.7033\r\n",
      "Train Epoch: 23 [62080/110534 (56%)]\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 23 [62720/110534 (57%)]\tClassification Loss: 1.6904\r\n",
      "Train Epoch: 23 [63360/110534 (57%)]\tClassification Loss: 1.2696\r\n",
      "Train Epoch: 23 [64000/110534 (58%)]\tClassification Loss: 1.4931\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_1000.pth.tar\r\n",
      "Train Epoch: 23 [64640/110534 (58%)]\tClassification Loss: 1.2745\r\n",
      "Train Epoch: 23 [65280/110534 (59%)]\tClassification Loss: 1.5362\r\n",
      "Train Epoch: 23 [65920/110534 (60%)]\tClassification Loss: 1.4820\r\n",
      "Train Epoch: 23 [66560/110534 (60%)]\tClassification Loss: 1.4785\r\n",
      "Train Epoch: 23 [67200/110534 (61%)]\tClassification Loss: 1.2907\r\n",
      "Train Epoch: 23 [67840/110534 (61%)]\tClassification Loss: 1.8117\r\n",
      "Train Epoch: 23 [68480/110534 (62%)]\tClassification Loss: 1.6071\r\n",
      "Train Epoch: 23 [69120/110534 (63%)]\tClassification Loss: 1.9331\r\n",
      "Train Epoch: 23 [69760/110534 (63%)]\tClassification Loss: 1.7592\r\n",
      "Train Epoch: 23 [70400/110534 (64%)]\tClassification Loss: 1.1988\r\n",
      "Train Epoch: 23 [71040/110534 (64%)]\tClassification Loss: 1.8855\r\n",
      "Train Epoch: 23 [71680/110534 (65%)]\tClassification Loss: 1.4748\r\n",
      "Train Epoch: 23 [72320/110534 (65%)]\tClassification Loss: 1.7400\r\n",
      "Train Epoch: 23 [72960/110534 (66%)]\tClassification Loss: 1.7612\r\n",
      "Train Epoch: 23 [73600/110534 (67%)]\tClassification Loss: 1.7314\r\n",
      "Train Epoch: 23 [74240/110534 (67%)]\tClassification Loss: 1.5385\r\n",
      "Train Epoch: 23 [74880/110534 (68%)]\tClassification Loss: 1.2434\r\n",
      "Train Epoch: 23 [75520/110534 (68%)]\tClassification Loss: 1.6102\r\n",
      "Train Epoch: 23 [76160/110534 (69%)]\tClassification Loss: 1.4338\r\n",
      "Train Epoch: 23 [76800/110534 (69%)]\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 23 [77440/110534 (70%)]\tClassification Loss: 1.3799\r\n",
      "Train Epoch: 23 [78080/110534 (71%)]\tClassification Loss: 1.4692\r\n",
      "Train Epoch: 23 [78720/110534 (71%)]\tClassification Loss: 1.3877\r\n",
      "Train Epoch: 23 [79360/110534 (72%)]\tClassification Loss: 1.4017\r\n",
      "Train Epoch: 23 [80000/110534 (72%)]\tClassification Loss: 1.4373\r\n",
      "Train Epoch: 23 [80640/110534 (73%)]\tClassification Loss: 1.3635\r\n",
      "Train Epoch: 23 [81280/110534 (74%)]\tClassification Loss: 1.8930\r\n",
      "Train Epoch: 23 [81920/110534 (74%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 23 [82560/110534 (75%)]\tClassification Loss: 1.6367\r\n",
      "Train Epoch: 23 [83200/110534 (75%)]\tClassification Loss: 1.4891\r\n",
      "Train Epoch: 23 [83840/110534 (76%)]\tClassification Loss: 1.8937\r\n",
      "Train Epoch: 23 [84480/110534 (76%)]\tClassification Loss: 1.7744\r\n",
      "Train Epoch: 23 [85120/110534 (77%)]\tClassification Loss: 1.5409\r\n",
      "Train Epoch: 23 [85760/110534 (78%)]\tClassification Loss: 1.6346\r\n",
      "Train Epoch: 23 [86400/110534 (78%)]\tClassification Loss: 1.6778\r\n",
      "Train Epoch: 23 [87040/110534 (79%)]\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 23 [87680/110534 (79%)]\tClassification Loss: 1.4635\r\n",
      "Train Epoch: 23 [88320/110534 (80%)]\tClassification Loss: 1.4453\r\n",
      "Train Epoch: 23 [88960/110534 (80%)]\tClassification Loss: 1.5660\r\n",
      "Train Epoch: 23 [89600/110534 (81%)]\tClassification Loss: 1.6849\r\n",
      "Train Epoch: 23 [90240/110534 (82%)]\tClassification Loss: 1.7622\r\n",
      "Train Epoch: 23 [90880/110534 (82%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 23 [91520/110534 (83%)]\tClassification Loss: 1.2185\r\n",
      "Train Epoch: 23 [92160/110534 (83%)]\tClassification Loss: 1.2705\r\n",
      "Train Epoch: 23 [92800/110534 (84%)]\tClassification Loss: 1.4668\r\n",
      "Train Epoch: 23 [93440/110534 (85%)]\tClassification Loss: 1.7939\r\n",
      "Train Epoch: 23 [94080/110534 (85%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 23 [94720/110534 (86%)]\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 23 [95360/110534 (86%)]\tClassification Loss: 1.3725\r\n",
      "Train Epoch: 23 [96000/110534 (87%)]\tClassification Loss: 1.3885\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_1500.pth.tar\r\n",
      "Train Epoch: 23 [96640/110534 (87%)]\tClassification Loss: 1.3311\r\n",
      "Train Epoch: 23 [97280/110534 (88%)]\tClassification Loss: 1.3663\r\n",
      "Train Epoch: 23 [97920/110534 (89%)]\tClassification Loss: 1.1560\r\n",
      "Train Epoch: 23 [98560/110534 (89%)]\tClassification Loss: 1.4970\r\n",
      "Train Epoch: 23 [99200/110534 (90%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 23 [99840/110534 (90%)]\tClassification Loss: 1.3850\r\n",
      "Train Epoch: 23 [100480/110534 (91%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 23 [101120/110534 (91%)]\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 23 [101760/110534 (92%)]\tClassification Loss: 1.4745\r\n",
      "Train Epoch: 23 [102400/110534 (93%)]\tClassification Loss: 1.4185\r\n",
      "Train Epoch: 23 [103040/110534 (93%)]\tClassification Loss: 1.4624\r\n",
      "Train Epoch: 23 [103680/110534 (94%)]\tClassification Loss: 1.7643\r\n",
      "Train Epoch: 23 [104320/110534 (94%)]\tClassification Loss: 1.3059\r\n",
      "Train Epoch: 23 [104960/110534 (95%)]\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 23 [105600/110534 (96%)]\tClassification Loss: 1.4305\r\n",
      "Train Epoch: 23 [106240/110534 (96%)]\tClassification Loss: 1.3091\r\n",
      "Train Epoch: 23 [106880/110534 (97%)]\tClassification Loss: 1.6360\r\n",
      "Train Epoch: 23 [107520/110534 (97%)]\tClassification Loss: 1.6432\r\n",
      "Train Epoch: 23 [108160/110534 (98%)]\tClassification Loss: 1.5022\r\n",
      "Train Epoch: 23 [108800/110534 (98%)]\tClassification Loss: 1.7991\r\n",
      "Train Epoch: 23 [109440/110534 (99%)]\tClassification Loss: 1.4669\r\n",
      "Train Epoch: 23 [110080/110534 (100%)]\tClassification Loss: 1.4357\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_final.pth.tar\r\n",
      "Train Epoch: 24 [0/110534 (0%)]\tClassification Loss: 1.5391\r\n",
      "\r\n",
      "Test set: Average loss: 1.4268, Accuracy: 22920/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 24 [640/110534 (1%)]\tClassification Loss: 1.2841\r\n",
      "Train Epoch: 24 [1280/110534 (1%)]\tClassification Loss: 1.9572\r\n",
      "Train Epoch: 24 [1920/110534 (2%)]\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 24 [2560/110534 (2%)]\tClassification Loss: 1.6849\r\n",
      "Train Epoch: 24 [3200/110534 (3%)]\tClassification Loss: 1.4955\r\n",
      "Train Epoch: 24 [3840/110534 (3%)]\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 24 [4480/110534 (4%)]\tClassification Loss: 1.4191\r\n",
      "Train Epoch: 24 [5120/110534 (5%)]\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 24 [5760/110534 (5%)]\tClassification Loss: 1.3853\r\n",
      "Train Epoch: 24 [6400/110534 (6%)]\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 24 [7040/110534 (6%)]\tClassification Loss: 1.5668\r\n",
      "Train Epoch: 24 [7680/110534 (7%)]\tClassification Loss: 1.6473\r\n",
      "Train Epoch: 24 [8320/110534 (8%)]\tClassification Loss: 1.7643\r\n",
      "Train Epoch: 24 [8960/110534 (8%)]\tClassification Loss: 1.6783\r\n",
      "Train Epoch: 24 [9600/110534 (9%)]\tClassification Loss: 1.4499\r\n",
      "Train Epoch: 24 [10240/110534 (9%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 24 [10880/110534 (10%)]\tClassification Loss: 1.5497\r\n",
      "Train Epoch: 24 [11520/110534 (10%)]\tClassification Loss: 1.5918\r\n",
      "Train Epoch: 24 [12160/110534 (11%)]\tClassification Loss: 1.3177\r\n",
      "Train Epoch: 24 [12800/110534 (12%)]\tClassification Loss: 1.5227\r\n",
      "Train Epoch: 24 [13440/110534 (12%)]\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 24 [14080/110534 (13%)]\tClassification Loss: 1.6449\r\n",
      "Train Epoch: 24 [14720/110534 (13%)]\tClassification Loss: 1.9349\r\n",
      "Train Epoch: 24 [15360/110534 (14%)]\tClassification Loss: 1.5364\r\n",
      "Train Epoch: 24 [16000/110534 (14%)]\tClassification Loss: 1.6150\r\n",
      "Train Epoch: 24 [16640/110534 (15%)]\tClassification Loss: 1.4994\r\n",
      "Train Epoch: 24 [17280/110534 (16%)]\tClassification Loss: 1.6073\r\n",
      "Train Epoch: 24 [17920/110534 (16%)]\tClassification Loss: 1.4133\r\n",
      "Train Epoch: 24 [18560/110534 (17%)]\tClassification Loss: 1.7058\r\n",
      "Train Epoch: 24 [19200/110534 (17%)]\tClassification Loss: 1.6850\r\n",
      "Train Epoch: 24 [19840/110534 (18%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 24 [20480/110534 (19%)]\tClassification Loss: 1.8093\r\n",
      "Train Epoch: 24 [21120/110534 (19%)]\tClassification Loss: 1.8099\r\n",
      "Train Epoch: 24 [21760/110534 (20%)]\tClassification Loss: 1.3487\r\n",
      "Train Epoch: 24 [22400/110534 (20%)]\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 24 [23040/110534 (21%)]\tClassification Loss: 1.2145\r\n",
      "Train Epoch: 24 [23680/110534 (21%)]\tClassification Loss: 1.6890\r\n",
      "Train Epoch: 24 [24320/110534 (22%)]\tClassification Loss: 1.2577\r\n",
      "Train Epoch: 24 [24960/110534 (23%)]\tClassification Loss: 1.7808\r\n",
      "Train Epoch: 24 [25600/110534 (23%)]\tClassification Loss: 1.3085\r\n",
      "Train Epoch: 24 [26240/110534 (24%)]\tClassification Loss: 1.4576\r\n",
      "Train Epoch: 24 [26880/110534 (24%)]\tClassification Loss: 1.7374\r\n",
      "Train Epoch: 24 [27520/110534 (25%)]\tClassification Loss: 1.3769\r\n",
      "Train Epoch: 24 [28160/110534 (25%)]\tClassification Loss: 1.7842\r\n",
      "Train Epoch: 24 [28800/110534 (26%)]\tClassification Loss: 1.7659\r\n",
      "Train Epoch: 24 [29440/110534 (27%)]\tClassification Loss: 1.5453\r\n",
      "Train Epoch: 24 [30080/110534 (27%)]\tClassification Loss: 1.6643\r\n",
      "Train Epoch: 24 [30720/110534 (28%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 24 [31360/110534 (28%)]\tClassification Loss: 1.6394\r\n",
      "Train Epoch: 24 [32000/110534 (29%)]\tClassification Loss: 1.4901\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_500.pth.tar\r\n",
      "Train Epoch: 24 [32640/110534 (30%)]\tClassification Loss: 1.3730\r\n",
      "Train Epoch: 24 [33280/110534 (30%)]\tClassification Loss: 1.5246\r\n",
      "Train Epoch: 24 [33920/110534 (31%)]\tClassification Loss: 1.6101\r\n",
      "Train Epoch: 24 [34560/110534 (31%)]\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 24 [35200/110534 (32%)]\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 24 [35840/110534 (32%)]\tClassification Loss: 1.2720\r\n",
      "Train Epoch: 24 [36480/110534 (33%)]\tClassification Loss: 1.3731\r\n",
      "Train Epoch: 24 [37120/110534 (34%)]\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 24 [37760/110534 (34%)]\tClassification Loss: 1.8658\r\n",
      "Train Epoch: 24 [38400/110534 (35%)]\tClassification Loss: 1.4849\r\n",
      "Train Epoch: 24 [39040/110534 (35%)]\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 24 [39680/110534 (36%)]\tClassification Loss: 1.5630\r\n",
      "Train Epoch: 24 [40320/110534 (36%)]\tClassification Loss: 1.6423\r\n",
      "Train Epoch: 24 [40960/110534 (37%)]\tClassification Loss: 1.4116\r\n",
      "Train Epoch: 24 [41600/110534 (38%)]\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 24 [42240/110534 (38%)]\tClassification Loss: 1.4307\r\n",
      "Train Epoch: 24 [42880/110534 (39%)]\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 24 [43520/110534 (39%)]\tClassification Loss: 1.3059\r\n",
      "Train Epoch: 24 [44160/110534 (40%)]\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 24 [44800/110534 (41%)]\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 24 [45440/110534 (41%)]\tClassification Loss: 1.5542\r\n",
      "Train Epoch: 24 [46080/110534 (42%)]\tClassification Loss: 1.3888\r\n",
      "Train Epoch: 24 [46720/110534 (42%)]\tClassification Loss: 1.5336\r\n",
      "Train Epoch: 24 [47360/110534 (43%)]\tClassification Loss: 1.5205\r\n",
      "Train Epoch: 24 [48000/110534 (43%)]\tClassification Loss: 1.5557\r\n",
      "Train Epoch: 24 [48640/110534 (44%)]\tClassification Loss: 1.3174\r\n",
      "Train Epoch: 24 [49280/110534 (45%)]\tClassification Loss: 1.6873\r\n",
      "Train Epoch: 24 [49920/110534 (45%)]\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 24 [50560/110534 (46%)]\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 24 [51200/110534 (46%)]\tClassification Loss: 1.3611\r\n",
      "Train Epoch: 24 [51840/110534 (47%)]\tClassification Loss: 1.4342\r\n",
      "Train Epoch: 24 [52480/110534 (47%)]\tClassification Loss: 1.5208\r\n",
      "Train Epoch: 24 [53120/110534 (48%)]\tClassification Loss: 1.1912\r\n",
      "Train Epoch: 24 [53760/110534 (49%)]\tClassification Loss: 1.6245\r\n",
      "Train Epoch: 24 [54400/110534 (49%)]\tClassification Loss: 1.4738\r\n",
      "Train Epoch: 24 [55040/110534 (50%)]\tClassification Loss: 1.6970\r\n",
      "Train Epoch: 24 [55680/110534 (50%)]\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 24 [56320/110534 (51%)]\tClassification Loss: 1.4838\r\n",
      "Train Epoch: 24 [56960/110534 (52%)]\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 24 [57600/110534 (52%)]\tClassification Loss: 1.7315\r\n",
      "Train Epoch: 24 [58240/110534 (53%)]\tClassification Loss: 1.4271\r\n",
      "Train Epoch: 24 [58880/110534 (53%)]\tClassification Loss: 1.6453\r\n",
      "Train Epoch: 24 [59520/110534 (54%)]\tClassification Loss: 1.3730\r\n",
      "Train Epoch: 24 [60160/110534 (54%)]\tClassification Loss: 1.4504\r\n",
      "Train Epoch: 24 [60800/110534 (55%)]\tClassification Loss: 1.6416\r\n",
      "Train Epoch: 24 [61440/110534 (56%)]\tClassification Loss: 1.2704\r\n",
      "Train Epoch: 24 [62080/110534 (56%)]\tClassification Loss: 1.5625\r\n",
      "Train Epoch: 24 [62720/110534 (57%)]\tClassification Loss: 1.8951\r\n",
      "Train Epoch: 24 [63360/110534 (57%)]\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 24 [64000/110534 (58%)]\tClassification Loss: 1.5370\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_1000.pth.tar\r\n",
      "Train Epoch: 24 [64640/110534 (58%)]\tClassification Loss: 1.1839\r\n",
      "Train Epoch: 24 [65280/110534 (59%)]\tClassification Loss: 1.7017\r\n",
      "Train Epoch: 24 [65920/110534 (60%)]\tClassification Loss: 1.5739\r\n",
      "Train Epoch: 24 [66560/110534 (60%)]\tClassification Loss: 1.4863\r\n",
      "Train Epoch: 24 [67200/110534 (61%)]\tClassification Loss: 1.2222\r\n",
      "Train Epoch: 24 [67840/110534 (61%)]\tClassification Loss: 1.6032\r\n",
      "Train Epoch: 24 [68480/110534 (62%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 24 [69120/110534 (63%)]\tClassification Loss: 1.8382\r\n",
      "Train Epoch: 24 [69760/110534 (63%)]\tClassification Loss: 1.6011\r\n",
      "Train Epoch: 24 [70400/110534 (64%)]\tClassification Loss: 1.3868\r\n",
      "Train Epoch: 24 [71040/110534 (64%)]\tClassification Loss: 1.9972\r\n",
      "Train Epoch: 24 [71680/110534 (65%)]\tClassification Loss: 1.3942\r\n",
      "Train Epoch: 24 [72320/110534 (65%)]\tClassification Loss: 1.4213\r\n",
      "Train Epoch: 24 [72960/110534 (66%)]\tClassification Loss: 1.7309\r\n",
      "Train Epoch: 24 [73600/110534 (67%)]\tClassification Loss: 1.7589\r\n",
      "Train Epoch: 24 [74240/110534 (67%)]\tClassification Loss: 1.7167\r\n",
      "Train Epoch: 24 [74880/110534 (68%)]\tClassification Loss: 1.1062\r\n",
      "Train Epoch: 24 [75520/110534 (68%)]\tClassification Loss: 1.5467\r\n",
      "Train Epoch: 24 [76160/110534 (69%)]\tClassification Loss: 1.3472\r\n",
      "Train Epoch: 24 [76800/110534 (69%)]\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 24 [77440/110534 (70%)]\tClassification Loss: 1.3773\r\n",
      "Train Epoch: 24 [78080/110534 (71%)]\tClassification Loss: 1.5213\r\n",
      "Train Epoch: 24 [78720/110534 (71%)]\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 24 [79360/110534 (72%)]\tClassification Loss: 1.3588\r\n",
      "Train Epoch: 24 [80000/110534 (72%)]\tClassification Loss: 1.3963\r\n",
      "Train Epoch: 24 [80640/110534 (73%)]\tClassification Loss: 1.3397\r\n",
      "Train Epoch: 24 [81280/110534 (74%)]\tClassification Loss: 1.7874\r\n",
      "Train Epoch: 24 [81920/110534 (74%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 24 [82560/110534 (75%)]\tClassification Loss: 1.6665\r\n",
      "Train Epoch: 24 [83200/110534 (75%)]\tClassification Loss: 1.4068\r\n",
      "Train Epoch: 24 [83840/110534 (76%)]\tClassification Loss: 1.8067\r\n",
      "Train Epoch: 24 [84480/110534 (76%)]\tClassification Loss: 1.7177\r\n",
      "Train Epoch: 24 [85120/110534 (77%)]\tClassification Loss: 1.4541\r\n",
      "Train Epoch: 24 [85760/110534 (78%)]\tClassification Loss: 1.5763\r\n",
      "Train Epoch: 24 [86400/110534 (78%)]\tClassification Loss: 1.6942\r\n",
      "Train Epoch: 24 [87040/110534 (79%)]\tClassification Loss: 1.3867\r\n",
      "Train Epoch: 24 [87680/110534 (79%)]\tClassification Loss: 1.4873\r\n",
      "Train Epoch: 24 [88320/110534 (80%)]\tClassification Loss: 1.6120\r\n",
      "Train Epoch: 24 [88960/110534 (80%)]\tClassification Loss: 1.7929\r\n",
      "Train Epoch: 24 [89600/110534 (81%)]\tClassification Loss: 1.9794\r\n",
      "Train Epoch: 24 [90240/110534 (82%)]\tClassification Loss: 1.7686\r\n",
      "Train Epoch: 24 [90880/110534 (82%)]\tClassification Loss: 1.5962\r\n",
      "Train Epoch: 24 [91520/110534 (83%)]\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 24 [92160/110534 (83%)]\tClassification Loss: 1.3925\r\n",
      "Train Epoch: 24 [92800/110534 (84%)]\tClassification Loss: 1.4238\r\n",
      "Train Epoch: 24 [93440/110534 (85%)]\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 24 [94080/110534 (85%)]\tClassification Loss: 1.4346\r\n",
      "Train Epoch: 24 [94720/110534 (86%)]\tClassification Loss: 1.2341\r\n",
      "Train Epoch: 24 [95360/110534 (86%)]\tClassification Loss: 1.4731\r\n",
      "Train Epoch: 24 [96000/110534 (87%)]\tClassification Loss: 1.6205\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_1500.pth.tar\r\n",
      "Train Epoch: 24 [96640/110534 (87%)]\tClassification Loss: 1.4139\r\n",
      "Train Epoch: 24 [97280/110534 (88%)]\tClassification Loss: 1.2325\r\n",
      "Train Epoch: 24 [97920/110534 (89%)]\tClassification Loss: 1.2240\r\n",
      "Train Epoch: 24 [98560/110534 (89%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 24 [99200/110534 (90%)]\tClassification Loss: 1.6365\r\n",
      "Train Epoch: 24 [99840/110534 (90%)]\tClassification Loss: 1.4523\r\n",
      "Train Epoch: 24 [100480/110534 (91%)]\tClassification Loss: 1.6659\r\n",
      "Train Epoch: 24 [101120/110534 (91%)]\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 24 [101760/110534 (92%)]\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 24 [102400/110534 (93%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 24 [103040/110534 (93%)]\tClassification Loss: 1.4966\r\n",
      "Train Epoch: 24 [103680/110534 (94%)]\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 24 [104320/110534 (94%)]\tClassification Loss: 1.3692\r\n",
      "Train Epoch: 24 [104960/110534 (95%)]\tClassification Loss: 1.6977\r\n",
      "Train Epoch: 24 [105600/110534 (96%)]\tClassification Loss: 1.4316\r\n",
      "Train Epoch: 24 [106240/110534 (96%)]\tClassification Loss: 1.2593\r\n",
      "Train Epoch: 24 [106880/110534 (97%)]\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 24 [107520/110534 (97%)]\tClassification Loss: 1.7734\r\n",
      "Train Epoch: 24 [108160/110534 (98%)]\tClassification Loss: 1.5651\r\n",
      "Train Epoch: 24 [108800/110534 (98%)]\tClassification Loss: 1.8019\r\n",
      "Train Epoch: 24 [109440/110534 (99%)]\tClassification Loss: 1.4411\r\n",
      "Train Epoch: 24 [110080/110534 (100%)]\tClassification Loss: 1.4053\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_final.pth.tar\r\n",
      "Train Epoch: 25 [0/110534 (0%)]\tClassification Loss: 1.6131\r\n",
      "\r\n",
      "Test set: Average loss: 1.4412, Accuracy: 22765/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 25 [640/110534 (1%)]\tClassification Loss: 1.3124\r\n",
      "Train Epoch: 25 [1280/110534 (1%)]\tClassification Loss: 1.9708\r\n",
      "Train Epoch: 25 [1920/110534 (2%)]\tClassification Loss: 1.5589\r\n",
      "Train Epoch: 25 [2560/110534 (2%)]\tClassification Loss: 1.7617\r\n",
      "Train Epoch: 25 [3200/110534 (3%)]\tClassification Loss: 1.5430\r\n",
      "Train Epoch: 25 [3840/110534 (3%)]\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 25 [4480/110534 (4%)]\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 25 [5120/110534 (5%)]\tClassification Loss: 1.7527\r\n",
      "Train Epoch: 25 [5760/110534 (5%)]\tClassification Loss: 1.5366\r\n",
      "Train Epoch: 25 [6400/110534 (6%)]\tClassification Loss: 1.4244\r\n",
      "Train Epoch: 25 [7040/110534 (6%)]\tClassification Loss: 1.4007\r\n",
      "Train Epoch: 25 [7680/110534 (7%)]\tClassification Loss: 1.4465\r\n",
      "Train Epoch: 25 [8320/110534 (8%)]\tClassification Loss: 1.7476\r\n",
      "Train Epoch: 25 [8960/110534 (8%)]\tClassification Loss: 1.5736\r\n",
      "Train Epoch: 25 [9600/110534 (9%)]\tClassification Loss: 1.4384\r\n",
      "Train Epoch: 25 [10240/110534 (9%)]\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 25 [10880/110534 (10%)]\tClassification Loss: 1.4088\r\n",
      "Train Epoch: 25 [11520/110534 (10%)]\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 25 [12160/110534 (11%)]\tClassification Loss: 1.4732\r\n",
      "Train Epoch: 25 [12800/110534 (12%)]\tClassification Loss: 1.6246\r\n",
      "Train Epoch: 25 [13440/110534 (12%)]\tClassification Loss: 1.4882\r\n",
      "Train Epoch: 25 [14080/110534 (13%)]\tClassification Loss: 1.4614\r\n",
      "Train Epoch: 25 [14720/110534 (13%)]\tClassification Loss: 1.8486\r\n",
      "Train Epoch: 25 [15360/110534 (14%)]\tClassification Loss: 1.4715\r\n",
      "Train Epoch: 25 [16000/110534 (14%)]\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 25 [16640/110534 (15%)]\tClassification Loss: 1.5870\r\n",
      "Train Epoch: 25 [17280/110534 (16%)]\tClassification Loss: 1.6040\r\n",
      "Train Epoch: 25 [17920/110534 (16%)]\tClassification Loss: 1.5047\r\n",
      "Train Epoch: 25 [18560/110534 (17%)]\tClassification Loss: 1.7550\r\n",
      "Train Epoch: 25 [19200/110534 (17%)]\tClassification Loss: 1.5455\r\n",
      "Train Epoch: 25 [19840/110534 (18%)]\tClassification Loss: 1.5016\r\n",
      "Train Epoch: 25 [20480/110534 (19%)]\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 25 [21120/110534 (19%)]\tClassification Loss: 1.6594\r\n",
      "Train Epoch: 25 [21760/110534 (20%)]\tClassification Loss: 1.4623\r\n",
      "Train Epoch: 25 [22400/110534 (20%)]\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 25 [23040/110534 (21%)]\tClassification Loss: 1.1960\r\n",
      "Train Epoch: 25 [23680/110534 (21%)]\tClassification Loss: 1.5877\r\n",
      "Train Epoch: 25 [24320/110534 (22%)]\tClassification Loss: 1.3095\r\n",
      "Train Epoch: 25 [24960/110534 (23%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 25 [25600/110534 (23%)]\tClassification Loss: 1.3912\r\n",
      "Train Epoch: 25 [26240/110534 (24%)]\tClassification Loss: 1.4224\r\n",
      "Train Epoch: 25 [26880/110534 (24%)]\tClassification Loss: 1.7120\r\n",
      "Train Epoch: 25 [27520/110534 (25%)]\tClassification Loss: 1.2410\r\n",
      "Train Epoch: 25 [28160/110534 (25%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 25 [28800/110534 (26%)]\tClassification Loss: 1.8027\r\n",
      "Train Epoch: 25 [29440/110534 (27%)]\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 25 [30080/110534 (27%)]\tClassification Loss: 1.7668\r\n",
      "Train Epoch: 25 [30720/110534 (28%)]\tClassification Loss: 1.5798\r\n",
      "Train Epoch: 25 [31360/110534 (28%)]\tClassification Loss: 1.7379\r\n",
      "Train Epoch: 25 [32000/110534 (29%)]\tClassification Loss: 1.3625\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_500.pth.tar\r\n",
      "Train Epoch: 25 [32640/110534 (30%)]\tClassification Loss: 1.4407\r\n",
      "Train Epoch: 25 [33280/110534 (30%)]\tClassification Loss: 1.4704\r\n",
      "Train Epoch: 25 [33920/110534 (31%)]\tClassification Loss: 1.6889\r\n",
      "Train Epoch: 25 [34560/110534 (31%)]\tClassification Loss: 1.7385\r\n",
      "Train Epoch: 25 [35200/110534 (32%)]\tClassification Loss: 1.5853\r\n",
      "Train Epoch: 25 [35840/110534 (32%)]\tClassification Loss: 1.2086\r\n",
      "Train Epoch: 25 [36480/110534 (33%)]\tClassification Loss: 1.3965\r\n",
      "Train Epoch: 25 [37120/110534 (34%)]\tClassification Loss: 1.6377\r\n",
      "Train Epoch: 25 [37760/110534 (34%)]\tClassification Loss: 1.8633\r\n",
      "Train Epoch: 25 [38400/110534 (35%)]\tClassification Loss: 1.4680\r\n",
      "Train Epoch: 25 [39040/110534 (35%)]\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 25 [39680/110534 (36%)]\tClassification Loss: 1.7199\r\n",
      "Train Epoch: 25 [40320/110534 (36%)]\tClassification Loss: 1.6035\r\n",
      "Train Epoch: 25 [40960/110534 (37%)]\tClassification Loss: 1.3154\r\n",
      "Train Epoch: 25 [41600/110534 (38%)]\tClassification Loss: 1.4165\r\n",
      "Train Epoch: 25 [42240/110534 (38%)]\tClassification Loss: 1.3425\r\n",
      "Train Epoch: 25 [42880/110534 (39%)]\tClassification Loss: 1.4506\r\n",
      "Train Epoch: 25 [43520/110534 (39%)]\tClassification Loss: 1.2621\r\n",
      "Train Epoch: 25 [44160/110534 (40%)]\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 25 [44800/110534 (41%)]\tClassification Loss: 1.8407\r\n",
      "Train Epoch: 25 [45440/110534 (41%)]\tClassification Loss: 1.5785\r\n",
      "Train Epoch: 25 [46080/110534 (42%)]\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 25 [46720/110534 (42%)]\tClassification Loss: 1.5839\r\n",
      "Train Epoch: 25 [47360/110534 (43%)]\tClassification Loss: 1.3810\r\n",
      "Train Epoch: 25 [48000/110534 (43%)]\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 25 [48640/110534 (44%)]\tClassification Loss: 1.3314\r\n",
      "Train Epoch: 25 [49280/110534 (45%)]\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 25 [49920/110534 (45%)]\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 25 [50560/110534 (46%)]\tClassification Loss: 1.5949\r\n",
      "Train Epoch: 25 [51200/110534 (46%)]\tClassification Loss: 1.2124\r\n",
      "Train Epoch: 25 [51840/110534 (47%)]\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 25 [52480/110534 (47%)]\tClassification Loss: 1.5110\r\n",
      "Train Epoch: 25 [53120/110534 (48%)]\tClassification Loss: 1.2692\r\n",
      "Train Epoch: 25 [53760/110534 (49%)]\tClassification Loss: 1.4236\r\n",
      "Train Epoch: 25 [54400/110534 (49%)]\tClassification Loss: 1.4057\r\n",
      "Train Epoch: 25 [55040/110534 (50%)]\tClassification Loss: 1.5980\r\n",
      "Train Epoch: 25 [55680/110534 (50%)]\tClassification Loss: 1.7226\r\n",
      "Train Epoch: 25 [56320/110534 (51%)]\tClassification Loss: 1.5380\r\n",
      "Train Epoch: 25 [56960/110534 (52%)]\tClassification Loss: 1.3928\r\n",
      "Train Epoch: 25 [57600/110534 (52%)]\tClassification Loss: 1.6163\r\n",
      "Train Epoch: 25 [58240/110534 (53%)]\tClassification Loss: 1.5365\r\n",
      "Train Epoch: 25 [58880/110534 (53%)]\tClassification Loss: 1.8698\r\n",
      "Train Epoch: 25 [59520/110534 (54%)]\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 25 [60160/110534 (54%)]\tClassification Loss: 1.3742\r\n",
      "Train Epoch: 25 [60800/110534 (55%)]\tClassification Loss: 1.5066\r\n",
      "Train Epoch: 25 [61440/110534 (56%)]\tClassification Loss: 1.5626\r\n",
      "Train Epoch: 25 [62080/110534 (56%)]\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 25 [62720/110534 (57%)]\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 25 [63360/110534 (57%)]\tClassification Loss: 1.2733\r\n",
      "Train Epoch: 25 [64000/110534 (58%)]\tClassification Loss: 1.6432\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_1000.pth.tar\r\n",
      "Train Epoch: 25 [64640/110534 (58%)]\tClassification Loss: 1.2555\r\n",
      "Train Epoch: 25 [65280/110534 (59%)]\tClassification Loss: 1.5700\r\n",
      "Train Epoch: 25 [65920/110534 (60%)]\tClassification Loss: 1.7205\r\n",
      "Train Epoch: 25 [66560/110534 (60%)]\tClassification Loss: 1.2334\r\n",
      "Train Epoch: 25 [67200/110534 (61%)]\tClassification Loss: 1.2924\r\n",
      "Train Epoch: 25 [67840/110534 (61%)]\tClassification Loss: 1.8891\r\n",
      "Train Epoch: 25 [68480/110534 (62%)]\tClassification Loss: 1.6682\r\n",
      "Train Epoch: 25 [69120/110534 (63%)]\tClassification Loss: 1.6761\r\n",
      "Train Epoch: 25 [69760/110534 (63%)]\tClassification Loss: 1.6407\r\n",
      "Train Epoch: 25 [70400/110534 (64%)]\tClassification Loss: 1.2124\r\n",
      "Train Epoch: 25 [71040/110534 (64%)]\tClassification Loss: 1.9947\r\n",
      "Train Epoch: 25 [71680/110534 (65%)]\tClassification Loss: 1.5561\r\n",
      "Train Epoch: 25 [72320/110534 (65%)]\tClassification Loss: 1.7450\r\n",
      "Train Epoch: 25 [72960/110534 (66%)]\tClassification Loss: 1.8870\r\n",
      "Train Epoch: 25 [73600/110534 (67%)]\tClassification Loss: 1.8750\r\n",
      "Train Epoch: 25 [74240/110534 (67%)]\tClassification Loss: 1.7719\r\n",
      "Train Epoch: 25 [74880/110534 (68%)]\tClassification Loss: 1.1661\r\n",
      "Train Epoch: 25 [75520/110534 (68%)]\tClassification Loss: 1.5016\r\n",
      "Train Epoch: 25 [76160/110534 (69%)]\tClassification Loss: 1.4123\r\n",
      "Train Epoch: 25 [76800/110534 (69%)]\tClassification Loss: 1.5016\r\n",
      "Train Epoch: 25 [77440/110534 (70%)]\tClassification Loss: 1.3318\r\n",
      "Train Epoch: 25 [78080/110534 (71%)]\tClassification Loss: 1.5464\r\n",
      "Train Epoch: 25 [78720/110534 (71%)]\tClassification Loss: 1.4412\r\n",
      "Train Epoch: 25 [79360/110534 (72%)]\tClassification Loss: 1.4120\r\n",
      "Train Epoch: 25 [80000/110534 (72%)]\tClassification Loss: 1.4591\r\n",
      "Train Epoch: 25 [80640/110534 (73%)]\tClassification Loss: 1.4386\r\n",
      "Train Epoch: 25 [81280/110534 (74%)]\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 25 [81920/110534 (74%)]\tClassification Loss: 1.6284\r\n",
      "Train Epoch: 25 [82560/110534 (75%)]\tClassification Loss: 1.6017\r\n",
      "Train Epoch: 25 [83200/110534 (75%)]\tClassification Loss: 1.4347\r\n",
      "Train Epoch: 25 [83840/110534 (76%)]\tClassification Loss: 1.8161\r\n",
      "Train Epoch: 25 [84480/110534 (76%)]\tClassification Loss: 1.7271\r\n",
      "Train Epoch: 25 [85120/110534 (77%)]\tClassification Loss: 1.5838\r\n",
      "Train Epoch: 25 [85760/110534 (78%)]\tClassification Loss: 1.5297\r\n",
      "Train Epoch: 25 [86400/110534 (78%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 25 [87040/110534 (79%)]\tClassification Loss: 1.5181\r\n",
      "Train Epoch: 25 [87680/110534 (79%)]\tClassification Loss: 1.4754\r\n",
      "Train Epoch: 25 [88320/110534 (80%)]\tClassification Loss: 1.5144\r\n",
      "Train Epoch: 25 [88960/110534 (80%)]\tClassification Loss: 1.6465\r\n",
      "Train Epoch: 25 [89600/110534 (81%)]\tClassification Loss: 1.7317\r\n",
      "Train Epoch: 25 [90240/110534 (82%)]\tClassification Loss: 1.7016\r\n",
      "Train Epoch: 25 [90880/110534 (82%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 25 [91520/110534 (83%)]\tClassification Loss: 1.2443\r\n",
      "Train Epoch: 25 [92160/110534 (83%)]\tClassification Loss: 1.3688\r\n",
      "Train Epoch: 25 [92800/110534 (84%)]\tClassification Loss: 1.3762\r\n",
      "Train Epoch: 25 [93440/110534 (85%)]\tClassification Loss: 1.7463\r\n",
      "Train Epoch: 25 [94080/110534 (85%)]\tClassification Loss: 1.7482\r\n",
      "Train Epoch: 25 [94720/110534 (86%)]\tClassification Loss: 1.2830\r\n",
      "Train Epoch: 25 [95360/110534 (86%)]\tClassification Loss: 1.3057\r\n",
      "Train Epoch: 25 [96000/110534 (87%)]\tClassification Loss: 1.4383\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_1500.pth.tar\r\n",
      "Train Epoch: 25 [96640/110534 (87%)]\tClassification Loss: 1.4496\r\n",
      "Train Epoch: 25 [97280/110534 (88%)]\tClassification Loss: 1.2327\r\n",
      "Train Epoch: 25 [97920/110534 (89%)]\tClassification Loss: 1.1565\r\n",
      "Train Epoch: 25 [98560/110534 (89%)]\tClassification Loss: 1.2990\r\n",
      "Train Epoch: 25 [99200/110534 (90%)]\tClassification Loss: 1.5434\r\n",
      "Train Epoch: 25 [99840/110534 (90%)]\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 25 [100480/110534 (91%)]\tClassification Loss: 1.6208\r\n",
      "Train Epoch: 25 [101120/110534 (91%)]\tClassification Loss: 1.3708\r\n",
      "Train Epoch: 25 [101760/110534 (92%)]\tClassification Loss: 1.7128\r\n",
      "Train Epoch: 25 [102400/110534 (93%)]\tClassification Loss: 1.4581\r\n",
      "Train Epoch: 25 [103040/110534 (93%)]\tClassification Loss: 1.5401\r\n",
      "Train Epoch: 25 [103680/110534 (94%)]\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 25 [104320/110534 (94%)]\tClassification Loss: 1.3228\r\n",
      "Train Epoch: 25 [104960/110534 (95%)]\tClassification Loss: 1.7052\r\n",
      "Train Epoch: 25 [105600/110534 (96%)]\tClassification Loss: 1.5112\r\n",
      "Train Epoch: 25 [106240/110534 (96%)]\tClassification Loss: 1.4767\r\n",
      "Train Epoch: 25 [106880/110534 (97%)]\tClassification Loss: 1.5571\r\n",
      "Train Epoch: 25 [107520/110534 (97%)]\tClassification Loss: 1.6817\r\n",
      "Train Epoch: 25 [108160/110534 (98%)]\tClassification Loss: 1.6517\r\n",
      "Train Epoch: 25 [108800/110534 (98%)]\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 25 [109440/110534 (99%)]\tClassification Loss: 1.4014\r\n",
      "Train Epoch: 25 [110080/110534 (100%)]\tClassification Loss: 1.3832\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_final.pth.tar\r\n",
      "Train Epoch: 26 [0/110534 (0%)]\tClassification Loss: 1.5678\r\n",
      "\r\n",
      "Test set: Average loss: 1.4226, Accuracy: 23006/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 26 [640/110534 (1%)]\tClassification Loss: 1.2849\r\n",
      "Train Epoch: 26 [1280/110534 (1%)]\tClassification Loss: 1.8836\r\n",
      "Train Epoch: 26 [1920/110534 (2%)]\tClassification Loss: 1.7599\r\n",
      "Train Epoch: 26 [2560/110534 (2%)]\tClassification Loss: 1.7628\r\n",
      "Train Epoch: 26 [3200/110534 (3%)]\tClassification Loss: 1.3173\r\n",
      "Train Epoch: 26 [3840/110534 (3%)]\tClassification Loss: 1.6070\r\n",
      "Train Epoch: 26 [4480/110534 (4%)]\tClassification Loss: 1.5658\r\n",
      "Train Epoch: 26 [5120/110534 (5%)]\tClassification Loss: 1.5608\r\n",
      "Train Epoch: 26 [5760/110534 (5%)]\tClassification Loss: 1.5102\r\n",
      "Train Epoch: 26 [6400/110534 (6%)]\tClassification Loss: 1.2830\r\n",
      "Train Epoch: 26 [7040/110534 (6%)]\tClassification Loss: 1.6029\r\n",
      "Train Epoch: 26 [7680/110534 (7%)]\tClassification Loss: 1.5569\r\n",
      "Train Epoch: 26 [8320/110534 (8%)]\tClassification Loss: 1.6645\r\n",
      "Train Epoch: 26 [8960/110534 (8%)]\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 26 [9600/110534 (9%)]\tClassification Loss: 1.3654\r\n",
      "Train Epoch: 26 [10240/110534 (9%)]\tClassification Loss: 1.4680\r\n",
      "Train Epoch: 26 [10880/110534 (10%)]\tClassification Loss: 1.5651\r\n",
      "Train Epoch: 26 [11520/110534 (10%)]\tClassification Loss: 1.6891\r\n",
      "Train Epoch: 26 [12160/110534 (11%)]\tClassification Loss: 1.2874\r\n",
      "Train Epoch: 26 [12800/110534 (12%)]\tClassification Loss: 1.6894\r\n",
      "Train Epoch: 26 [13440/110534 (12%)]\tClassification Loss: 1.4082\r\n",
      "Train Epoch: 26 [14080/110534 (13%)]\tClassification Loss: 1.6126\r\n",
      "Train Epoch: 26 [14720/110534 (13%)]\tClassification Loss: 1.7457\r\n",
      "Train Epoch: 26 [15360/110534 (14%)]\tClassification Loss: 1.4880\r\n",
      "Train Epoch: 26 [16000/110534 (14%)]\tClassification Loss: 1.7497\r\n",
      "Train Epoch: 26 [16640/110534 (15%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 26 [17280/110534 (16%)]\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 26 [17920/110534 (16%)]\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 26 [18560/110534 (17%)]\tClassification Loss: 1.6534\r\n",
      "Train Epoch: 26 [19200/110534 (17%)]\tClassification Loss: 1.6318\r\n",
      "Train Epoch: 26 [19840/110534 (18%)]\tClassification Loss: 1.4914\r\n",
      "Train Epoch: 26 [20480/110534 (19%)]\tClassification Loss: 1.7347\r\n",
      "Train Epoch: 26 [21120/110534 (19%)]\tClassification Loss: 1.6447\r\n",
      "Train Epoch: 26 [21760/110534 (20%)]\tClassification Loss: 1.4233\r\n",
      "Train Epoch: 26 [22400/110534 (20%)]\tClassification Loss: 1.5909\r\n",
      "Train Epoch: 26 [23040/110534 (21%)]\tClassification Loss: 1.0838\r\n",
      "Train Epoch: 26 [23680/110534 (21%)]\tClassification Loss: 1.5639\r\n",
      "Train Epoch: 26 [24320/110534 (22%)]\tClassification Loss: 1.4375\r\n",
      "Train Epoch: 26 [24960/110534 (23%)]\tClassification Loss: 1.5787\r\n",
      "Train Epoch: 26 [25600/110534 (23%)]\tClassification Loss: 1.4238\r\n",
      "Train Epoch: 26 [26240/110534 (24%)]\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 26 [26880/110534 (24%)]\tClassification Loss: 1.7312\r\n",
      "Train Epoch: 26 [27520/110534 (25%)]\tClassification Loss: 1.4474\r\n",
      "Train Epoch: 26 [28160/110534 (25%)]\tClassification Loss: 1.8129\r\n",
      "Train Epoch: 26 [28800/110534 (26%)]\tClassification Loss: 1.7293\r\n",
      "Train Epoch: 26 [29440/110534 (27%)]\tClassification Loss: 1.4575\r\n",
      "Train Epoch: 26 [30080/110534 (27%)]\tClassification Loss: 1.9994\r\n",
      "Train Epoch: 26 [30720/110534 (28%)]\tClassification Loss: 1.3873\r\n",
      "Train Epoch: 26 [31360/110534 (28%)]\tClassification Loss: 1.5268\r\n",
      "Train Epoch: 26 [32000/110534 (29%)]\tClassification Loss: 1.4313\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_500.pth.tar\r\n",
      "Train Epoch: 26 [32640/110534 (30%)]\tClassification Loss: 1.3703\r\n",
      "Train Epoch: 26 [33280/110534 (30%)]\tClassification Loss: 1.4020\r\n",
      "Train Epoch: 26 [33920/110534 (31%)]\tClassification Loss: 1.5616\r\n",
      "Train Epoch: 26 [34560/110534 (31%)]\tClassification Loss: 1.7833\r\n",
      "Train Epoch: 26 [35200/110534 (32%)]\tClassification Loss: 1.5286\r\n",
      "Train Epoch: 26 [35840/110534 (32%)]\tClassification Loss: 1.1888\r\n",
      "Train Epoch: 26 [36480/110534 (33%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 26 [37120/110534 (34%)]\tClassification Loss: 1.5567\r\n",
      "Train Epoch: 26 [37760/110534 (34%)]\tClassification Loss: 1.9665\r\n",
      "Train Epoch: 26 [38400/110534 (35%)]\tClassification Loss: 1.4671\r\n",
      "Train Epoch: 26 [39040/110534 (35%)]\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 26 [39680/110534 (36%)]\tClassification Loss: 1.5647\r\n",
      "Train Epoch: 26 [40320/110534 (36%)]\tClassification Loss: 1.5313\r\n",
      "Train Epoch: 26 [40960/110534 (37%)]\tClassification Loss: 1.3660\r\n",
      "Train Epoch: 26 [41600/110534 (38%)]\tClassification Loss: 1.4800\r\n",
      "Train Epoch: 26 [42240/110534 (38%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 26 [42880/110534 (39%)]\tClassification Loss: 1.3474\r\n",
      "Train Epoch: 26 [43520/110534 (39%)]\tClassification Loss: 1.2980\r\n",
      "Train Epoch: 26 [44160/110534 (40%)]\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 26 [44800/110534 (41%)]\tClassification Loss: 1.6320\r\n",
      "Train Epoch: 26 [45440/110534 (41%)]\tClassification Loss: 1.4733\r\n",
      "Train Epoch: 26 [46080/110534 (42%)]\tClassification Loss: 1.4812\r\n",
      "Train Epoch: 26 [46720/110534 (42%)]\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 26 [47360/110534 (43%)]\tClassification Loss: 1.4510\r\n",
      "Train Epoch: 26 [48000/110534 (43%)]\tClassification Loss: 1.6576\r\n",
      "Train Epoch: 26 [48640/110534 (44%)]\tClassification Loss: 1.5419\r\n",
      "Train Epoch: 26 [49280/110534 (45%)]\tClassification Loss: 1.5299\r\n",
      "Train Epoch: 26 [49920/110534 (45%)]\tClassification Loss: 1.6925\r\n",
      "Train Epoch: 26 [50560/110534 (46%)]\tClassification Loss: 1.5806\r\n",
      "Train Epoch: 26 [51200/110534 (46%)]\tClassification Loss: 1.3668\r\n",
      "Train Epoch: 26 [51840/110534 (47%)]\tClassification Loss: 1.5747\r\n",
      "Train Epoch: 26 [52480/110534 (47%)]\tClassification Loss: 1.4395\r\n",
      "Train Epoch: 26 [53120/110534 (48%)]\tClassification Loss: 1.1456\r\n",
      "Train Epoch: 26 [53760/110534 (49%)]\tClassification Loss: 1.6948\r\n",
      "Train Epoch: 26 [54400/110534 (49%)]\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 26 [55040/110534 (50%)]\tClassification Loss: 1.6979\r\n",
      "Train Epoch: 26 [55680/110534 (50%)]\tClassification Loss: 1.6218\r\n",
      "Train Epoch: 26 [56320/110534 (51%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 26 [56960/110534 (52%)]\tClassification Loss: 1.5364\r\n",
      "Train Epoch: 26 [57600/110534 (52%)]\tClassification Loss: 1.6014\r\n",
      "Train Epoch: 26 [58240/110534 (53%)]\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 26 [58880/110534 (53%)]\tClassification Loss: 1.6792\r\n",
      "Train Epoch: 26 [59520/110534 (54%)]\tClassification Loss: 1.3026\r\n",
      "Train Epoch: 26 [60160/110534 (54%)]\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 26 [60800/110534 (55%)]\tClassification Loss: 1.5705\r\n",
      "Train Epoch: 26 [61440/110534 (56%)]\tClassification Loss: 1.4612\r\n",
      "Train Epoch: 26 [62080/110534 (56%)]\tClassification Loss: 1.4003\r\n",
      "Train Epoch: 26 [62720/110534 (57%)]\tClassification Loss: 1.4682\r\n",
      "Train Epoch: 26 [63360/110534 (57%)]\tClassification Loss: 1.2732\r\n",
      "Train Epoch: 26 [64000/110534 (58%)]\tClassification Loss: 1.6756\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_1000.pth.tar\r\n",
      "Train Epoch: 26 [64640/110534 (58%)]\tClassification Loss: 1.2511\r\n",
      "Train Epoch: 26 [65280/110534 (59%)]\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 26 [65920/110534 (60%)]\tClassification Loss: 1.3043\r\n",
      "Train Epoch: 26 [66560/110534 (60%)]\tClassification Loss: 1.3178\r\n",
      "Train Epoch: 26 [67200/110534 (61%)]\tClassification Loss: 1.3190\r\n",
      "Train Epoch: 26 [67840/110534 (61%)]\tClassification Loss: 1.8477\r\n",
      "Train Epoch: 26 [68480/110534 (62%)]\tClassification Loss: 1.5248\r\n",
      "Train Epoch: 26 [69120/110534 (63%)]\tClassification Loss: 1.8471\r\n",
      "Train Epoch: 26 [69760/110534 (63%)]\tClassification Loss: 1.4388\r\n",
      "Train Epoch: 26 [70400/110534 (64%)]\tClassification Loss: 1.2088\r\n",
      "Train Epoch: 26 [71040/110534 (64%)]\tClassification Loss: 1.6661\r\n",
      "Train Epoch: 26 [71680/110534 (65%)]\tClassification Loss: 1.4887\r\n",
      "Train Epoch: 26 [72320/110534 (65%)]\tClassification Loss: 1.5664\r\n",
      "Train Epoch: 26 [72960/110534 (66%)]\tClassification Loss: 1.8365\r\n",
      "Train Epoch: 26 [73600/110534 (67%)]\tClassification Loss: 1.6321\r\n",
      "Train Epoch: 26 [74240/110534 (67%)]\tClassification Loss: 1.6738\r\n",
      "Train Epoch: 26 [74880/110534 (68%)]\tClassification Loss: 1.1207\r\n",
      "Train Epoch: 26 [75520/110534 (68%)]\tClassification Loss: 1.3355\r\n",
      "Train Epoch: 26 [76160/110534 (69%)]\tClassification Loss: 1.3458\r\n",
      "Train Epoch: 26 [76800/110534 (69%)]\tClassification Loss: 1.5490\r\n",
      "Train Epoch: 26 [77440/110534 (70%)]\tClassification Loss: 1.4183\r\n",
      "Train Epoch: 26 [78080/110534 (71%)]\tClassification Loss: 1.5615\r\n",
      "Train Epoch: 26 [78720/110534 (71%)]\tClassification Loss: 1.4163\r\n",
      "Train Epoch: 26 [79360/110534 (72%)]\tClassification Loss: 1.4756\r\n",
      "Train Epoch: 26 [80000/110534 (72%)]\tClassification Loss: 1.3908\r\n",
      "Train Epoch: 26 [80640/110534 (73%)]\tClassification Loss: 1.4188\r\n",
      "Train Epoch: 26 [81280/110534 (74%)]\tClassification Loss: 1.9344\r\n",
      "Train Epoch: 26 [81920/110534 (74%)]\tClassification Loss: 1.5381\r\n",
      "Train Epoch: 26 [82560/110534 (75%)]\tClassification Loss: 1.5993\r\n",
      "Train Epoch: 26 [83200/110534 (75%)]\tClassification Loss: 1.4059\r\n",
      "Train Epoch: 26 [83840/110534 (76%)]\tClassification Loss: 1.6780\r\n",
      "Train Epoch: 26 [84480/110534 (76%)]\tClassification Loss: 1.5757\r\n",
      "Train Epoch: 26 [85120/110534 (77%)]\tClassification Loss: 1.5025\r\n",
      "Train Epoch: 26 [85760/110534 (78%)]\tClassification Loss: 1.4504\r\n",
      "Train Epoch: 26 [86400/110534 (78%)]\tClassification Loss: 1.5686\r\n",
      "Train Epoch: 26 [87040/110534 (79%)]\tClassification Loss: 1.5922\r\n",
      "Train Epoch: 26 [87680/110534 (79%)]\tClassification Loss: 1.3934\r\n",
      "Train Epoch: 26 [88320/110534 (80%)]\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 26 [88960/110534 (80%)]\tClassification Loss: 1.4541\r\n",
      "Train Epoch: 26 [89600/110534 (81%)]\tClassification Loss: 1.7195\r\n",
      "Train Epoch: 26 [90240/110534 (82%)]\tClassification Loss: 1.6767\r\n",
      "Train Epoch: 26 [90880/110534 (82%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 26 [91520/110534 (83%)]\tClassification Loss: 1.2553\r\n",
      "Train Epoch: 26 [92160/110534 (83%)]\tClassification Loss: 1.3206\r\n",
      "Train Epoch: 26 [92800/110534 (84%)]\tClassification Loss: 1.5589\r\n",
      "Train Epoch: 26 [93440/110534 (85%)]\tClassification Loss: 1.7129\r\n",
      "Train Epoch: 26 [94080/110534 (85%)]\tClassification Loss: 1.7670\r\n",
      "Train Epoch: 26 [94720/110534 (86%)]\tClassification Loss: 1.2226\r\n",
      "Train Epoch: 26 [95360/110534 (86%)]\tClassification Loss: 1.3125\r\n",
      "Train Epoch: 26 [96000/110534 (87%)]\tClassification Loss: 1.4338\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_1500.pth.tar\r\n",
      "Train Epoch: 26 [96640/110534 (87%)]\tClassification Loss: 1.2710\r\n",
      "Train Epoch: 26 [97280/110534 (88%)]\tClassification Loss: 1.2991\r\n",
      "Train Epoch: 26 [97920/110534 (89%)]\tClassification Loss: 1.3060\r\n",
      "Train Epoch: 26 [98560/110534 (89%)]\tClassification Loss: 1.6083\r\n",
      "Train Epoch: 26 [99200/110534 (90%)]\tClassification Loss: 1.6222\r\n",
      "Train Epoch: 26 [99840/110534 (90%)]\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 26 [100480/110534 (91%)]\tClassification Loss: 1.6552\r\n",
      "Train Epoch: 26 [101120/110534 (91%)]\tClassification Loss: 1.3655\r\n",
      "Train Epoch: 26 [101760/110534 (92%)]\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 26 [102400/110534 (93%)]\tClassification Loss: 1.6195\r\n",
      "Train Epoch: 26 [103040/110534 (93%)]\tClassification Loss: 1.4013\r\n",
      "Train Epoch: 26 [103680/110534 (94%)]\tClassification Loss: 1.7852\r\n",
      "Train Epoch: 26 [104320/110534 (94%)]\tClassification Loss: 1.2053\r\n",
      "Train Epoch: 26 [104960/110534 (95%)]\tClassification Loss: 1.4972\r\n",
      "Train Epoch: 26 [105600/110534 (96%)]\tClassification Loss: 1.3686\r\n",
      "Train Epoch: 26 [106240/110534 (96%)]\tClassification Loss: 1.2956\r\n",
      "Train Epoch: 26 [106880/110534 (97%)]\tClassification Loss: 1.6400\r\n",
      "Train Epoch: 26 [107520/110534 (97%)]\tClassification Loss: 1.7495\r\n",
      "Train Epoch: 26 [108160/110534 (98%)]\tClassification Loss: 1.4787\r\n",
      "Train Epoch: 26 [108800/110534 (98%)]\tClassification Loss: 1.7131\r\n",
      "Train Epoch: 26 [109440/110534 (99%)]\tClassification Loss: 1.5537\r\n",
      "Train Epoch: 26 [110080/110534 (100%)]\tClassification Loss: 1.3744\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_final.pth.tar\r\n",
      "Train Epoch: 27 [0/110534 (0%)]\tClassification Loss: 1.5648\r\n",
      "\r\n",
      "Test set: Average loss: 1.4293, Accuracy: 22892/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 27 [640/110534 (1%)]\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 27 [1280/110534 (1%)]\tClassification Loss: 2.0825\r\n",
      "Train Epoch: 27 [1920/110534 (2%)]\tClassification Loss: 1.5952\r\n",
      "Train Epoch: 27 [2560/110534 (2%)]\tClassification Loss: 1.7889\r\n",
      "Train Epoch: 27 [3200/110534 (3%)]\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 27 [3840/110534 (3%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 27 [4480/110534 (4%)]\tClassification Loss: 1.4115\r\n",
      "Train Epoch: 27 [5120/110534 (5%)]\tClassification Loss: 1.5494\r\n",
      "Train Epoch: 27 [5760/110534 (5%)]\tClassification Loss: 1.5504\r\n",
      "Train Epoch: 27 [6400/110534 (6%)]\tClassification Loss: 1.3227\r\n",
      "Train Epoch: 27 [7040/110534 (6%)]\tClassification Loss: 1.5149\r\n",
      "Train Epoch: 27 [7680/110534 (7%)]\tClassification Loss: 1.6439\r\n",
      "Train Epoch: 27 [8320/110534 (8%)]\tClassification Loss: 1.8115\r\n",
      "Train Epoch: 27 [8960/110534 (8%)]\tClassification Loss: 1.7777\r\n",
      "Train Epoch: 27 [9600/110534 (9%)]\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 27 [10240/110534 (9%)]\tClassification Loss: 1.5117\r\n",
      "Train Epoch: 27 [10880/110534 (10%)]\tClassification Loss: 1.5199\r\n",
      "Train Epoch: 27 [11520/110534 (10%)]\tClassification Loss: 1.4795\r\n",
      "Train Epoch: 27 [12160/110534 (11%)]\tClassification Loss: 1.4987\r\n",
      "Train Epoch: 27 [12800/110534 (12%)]\tClassification Loss: 1.5237\r\n",
      "Train Epoch: 27 [13440/110534 (12%)]\tClassification Loss: 1.4472\r\n",
      "Train Epoch: 27 [14080/110534 (13%)]\tClassification Loss: 1.6928\r\n",
      "Train Epoch: 27 [14720/110534 (13%)]\tClassification Loss: 1.7900\r\n",
      "Train Epoch: 27 [15360/110534 (14%)]\tClassification Loss: 1.6265\r\n",
      "Train Epoch: 27 [16000/110534 (14%)]\tClassification Loss: 1.8257\r\n",
      "Train Epoch: 27 [16640/110534 (15%)]\tClassification Loss: 1.4310\r\n",
      "Train Epoch: 27 [17280/110534 (16%)]\tClassification Loss: 1.5708\r\n",
      "Train Epoch: 27 [17920/110534 (16%)]\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 27 [18560/110534 (17%)]\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 27 [19200/110534 (17%)]\tClassification Loss: 1.6078\r\n",
      "Train Epoch: 27 [19840/110534 (18%)]\tClassification Loss: 1.5825\r\n",
      "Train Epoch: 27 [20480/110534 (19%)]\tClassification Loss: 1.8456\r\n",
      "Train Epoch: 27 [21120/110534 (19%)]\tClassification Loss: 1.7612\r\n",
      "Train Epoch: 27 [21760/110534 (20%)]\tClassification Loss: 1.3137\r\n",
      "Train Epoch: 27 [22400/110534 (20%)]\tClassification Loss: 1.5484\r\n",
      "Train Epoch: 27 [23040/110534 (21%)]\tClassification Loss: 1.1958\r\n",
      "Train Epoch: 27 [23680/110534 (21%)]\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 27 [24320/110534 (22%)]\tClassification Loss: 1.1498\r\n",
      "Train Epoch: 27 [24960/110534 (23%)]\tClassification Loss: 1.6571\r\n",
      "Train Epoch: 27 [25600/110534 (23%)]\tClassification Loss: 1.3594\r\n",
      "Train Epoch: 27 [26240/110534 (24%)]\tClassification Loss: 1.4044\r\n",
      "Train Epoch: 27 [26880/110534 (24%)]\tClassification Loss: 1.6250\r\n",
      "Train Epoch: 27 [27520/110534 (25%)]\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 27 [28160/110534 (25%)]\tClassification Loss: 1.6872\r\n",
      "Train Epoch: 27 [28800/110534 (26%)]\tClassification Loss: 1.7444\r\n",
      "Train Epoch: 27 [29440/110534 (27%)]\tClassification Loss: 1.5592\r\n",
      "Train Epoch: 27 [30080/110534 (27%)]\tClassification Loss: 1.9344\r\n",
      "Train Epoch: 27 [30720/110534 (28%)]\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 27 [31360/110534 (28%)]\tClassification Loss: 1.5372\r\n",
      "Train Epoch: 27 [32000/110534 (29%)]\tClassification Loss: 1.4095\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_500.pth.tar\r\n",
      "Train Epoch: 27 [32640/110534 (30%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 27 [33280/110534 (30%)]\tClassification Loss: 1.4847\r\n",
      "Train Epoch: 27 [33920/110534 (31%)]\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 27 [34560/110534 (31%)]\tClassification Loss: 1.7169\r\n",
      "Train Epoch: 27 [35200/110534 (32%)]\tClassification Loss: 1.4520\r\n",
      "Train Epoch: 27 [35840/110534 (32%)]\tClassification Loss: 1.2662\r\n",
      "Train Epoch: 27 [36480/110534 (33%)]\tClassification Loss: 1.4423\r\n",
      "Train Epoch: 27 [37120/110534 (34%)]\tClassification Loss: 1.7720\r\n",
      "Train Epoch: 27 [37760/110534 (34%)]\tClassification Loss: 1.8815\r\n",
      "Train Epoch: 27 [38400/110534 (35%)]\tClassification Loss: 1.4739\r\n",
      "Train Epoch: 27 [39040/110534 (35%)]\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 27 [39680/110534 (36%)]\tClassification Loss: 1.5904\r\n",
      "Train Epoch: 27 [40320/110534 (36%)]\tClassification Loss: 1.5562\r\n",
      "Train Epoch: 27 [40960/110534 (37%)]\tClassification Loss: 1.4786\r\n",
      "Train Epoch: 27 [41600/110534 (38%)]\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 27 [42240/110534 (38%)]\tClassification Loss: 1.3170\r\n",
      "Train Epoch: 27 [42880/110534 (39%)]\tClassification Loss: 1.3758\r\n",
      "Train Epoch: 27 [43520/110534 (39%)]\tClassification Loss: 1.3284\r\n",
      "Train Epoch: 27 [44160/110534 (40%)]\tClassification Loss: 1.3872\r\n",
      "Train Epoch: 27 [44800/110534 (41%)]\tClassification Loss: 1.7751\r\n",
      "Train Epoch: 27 [45440/110534 (41%)]\tClassification Loss: 1.5586\r\n",
      "Train Epoch: 27 [46080/110534 (42%)]\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 27 [46720/110534 (42%)]\tClassification Loss: 1.4836\r\n",
      "Train Epoch: 27 [47360/110534 (43%)]\tClassification Loss: 1.5208\r\n",
      "Train Epoch: 27 [48000/110534 (43%)]\tClassification Loss: 1.5758\r\n",
      "Train Epoch: 27 [48640/110534 (44%)]\tClassification Loss: 1.3383\r\n",
      "Train Epoch: 27 [49280/110534 (45%)]\tClassification Loss: 1.4222\r\n",
      "Train Epoch: 27 [49920/110534 (45%)]\tClassification Loss: 1.5915\r\n",
      "Train Epoch: 27 [50560/110534 (46%)]\tClassification Loss: 1.5678\r\n",
      "Train Epoch: 27 [51200/110534 (46%)]\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 27 [51840/110534 (47%)]\tClassification Loss: 1.4342\r\n",
      "Train Epoch: 27 [52480/110534 (47%)]\tClassification Loss: 1.5850\r\n",
      "Train Epoch: 27 [53120/110534 (48%)]\tClassification Loss: 1.1890\r\n",
      "Train Epoch: 27 [53760/110534 (49%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 27 [54400/110534 (49%)]\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 27 [55040/110534 (50%)]\tClassification Loss: 1.5210\r\n",
      "Train Epoch: 27 [55680/110534 (50%)]\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 27 [56320/110534 (51%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 27 [56960/110534 (52%)]\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 27 [57600/110534 (52%)]\tClassification Loss: 1.5298\r\n",
      "Train Epoch: 27 [58240/110534 (53%)]\tClassification Loss: 1.5644\r\n",
      "Train Epoch: 27 [58880/110534 (53%)]\tClassification Loss: 1.8236\r\n",
      "Train Epoch: 27 [59520/110534 (54%)]\tClassification Loss: 1.3713\r\n",
      "Train Epoch: 27 [60160/110534 (54%)]\tClassification Loss: 1.3568\r\n",
      "Train Epoch: 27 [60800/110534 (55%)]\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 27 [61440/110534 (56%)]\tClassification Loss: 1.5558\r\n",
      "Train Epoch: 27 [62080/110534 (56%)]\tClassification Loss: 1.4132\r\n",
      "Train Epoch: 27 [62720/110534 (57%)]\tClassification Loss: 1.5726\r\n",
      "Train Epoch: 27 [63360/110534 (57%)]\tClassification Loss: 1.3038\r\n",
      "Train Epoch: 27 [64000/110534 (58%)]\tClassification Loss: 1.5888\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_1000.pth.tar\r\n",
      "Train Epoch: 27 [64640/110534 (58%)]\tClassification Loss: 1.2619\r\n",
      "Train Epoch: 27 [65280/110534 (59%)]\tClassification Loss: 1.4648\r\n",
      "Train Epoch: 27 [65920/110534 (60%)]\tClassification Loss: 1.5891\r\n",
      "Train Epoch: 27 [66560/110534 (60%)]\tClassification Loss: 1.3623\r\n",
      "Train Epoch: 27 [67200/110534 (61%)]\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 27 [67840/110534 (61%)]\tClassification Loss: 1.8978\r\n",
      "Train Epoch: 27 [68480/110534 (62%)]\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 27 [69120/110534 (63%)]\tClassification Loss: 1.9264\r\n",
      "Train Epoch: 27 [69760/110534 (63%)]\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 27 [70400/110534 (64%)]\tClassification Loss: 1.1291\r\n",
      "Train Epoch: 27 [71040/110534 (64%)]\tClassification Loss: 1.8946\r\n",
      "Train Epoch: 27 [71680/110534 (65%)]\tClassification Loss: 1.5233\r\n",
      "Train Epoch: 27 [72320/110534 (65%)]\tClassification Loss: 1.6211\r\n",
      "Train Epoch: 27 [72960/110534 (66%)]\tClassification Loss: 1.6983\r\n",
      "Train Epoch: 27 [73600/110534 (67%)]\tClassification Loss: 1.8663\r\n",
      "Train Epoch: 27 [74240/110534 (67%)]\tClassification Loss: 1.5969\r\n",
      "Train Epoch: 27 [74880/110534 (68%)]\tClassification Loss: 1.0814\r\n",
      "Train Epoch: 27 [75520/110534 (68%)]\tClassification Loss: 1.3412\r\n",
      "Train Epoch: 27 [76160/110534 (69%)]\tClassification Loss: 1.4237\r\n",
      "Train Epoch: 27 [76800/110534 (69%)]\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 27 [77440/110534 (70%)]\tClassification Loss: 1.3661\r\n",
      "Train Epoch: 27 [78080/110534 (71%)]\tClassification Loss: 1.4903\r\n",
      "Train Epoch: 27 [78720/110534 (71%)]\tClassification Loss: 1.3914\r\n",
      "Train Epoch: 27 [79360/110534 (72%)]\tClassification Loss: 1.2502\r\n",
      "Train Epoch: 27 [80000/110534 (72%)]\tClassification Loss: 1.2960\r\n",
      "Train Epoch: 27 [80640/110534 (73%)]\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 27 [81280/110534 (74%)]\tClassification Loss: 1.8970\r\n",
      "Train Epoch: 27 [81920/110534 (74%)]\tClassification Loss: 1.5249\r\n",
      "Train Epoch: 27 [82560/110534 (75%)]\tClassification Loss: 1.6236\r\n",
      "Train Epoch: 27 [83200/110534 (75%)]\tClassification Loss: 1.4770\r\n",
      "Train Epoch: 27 [83840/110534 (76%)]\tClassification Loss: 1.7622\r\n",
      "Train Epoch: 27 [84480/110534 (76%)]\tClassification Loss: 1.5350\r\n",
      "Train Epoch: 27 [85120/110534 (77%)]\tClassification Loss: 1.3725\r\n",
      "Train Epoch: 27 [85760/110534 (78%)]\tClassification Loss: 1.4543\r\n",
      "Train Epoch: 27 [86400/110534 (78%)]\tClassification Loss: 1.6256\r\n",
      "Train Epoch: 27 [87040/110534 (79%)]\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 27 [87680/110534 (79%)]\tClassification Loss: 1.3727\r\n",
      "Train Epoch: 27 [88320/110534 (80%)]\tClassification Loss: 1.4917\r\n",
      "Train Epoch: 27 [88960/110534 (80%)]\tClassification Loss: 1.6408\r\n",
      "Train Epoch: 27 [89600/110534 (81%)]\tClassification Loss: 1.7988\r\n",
      "Train Epoch: 27 [90240/110534 (82%)]\tClassification Loss: 1.8596\r\n",
      "Train Epoch: 27 [90880/110534 (82%)]\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 27 [91520/110534 (83%)]\tClassification Loss: 1.1921\r\n",
      "Train Epoch: 27 [92160/110534 (83%)]\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 27 [92800/110534 (84%)]\tClassification Loss: 1.4644\r\n",
      "Train Epoch: 27 [93440/110534 (85%)]\tClassification Loss: 1.6402\r\n",
      "Train Epoch: 27 [94080/110534 (85%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 27 [94720/110534 (86%)]\tClassification Loss: 1.3870\r\n",
      "Train Epoch: 27 [95360/110534 (86%)]\tClassification Loss: 1.4879\r\n",
      "Train Epoch: 27 [96000/110534 (87%)]\tClassification Loss: 1.3688\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_1500.pth.tar\r\n",
      "Train Epoch: 27 [96640/110534 (87%)]\tClassification Loss: 1.3086\r\n",
      "Train Epoch: 27 [97280/110534 (88%)]\tClassification Loss: 1.3235\r\n",
      "Train Epoch: 27 [97920/110534 (89%)]\tClassification Loss: 1.2185\r\n",
      "Train Epoch: 27 [98560/110534 (89%)]\tClassification Loss: 1.5088\r\n",
      "Train Epoch: 27 [99200/110534 (90%)]\tClassification Loss: 1.5043\r\n",
      "Train Epoch: 27 [99840/110534 (90%)]\tClassification Loss: 1.5174\r\n",
      "Train Epoch: 27 [100480/110534 (91%)]\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 27 [101120/110534 (91%)]\tClassification Loss: 1.3961\r\n",
      "Train Epoch: 27 [101760/110534 (92%)]\tClassification Loss: 1.6069\r\n",
      "Train Epoch: 27 [102400/110534 (93%)]\tClassification Loss: 1.4665\r\n",
      "Train Epoch: 27 [103040/110534 (93%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 27 [103680/110534 (94%)]\tClassification Loss: 1.8251\r\n",
      "Train Epoch: 27 [104320/110534 (94%)]\tClassification Loss: 1.3164\r\n",
      "Train Epoch: 27 [104960/110534 (95%)]\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 27 [105600/110534 (96%)]\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 27 [106240/110534 (96%)]\tClassification Loss: 1.4678\r\n",
      "Train Epoch: 27 [106880/110534 (97%)]\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 27 [107520/110534 (97%)]\tClassification Loss: 1.6937\r\n",
      "Train Epoch: 27 [108160/110534 (98%)]\tClassification Loss: 1.4151\r\n",
      "Train Epoch: 27 [108800/110534 (98%)]\tClassification Loss: 1.6234\r\n",
      "Train Epoch: 27 [109440/110534 (99%)]\tClassification Loss: 1.6628\r\n",
      "Train Epoch: 27 [110080/110534 (100%)]\tClassification Loss: 1.4457\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_final.pth.tar\r\n",
      "Train Epoch: 28 [0/110534 (0%)]\tClassification Loss: 1.5025\r\n",
      "\r\n",
      "Test set: Average loss: 1.4253, Accuracy: 22955/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 28 [640/110534 (1%)]\tClassification Loss: 1.2797\r\n",
      "Train Epoch: 28 [1280/110534 (1%)]\tClassification Loss: 1.9167\r\n",
      "Train Epoch: 28 [1920/110534 (2%)]\tClassification Loss: 1.5716\r\n",
      "Train Epoch: 28 [2560/110534 (2%)]\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 28 [3200/110534 (3%)]\tClassification Loss: 1.4617\r\n",
      "Train Epoch: 28 [3840/110534 (3%)]\tClassification Loss: 1.2493\r\n",
      "Train Epoch: 28 [4480/110534 (4%)]\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 28 [5120/110534 (5%)]\tClassification Loss: 1.6441\r\n",
      "Train Epoch: 28 [5760/110534 (5%)]\tClassification Loss: 1.6038\r\n",
      "Train Epoch: 28 [6400/110534 (6%)]\tClassification Loss: 1.2692\r\n",
      "Train Epoch: 28 [7040/110534 (6%)]\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 28 [7680/110534 (7%)]\tClassification Loss: 1.5098\r\n",
      "Train Epoch: 28 [8320/110534 (8%)]\tClassification Loss: 1.6916\r\n",
      "Train Epoch: 28 [8960/110534 (8%)]\tClassification Loss: 1.7643\r\n",
      "Train Epoch: 28 [9600/110534 (9%)]\tClassification Loss: 1.3267\r\n",
      "Train Epoch: 28 [10240/110534 (9%)]\tClassification Loss: 1.4590\r\n",
      "Train Epoch: 28 [10880/110534 (10%)]\tClassification Loss: 1.5201\r\n",
      "Train Epoch: 28 [11520/110534 (10%)]\tClassification Loss: 1.5038\r\n",
      "Train Epoch: 28 [12160/110534 (11%)]\tClassification Loss: 1.2486\r\n",
      "Train Epoch: 28 [12800/110534 (12%)]\tClassification Loss: 1.8680\r\n",
      "Train Epoch: 28 [13440/110534 (12%)]\tClassification Loss: 1.2697\r\n",
      "Train Epoch: 28 [14080/110534 (13%)]\tClassification Loss: 1.6444\r\n",
      "Train Epoch: 28 [14720/110534 (13%)]\tClassification Loss: 1.7265\r\n",
      "Train Epoch: 28 [15360/110534 (14%)]\tClassification Loss: 1.6863\r\n",
      "Train Epoch: 28 [16000/110534 (14%)]\tClassification Loss: 1.7204\r\n",
      "Train Epoch: 28 [16640/110534 (15%)]\tClassification Loss: 1.5546\r\n",
      "Train Epoch: 28 [17280/110534 (16%)]\tClassification Loss: 1.6628\r\n",
      "Train Epoch: 28 [17920/110534 (16%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 28 [18560/110534 (17%)]\tClassification Loss: 1.7605\r\n",
      "Train Epoch: 28 [19200/110534 (17%)]\tClassification Loss: 1.6890\r\n",
      "Train Epoch: 28 [19840/110534 (18%)]\tClassification Loss: 1.6189\r\n",
      "Train Epoch: 28 [20480/110534 (19%)]\tClassification Loss: 1.8096\r\n",
      "Train Epoch: 28 [21120/110534 (19%)]\tClassification Loss: 1.7031\r\n",
      "Train Epoch: 28 [21760/110534 (20%)]\tClassification Loss: 1.2793\r\n",
      "Train Epoch: 28 [22400/110534 (20%)]\tClassification Loss: 1.4983\r\n",
      "Train Epoch: 28 [23040/110534 (21%)]\tClassification Loss: 1.1278\r\n",
      "Train Epoch: 28 [23680/110534 (21%)]\tClassification Loss: 1.5692\r\n",
      "Train Epoch: 28 [24320/110534 (22%)]\tClassification Loss: 1.4253\r\n",
      "Train Epoch: 28 [24960/110534 (23%)]\tClassification Loss: 1.7967\r\n",
      "Train Epoch: 28 [25600/110534 (23%)]\tClassification Loss: 1.2416\r\n",
      "Train Epoch: 28 [26240/110534 (24%)]\tClassification Loss: 1.4239\r\n",
      "Train Epoch: 28 [26880/110534 (24%)]\tClassification Loss: 1.6443\r\n",
      "Train Epoch: 28 [27520/110534 (25%)]\tClassification Loss: 1.2116\r\n",
      "Train Epoch: 28 [28160/110534 (25%)]\tClassification Loss: 1.8151\r\n",
      "Train Epoch: 28 [28800/110534 (26%)]\tClassification Loss: 1.7496\r\n",
      "Train Epoch: 28 [29440/110534 (27%)]\tClassification Loss: 1.7140\r\n",
      "Train Epoch: 28 [30080/110534 (27%)]\tClassification Loss: 1.6622\r\n",
      "Train Epoch: 28 [30720/110534 (28%)]\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 28 [31360/110534 (28%)]\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 28 [32000/110534 (29%)]\tClassification Loss: 1.5806\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_500.pth.tar\r\n",
      "Train Epoch: 28 [32640/110534 (30%)]\tClassification Loss: 1.3731\r\n",
      "Train Epoch: 28 [33280/110534 (30%)]\tClassification Loss: 1.4725\r\n",
      "Train Epoch: 28 [33920/110534 (31%)]\tClassification Loss: 1.5314\r\n",
      "Train Epoch: 28 [34560/110534 (31%)]\tClassification Loss: 1.9021\r\n",
      "Train Epoch: 28 [35200/110534 (32%)]\tClassification Loss: 1.4188\r\n",
      "Train Epoch: 28 [35840/110534 (32%)]\tClassification Loss: 1.2786\r\n",
      "Train Epoch: 28 [36480/110534 (33%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 28 [37120/110534 (34%)]\tClassification Loss: 1.5998\r\n",
      "Train Epoch: 28 [37760/110534 (34%)]\tClassification Loss: 1.8782\r\n",
      "Train Epoch: 28 [38400/110534 (35%)]\tClassification Loss: 1.4930\r\n",
      "Train Epoch: 28 [39040/110534 (35%)]\tClassification Loss: 1.5058\r\n",
      "Train Epoch: 28 [39680/110534 (36%)]\tClassification Loss: 1.6903\r\n",
      "Train Epoch: 28 [40320/110534 (36%)]\tClassification Loss: 1.5805\r\n",
      "Train Epoch: 28 [40960/110534 (37%)]\tClassification Loss: 1.2969\r\n",
      "Train Epoch: 28 [41600/110534 (38%)]\tClassification Loss: 1.4126\r\n",
      "Train Epoch: 28 [42240/110534 (38%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 28 [42880/110534 (39%)]\tClassification Loss: 1.4096\r\n",
      "Train Epoch: 28 [43520/110534 (39%)]\tClassification Loss: 1.1742\r\n",
      "Train Epoch: 28 [44160/110534 (40%)]\tClassification Loss: 1.5172\r\n",
      "Train Epoch: 28 [44800/110534 (41%)]\tClassification Loss: 1.6773\r\n",
      "Train Epoch: 28 [45440/110534 (41%)]\tClassification Loss: 1.5572\r\n",
      "Train Epoch: 28 [46080/110534 (42%)]\tClassification Loss: 1.3644\r\n",
      "Train Epoch: 28 [46720/110534 (42%)]\tClassification Loss: 1.5828\r\n",
      "Train Epoch: 28 [47360/110534 (43%)]\tClassification Loss: 1.5389\r\n",
      "Train Epoch: 28 [48000/110534 (43%)]\tClassification Loss: 1.7105\r\n",
      "Train Epoch: 28 [48640/110534 (44%)]\tClassification Loss: 1.2972\r\n",
      "Train Epoch: 28 [49280/110534 (45%)]\tClassification Loss: 1.5718\r\n",
      "Train Epoch: 28 [49920/110534 (45%)]\tClassification Loss: 1.6478\r\n",
      "Train Epoch: 28 [50560/110534 (46%)]\tClassification Loss: 1.4933\r\n",
      "Train Epoch: 28 [51200/110534 (46%)]\tClassification Loss: 1.3622\r\n",
      "Train Epoch: 28 [51840/110534 (47%)]\tClassification Loss: 1.3759\r\n",
      "Train Epoch: 28 [52480/110534 (47%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 28 [53120/110534 (48%)]\tClassification Loss: 1.1919\r\n",
      "Train Epoch: 28 [53760/110534 (49%)]\tClassification Loss: 1.4890\r\n",
      "Train Epoch: 28 [54400/110534 (49%)]\tClassification Loss: 1.6279\r\n",
      "Train Epoch: 28 [55040/110534 (50%)]\tClassification Loss: 1.5937\r\n",
      "Train Epoch: 28 [55680/110534 (50%)]\tClassification Loss: 1.6964\r\n",
      "Train Epoch: 28 [56320/110534 (51%)]\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 28 [56960/110534 (52%)]\tClassification Loss: 1.4969\r\n",
      "Train Epoch: 28 [57600/110534 (52%)]\tClassification Loss: 1.7001\r\n",
      "Train Epoch: 28 [58240/110534 (53%)]\tClassification Loss: 1.6066\r\n",
      "Train Epoch: 28 [58880/110534 (53%)]\tClassification Loss: 1.7716\r\n",
      "Train Epoch: 28 [59520/110534 (54%)]\tClassification Loss: 1.4185\r\n",
      "Train Epoch: 28 [60160/110534 (54%)]\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 28 [60800/110534 (55%)]\tClassification Loss: 1.3785\r\n",
      "Train Epoch: 28 [61440/110534 (56%)]\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 28 [62080/110534 (56%)]\tClassification Loss: 1.4945\r\n",
      "Train Epoch: 28 [62720/110534 (57%)]\tClassification Loss: 1.5689\r\n",
      "Train Epoch: 28 [63360/110534 (57%)]\tClassification Loss: 1.2450\r\n",
      "Train Epoch: 28 [64000/110534 (58%)]\tClassification Loss: 1.5891\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_1000.pth.tar\r\n",
      "Train Epoch: 28 [64640/110534 (58%)]\tClassification Loss: 1.1867\r\n",
      "Train Epoch: 28 [65280/110534 (59%)]\tClassification Loss: 1.4701\r\n",
      "Train Epoch: 28 [65920/110534 (60%)]\tClassification Loss: 1.5150\r\n",
      "Train Epoch: 28 [66560/110534 (60%)]\tClassification Loss: 1.3825\r\n",
      "Train Epoch: 28 [67200/110534 (61%)]\tClassification Loss: 1.4122\r\n",
      "Train Epoch: 28 [67840/110534 (61%)]\tClassification Loss: 1.7355\r\n",
      "Train Epoch: 28 [68480/110534 (62%)]\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 28 [69120/110534 (63%)]\tClassification Loss: 1.8566\r\n",
      "Train Epoch: 28 [69760/110534 (63%)]\tClassification Loss: 1.7141\r\n",
      "Train Epoch: 28 [70400/110534 (64%)]\tClassification Loss: 1.3329\r\n",
      "Train Epoch: 28 [71040/110534 (64%)]\tClassification Loss: 1.9241\r\n",
      "Train Epoch: 28 [71680/110534 (65%)]\tClassification Loss: 1.4778\r\n",
      "Train Epoch: 28 [72320/110534 (65%)]\tClassification Loss: 1.5866\r\n",
      "Train Epoch: 28 [72960/110534 (66%)]\tClassification Loss: 1.8859\r\n",
      "Train Epoch: 28 [73600/110534 (67%)]\tClassification Loss: 1.7574\r\n",
      "Train Epoch: 28 [74240/110534 (67%)]\tClassification Loss: 1.5827\r\n",
      "Train Epoch: 28 [74880/110534 (68%)]\tClassification Loss: 1.1861\r\n",
      "Train Epoch: 28 [75520/110534 (68%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 28 [76160/110534 (69%)]\tClassification Loss: 1.4607\r\n",
      "Train Epoch: 28 [76800/110534 (69%)]\tClassification Loss: 1.2741\r\n",
      "Train Epoch: 28 [77440/110534 (70%)]\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 28 [78080/110534 (71%)]\tClassification Loss: 1.3854\r\n",
      "Train Epoch: 28 [78720/110534 (71%)]\tClassification Loss: 1.4052\r\n",
      "Train Epoch: 28 [79360/110534 (72%)]\tClassification Loss: 1.4530\r\n",
      "Train Epoch: 28 [80000/110534 (72%)]\tClassification Loss: 1.5157\r\n",
      "Train Epoch: 28 [80640/110534 (73%)]\tClassification Loss: 1.3786\r\n",
      "Train Epoch: 28 [81280/110534 (74%)]\tClassification Loss: 1.8221\r\n",
      "Train Epoch: 28 [81920/110534 (74%)]\tClassification Loss: 1.5170\r\n",
      "Train Epoch: 28 [82560/110534 (75%)]\tClassification Loss: 1.5669\r\n",
      "Train Epoch: 28 [83200/110534 (75%)]\tClassification Loss: 1.3487\r\n",
      "Train Epoch: 28 [83840/110534 (76%)]\tClassification Loss: 1.8726\r\n",
      "Train Epoch: 28 [84480/110534 (76%)]\tClassification Loss: 1.6620\r\n",
      "Train Epoch: 28 [85120/110534 (77%)]\tClassification Loss: 1.5369\r\n",
      "Train Epoch: 28 [85760/110534 (78%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 28 [86400/110534 (78%)]\tClassification Loss: 1.7333\r\n",
      "Train Epoch: 28 [87040/110534 (79%)]\tClassification Loss: 1.5301\r\n",
      "Train Epoch: 28 [87680/110534 (79%)]\tClassification Loss: 1.4456\r\n",
      "Train Epoch: 28 [88320/110534 (80%)]\tClassification Loss: 1.5277\r\n",
      "Train Epoch: 28 [88960/110534 (80%)]\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 28 [89600/110534 (81%)]\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 28 [90240/110534 (82%)]\tClassification Loss: 1.8157\r\n",
      "Train Epoch: 28 [90880/110534 (82%)]\tClassification Loss: 1.6779\r\n",
      "Train Epoch: 28 [91520/110534 (83%)]\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 28 [92160/110534 (83%)]\tClassification Loss: 1.3006\r\n",
      "Train Epoch: 28 [92800/110534 (84%)]\tClassification Loss: 1.2757\r\n",
      "Train Epoch: 28 [93440/110534 (85%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 28 [94080/110534 (85%)]\tClassification Loss: 1.7526\r\n",
      "Train Epoch: 28 [94720/110534 (86%)]\tClassification Loss: 1.3485\r\n",
      "Train Epoch: 28 [95360/110534 (86%)]\tClassification Loss: 1.3855\r\n",
      "Train Epoch: 28 [96000/110534 (87%)]\tClassification Loss: 1.4668\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_1500.pth.tar\r\n",
      "Train Epoch: 28 [96640/110534 (87%)]\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 28 [97280/110534 (88%)]\tClassification Loss: 1.3482\r\n",
      "Train Epoch: 28 [97920/110534 (89%)]\tClassification Loss: 1.2888\r\n",
      "Train Epoch: 28 [98560/110534 (89%)]\tClassification Loss: 1.4067\r\n",
      "Train Epoch: 28 [99200/110534 (90%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 28 [99840/110534 (90%)]\tClassification Loss: 1.3628\r\n",
      "Train Epoch: 28 [100480/110534 (91%)]\tClassification Loss: 1.5796\r\n",
      "Train Epoch: 28 [101120/110534 (91%)]\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 28 [101760/110534 (92%)]\tClassification Loss: 1.6730\r\n",
      "Train Epoch: 28 [102400/110534 (93%)]\tClassification Loss: 1.6084\r\n",
      "Train Epoch: 28 [103040/110534 (93%)]\tClassification Loss: 1.5526\r\n",
      "Train Epoch: 28 [103680/110534 (94%)]\tClassification Loss: 1.8245\r\n",
      "Train Epoch: 28 [104320/110534 (94%)]\tClassification Loss: 1.3858\r\n",
      "Train Epoch: 28 [104960/110534 (95%)]\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 28 [105600/110534 (96%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 28 [106240/110534 (96%)]\tClassification Loss: 1.3233\r\n",
      "Train Epoch: 28 [106880/110534 (97%)]\tClassification Loss: 1.6018\r\n",
      "Train Epoch: 28 [107520/110534 (97%)]\tClassification Loss: 1.7283\r\n",
      "Train Epoch: 28 [108160/110534 (98%)]\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 28 [108800/110534 (98%)]\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 28 [109440/110534 (99%)]\tClassification Loss: 1.2824\r\n",
      "Train Epoch: 28 [110080/110534 (100%)]\tClassification Loss: 1.3985\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_final.pth.tar\r\n",
      "Train Epoch: 29 [0/110534 (0%)]\tClassification Loss: 1.5827\r\n",
      "\r\n",
      "Test set: Average loss: 1.4235, Accuracy: 22940/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 29 [640/110534 (1%)]\tClassification Loss: 1.3655\r\n",
      "Train Epoch: 29 [1280/110534 (1%)]\tClassification Loss: 2.1513\r\n",
      "Train Epoch: 29 [1920/110534 (2%)]\tClassification Loss: 1.5692\r\n",
      "Train Epoch: 29 [2560/110534 (2%)]\tClassification Loss: 1.7461\r\n",
      "Train Epoch: 29 [3200/110534 (3%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 29 [3840/110534 (3%)]\tClassification Loss: 1.4814\r\n",
      "Train Epoch: 29 [4480/110534 (4%)]\tClassification Loss: 1.5840\r\n",
      "Train Epoch: 29 [5120/110534 (5%)]\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 29 [5760/110534 (5%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 29 [6400/110534 (6%)]\tClassification Loss: 1.3274\r\n",
      "Train Epoch: 29 [7040/110534 (6%)]\tClassification Loss: 1.6070\r\n",
      "Train Epoch: 29 [7680/110534 (7%)]\tClassification Loss: 1.6654\r\n",
      "Train Epoch: 29 [8320/110534 (8%)]\tClassification Loss: 1.8333\r\n",
      "Train Epoch: 29 [8960/110534 (8%)]\tClassification Loss: 1.5643\r\n",
      "Train Epoch: 29 [9600/110534 (9%)]\tClassification Loss: 1.5174\r\n",
      "Train Epoch: 29 [10240/110534 (9%)]\tClassification Loss: 1.5198\r\n",
      "Train Epoch: 29 [10880/110534 (10%)]\tClassification Loss: 1.3341\r\n",
      "Train Epoch: 29 [11520/110534 (10%)]\tClassification Loss: 1.7048\r\n",
      "Train Epoch: 29 [12160/110534 (11%)]\tClassification Loss: 1.4307\r\n",
      "Train Epoch: 29 [12800/110534 (12%)]\tClassification Loss: 1.7053\r\n",
      "Train Epoch: 29 [13440/110534 (12%)]\tClassification Loss: 1.3687\r\n",
      "Train Epoch: 29 [14080/110534 (13%)]\tClassification Loss: 1.4917\r\n",
      "Train Epoch: 29 [14720/110534 (13%)]\tClassification Loss: 1.7173\r\n",
      "Train Epoch: 29 [15360/110534 (14%)]\tClassification Loss: 1.4206\r\n",
      "Train Epoch: 29 [16000/110534 (14%)]\tClassification Loss: 1.6036\r\n",
      "Train Epoch: 29 [16640/110534 (15%)]\tClassification Loss: 1.4400\r\n",
      "Train Epoch: 29 [17280/110534 (16%)]\tClassification Loss: 1.7785\r\n",
      "Train Epoch: 29 [17920/110534 (16%)]\tClassification Loss: 1.6096\r\n",
      "Train Epoch: 29 [18560/110534 (17%)]\tClassification Loss: 1.7610\r\n",
      "Train Epoch: 29 [19200/110534 (17%)]\tClassification Loss: 1.4656\r\n",
      "Train Epoch: 29 [19840/110534 (18%)]\tClassification Loss: 1.5028\r\n",
      "Train Epoch: 29 [20480/110534 (19%)]\tClassification Loss: 1.7623\r\n",
      "Train Epoch: 29 [21120/110534 (19%)]\tClassification Loss: 1.7656\r\n",
      "Train Epoch: 29 [21760/110534 (20%)]\tClassification Loss: 1.4418\r\n",
      "Train Epoch: 29 [22400/110534 (20%)]\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 29 [23040/110534 (21%)]\tClassification Loss: 1.1624\r\n",
      "Train Epoch: 29 [23680/110534 (21%)]\tClassification Loss: 1.7068\r\n",
      "Train Epoch: 29 [24320/110534 (22%)]\tClassification Loss: 1.2483\r\n",
      "Train Epoch: 29 [24960/110534 (23%)]\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 29 [25600/110534 (23%)]\tClassification Loss: 1.4000\r\n",
      "Train Epoch: 29 [26240/110534 (24%)]\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 29 [26880/110534 (24%)]\tClassification Loss: 1.5278\r\n",
      "Train Epoch: 29 [27520/110534 (25%)]\tClassification Loss: 1.3680\r\n",
      "Train Epoch: 29 [28160/110534 (25%)]\tClassification Loss: 1.7483\r\n",
      "Train Epoch: 29 [28800/110534 (26%)]\tClassification Loss: 1.8273\r\n",
      "Train Epoch: 29 [29440/110534 (27%)]\tClassification Loss: 1.6328\r\n",
      "Train Epoch: 29 [30080/110534 (27%)]\tClassification Loss: 1.6855\r\n",
      "Train Epoch: 29 [30720/110534 (28%)]\tClassification Loss: 1.4795\r\n",
      "Train Epoch: 29 [31360/110534 (28%)]\tClassification Loss: 1.5709\r\n",
      "Train Epoch: 29 [32000/110534 (29%)]\tClassification Loss: 1.4166\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_500.pth.tar\r\n",
      "Train Epoch: 29 [32640/110534 (30%)]\tClassification Loss: 1.5136\r\n",
      "Train Epoch: 29 [33280/110534 (30%)]\tClassification Loss: 1.4800\r\n",
      "Train Epoch: 29 [33920/110534 (31%)]\tClassification Loss: 1.5097\r\n",
      "Train Epoch: 29 [34560/110534 (31%)]\tClassification Loss: 1.7689\r\n",
      "Train Epoch: 29 [35200/110534 (32%)]\tClassification Loss: 1.4443\r\n",
      "Train Epoch: 29 [35840/110534 (32%)]\tClassification Loss: 1.2965\r\n",
      "Train Epoch: 29 [36480/110534 (33%)]\tClassification Loss: 1.4154\r\n",
      "Train Epoch: 29 [37120/110534 (34%)]\tClassification Loss: 1.7174\r\n",
      "Train Epoch: 29 [37760/110534 (34%)]\tClassification Loss: 1.9165\r\n",
      "Train Epoch: 29 [38400/110534 (35%)]\tClassification Loss: 1.3056\r\n",
      "Train Epoch: 29 [39040/110534 (35%)]\tClassification Loss: 1.5021\r\n",
      "Train Epoch: 29 [39680/110534 (36%)]\tClassification Loss: 1.5243\r\n",
      "Train Epoch: 29 [40320/110534 (36%)]\tClassification Loss: 1.6043\r\n",
      "Train Epoch: 29 [40960/110534 (37%)]\tClassification Loss: 1.4570\r\n",
      "Train Epoch: 29 [41600/110534 (38%)]\tClassification Loss: 1.3163\r\n",
      "Train Epoch: 29 [42240/110534 (38%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 29 [42880/110534 (39%)]\tClassification Loss: 1.4349\r\n",
      "Train Epoch: 29 [43520/110534 (39%)]\tClassification Loss: 1.3607\r\n",
      "Train Epoch: 29 [44160/110534 (40%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 29 [44800/110534 (41%)]\tClassification Loss: 1.6410\r\n",
      "Train Epoch: 29 [45440/110534 (41%)]\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 29 [46080/110534 (42%)]\tClassification Loss: 1.3720\r\n",
      "Train Epoch: 29 [46720/110534 (42%)]\tClassification Loss: 1.5319\r\n",
      "Train Epoch: 29 [47360/110534 (43%)]\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 29 [48000/110534 (43%)]\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 29 [48640/110534 (44%)]\tClassification Loss: 1.4273\r\n",
      "Train Epoch: 29 [49280/110534 (45%)]\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 29 [49920/110534 (45%)]\tClassification Loss: 1.4318\r\n",
      "Train Epoch: 29 [50560/110534 (46%)]\tClassification Loss: 1.6413\r\n",
      "Train Epoch: 29 [51200/110534 (46%)]\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 29 [51840/110534 (47%)]\tClassification Loss: 1.5355\r\n",
      "Train Epoch: 29 [52480/110534 (47%)]\tClassification Loss: 1.4571\r\n",
      "Train Epoch: 29 [53120/110534 (48%)]\tClassification Loss: 1.2299\r\n",
      "Train Epoch: 29 [53760/110534 (49%)]\tClassification Loss: 1.3968\r\n",
      "Train Epoch: 29 [54400/110534 (49%)]\tClassification Loss: 1.3819\r\n",
      "Train Epoch: 29 [55040/110534 (50%)]\tClassification Loss: 1.7522\r\n",
      "Train Epoch: 29 [55680/110534 (50%)]\tClassification Loss: 1.6257\r\n",
      "Train Epoch: 29 [56320/110534 (51%)]\tClassification Loss: 1.3968\r\n",
      "Train Epoch: 29 [56960/110534 (52%)]\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 29 [57600/110534 (52%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 29 [58240/110534 (53%)]\tClassification Loss: 1.4009\r\n",
      "Train Epoch: 29 [58880/110534 (53%)]\tClassification Loss: 1.9025\r\n",
      "Train Epoch: 29 [59520/110534 (54%)]\tClassification Loss: 1.4074\r\n",
      "Train Epoch: 29 [60160/110534 (54%)]\tClassification Loss: 1.3475\r\n",
      "Train Epoch: 29 [60800/110534 (55%)]\tClassification Loss: 1.5962\r\n",
      "Train Epoch: 29 [61440/110534 (56%)]\tClassification Loss: 1.5362\r\n",
      "Train Epoch: 29 [62080/110534 (56%)]\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 29 [62720/110534 (57%)]\tClassification Loss: 1.5030\r\n",
      "Train Epoch: 29 [63360/110534 (57%)]\tClassification Loss: 1.2404\r\n",
      "Train Epoch: 29 [64000/110534 (58%)]\tClassification Loss: 1.5753\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_1000.pth.tar\r\n",
      "Train Epoch: 29 [64640/110534 (58%)]\tClassification Loss: 1.3108\r\n",
      "Train Epoch: 29 [65280/110534 (59%)]\tClassification Loss: 1.3760\r\n",
      "Train Epoch: 29 [65920/110534 (60%)]\tClassification Loss: 1.4788\r\n",
      "Train Epoch: 29 [66560/110534 (60%)]\tClassification Loss: 1.3782\r\n",
      "Train Epoch: 29 [67200/110534 (61%)]\tClassification Loss: 1.3506\r\n",
      "Train Epoch: 29 [67840/110534 (61%)]\tClassification Loss: 1.9470\r\n",
      "Train Epoch: 29 [68480/110534 (62%)]\tClassification Loss: 1.6331\r\n",
      "Train Epoch: 29 [69120/110534 (63%)]\tClassification Loss: 1.7421\r\n",
      "Train Epoch: 29 [69760/110534 (63%)]\tClassification Loss: 1.6411\r\n",
      "Train Epoch: 29 [70400/110534 (64%)]\tClassification Loss: 1.2523\r\n",
      "Train Epoch: 29 [71040/110534 (64%)]\tClassification Loss: 1.8157\r\n",
      "Train Epoch: 29 [71680/110534 (65%)]\tClassification Loss: 1.4671\r\n",
      "Train Epoch: 29 [72320/110534 (65%)]\tClassification Loss: 1.6321\r\n",
      "Train Epoch: 29 [72960/110534 (66%)]\tClassification Loss: 1.7460\r\n",
      "Train Epoch: 29 [73600/110534 (67%)]\tClassification Loss: 1.7818\r\n",
      "Train Epoch: 29 [74240/110534 (67%)]\tClassification Loss: 1.7396\r\n",
      "Train Epoch: 29 [74880/110534 (68%)]\tClassification Loss: 1.1555\r\n",
      "Train Epoch: 29 [75520/110534 (68%)]\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 29 [76160/110534 (69%)]\tClassification Loss: 1.4931\r\n",
      "Train Epoch: 29 [76800/110534 (69%)]\tClassification Loss: 1.3450\r\n",
      "Train Epoch: 29 [77440/110534 (70%)]\tClassification Loss: 1.3586\r\n",
      "Train Epoch: 29 [78080/110534 (71%)]\tClassification Loss: 1.4239\r\n",
      "Train Epoch: 29 [78720/110534 (71%)]\tClassification Loss: 1.4877\r\n",
      "Train Epoch: 29 [79360/110534 (72%)]\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 29 [80000/110534 (72%)]\tClassification Loss: 1.3249\r\n",
      "Train Epoch: 29 [80640/110534 (73%)]\tClassification Loss: 1.3511\r\n",
      "Train Epoch: 29 [81280/110534 (74%)]\tClassification Loss: 1.6826\r\n",
      "Train Epoch: 29 [81920/110534 (74%)]\tClassification Loss: 1.5091\r\n",
      "Train Epoch: 29 [82560/110534 (75%)]\tClassification Loss: 1.7106\r\n",
      "Train Epoch: 29 [83200/110534 (75%)]\tClassification Loss: 1.3473\r\n",
      "Train Epoch: 29 [83840/110534 (76%)]\tClassification Loss: 1.7974\r\n",
      "Train Epoch: 29 [84480/110534 (76%)]\tClassification Loss: 1.8473\r\n",
      "Train Epoch: 29 [85120/110534 (77%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 29 [85760/110534 (78%)]\tClassification Loss: 1.4579\r\n",
      "Train Epoch: 29 [86400/110534 (78%)]\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 29 [87040/110534 (79%)]\tClassification Loss: 1.6248\r\n",
      "Train Epoch: 29 [87680/110534 (79%)]\tClassification Loss: 1.4548\r\n",
      "Train Epoch: 29 [88320/110534 (80%)]\tClassification Loss: 1.4258\r\n",
      "Train Epoch: 29 [88960/110534 (80%)]\tClassification Loss: 1.7845\r\n",
      "Train Epoch: 29 [89600/110534 (81%)]\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 29 [90240/110534 (82%)]\tClassification Loss: 1.7712\r\n",
      "Train Epoch: 29 [90880/110534 (82%)]\tClassification Loss: 1.5340\r\n",
      "Train Epoch: 29 [91520/110534 (83%)]\tClassification Loss: 1.2121\r\n",
      "Train Epoch: 29 [92160/110534 (83%)]\tClassification Loss: 1.3315\r\n",
      "Train Epoch: 29 [92800/110534 (84%)]\tClassification Loss: 1.2759\r\n",
      "Train Epoch: 29 [93440/110534 (85%)]\tClassification Loss: 1.6647\r\n",
      "Train Epoch: 29 [94080/110534 (85%)]\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 29 [94720/110534 (86%)]\tClassification Loss: 1.2455\r\n",
      "Train Epoch: 29 [95360/110534 (86%)]\tClassification Loss: 1.3179\r\n",
      "Train Epoch: 29 [96000/110534 (87%)]\tClassification Loss: 1.3615\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_1500.pth.tar\r\n",
      "Train Epoch: 29 [96640/110534 (87%)]\tClassification Loss: 1.3740\r\n",
      "Train Epoch: 29 [97280/110534 (88%)]\tClassification Loss: 1.3037\r\n",
      "Train Epoch: 29 [97920/110534 (89%)]\tClassification Loss: 1.2760\r\n",
      "Train Epoch: 29 [98560/110534 (89%)]\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 29 [99200/110534 (90%)]\tClassification Loss: 1.6666\r\n",
      "Train Epoch: 29 [99840/110534 (90%)]\tClassification Loss: 1.5192\r\n",
      "Train Epoch: 29 [100480/110534 (91%)]\tClassification Loss: 1.5672\r\n",
      "Train Epoch: 29 [101120/110534 (91%)]\tClassification Loss: 1.5358\r\n",
      "Train Epoch: 29 [101760/110534 (92%)]\tClassification Loss: 1.6555\r\n",
      "Train Epoch: 29 [102400/110534 (93%)]\tClassification Loss: 1.2912\r\n",
      "Train Epoch: 29 [103040/110534 (93%)]\tClassification Loss: 1.4236\r\n",
      "Train Epoch: 29 [103680/110534 (94%)]\tClassification Loss: 1.6353\r\n",
      "Train Epoch: 29 [104320/110534 (94%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 29 [104960/110534 (95%)]\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 29 [105600/110534 (96%)]\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 29 [106240/110534 (96%)]\tClassification Loss: 1.2650\r\n",
      "Train Epoch: 29 [106880/110534 (97%)]\tClassification Loss: 1.6824\r\n",
      "Train Epoch: 29 [107520/110534 (97%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 29 [108160/110534 (98%)]\tClassification Loss: 1.4290\r\n",
      "Train Epoch: 29 [108800/110534 (98%)]\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 29 [109440/110534 (99%)]\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 29 [110080/110534 (100%)]\tClassification Loss: 1.4236\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_final.pth.tar\r\n",
      "Train Epoch: 30 [0/110534 (0%)]\tClassification Loss: 1.5501\r\n",
      "\r\n",
      "Test set: Average loss: 1.4252, Accuracy: 22934/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 30 [640/110534 (1%)]\tClassification Loss: 1.3201\r\n",
      "Train Epoch: 30 [1280/110534 (1%)]\tClassification Loss: 1.9963\r\n",
      "Train Epoch: 30 [1920/110534 (2%)]\tClassification Loss: 1.4632\r\n",
      "Train Epoch: 30 [2560/110534 (2%)]\tClassification Loss: 1.5956\r\n",
      "Train Epoch: 30 [3200/110534 (3%)]\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 30 [3840/110534 (3%)]\tClassification Loss: 1.3318\r\n",
      "Train Epoch: 30 [4480/110534 (4%)]\tClassification Loss: 1.6230\r\n",
      "Train Epoch: 30 [5120/110534 (5%)]\tClassification Loss: 1.5308\r\n",
      "Train Epoch: 30 [5760/110534 (5%)]\tClassification Loss: 1.5696\r\n",
      "Train Epoch: 30 [6400/110534 (6%)]\tClassification Loss: 1.3214\r\n",
      "Train Epoch: 30 [7040/110534 (6%)]\tClassification Loss: 1.3621\r\n",
      "Train Epoch: 30 [7680/110534 (7%)]\tClassification Loss: 1.4602\r\n",
      "Train Epoch: 30 [8320/110534 (8%)]\tClassification Loss: 1.7652\r\n",
      "Train Epoch: 30 [8960/110534 (8%)]\tClassification Loss: 1.6874\r\n",
      "Train Epoch: 30 [9600/110534 (9%)]\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 30 [10240/110534 (9%)]\tClassification Loss: 1.5051\r\n",
      "Train Epoch: 30 [10880/110534 (10%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 30 [11520/110534 (10%)]\tClassification Loss: 1.5817\r\n",
      "Train Epoch: 30 [12160/110534 (11%)]\tClassification Loss: 1.4005\r\n",
      "Train Epoch: 30 [12800/110534 (12%)]\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 30 [13440/110534 (12%)]\tClassification Loss: 1.4836\r\n",
      "Train Epoch: 30 [14080/110534 (13%)]\tClassification Loss: 1.4547\r\n",
      "Train Epoch: 30 [14720/110534 (13%)]\tClassification Loss: 1.7067\r\n",
      "Train Epoch: 30 [15360/110534 (14%)]\tClassification Loss: 1.6104\r\n",
      "Train Epoch: 30 [16000/110534 (14%)]\tClassification Loss: 1.5951\r\n",
      "Train Epoch: 30 [16640/110534 (15%)]\tClassification Loss: 1.3699\r\n",
      "Train Epoch: 30 [17280/110534 (16%)]\tClassification Loss: 1.6418\r\n",
      "Train Epoch: 30 [17920/110534 (16%)]\tClassification Loss: 1.5246\r\n",
      "Train Epoch: 30 [18560/110534 (17%)]\tClassification Loss: 1.7231\r\n",
      "Train Epoch: 30 [19200/110534 (17%)]\tClassification Loss: 1.6011\r\n",
      "Train Epoch: 30 [19840/110534 (18%)]\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 30 [20480/110534 (19%)]\tClassification Loss: 1.8645\r\n",
      "Train Epoch: 30 [21120/110534 (19%)]\tClassification Loss: 1.7700\r\n",
      "Train Epoch: 30 [21760/110534 (20%)]\tClassification Loss: 1.4485\r\n",
      "Train Epoch: 30 [22400/110534 (20%)]\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 30 [23040/110534 (21%)]\tClassification Loss: 1.1737\r\n",
      "Train Epoch: 30 [23680/110534 (21%)]\tClassification Loss: 1.6525\r\n",
      "Train Epoch: 30 [24320/110534 (22%)]\tClassification Loss: 1.2023\r\n",
      "Train Epoch: 30 [24960/110534 (23%)]\tClassification Loss: 1.6887\r\n",
      "Train Epoch: 30 [25600/110534 (23%)]\tClassification Loss: 1.3373\r\n",
      "Train Epoch: 30 [26240/110534 (24%)]\tClassification Loss: 1.5375\r\n",
      "Train Epoch: 30 [26880/110534 (24%)]\tClassification Loss: 1.7510\r\n",
      "Train Epoch: 30 [27520/110534 (25%)]\tClassification Loss: 1.3576\r\n",
      "Train Epoch: 30 [28160/110534 (25%)]\tClassification Loss: 1.7440\r\n",
      "Train Epoch: 30 [28800/110534 (26%)]\tClassification Loss: 1.7205\r\n",
      "Train Epoch: 30 [29440/110534 (27%)]\tClassification Loss: 1.6155\r\n",
      "Train Epoch: 30 [30080/110534 (27%)]\tClassification Loss: 1.7927\r\n",
      "Train Epoch: 30 [30720/110534 (28%)]\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 30 [31360/110534 (28%)]\tClassification Loss: 1.7497\r\n",
      "Train Epoch: 30 [32000/110534 (29%)]\tClassification Loss: 1.4899\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_500.pth.tar\r\n",
      "Train Epoch: 30 [32640/110534 (30%)]\tClassification Loss: 1.4319\r\n",
      "Train Epoch: 30 [33280/110534 (30%)]\tClassification Loss: 1.4138\r\n",
      "Train Epoch: 30 [33920/110534 (31%)]\tClassification Loss: 1.7572\r\n",
      "Train Epoch: 30 [34560/110534 (31%)]\tClassification Loss: 1.7809\r\n",
      "Train Epoch: 30 [35200/110534 (32%)]\tClassification Loss: 1.4533\r\n",
      "Train Epoch: 30 [35840/110534 (32%)]\tClassification Loss: 1.2815\r\n",
      "Train Epoch: 30 [36480/110534 (33%)]\tClassification Loss: 1.3614\r\n",
      "Train Epoch: 30 [37120/110534 (34%)]\tClassification Loss: 1.5467\r\n",
      "Train Epoch: 30 [37760/110534 (34%)]\tClassification Loss: 1.8614\r\n",
      "Train Epoch: 30 [38400/110534 (35%)]\tClassification Loss: 1.4456\r\n",
      "Train Epoch: 30 [39040/110534 (35%)]\tClassification Loss: 1.5312\r\n",
      "Train Epoch: 30 [39680/110534 (36%)]\tClassification Loss: 1.5756\r\n",
      "Train Epoch: 30 [40320/110534 (36%)]\tClassification Loss: 1.5368\r\n",
      "Train Epoch: 30 [40960/110534 (37%)]\tClassification Loss: 1.3095\r\n",
      "Train Epoch: 30 [41600/110534 (38%)]\tClassification Loss: 1.4325\r\n",
      "Train Epoch: 30 [42240/110534 (38%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 30 [42880/110534 (39%)]\tClassification Loss: 1.4887\r\n",
      "Train Epoch: 30 [43520/110534 (39%)]\tClassification Loss: 1.2837\r\n",
      "Train Epoch: 30 [44160/110534 (40%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 30 [44800/110534 (41%)]\tClassification Loss: 1.6372\r\n",
      "Train Epoch: 30 [45440/110534 (41%)]\tClassification Loss: 1.5842\r\n",
      "Train Epoch: 30 [46080/110534 (42%)]\tClassification Loss: 1.4231\r\n",
      "Train Epoch: 30 [46720/110534 (42%)]\tClassification Loss: 1.5014\r\n",
      "Train Epoch: 30 [47360/110534 (43%)]\tClassification Loss: 1.4564\r\n",
      "Train Epoch: 30 [48000/110534 (43%)]\tClassification Loss: 1.5758\r\n",
      "Train Epoch: 30 [48640/110534 (44%)]\tClassification Loss: 1.3839\r\n",
      "Train Epoch: 30 [49280/110534 (45%)]\tClassification Loss: 1.5336\r\n",
      "Train Epoch: 30 [49920/110534 (45%)]\tClassification Loss: 1.5724\r\n",
      "Train Epoch: 30 [50560/110534 (46%)]\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 30 [51200/110534 (46%)]\tClassification Loss: 1.2372\r\n",
      "Train Epoch: 30 [51840/110534 (47%)]\tClassification Loss: 1.4096\r\n",
      "Train Epoch: 30 [52480/110534 (47%)]\tClassification Loss: 1.4192\r\n",
      "Train Epoch: 30 [53120/110534 (48%)]\tClassification Loss: 1.1764\r\n",
      "Train Epoch: 30 [53760/110534 (49%)]\tClassification Loss: 1.3685\r\n",
      "Train Epoch: 30 [54400/110534 (49%)]\tClassification Loss: 1.6184\r\n",
      "Train Epoch: 30 [55040/110534 (50%)]\tClassification Loss: 1.8515\r\n",
      "Train Epoch: 30 [55680/110534 (50%)]\tClassification Loss: 1.6895\r\n",
      "Train Epoch: 30 [56320/110534 (51%)]\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 30 [56960/110534 (52%)]\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 30 [57600/110534 (52%)]\tClassification Loss: 1.6629\r\n",
      "Train Epoch: 30 [58240/110534 (53%)]\tClassification Loss: 1.5558\r\n",
      "Train Epoch: 30 [58880/110534 (53%)]\tClassification Loss: 1.8179\r\n",
      "Train Epoch: 30 [59520/110534 (54%)]\tClassification Loss: 1.2762\r\n",
      "Train Epoch: 30 [60160/110534 (54%)]\tClassification Loss: 1.4324\r\n",
      "Train Epoch: 30 [60800/110534 (55%)]\tClassification Loss: 1.5736\r\n",
      "Train Epoch: 30 [61440/110534 (56%)]\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 30 [62080/110534 (56%)]\tClassification Loss: 1.4841\r\n",
      "Train Epoch: 30 [62720/110534 (57%)]\tClassification Loss: 1.6179\r\n",
      "Train Epoch: 30 [63360/110534 (57%)]\tClassification Loss: 1.3799\r\n",
      "Train Epoch: 30 [64000/110534 (58%)]\tClassification Loss: 1.7150\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_1000.pth.tar\r\n",
      "Train Epoch: 30 [64640/110534 (58%)]\tClassification Loss: 1.3829\r\n",
      "Train Epoch: 30 [65280/110534 (59%)]\tClassification Loss: 1.4801\r\n",
      "Train Epoch: 30 [65920/110534 (60%)]\tClassification Loss: 1.7079\r\n",
      "Train Epoch: 30 [66560/110534 (60%)]\tClassification Loss: 1.1528\r\n",
      "Train Epoch: 30 [67200/110534 (61%)]\tClassification Loss: 1.1950\r\n",
      "Train Epoch: 30 [67840/110534 (61%)]\tClassification Loss: 2.1274\r\n",
      "Train Epoch: 30 [68480/110534 (62%)]\tClassification Loss: 1.5697\r\n",
      "Train Epoch: 30 [69120/110534 (63%)]\tClassification Loss: 1.7258\r\n",
      "Train Epoch: 30 [69760/110534 (63%)]\tClassification Loss: 1.5847\r\n",
      "Train Epoch: 30 [70400/110534 (64%)]\tClassification Loss: 1.1853\r\n",
      "Train Epoch: 30 [71040/110534 (64%)]\tClassification Loss: 1.8598\r\n",
      "Train Epoch: 30 [71680/110534 (65%)]\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 30 [72320/110534 (65%)]\tClassification Loss: 1.4624\r\n",
      "Train Epoch: 30 [72960/110534 (66%)]\tClassification Loss: 1.7113\r\n",
      "Train Epoch: 30 [73600/110534 (67%)]\tClassification Loss: 1.7380\r\n",
      "Train Epoch: 30 [74240/110534 (67%)]\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 30 [74880/110534 (68%)]\tClassification Loss: 1.2929\r\n",
      "Train Epoch: 30 [75520/110534 (68%)]\tClassification Loss: 1.4846\r\n",
      "Train Epoch: 30 [76160/110534 (69%)]\tClassification Loss: 1.3883\r\n",
      "Train Epoch: 30 [76800/110534 (69%)]\tClassification Loss: 1.2183\r\n",
      "Train Epoch: 30 [77440/110534 (70%)]\tClassification Loss: 1.4805\r\n",
      "Train Epoch: 30 [78080/110534 (71%)]\tClassification Loss: 1.5598\r\n",
      "Train Epoch: 30 [78720/110534 (71%)]\tClassification Loss: 1.4065\r\n",
      "Train Epoch: 30 [79360/110534 (72%)]\tClassification Loss: 1.3246\r\n",
      "Train Epoch: 30 [80000/110534 (72%)]\tClassification Loss: 1.3093\r\n",
      "Train Epoch: 30 [80640/110534 (73%)]\tClassification Loss: 1.3772\r\n",
      "Train Epoch: 30 [81280/110534 (74%)]\tClassification Loss: 1.8499\r\n",
      "Train Epoch: 30 [81920/110534 (74%)]\tClassification Loss: 1.4592\r\n",
      "Train Epoch: 30 [82560/110534 (75%)]\tClassification Loss: 1.7483\r\n",
      "Train Epoch: 30 [83200/110534 (75%)]\tClassification Loss: 1.5974\r\n",
      "Train Epoch: 30 [83840/110534 (76%)]\tClassification Loss: 1.7818\r\n",
      "Train Epoch: 30 [84480/110534 (76%)]\tClassification Loss: 1.8120\r\n",
      "Train Epoch: 30 [85120/110534 (77%)]\tClassification Loss: 1.4448\r\n",
      "Train Epoch: 30 [85760/110534 (78%)]\tClassification Loss: 1.5389\r\n",
      "Train Epoch: 30 [86400/110534 (78%)]\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 30 [87040/110534 (79%)]\tClassification Loss: 1.4631\r\n",
      "Train Epoch: 30 [87680/110534 (79%)]\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 30 [88320/110534 (80%)]\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 30 [88960/110534 (80%)]\tClassification Loss: 1.8317\r\n",
      "Train Epoch: 30 [89600/110534 (81%)]\tClassification Loss: 1.7353\r\n",
      "Train Epoch: 30 [90240/110534 (82%)]\tClassification Loss: 1.7329\r\n",
      "Train Epoch: 30 [90880/110534 (82%)]\tClassification Loss: 1.7126\r\n",
      "Train Epoch: 30 [91520/110534 (83%)]\tClassification Loss: 1.2850\r\n",
      "Train Epoch: 30 [92160/110534 (83%)]\tClassification Loss: 1.3316\r\n",
      "Train Epoch: 30 [92800/110534 (84%)]\tClassification Loss: 1.4835\r\n",
      "Train Epoch: 30 [93440/110534 (85%)]\tClassification Loss: 1.5240\r\n",
      "Train Epoch: 30 [94080/110534 (85%)]\tClassification Loss: 1.5337\r\n",
      "Train Epoch: 30 [94720/110534 (86%)]\tClassification Loss: 1.2839\r\n",
      "Train Epoch: 30 [95360/110534 (86%)]\tClassification Loss: 1.3782\r\n",
      "Train Epoch: 30 [96000/110534 (87%)]\tClassification Loss: 1.4189\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_1500.pth.tar\r\n",
      "Train Epoch: 30 [96640/110534 (87%)]\tClassification Loss: 1.3685\r\n",
      "Train Epoch: 30 [97280/110534 (88%)]\tClassification Loss: 1.2975\r\n",
      "Train Epoch: 30 [97920/110534 (89%)]\tClassification Loss: 1.2189\r\n",
      "Train Epoch: 30 [98560/110534 (89%)]\tClassification Loss: 1.5445\r\n",
      "Train Epoch: 30 [99200/110534 (90%)]\tClassification Loss: 1.5571\r\n",
      "Train Epoch: 30 [99840/110534 (90%)]\tClassification Loss: 1.5946\r\n",
      "Train Epoch: 30 [100480/110534 (91%)]\tClassification Loss: 1.7040\r\n",
      "Train Epoch: 30 [101120/110534 (91%)]\tClassification Loss: 1.5845\r\n",
      "Train Epoch: 30 [101760/110534 (92%)]\tClassification Loss: 1.6649\r\n",
      "Train Epoch: 30 [102400/110534 (93%)]\tClassification Loss: 1.4933\r\n",
      "Train Epoch: 30 [103040/110534 (93%)]\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 30 [103680/110534 (94%)]\tClassification Loss: 1.6788\r\n",
      "Train Epoch: 30 [104320/110534 (94%)]\tClassification Loss: 1.3029\r\n",
      "Train Epoch: 30 [104960/110534 (95%)]\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 30 [105600/110534 (96%)]\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 30 [106240/110534 (96%)]\tClassification Loss: 1.2629\r\n",
      "Train Epoch: 30 [106880/110534 (97%)]\tClassification Loss: 1.5935\r\n",
      "Train Epoch: 30 [107520/110534 (97%)]\tClassification Loss: 1.6496\r\n",
      "Train Epoch: 30 [108160/110534 (98%)]\tClassification Loss: 1.4055\r\n",
      "Train Epoch: 30 [108800/110534 (98%)]\tClassification Loss: 1.7341\r\n",
      "Train Epoch: 30 [109440/110534 (99%)]\tClassification Loss: 1.5152\r\n",
      "Train Epoch: 30 [110080/110534 (100%)]\tClassification Loss: 1.4242\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_final.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "# FREEZE = False. LR=0.003. in-shop=False. 30 epochs\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/110534 (0%)]\tClassification Loss: 3.2574\r\n",
      "train.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.2369, Accuracy: 587/42368 (1%)\r\n",
      "\r\n",
      "Train Epoch: 1 [1920/110534 (2%)]\tClassification Loss: 2.7135\r\n",
      "Train Epoch: 1 [3840/110534 (3%)]\tClassification Loss: 2.5495\r\n",
      "Train Epoch: 1 [5760/110534 (5%)]\tClassification Loss: 2.6430\r\n",
      "Train Epoch: 1 [7680/110534 (7%)]\tClassification Loss: 2.4120\r\n",
      "Train Epoch: 1 [9600/110534 (9%)]\tClassification Loss: 2.3391\r\n",
      "Train Epoch: 1 [11520/110534 (10%)]\tClassification Loss: 2.3827\r\n",
      "Train Epoch: 1 [13440/110534 (12%)]\tClassification Loss: 2.4087\r\n",
      "Train Epoch: 1 [15360/110534 (14%)]\tClassification Loss: 2.3085\r\n",
      "Train Epoch: 1 [17280/110534 (16%)]\tClassification Loss: 2.3360\r\n",
      "Train Epoch: 1 [19200/110534 (17%)]\tClassification Loss: 2.2704\r\n",
      "Train Epoch: 1 [21120/110534 (19%)]\tClassification Loss: 2.3272\r\n",
      "Train Epoch: 1 [23040/110534 (21%)]\tClassification Loss: 2.0667\r\n",
      "Train Epoch: 1 [24960/110534 (23%)]\tClassification Loss: 2.2104\r\n",
      "Train Epoch: 1 [26880/110534 (24%)]\tClassification Loss: 2.2172\r\n",
      "Train Epoch: 1 [28800/110534 (26%)]\tClassification Loss: 2.3362\r\n",
      "Train Epoch: 1 [30720/110534 (28%)]\tClassification Loss: 2.2286\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_500.pth.tar\r\n",
      "Train Epoch: 1 [32640/110534 (30%)]\tClassification Loss: 1.7893\r\n",
      "Train Epoch: 1 [34560/110534 (31%)]\tClassification Loss: 1.8705\r\n",
      "Train Epoch: 1 [36480/110534 (33%)]\tClassification Loss: 1.7958\r\n",
      "Train Epoch: 1 [38400/110534 (35%)]\tClassification Loss: 2.1396\r\n",
      "Train Epoch: 1 [40320/110534 (36%)]\tClassification Loss: 2.0229\r\n",
      "Train Epoch: 1 [42240/110534 (38%)]\tClassification Loss: 2.0080\r\n",
      "Train Epoch: 1 [44160/110534 (40%)]\tClassification Loss: 1.9624\r\n",
      "Train Epoch: 1 [46080/110534 (42%)]\tClassification Loss: 2.1007\r\n",
      "Train Epoch: 1 [48000/110534 (43%)]\tClassification Loss: 2.1294\r\n",
      "Train Epoch: 1 [49920/110534 (45%)]\tClassification Loss: 2.0765\r\n",
      "Train Epoch: 1 [51840/110534 (47%)]\tClassification Loss: 2.2344\r\n",
      "Train Epoch: 1 [53760/110534 (49%)]\tClassification Loss: 1.8529\r\n",
      "Train Epoch: 1 [55680/110534 (50%)]\tClassification Loss: 1.8122\r\n",
      "Train Epoch: 1 [57600/110534 (52%)]\tClassification Loss: 2.0477\r\n",
      "Train Epoch: 1 [59520/110534 (54%)]\tClassification Loss: 1.9866\r\n",
      "Train Epoch: 1 [61440/110534 (56%)]\tClassification Loss: 1.8276\r\n",
      "Train Epoch: 1 [63360/110534 (57%)]\tClassification Loss: 2.0709\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1000.pth.tar\r\n",
      "Train Epoch: 1 [65280/110534 (59%)]\tClassification Loss: 1.9061\r\n",
      "Train Epoch: 1 [67200/110534 (61%)]\tClassification Loss: 1.7554\r\n",
      "Train Epoch: 1 [69120/110534 (63%)]\tClassification Loss: 1.8559\r\n",
      "Train Epoch: 1 [71040/110534 (64%)]\tClassification Loss: 1.7782\r\n",
      "Train Epoch: 1 [72960/110534 (66%)]\tClassification Loss: 1.7498\r\n",
      "Train Epoch: 1 [74880/110534 (68%)]\tClassification Loss: 1.6683\r\n",
      "Train Epoch: 1 [76800/110534 (69%)]\tClassification Loss: 1.6660\r\n",
      "Train Epoch: 1 [78720/110534 (71%)]\tClassification Loss: 1.8147\r\n",
      "Train Epoch: 1 [80640/110534 (73%)]\tClassification Loss: 1.8789\r\n",
      "Train Epoch: 1 [82560/110534 (75%)]\tClassification Loss: 1.6688\r\n",
      "Train Epoch: 1 [84480/110534 (76%)]\tClassification Loss: 1.6579\r\n",
      "Train Epoch: 1 [86400/110534 (78%)]\tClassification Loss: 1.8037\r\n",
      "Train Epoch: 1 [88320/110534 (80%)]\tClassification Loss: 1.9795\r\n",
      "Train Epoch: 1 [90240/110534 (82%)]\tClassification Loss: 2.0085\r\n",
      "Train Epoch: 1 [92160/110534 (83%)]\tClassification Loss: 1.9744\r\n",
      "Train Epoch: 1 [94080/110534 (85%)]\tClassification Loss: 1.8853\r\n",
      "Train Epoch: 1 [96000/110534 (87%)]\tClassification Loss: 1.9320\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [97920/110534 (89%)]\tClassification Loss: 1.7821\r\n",
      "Train Epoch: 1 [99840/110534 (90%)]\tClassification Loss: 2.0308\r\n",
      "Train Epoch: 1 [101760/110534 (92%)]\tClassification Loss: 1.7555\r\n",
      "Train Epoch: 1 [103680/110534 (94%)]\tClassification Loss: 1.8541\r\n",
      "Train Epoch: 1 [105600/110534 (96%)]\tClassification Loss: 1.8268\r\n",
      "Train Epoch: 1 [107520/110534 (97%)]\tClassification Loss: 1.7398\r\n",
      "Train Epoch: 1 [109440/110534 (99%)]\tClassification Loss: 1.8591\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/110534 (0%)]\tClassification Loss: 2.0089\r\n",
      "\r\n",
      "Test set: Average loss: 1.6983, Accuracy: 20827/42368 (49%)\r\n",
      "\r\n",
      "Train Epoch: 2 [1920/110534 (2%)]\tClassification Loss: 1.7425\r\n",
      "Train Epoch: 2 [3840/110534 (3%)]\tClassification Loss: 1.8795\r\n",
      "Train Epoch: 2 [5760/110534 (5%)]\tClassification Loss: 1.9725\r\n",
      "Train Epoch: 2 [7680/110534 (7%)]\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 2 [9600/110534 (9%)]\tClassification Loss: 1.7991\r\n",
      "Train Epoch: 2 [11520/110534 (10%)]\tClassification Loss: 1.8117\r\n",
      "Train Epoch: 2 [13440/110534 (12%)]\tClassification Loss: 1.7238\r\n",
      "Train Epoch: 2 [15360/110534 (14%)]\tClassification Loss: 1.9071\r\n",
      "Train Epoch: 2 [17280/110534 (16%)]\tClassification Loss: 1.8593\r\n",
      "Train Epoch: 2 [19200/110534 (17%)]\tClassification Loss: 1.9115\r\n",
      "Train Epoch: 2 [21120/110534 (19%)]\tClassification Loss: 1.8727\r\n",
      "Train Epoch: 2 [23040/110534 (21%)]\tClassification Loss: 1.7022\r\n",
      "Train Epoch: 2 [24960/110534 (23%)]\tClassification Loss: 1.9054\r\n",
      "Train Epoch: 2 [26880/110534 (24%)]\tClassification Loss: 1.7970\r\n",
      "Train Epoch: 2 [28800/110534 (26%)]\tClassification Loss: 2.0521\r\n",
      "Train Epoch: 2 [30720/110534 (28%)]\tClassification Loss: 1.8467\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_500.pth.tar\r\n",
      "Train Epoch: 2 [32640/110534 (30%)]\tClassification Loss: 1.6296\r\n",
      "Train Epoch: 2 [34560/110534 (31%)]\tClassification Loss: 1.7080\r\n",
      "Train Epoch: 2 [36480/110534 (33%)]\tClassification Loss: 1.5081\r\n",
      "Train Epoch: 2 [38400/110534 (35%)]\tClassification Loss: 1.7417\r\n",
      "Train Epoch: 2 [40320/110534 (36%)]\tClassification Loss: 1.6071\r\n",
      "Train Epoch: 2 [42240/110534 (38%)]\tClassification Loss: 1.7030\r\n",
      "Train Epoch: 2 [44160/110534 (40%)]\tClassification Loss: 1.5465\r\n",
      "Train Epoch: 2 [46080/110534 (42%)]\tClassification Loss: 1.7607\r\n",
      "Train Epoch: 2 [48000/110534 (43%)]\tClassification Loss: 1.8120\r\n",
      "Train Epoch: 2 [49920/110534 (45%)]\tClassification Loss: 1.8107\r\n",
      "Train Epoch: 2 [51840/110534 (47%)]\tClassification Loss: 1.9696\r\n",
      "Train Epoch: 2 [53760/110534 (49%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 2 [55680/110534 (50%)]\tClassification Loss: 1.6108\r\n",
      "Train Epoch: 2 [57600/110534 (52%)]\tClassification Loss: 1.8019\r\n",
      "Train Epoch: 2 [59520/110534 (54%)]\tClassification Loss: 1.8865\r\n",
      "Train Epoch: 2 [61440/110534 (56%)]\tClassification Loss: 1.7418\r\n",
      "Train Epoch: 2 [63360/110534 (57%)]\tClassification Loss: 1.8634\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1000.pth.tar\r\n",
      "Train Epoch: 2 [65280/110534 (59%)]\tClassification Loss: 1.7033\r\n",
      "Train Epoch: 2 [67200/110534 (61%)]\tClassification Loss: 1.5937\r\n",
      "Train Epoch: 2 [69120/110534 (63%)]\tClassification Loss: 1.5183\r\n",
      "Train Epoch: 2 [71040/110534 (64%)]\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 2 [72960/110534 (66%)]\tClassification Loss: 1.5950\r\n",
      "Train Epoch: 2 [74880/110534 (68%)]\tClassification Loss: 1.3971\r\n",
      "Train Epoch: 2 [76800/110534 (69%)]\tClassification Loss: 1.6869\r\n",
      "Train Epoch: 2 [78720/110534 (71%)]\tClassification Loss: 1.6968\r\n",
      "Train Epoch: 2 [80640/110534 (73%)]\tClassification Loss: 1.7346\r\n",
      "Train Epoch: 2 [82560/110534 (75%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 2 [84480/110534 (76%)]\tClassification Loss: 1.5573\r\n",
      "Train Epoch: 2 [86400/110534 (78%)]\tClassification Loss: 1.6625\r\n",
      "Train Epoch: 2 [88320/110534 (80%)]\tClassification Loss: 1.9199\r\n",
      "Train Epoch: 2 [90240/110534 (82%)]\tClassification Loss: 1.7173\r\n",
      "Train Epoch: 2 [92160/110534 (83%)]\tClassification Loss: 1.7764\r\n",
      "Train Epoch: 2 [94080/110534 (85%)]\tClassification Loss: 1.8559\r\n",
      "Train Epoch: 2 [96000/110534 (87%)]\tClassification Loss: 1.8951\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [97920/110534 (89%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 2 [99840/110534 (90%)]\tClassification Loss: 1.9700\r\n",
      "Train Epoch: 2 [101760/110534 (92%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 2 [103680/110534 (94%)]\tClassification Loss: 1.8495\r\n",
      "Train Epoch: 2 [105600/110534 (96%)]\tClassification Loss: 1.6895\r\n",
      "Train Epoch: 2 [107520/110534 (97%)]\tClassification Loss: 1.6002\r\n",
      "Train Epoch: 2 [109440/110534 (99%)]\tClassification Loss: 1.7988\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/110534 (0%)]\tClassification Loss: 1.8507\r\n",
      "\r\n",
      "Test set: Average loss: 1.5878, Accuracy: 21574/42368 (51%)\r\n",
      "\r\n",
      "Train Epoch: 3 [1920/110534 (2%)]\tClassification Loss: 1.5460\r\n",
      "Train Epoch: 3 [3840/110534 (3%)]\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 3 [5760/110534 (5%)]\tClassification Loss: 1.7913\r\n",
      "Train Epoch: 3 [7680/110534 (7%)]\tClassification Loss: 1.5827\r\n",
      "Train Epoch: 3 [9600/110534 (9%)]\tClassification Loss: 1.8094\r\n",
      "Train Epoch: 3 [11520/110534 (10%)]\tClassification Loss: 1.7262\r\n",
      "Train Epoch: 3 [13440/110534 (12%)]\tClassification Loss: 1.6378\r\n",
      "Train Epoch: 3 [15360/110534 (14%)]\tClassification Loss: 1.9471\r\n",
      "Train Epoch: 3 [17280/110534 (16%)]\tClassification Loss: 1.9474\r\n",
      "Train Epoch: 3 [19200/110534 (17%)]\tClassification Loss: 1.9702\r\n",
      "Train Epoch: 3 [21120/110534 (19%)]\tClassification Loss: 1.7683\r\n",
      "Train Epoch: 3 [23040/110534 (21%)]\tClassification Loss: 1.6599\r\n",
      "Train Epoch: 3 [24960/110534 (23%)]\tClassification Loss: 1.7177\r\n",
      "Train Epoch: 3 [26880/110534 (24%)]\tClassification Loss: 1.7655\r\n",
      "Train Epoch: 3 [28800/110534 (26%)]\tClassification Loss: 2.1163\r\n",
      "Train Epoch: 3 [30720/110534 (28%)]\tClassification Loss: 1.7610\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_500.pth.tar\r\n",
      "Train Epoch: 3 [32640/110534 (30%)]\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 3 [34560/110534 (31%)]\tClassification Loss: 1.4326\r\n",
      "Train Epoch: 3 [36480/110534 (33%)]\tClassification Loss: 1.4935\r\n",
      "Train Epoch: 3 [38400/110534 (35%)]\tClassification Loss: 1.7081\r\n",
      "Train Epoch: 3 [40320/110534 (36%)]\tClassification Loss: 1.6313\r\n",
      "Train Epoch: 3 [42240/110534 (38%)]\tClassification Loss: 1.6682\r\n",
      "Train Epoch: 3 [44160/110534 (40%)]\tClassification Loss: 1.6016\r\n",
      "Train Epoch: 3 [46080/110534 (42%)]\tClassification Loss: 1.7387\r\n",
      "Train Epoch: 3 [48000/110534 (43%)]\tClassification Loss: 1.7153\r\n",
      "Train Epoch: 3 [49920/110534 (45%)]\tClassification Loss: 1.8567\r\n",
      "Train Epoch: 3 [51840/110534 (47%)]\tClassification Loss: 1.9414\r\n",
      "Train Epoch: 3 [53760/110534 (49%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 3 [55680/110534 (50%)]\tClassification Loss: 1.6115\r\n",
      "Train Epoch: 3 [57600/110534 (52%)]\tClassification Loss: 1.6686\r\n",
      "Train Epoch: 3 [59520/110534 (54%)]\tClassification Loss: 1.9132\r\n",
      "Train Epoch: 3 [61440/110534 (56%)]\tClassification Loss: 1.6305\r\n",
      "Train Epoch: 3 [63360/110534 (57%)]\tClassification Loss: 1.9191\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1000.pth.tar\r\n",
      "Train Epoch: 3 [65280/110534 (59%)]\tClassification Loss: 1.6090\r\n",
      "Train Epoch: 3 [67200/110534 (61%)]\tClassification Loss: 1.4927\r\n",
      "Train Epoch: 3 [69120/110534 (63%)]\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 3 [71040/110534 (64%)]\tClassification Loss: 1.6087\r\n",
      "Train Epoch: 3 [72960/110534 (66%)]\tClassification Loss: 1.4771\r\n",
      "Train Epoch: 3 [74880/110534 (68%)]\tClassification Loss: 1.4339\r\n",
      "Train Epoch: 3 [76800/110534 (69%)]\tClassification Loss: 1.5245\r\n",
      "Train Epoch: 3 [78720/110534 (71%)]\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 3 [80640/110534 (73%)]\tClassification Loss: 1.6665\r\n",
      "Train Epoch: 3 [82560/110534 (75%)]\tClassification Loss: 1.4420\r\n",
      "Train Epoch: 3 [84480/110534 (76%)]\tClassification Loss: 1.4501\r\n",
      "Train Epoch: 3 [86400/110534 (78%)]\tClassification Loss: 1.5497\r\n",
      "Train Epoch: 3 [88320/110534 (80%)]\tClassification Loss: 1.8845\r\n",
      "Train Epoch: 3 [90240/110534 (82%)]\tClassification Loss: 1.8214\r\n",
      "Train Epoch: 3 [92160/110534 (83%)]\tClassification Loss: 1.7950\r\n",
      "Train Epoch: 3 [94080/110534 (85%)]\tClassification Loss: 1.6506\r\n",
      "Train Epoch: 3 [96000/110534 (87%)]\tClassification Loss: 1.9763\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [97920/110534 (89%)]\tClassification Loss: 1.6520\r\n",
      "Train Epoch: 3 [99840/110534 (90%)]\tClassification Loss: 1.8456\r\n",
      "Train Epoch: 3 [101760/110534 (92%)]\tClassification Loss: 1.5447\r\n",
      "Train Epoch: 3 [103680/110534 (94%)]\tClassification Loss: 1.7445\r\n",
      "Train Epoch: 3 [105600/110534 (96%)]\tClassification Loss: 1.5818\r\n",
      "Train Epoch: 3 [107520/110534 (97%)]\tClassification Loss: 1.6405\r\n",
      "Train Epoch: 3 [109440/110534 (99%)]\tClassification Loss: 1.7046\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/110534 (0%)]\tClassification Loss: 1.7847\r\n",
      "\r\n",
      "Test set: Average loss: 1.5433, Accuracy: 22000/42368 (52%)\r\n",
      "\r\n",
      "Train Epoch: 4 [1920/110534 (2%)]\tClassification Loss: 1.6675\r\n",
      "Train Epoch: 4 [3840/110534 (3%)]\tClassification Loss: 1.6250\r\n",
      "Train Epoch: 4 [5760/110534 (5%)]\tClassification Loss: 1.8452\r\n",
      "Train Epoch: 4 [7680/110534 (7%)]\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 4 [9600/110534 (9%)]\tClassification Loss: 1.7212\r\n",
      "Train Epoch: 4 [11520/110534 (10%)]\tClassification Loss: 1.8816\r\n",
      "Train Epoch: 4 [13440/110534 (12%)]\tClassification Loss: 1.6703\r\n",
      "Train Epoch: 4 [15360/110534 (14%)]\tClassification Loss: 1.9035\r\n",
      "Train Epoch: 4 [17280/110534 (16%)]\tClassification Loss: 1.7969\r\n",
      "Train Epoch: 4 [19200/110534 (17%)]\tClassification Loss: 1.9288\r\n",
      "Train Epoch: 4 [21120/110534 (19%)]\tClassification Loss: 1.6685\r\n",
      "Train Epoch: 4 [23040/110534 (21%)]\tClassification Loss: 1.5860\r\n",
      "Train Epoch: 4 [24960/110534 (23%)]\tClassification Loss: 1.6686\r\n",
      "Train Epoch: 4 [26880/110534 (24%)]\tClassification Loss: 1.7058\r\n",
      "Train Epoch: 4 [28800/110534 (26%)]\tClassification Loss: 1.9711\r\n",
      "Train Epoch: 4 [30720/110534 (28%)]\tClassification Loss: 1.5142\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_500.pth.tar\r\n",
      "Train Epoch: 4 [32640/110534 (30%)]\tClassification Loss: 1.6061\r\n",
      "Train Epoch: 4 [34560/110534 (31%)]\tClassification Loss: 1.3723\r\n",
      "Train Epoch: 4 [36480/110534 (33%)]\tClassification Loss: 1.3990\r\n",
      "Train Epoch: 4 [38400/110534 (35%)]\tClassification Loss: 1.6057\r\n",
      "Train Epoch: 4 [40320/110534 (36%)]\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 4 [42240/110534 (38%)]\tClassification Loss: 1.6825\r\n",
      "Train Epoch: 4 [44160/110534 (40%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 4 [46080/110534 (42%)]\tClassification Loss: 1.6000\r\n",
      "Train Epoch: 4 [48000/110534 (43%)]\tClassification Loss: 1.7210\r\n",
      "Train Epoch: 4 [49920/110534 (45%)]\tClassification Loss: 1.7988\r\n",
      "Train Epoch: 4 [51840/110534 (47%)]\tClassification Loss: 1.9048\r\n",
      "Train Epoch: 4 [53760/110534 (49%)]\tClassification Loss: 1.5162\r\n",
      "Train Epoch: 4 [55680/110534 (50%)]\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 4 [57600/110534 (52%)]\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 4 [59520/110534 (54%)]\tClassification Loss: 2.0241\r\n",
      "Train Epoch: 4 [61440/110534 (56%)]\tClassification Loss: 1.6318\r\n",
      "Train Epoch: 4 [63360/110534 (57%)]\tClassification Loss: 1.9270\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1000.pth.tar\r\n",
      "Train Epoch: 4 [65280/110534 (59%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 4 [67200/110534 (61%)]\tClassification Loss: 1.3887\r\n",
      "Train Epoch: 4 [69120/110534 (63%)]\tClassification Loss: 1.5611\r\n",
      "Train Epoch: 4 [71040/110534 (64%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 4 [72960/110534 (66%)]\tClassification Loss: 1.4370\r\n",
      "Train Epoch: 4 [74880/110534 (68%)]\tClassification Loss: 1.3574\r\n",
      "Train Epoch: 4 [76800/110534 (69%)]\tClassification Loss: 1.4811\r\n",
      "Train Epoch: 4 [78720/110534 (71%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 4 [80640/110534 (73%)]\tClassification Loss: 1.6762\r\n",
      "Train Epoch: 4 [82560/110534 (75%)]\tClassification Loss: 1.4317\r\n",
      "Train Epoch: 4 [84480/110534 (76%)]\tClassification Loss: 1.4894\r\n",
      "Train Epoch: 4 [86400/110534 (78%)]\tClassification Loss: 1.5185\r\n",
      "Train Epoch: 4 [88320/110534 (80%)]\tClassification Loss: 1.8743\r\n",
      "Train Epoch: 4 [90240/110534 (82%)]\tClassification Loss: 1.8281\r\n",
      "Train Epoch: 4 [92160/110534 (83%)]\tClassification Loss: 1.7372\r\n",
      "Train Epoch: 4 [94080/110534 (85%)]\tClassification Loss: 1.6431\r\n",
      "Train Epoch: 4 [96000/110534 (87%)]\tClassification Loss: 1.8722\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [97920/110534 (89%)]\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 4 [99840/110534 (90%)]\tClassification Loss: 1.6592\r\n",
      "Train Epoch: 4 [101760/110534 (92%)]\tClassification Loss: 1.4154\r\n",
      "Train Epoch: 4 [103680/110534 (94%)]\tClassification Loss: 1.7662\r\n",
      "Train Epoch: 4 [105600/110534 (96%)]\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 4 [107520/110534 (97%)]\tClassification Loss: 1.6688\r\n",
      "Train Epoch: 4 [109440/110534 (99%)]\tClassification Loss: 1.8074\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/110534 (0%)]\tClassification Loss: 1.8967\r\n",
      "\r\n",
      "Test set: Average loss: 1.5175, Accuracy: 22253/42368 (53%)\r\n",
      "\r\n",
      "Train Epoch: 5 [1920/110534 (2%)]\tClassification Loss: 1.5911\r\n",
      "Train Epoch: 5 [3840/110534 (3%)]\tClassification Loss: 1.5268\r\n",
      "Train Epoch: 5 [5760/110534 (5%)]\tClassification Loss: 1.7718\r\n",
      "Train Epoch: 5 [7680/110534 (7%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 5 [9600/110534 (9%)]\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 5 [11520/110534 (10%)]\tClassification Loss: 1.8541\r\n",
      "Train Epoch: 5 [13440/110534 (12%)]\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 5 [15360/110534 (14%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 5 [17280/110534 (16%)]\tClassification Loss: 1.7756\r\n",
      "Train Epoch: 5 [19200/110534 (17%)]\tClassification Loss: 1.8360\r\n",
      "Train Epoch: 5 [21120/110534 (19%)]\tClassification Loss: 1.7098\r\n",
      "Train Epoch: 5 [23040/110534 (21%)]\tClassification Loss: 1.6022\r\n",
      "Train Epoch: 5 [24960/110534 (23%)]\tClassification Loss: 1.7831\r\n",
      "Train Epoch: 5 [26880/110534 (24%)]\tClassification Loss: 1.7222\r\n",
      "Train Epoch: 5 [28800/110534 (26%)]\tClassification Loss: 1.9366\r\n",
      "Train Epoch: 5 [30720/110534 (28%)]\tClassification Loss: 1.5841\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_500.pth.tar\r\n",
      "Train Epoch: 5 [32640/110534 (30%)]\tClassification Loss: 1.5612\r\n",
      "Train Epoch: 5 [34560/110534 (31%)]\tClassification Loss: 1.4411\r\n",
      "Train Epoch: 5 [36480/110534 (33%)]\tClassification Loss: 1.2888\r\n",
      "Train Epoch: 5 [38400/110534 (35%)]\tClassification Loss: 1.5774\r\n",
      "Train Epoch: 5 [40320/110534 (36%)]\tClassification Loss: 1.6569\r\n",
      "Train Epoch: 5 [42240/110534 (38%)]\tClassification Loss: 1.6085\r\n",
      "Train Epoch: 5 [44160/110534 (40%)]\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 5 [46080/110534 (42%)]\tClassification Loss: 1.6975\r\n",
      "Train Epoch: 5 [48000/110534 (43%)]\tClassification Loss: 1.6413\r\n",
      "Train Epoch: 5 [49920/110534 (45%)]\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 5 [51840/110534 (47%)]\tClassification Loss: 1.9578\r\n",
      "Train Epoch: 5 [53760/110534 (49%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 5 [55680/110534 (50%)]\tClassification Loss: 1.5651\r\n",
      "Train Epoch: 5 [57600/110534 (52%)]\tClassification Loss: 1.6745\r\n",
      "Train Epoch: 5 [59520/110534 (54%)]\tClassification Loss: 1.7771\r\n",
      "Train Epoch: 5 [61440/110534 (56%)]\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 5 [63360/110534 (57%)]\tClassification Loss: 1.8376\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1000.pth.tar\r\n",
      "Train Epoch: 5 [65280/110534 (59%)]\tClassification Loss: 1.6845\r\n",
      "Train Epoch: 5 [67200/110534 (61%)]\tClassification Loss: 1.4422\r\n",
      "Train Epoch: 5 [69120/110534 (63%)]\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 5 [71040/110534 (64%)]\tClassification Loss: 1.4050\r\n",
      "Train Epoch: 5 [72960/110534 (66%)]\tClassification Loss: 1.4743\r\n",
      "Train Epoch: 5 [74880/110534 (68%)]\tClassification Loss: 1.5088\r\n",
      "Train Epoch: 5 [76800/110534 (69%)]\tClassification Loss: 1.3641\r\n",
      "Train Epoch: 5 [78720/110534 (71%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 5 [80640/110534 (73%)]\tClassification Loss: 1.6534\r\n",
      "Train Epoch: 5 [82560/110534 (75%)]\tClassification Loss: 1.4079\r\n",
      "Train Epoch: 5 [84480/110534 (76%)]\tClassification Loss: 1.4596\r\n",
      "Train Epoch: 5 [86400/110534 (78%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 5 [88320/110534 (80%)]\tClassification Loss: 1.7924\r\n",
      "Train Epoch: 5 [90240/110534 (82%)]\tClassification Loss: 1.6766\r\n",
      "Train Epoch: 5 [92160/110534 (83%)]\tClassification Loss: 1.6712\r\n",
      "Train Epoch: 5 [94080/110534 (85%)]\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 5 [96000/110534 (87%)]\tClassification Loss: 1.7391\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [97920/110534 (89%)]\tClassification Loss: 1.5572\r\n",
      "Train Epoch: 5 [99840/110534 (90%)]\tClassification Loss: 1.6362\r\n",
      "Train Epoch: 5 [101760/110534 (92%)]\tClassification Loss: 1.5667\r\n",
      "Train Epoch: 5 [103680/110534 (94%)]\tClassification Loss: 1.7213\r\n",
      "Train Epoch: 5 [105600/110534 (96%)]\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 5 [107520/110534 (97%)]\tClassification Loss: 1.5027\r\n",
      "Train Epoch: 5 [109440/110534 (99%)]\tClassification Loss: 1.7817\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/110534 (0%)]\tClassification Loss: 1.6679\r\n",
      "\r\n",
      "Test set: Average loss: 1.5019, Accuracy: 22453/42368 (53%)\r\n",
      "\r\n",
      "Train Epoch: 6 [1920/110534 (2%)]\tClassification Loss: 1.4779\r\n",
      "Train Epoch: 6 [3840/110534 (3%)]\tClassification Loss: 1.6093\r\n",
      "Train Epoch: 6 [5760/110534 (5%)]\tClassification Loss: 1.9675\r\n",
      "Train Epoch: 6 [7680/110534 (7%)]\tClassification Loss: 1.4451\r\n",
      "Train Epoch: 6 [9600/110534 (9%)]\tClassification Loss: 1.6715\r\n",
      "Train Epoch: 6 [11520/110534 (10%)]\tClassification Loss: 1.7027\r\n",
      "Train Epoch: 6 [13440/110534 (12%)]\tClassification Loss: 1.6060\r\n",
      "Train Epoch: 6 [15360/110534 (14%)]\tClassification Loss: 1.7395\r\n",
      "Train Epoch: 6 [17280/110534 (16%)]\tClassification Loss: 1.8150\r\n",
      "Train Epoch: 6 [19200/110534 (17%)]\tClassification Loss: 1.8024\r\n",
      "Train Epoch: 6 [21120/110534 (19%)]\tClassification Loss: 1.7538\r\n",
      "Train Epoch: 6 [23040/110534 (21%)]\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 6 [24960/110534 (23%)]\tClassification Loss: 1.7764\r\n",
      "Train Epoch: 6 [26880/110534 (24%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 6 [28800/110534 (26%)]\tClassification Loss: 1.8129\r\n",
      "Train Epoch: 6 [30720/110534 (28%)]\tClassification Loss: 1.6740\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_500.pth.tar\r\n",
      "Train Epoch: 6 [32640/110534 (30%)]\tClassification Loss: 1.6007\r\n",
      "Train Epoch: 6 [34560/110534 (31%)]\tClassification Loss: 1.3576\r\n",
      "Train Epoch: 6 [36480/110534 (33%)]\tClassification Loss: 1.4003\r\n",
      "Train Epoch: 6 [38400/110534 (35%)]\tClassification Loss: 1.6587\r\n",
      "Train Epoch: 6 [40320/110534 (36%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 6 [42240/110534 (38%)]\tClassification Loss: 1.7516\r\n",
      "Train Epoch: 6 [44160/110534 (40%)]\tClassification Loss: 1.6180\r\n",
      "Train Epoch: 6 [46080/110534 (42%)]\tClassification Loss: 1.4535\r\n",
      "Train Epoch: 6 [48000/110534 (43%)]\tClassification Loss: 1.7189\r\n",
      "Train Epoch: 6 [49920/110534 (45%)]\tClassification Loss: 1.6897\r\n",
      "Train Epoch: 6 [51840/110534 (47%)]\tClassification Loss: 1.7409\r\n",
      "Train Epoch: 6 [53760/110534 (49%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 6 [55680/110534 (50%)]\tClassification Loss: 1.6121\r\n",
      "Train Epoch: 6 [57600/110534 (52%)]\tClassification Loss: 1.5248\r\n",
      "Train Epoch: 6 [59520/110534 (54%)]\tClassification Loss: 2.0240\r\n",
      "Train Epoch: 6 [61440/110534 (56%)]\tClassification Loss: 1.5537\r\n",
      "Train Epoch: 6 [63360/110534 (57%)]\tClassification Loss: 1.8792\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1000.pth.tar\r\n",
      "Train Epoch: 6 [65280/110534 (59%)]\tClassification Loss: 1.5754\r\n",
      "Train Epoch: 6 [67200/110534 (61%)]\tClassification Loss: 1.4508\r\n",
      "Train Epoch: 6 [69120/110534 (63%)]\tClassification Loss: 1.5269\r\n",
      "Train Epoch: 6 [71040/110534 (64%)]\tClassification Loss: 1.5441\r\n",
      "Train Epoch: 6 [72960/110534 (66%)]\tClassification Loss: 1.4529\r\n",
      "Train Epoch: 6 [74880/110534 (68%)]\tClassification Loss: 1.4409\r\n",
      "Train Epoch: 6 [76800/110534 (69%)]\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 6 [78720/110534 (71%)]\tClassification Loss: 1.5746\r\n",
      "Train Epoch: 6 [80640/110534 (73%)]\tClassification Loss: 1.6634\r\n",
      "Train Epoch: 6 [82560/110534 (75%)]\tClassification Loss: 1.3312\r\n",
      "Train Epoch: 6 [84480/110534 (76%)]\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 6 [86400/110534 (78%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 6 [88320/110534 (80%)]\tClassification Loss: 1.7003\r\n",
      "Train Epoch: 6 [90240/110534 (82%)]\tClassification Loss: 1.7510\r\n",
      "Train Epoch: 6 [92160/110534 (83%)]\tClassification Loss: 1.6860\r\n",
      "Train Epoch: 6 [94080/110534 (85%)]\tClassification Loss: 1.7391\r\n",
      "Train Epoch: 6 [96000/110534 (87%)]\tClassification Loss: 1.8180\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [97920/110534 (89%)]\tClassification Loss: 1.5925\r\n",
      "Train Epoch: 6 [99840/110534 (90%)]\tClassification Loss: 1.7230\r\n",
      "Train Epoch: 6 [101760/110534 (92%)]\tClassification Loss: 1.4502\r\n",
      "Train Epoch: 6 [103680/110534 (94%)]\tClassification Loss: 1.7223\r\n",
      "Train Epoch: 6 [105600/110534 (96%)]\tClassification Loss: 1.5332\r\n",
      "Train Epoch: 6 [107520/110534 (97%)]\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 6 [109440/110534 (99%)]\tClassification Loss: 1.7340\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_final.pth.tar\r\n",
      "Train Epoch: 7 [0/110534 (0%)]\tClassification Loss: 1.6601\r\n",
      "\r\n",
      "Test set: Average loss: 1.4915, Accuracy: 22541/42368 (53%)\r\n",
      "\r\n",
      "Train Epoch: 7 [1920/110534 (2%)]\tClassification Loss: 1.3981\r\n",
      "Train Epoch: 7 [3840/110534 (3%)]\tClassification Loss: 1.6063\r\n",
      "Train Epoch: 7 [5760/110534 (5%)]\tClassification Loss: 1.7852\r\n",
      "Train Epoch: 7 [7680/110534 (7%)]\tClassification Loss: 1.7070\r\n",
      "Train Epoch: 7 [9600/110534 (9%)]\tClassification Loss: 1.6199\r\n",
      "Train Epoch: 7 [11520/110534 (10%)]\tClassification Loss: 1.6829\r\n",
      "Train Epoch: 7 [13440/110534 (12%)]\tClassification Loss: 1.5632\r\n",
      "Train Epoch: 7 [15360/110534 (14%)]\tClassification Loss: 1.7020\r\n",
      "Train Epoch: 7 [17280/110534 (16%)]\tClassification Loss: 1.7856\r\n",
      "Train Epoch: 7 [19200/110534 (17%)]\tClassification Loss: 1.7426\r\n",
      "Train Epoch: 7 [21120/110534 (19%)]\tClassification Loss: 1.5806\r\n",
      "Train Epoch: 7 [23040/110534 (21%)]\tClassification Loss: 1.5805\r\n",
      "Train Epoch: 7 [24960/110534 (23%)]\tClassification Loss: 1.5828\r\n",
      "Train Epoch: 7 [26880/110534 (24%)]\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 7 [28800/110534 (26%)]\tClassification Loss: 1.8421\r\n",
      "Train Epoch: 7 [30720/110534 (28%)]\tClassification Loss: 1.6657\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_500.pth.tar\r\n",
      "Train Epoch: 7 [32640/110534 (30%)]\tClassification Loss: 1.5894\r\n",
      "Train Epoch: 7 [34560/110534 (31%)]\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 7 [36480/110534 (33%)]\tClassification Loss: 1.4301\r\n",
      "Train Epoch: 7 [38400/110534 (35%)]\tClassification Loss: 1.4958\r\n",
      "Train Epoch: 7 [40320/110534 (36%)]\tClassification Loss: 1.7274\r\n",
      "Train Epoch: 7 [42240/110534 (38%)]\tClassification Loss: 1.6816\r\n",
      "Train Epoch: 7 [44160/110534 (40%)]\tClassification Loss: 1.3438\r\n",
      "Train Epoch: 7 [46080/110534 (42%)]\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 7 [48000/110534 (43%)]\tClassification Loss: 1.6908\r\n",
      "Train Epoch: 7 [49920/110534 (45%)]\tClassification Loss: 1.6354\r\n",
      "Train Epoch: 7 [51840/110534 (47%)]\tClassification Loss: 1.9599\r\n",
      "Train Epoch: 7 [53760/110534 (49%)]\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 7 [55680/110534 (50%)]\tClassification Loss: 1.4750\r\n",
      "Train Epoch: 7 [57600/110534 (52%)]\tClassification Loss: 1.5944\r\n",
      "Train Epoch: 7 [59520/110534 (54%)]\tClassification Loss: 2.0211\r\n",
      "Train Epoch: 7 [61440/110534 (56%)]\tClassification Loss: 1.5957\r\n",
      "Train Epoch: 7 [63360/110534 (57%)]\tClassification Loss: 1.9032\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1000.pth.tar\r\n",
      "Train Epoch: 7 [65280/110534 (59%)]\tClassification Loss: 1.7887\r\n",
      "Train Epoch: 7 [67200/110534 (61%)]\tClassification Loss: 1.3477\r\n",
      "Train Epoch: 7 [69120/110534 (63%)]\tClassification Loss: 1.4545\r\n",
      "Train Epoch: 7 [71040/110534 (64%)]\tClassification Loss: 1.6475\r\n",
      "Train Epoch: 7 [72960/110534 (66%)]\tClassification Loss: 1.4041\r\n",
      "Train Epoch: 7 [74880/110534 (68%)]\tClassification Loss: 1.3599\r\n",
      "Train Epoch: 7 [76800/110534 (69%)]\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 7 [78720/110534 (71%)]\tClassification Loss: 1.6013\r\n",
      "Train Epoch: 7 [80640/110534 (73%)]\tClassification Loss: 1.6290\r\n",
      "Train Epoch: 7 [82560/110534 (75%)]\tClassification Loss: 1.5562\r\n",
      "Train Epoch: 7 [84480/110534 (76%)]\tClassification Loss: 1.5185\r\n",
      "Train Epoch: 7 [86400/110534 (78%)]\tClassification Loss: 1.5345\r\n",
      "Train Epoch: 7 [88320/110534 (80%)]\tClassification Loss: 1.8164\r\n",
      "Train Epoch: 7 [90240/110534 (82%)]\tClassification Loss: 1.7678\r\n",
      "Train Epoch: 7 [92160/110534 (83%)]\tClassification Loss: 1.9201\r\n",
      "Train Epoch: 7 [94080/110534 (85%)]\tClassification Loss: 1.6361\r\n",
      "Train Epoch: 7 [96000/110534 (87%)]\tClassification Loss: 1.7583\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_1500.pth.tar\r\n",
      "Train Epoch: 7 [97920/110534 (89%)]\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 7 [99840/110534 (90%)]\tClassification Loss: 1.7407\r\n",
      "Train Epoch: 7 [101760/110534 (92%)]\tClassification Loss: 1.5583\r\n",
      "Train Epoch: 7 [103680/110534 (94%)]\tClassification Loss: 1.6763\r\n",
      "Train Epoch: 7 [105600/110534 (96%)]\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 7 [107520/110534 (97%)]\tClassification Loss: 1.7149\r\n",
      "Train Epoch: 7 [109440/110534 (99%)]\tClassification Loss: 1.5850\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_7_final.pth.tar\r\n",
      "Train Epoch: 8 [0/110534 (0%)]\tClassification Loss: 1.7301\r\n",
      "\r\n",
      "Test set: Average loss: 1.4826, Accuracy: 22675/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 8 [1920/110534 (2%)]\tClassification Loss: 1.5227\r\n",
      "Train Epoch: 8 [3840/110534 (3%)]\tClassification Loss: 1.5864\r\n",
      "Train Epoch: 8 [5760/110534 (5%)]\tClassification Loss: 1.7470\r\n",
      "Train Epoch: 8 [7680/110534 (7%)]\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 8 [9600/110534 (9%)]\tClassification Loss: 1.6213\r\n",
      "Train Epoch: 8 [11520/110534 (10%)]\tClassification Loss: 1.7706\r\n",
      "Train Epoch: 8 [13440/110534 (12%)]\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 8 [15360/110534 (14%)]\tClassification Loss: 1.7304\r\n",
      "Train Epoch: 8 [17280/110534 (16%)]\tClassification Loss: 1.9131\r\n",
      "Train Epoch: 8 [19200/110534 (17%)]\tClassification Loss: 1.8148\r\n",
      "Train Epoch: 8 [21120/110534 (19%)]\tClassification Loss: 1.6195\r\n",
      "Train Epoch: 8 [23040/110534 (21%)]\tClassification Loss: 1.4502\r\n",
      "Train Epoch: 8 [24960/110534 (23%)]\tClassification Loss: 1.7225\r\n",
      "Train Epoch: 8 [26880/110534 (24%)]\tClassification Loss: 1.6633\r\n",
      "Train Epoch: 8 [28800/110534 (26%)]\tClassification Loss: 1.8913\r\n",
      "Train Epoch: 8 [30720/110534 (28%)]\tClassification Loss: 1.5155\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_500.pth.tar\r\n",
      "Train Epoch: 8 [32640/110534 (30%)]\tClassification Loss: 1.5959\r\n",
      "Train Epoch: 8 [34560/110534 (31%)]\tClassification Loss: 1.2363\r\n",
      "Train Epoch: 8 [36480/110534 (33%)]\tClassification Loss: 1.3970\r\n",
      "Train Epoch: 8 [38400/110534 (35%)]\tClassification Loss: 1.5872\r\n",
      "Train Epoch: 8 [40320/110534 (36%)]\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 8 [42240/110534 (38%)]\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 8 [44160/110534 (40%)]\tClassification Loss: 1.3281\r\n",
      "Train Epoch: 8 [46080/110534 (42%)]\tClassification Loss: 1.4293\r\n",
      "Train Epoch: 8 [48000/110534 (43%)]\tClassification Loss: 1.7160\r\n",
      "Train Epoch: 8 [49920/110534 (45%)]\tClassification Loss: 1.7630\r\n",
      "Train Epoch: 8 [51840/110534 (47%)]\tClassification Loss: 1.7026\r\n",
      "Train Epoch: 8 [53760/110534 (49%)]\tClassification Loss: 1.4639\r\n",
      "Train Epoch: 8 [55680/110534 (50%)]\tClassification Loss: 1.6186\r\n",
      "Train Epoch: 8 [57600/110534 (52%)]\tClassification Loss: 1.5893\r\n",
      "Train Epoch: 8 [59520/110534 (54%)]\tClassification Loss: 1.8891\r\n",
      "Train Epoch: 8 [61440/110534 (56%)]\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 8 [63360/110534 (57%)]\tClassification Loss: 1.7944\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1000.pth.tar\r\n",
      "Train Epoch: 8 [65280/110534 (59%)]\tClassification Loss: 1.6593\r\n",
      "Train Epoch: 8 [67200/110534 (61%)]\tClassification Loss: 1.4805\r\n",
      "Train Epoch: 8 [69120/110534 (63%)]\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 8 [71040/110534 (64%)]\tClassification Loss: 1.5296\r\n",
      "Train Epoch: 8 [72960/110534 (66%)]\tClassification Loss: 1.4786\r\n",
      "Train Epoch: 8 [74880/110534 (68%)]\tClassification Loss: 1.4369\r\n",
      "Train Epoch: 8 [76800/110534 (69%)]\tClassification Loss: 1.4578\r\n",
      "Train Epoch: 8 [78720/110534 (71%)]\tClassification Loss: 1.4607\r\n",
      "Train Epoch: 8 [80640/110534 (73%)]\tClassification Loss: 1.7224\r\n",
      "Train Epoch: 8 [82560/110534 (75%)]\tClassification Loss: 1.4614\r\n",
      "Train Epoch: 8 [84480/110534 (76%)]\tClassification Loss: 1.4335\r\n",
      "Train Epoch: 8 [86400/110534 (78%)]\tClassification Loss: 1.4663\r\n",
      "Train Epoch: 8 [88320/110534 (80%)]\tClassification Loss: 1.8775\r\n",
      "Train Epoch: 8 [90240/110534 (82%)]\tClassification Loss: 1.7165\r\n",
      "Train Epoch: 8 [92160/110534 (83%)]\tClassification Loss: 1.9566\r\n",
      "Train Epoch: 8 [94080/110534 (85%)]\tClassification Loss: 1.6819\r\n",
      "Train Epoch: 8 [96000/110534 (87%)]\tClassification Loss: 1.8782\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_1500.pth.tar\r\n",
      "Train Epoch: 8 [97920/110534 (89%)]\tClassification Loss: 1.5429\r\n",
      "Train Epoch: 8 [99840/110534 (90%)]\tClassification Loss: 1.7826\r\n",
      "Train Epoch: 8 [101760/110534 (92%)]\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 8 [103680/110534 (94%)]\tClassification Loss: 1.7432\r\n",
      "Train Epoch: 8 [105600/110534 (96%)]\tClassification Loss: 1.5857\r\n",
      "Train Epoch: 8 [107520/110534 (97%)]\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 8 [109440/110534 (99%)]\tClassification Loss: 1.6682\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_8_final.pth.tar\r\n",
      "Train Epoch: 9 [0/110534 (0%)]\tClassification Loss: 1.6203\r\n",
      "\r\n",
      "Test set: Average loss: 1.4767, Accuracy: 22722/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 9 [1920/110534 (2%)]\tClassification Loss: 1.4905\r\n",
      "Train Epoch: 9 [3840/110534 (3%)]\tClassification Loss: 1.5836\r\n",
      "Train Epoch: 9 [5760/110534 (5%)]\tClassification Loss: 1.6874\r\n",
      "Train Epoch: 9 [7680/110534 (7%)]\tClassification Loss: 1.5623\r\n",
      "Train Epoch: 9 [9600/110534 (9%)]\tClassification Loss: 1.6884\r\n",
      "Train Epoch: 9 [11520/110534 (10%)]\tClassification Loss: 1.7104\r\n",
      "Train Epoch: 9 [13440/110534 (12%)]\tClassification Loss: 1.5731\r\n",
      "Train Epoch: 9 [15360/110534 (14%)]\tClassification Loss: 1.7001\r\n",
      "Train Epoch: 9 [17280/110534 (16%)]\tClassification Loss: 1.7851\r\n",
      "Train Epoch: 9 [19200/110534 (17%)]\tClassification Loss: 1.9936\r\n",
      "Train Epoch: 9 [21120/110534 (19%)]\tClassification Loss: 1.5973\r\n",
      "Train Epoch: 9 [23040/110534 (21%)]\tClassification Loss: 1.4969\r\n",
      "Train Epoch: 9 [24960/110534 (23%)]\tClassification Loss: 1.5815\r\n",
      "Train Epoch: 9 [26880/110534 (24%)]\tClassification Loss: 1.6512\r\n",
      "Train Epoch: 9 [28800/110534 (26%)]\tClassification Loss: 1.7903\r\n",
      "Train Epoch: 9 [30720/110534 (28%)]\tClassification Loss: 1.6086\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_500.pth.tar\r\n",
      "Train Epoch: 9 [32640/110534 (30%)]\tClassification Loss: 1.6543\r\n",
      "Train Epoch: 9 [34560/110534 (31%)]\tClassification Loss: 1.4091\r\n",
      "Train Epoch: 9 [36480/110534 (33%)]\tClassification Loss: 1.2684\r\n",
      "Train Epoch: 9 [38400/110534 (35%)]\tClassification Loss: 1.3951\r\n",
      "Train Epoch: 9 [40320/110534 (36%)]\tClassification Loss: 1.5030\r\n",
      "Train Epoch: 9 [42240/110534 (38%)]\tClassification Loss: 1.5586\r\n",
      "Train Epoch: 9 [44160/110534 (40%)]\tClassification Loss: 1.4728\r\n",
      "Train Epoch: 9 [46080/110534 (42%)]\tClassification Loss: 1.5976\r\n",
      "Train Epoch: 9 [48000/110534 (43%)]\tClassification Loss: 1.5869\r\n",
      "Train Epoch: 9 [49920/110534 (45%)]\tClassification Loss: 1.7978\r\n",
      "Train Epoch: 9 [51840/110534 (47%)]\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 9 [53760/110534 (49%)]\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 9 [55680/110534 (50%)]\tClassification Loss: 1.6244\r\n",
      "Train Epoch: 9 [57600/110534 (52%)]\tClassification Loss: 1.4606\r\n",
      "Train Epoch: 9 [59520/110534 (54%)]\tClassification Loss: 1.9559\r\n",
      "Train Epoch: 9 [61440/110534 (56%)]\tClassification Loss: 1.6756\r\n",
      "Train Epoch: 9 [63360/110534 (57%)]\tClassification Loss: 1.8177\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1000.pth.tar\r\n",
      "Train Epoch: 9 [65280/110534 (59%)]\tClassification Loss: 1.6571\r\n",
      "Train Epoch: 9 [67200/110534 (61%)]\tClassification Loss: 1.3526\r\n",
      "Train Epoch: 9 [69120/110534 (63%)]\tClassification Loss: 1.5139\r\n",
      "Train Epoch: 9 [71040/110534 (64%)]\tClassification Loss: 1.5220\r\n",
      "Train Epoch: 9 [72960/110534 (66%)]\tClassification Loss: 1.4837\r\n",
      "Train Epoch: 9 [74880/110534 (68%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 9 [76800/110534 (69%)]\tClassification Loss: 1.4736\r\n",
      "Train Epoch: 9 [78720/110534 (71%)]\tClassification Loss: 1.4265\r\n",
      "Train Epoch: 9 [80640/110534 (73%)]\tClassification Loss: 1.5763\r\n",
      "Train Epoch: 9 [82560/110534 (75%)]\tClassification Loss: 1.4063\r\n",
      "Train Epoch: 9 [84480/110534 (76%)]\tClassification Loss: 1.5077\r\n",
      "Train Epoch: 9 [86400/110534 (78%)]\tClassification Loss: 1.4269\r\n",
      "Train Epoch: 9 [88320/110534 (80%)]\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 9 [90240/110534 (82%)]\tClassification Loss: 1.7071\r\n",
      "Train Epoch: 9 [92160/110534 (83%)]\tClassification Loss: 1.9069\r\n",
      "Train Epoch: 9 [94080/110534 (85%)]\tClassification Loss: 1.6724\r\n",
      "Train Epoch: 9 [96000/110534 (87%)]\tClassification Loss: 1.6800\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_1500.pth.tar\r\n",
      "Train Epoch: 9 [97920/110534 (89%)]\tClassification Loss: 1.4820\r\n",
      "Train Epoch: 9 [99840/110534 (90%)]\tClassification Loss: 1.7627\r\n",
      "Train Epoch: 9 [101760/110534 (92%)]\tClassification Loss: 1.3753\r\n",
      "Train Epoch: 9 [103680/110534 (94%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 9 [105600/110534 (96%)]\tClassification Loss: 1.5563\r\n",
      "Train Epoch: 9 [107520/110534 (97%)]\tClassification Loss: 1.5213\r\n",
      "Train Epoch: 9 [109440/110534 (99%)]\tClassification Loss: 1.6601\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_9_final.pth.tar\r\n",
      "Train Epoch: 10 [0/110534 (0%)]\tClassification Loss: 1.6326\r\n",
      "\r\n",
      "Test set: Average loss: 1.4710, Accuracy: 22786/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 10 [1920/110534 (2%)]\tClassification Loss: 1.4986\r\n",
      "Train Epoch: 10 [3840/110534 (3%)]\tClassification Loss: 1.6241\r\n",
      "Train Epoch: 10 [5760/110534 (5%)]\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 10 [7680/110534 (7%)]\tClassification Loss: 1.4604\r\n",
      "Train Epoch: 10 [9600/110534 (9%)]\tClassification Loss: 1.7272\r\n",
      "Train Epoch: 10 [11520/110534 (10%)]\tClassification Loss: 1.8953\r\n",
      "Train Epoch: 10 [13440/110534 (12%)]\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 10 [15360/110534 (14%)]\tClassification Loss: 1.9192\r\n",
      "Train Epoch: 10 [17280/110534 (16%)]\tClassification Loss: 1.8978\r\n",
      "Train Epoch: 10 [19200/110534 (17%)]\tClassification Loss: 1.8165\r\n",
      "Train Epoch: 10 [21120/110534 (19%)]\tClassification Loss: 1.5775\r\n",
      "Train Epoch: 10 [23040/110534 (21%)]\tClassification Loss: 1.6682\r\n",
      "Train Epoch: 10 [24960/110534 (23%)]\tClassification Loss: 1.7038\r\n",
      "Train Epoch: 10 [26880/110534 (24%)]\tClassification Loss: 1.7461\r\n",
      "Train Epoch: 10 [28800/110534 (26%)]\tClassification Loss: 2.0021\r\n",
      "Train Epoch: 10 [30720/110534 (28%)]\tClassification Loss: 1.5295\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_500.pth.tar\r\n",
      "Train Epoch: 10 [32640/110534 (30%)]\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 10 [34560/110534 (31%)]\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 10 [36480/110534 (33%)]\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 10 [38400/110534 (35%)]\tClassification Loss: 1.4760\r\n",
      "Train Epoch: 10 [40320/110534 (36%)]\tClassification Loss: 1.5826\r\n",
      "Train Epoch: 10 [42240/110534 (38%)]\tClassification Loss: 1.5302\r\n",
      "Train Epoch: 10 [44160/110534 (40%)]\tClassification Loss: 1.5524\r\n",
      "Train Epoch: 10 [46080/110534 (42%)]\tClassification Loss: 1.6222\r\n",
      "Train Epoch: 10 [48000/110534 (43%)]\tClassification Loss: 1.7977\r\n",
      "Train Epoch: 10 [49920/110534 (45%)]\tClassification Loss: 1.6971\r\n",
      "Train Epoch: 10 [51840/110534 (47%)]\tClassification Loss: 1.8669\r\n",
      "Train Epoch: 10 [53760/110534 (49%)]\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 10 [55680/110534 (50%)]\tClassification Loss: 1.4514\r\n",
      "Train Epoch: 10 [57600/110534 (52%)]\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 10 [59520/110534 (54%)]\tClassification Loss: 1.9293\r\n",
      "Train Epoch: 10 [61440/110534 (56%)]\tClassification Loss: 1.3909\r\n",
      "Train Epoch: 10 [63360/110534 (57%)]\tClassification Loss: 1.8033\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1000.pth.tar\r\n",
      "Train Epoch: 10 [65280/110534 (59%)]\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 10 [67200/110534 (61%)]\tClassification Loss: 1.3218\r\n",
      "Train Epoch: 10 [69120/110534 (63%)]\tClassification Loss: 1.4544\r\n",
      "Train Epoch: 10 [71040/110534 (64%)]\tClassification Loss: 1.5263\r\n",
      "Train Epoch: 10 [72960/110534 (66%)]\tClassification Loss: 1.5746\r\n",
      "Train Epoch: 10 [74880/110534 (68%)]\tClassification Loss: 1.3982\r\n",
      "Train Epoch: 10 [76800/110534 (69%)]\tClassification Loss: 1.4845\r\n",
      "Train Epoch: 10 [78720/110534 (71%)]\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 10 [80640/110534 (73%)]\tClassification Loss: 1.7241\r\n",
      "Train Epoch: 10 [82560/110534 (75%)]\tClassification Loss: 1.2906\r\n",
      "Train Epoch: 10 [84480/110534 (76%)]\tClassification Loss: 1.4003\r\n",
      "Train Epoch: 10 [86400/110534 (78%)]\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 10 [88320/110534 (80%)]\tClassification Loss: 1.7820\r\n",
      "Train Epoch: 10 [90240/110534 (82%)]\tClassification Loss: 1.6376\r\n",
      "Train Epoch: 10 [92160/110534 (83%)]\tClassification Loss: 1.8156\r\n",
      "Train Epoch: 10 [94080/110534 (85%)]\tClassification Loss: 1.6669\r\n",
      "Train Epoch: 10 [96000/110534 (87%)]\tClassification Loss: 1.7744\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_1500.pth.tar\r\n",
      "Train Epoch: 10 [97920/110534 (89%)]\tClassification Loss: 1.5397\r\n",
      "Train Epoch: 10 [99840/110534 (90%)]\tClassification Loss: 1.6063\r\n",
      "Train Epoch: 10 [101760/110534 (92%)]\tClassification Loss: 1.4146\r\n",
      "Train Epoch: 10 [103680/110534 (94%)]\tClassification Loss: 1.6560\r\n",
      "Train Epoch: 10 [105600/110534 (96%)]\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 10 [107520/110534 (97%)]\tClassification Loss: 1.6533\r\n",
      "Train Epoch: 10 [109440/110534 (99%)]\tClassification Loss: 1.6244\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_10_final.pth.tar\r\n",
      "Train Epoch: 11 [0/110534 (0%)]\tClassification Loss: 1.6191\r\n",
      "\r\n",
      "Test set: Average loss: 1.4690, Accuracy: 22812/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 11 [1920/110534 (2%)]\tClassification Loss: 1.4494\r\n",
      "Train Epoch: 11 [3840/110534 (3%)]\tClassification Loss: 1.5130\r\n",
      "Train Epoch: 11 [5760/110534 (5%)]\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 11 [7680/110534 (7%)]\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 11 [9600/110534 (9%)]\tClassification Loss: 1.5728\r\n",
      "Train Epoch: 11 [11520/110534 (10%)]\tClassification Loss: 1.6317\r\n",
      "Train Epoch: 11 [13440/110534 (12%)]\tClassification Loss: 1.5874\r\n",
      "Train Epoch: 11 [15360/110534 (14%)]\tClassification Loss: 1.8860\r\n",
      "Train Epoch: 11 [17280/110534 (16%)]\tClassification Loss: 1.8623\r\n",
      "Train Epoch: 11 [19200/110534 (17%)]\tClassification Loss: 1.8559\r\n",
      "Train Epoch: 11 [21120/110534 (19%)]\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 11 [23040/110534 (21%)]\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 11 [24960/110534 (23%)]\tClassification Loss: 1.6487\r\n",
      "Train Epoch: 11 [26880/110534 (24%)]\tClassification Loss: 1.6362\r\n",
      "Train Epoch: 11 [28800/110534 (26%)]\tClassification Loss: 1.8532\r\n",
      "Train Epoch: 11 [30720/110534 (28%)]\tClassification Loss: 1.4460\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_500.pth.tar\r\n",
      "Train Epoch: 11 [32640/110534 (30%)]\tClassification Loss: 1.6469\r\n",
      "Train Epoch: 11 [34560/110534 (31%)]\tClassification Loss: 1.4719\r\n",
      "Train Epoch: 11 [36480/110534 (33%)]\tClassification Loss: 1.4967\r\n",
      "Train Epoch: 11 [38400/110534 (35%)]\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 11 [40320/110534 (36%)]\tClassification Loss: 1.4830\r\n",
      "Train Epoch: 11 [42240/110534 (38%)]\tClassification Loss: 1.4496\r\n",
      "Train Epoch: 11 [44160/110534 (40%)]\tClassification Loss: 1.6158\r\n",
      "Train Epoch: 11 [46080/110534 (42%)]\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 11 [48000/110534 (43%)]\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 11 [49920/110534 (45%)]\tClassification Loss: 1.6811\r\n",
      "Train Epoch: 11 [51840/110534 (47%)]\tClassification Loss: 1.9065\r\n",
      "Train Epoch: 11 [53760/110534 (49%)]\tClassification Loss: 1.3980\r\n",
      "Train Epoch: 11 [55680/110534 (50%)]\tClassification Loss: 1.4945\r\n",
      "Train Epoch: 11 [57600/110534 (52%)]\tClassification Loss: 1.6197\r\n",
      "Train Epoch: 11 [59520/110534 (54%)]\tClassification Loss: 1.8418\r\n",
      "Train Epoch: 11 [61440/110534 (56%)]\tClassification Loss: 1.5738\r\n",
      "Train Epoch: 11 [63360/110534 (57%)]\tClassification Loss: 1.8102\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_1000.pth.tar\r\n",
      "Train Epoch: 11 [65280/110534 (59%)]\tClassification Loss: 1.6476\r\n",
      "Train Epoch: 11 [67200/110534 (61%)]\tClassification Loss: 1.4301\r\n",
      "Train Epoch: 11 [69120/110534 (63%)]\tClassification Loss: 1.6587\r\n",
      "Train Epoch: 11 [71040/110534 (64%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 11 [72960/110534 (66%)]\tClassification Loss: 1.3989\r\n",
      "Train Epoch: 11 [74880/110534 (68%)]\tClassification Loss: 1.3852\r\n",
      "Train Epoch: 11 [76800/110534 (69%)]\tClassification Loss: 1.5545\r\n",
      "Train Epoch: 11 [78720/110534 (71%)]\tClassification Loss: 1.5886\r\n",
      "Train Epoch: 11 [80640/110534 (73%)]\tClassification Loss: 1.5935\r\n",
      "Train Epoch: 11 [82560/110534 (75%)]\tClassification Loss: 1.3822\r\n",
      "Train Epoch: 11 [84480/110534 (76%)]\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 11 [86400/110534 (78%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 11 [88320/110534 (80%)]\tClassification Loss: 1.9307\r\n",
      "Train Epoch: 11 [90240/110534 (82%)]\tClassification Loss: 1.6768\r\n",
      "Train Epoch: 11 [92160/110534 (83%)]\tClassification Loss: 1.8503\r\n",
      "Train Epoch: 11 [94080/110534 (85%)]\tClassification Loss: 1.7261\r\n",
      "Train Epoch: 11 [96000/110534 (87%)]\tClassification Loss: 1.8220\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_1500.pth.tar\r\n",
      "Train Epoch: 11 [97920/110534 (89%)]\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 11 [99840/110534 (90%)]\tClassification Loss: 1.6612\r\n",
      "Train Epoch: 11 [101760/110534 (92%)]\tClassification Loss: 1.4554\r\n",
      "Train Epoch: 11 [103680/110534 (94%)]\tClassification Loss: 1.7553\r\n",
      "Train Epoch: 11 [105600/110534 (96%)]\tClassification Loss: 1.5892\r\n",
      "Train Epoch: 11 [107520/110534 (97%)]\tClassification Loss: 1.4169\r\n",
      "Train Epoch: 11 [109440/110534 (99%)]\tClassification Loss: 1.5526\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_11_final.pth.tar\r\n",
      "Train Epoch: 12 [0/110534 (0%)]\tClassification Loss: 1.6434\r\n",
      "\r\n",
      "Test set: Average loss: 1.4665, Accuracy: 22818/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 12 [1920/110534 (2%)]\tClassification Loss: 1.3535\r\n",
      "Train Epoch: 12 [3840/110534 (3%)]\tClassification Loss: 1.5947\r\n",
      "Train Epoch: 12 [5760/110534 (5%)]\tClassification Loss: 1.7704\r\n",
      "Train Epoch: 12 [7680/110534 (7%)]\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 12 [9600/110534 (9%)]\tClassification Loss: 1.5759\r\n",
      "Train Epoch: 12 [11520/110534 (10%)]\tClassification Loss: 1.8303\r\n",
      "Train Epoch: 12 [13440/110534 (12%)]\tClassification Loss: 1.6766\r\n",
      "Train Epoch: 12 [15360/110534 (14%)]\tClassification Loss: 1.7707\r\n",
      "Train Epoch: 12 [17280/110534 (16%)]\tClassification Loss: 1.7196\r\n",
      "Train Epoch: 12 [19200/110534 (17%)]\tClassification Loss: 1.8251\r\n",
      "Train Epoch: 12 [21120/110534 (19%)]\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 12 [23040/110534 (21%)]\tClassification Loss: 1.3804\r\n",
      "Train Epoch: 12 [24960/110534 (23%)]\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 12 [26880/110534 (24%)]\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 12 [28800/110534 (26%)]\tClassification Loss: 1.9912\r\n",
      "Train Epoch: 12 [30720/110534 (28%)]\tClassification Loss: 1.5153\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_500.pth.tar\r\n",
      "Train Epoch: 12 [32640/110534 (30%)]\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 12 [34560/110534 (31%)]\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 12 [36480/110534 (33%)]\tClassification Loss: 1.2924\r\n",
      "Train Epoch: 12 [38400/110534 (35%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 12 [40320/110534 (36%)]\tClassification Loss: 1.5428\r\n",
      "Train Epoch: 12 [42240/110534 (38%)]\tClassification Loss: 1.5992\r\n",
      "Train Epoch: 12 [44160/110534 (40%)]\tClassification Loss: 1.4041\r\n",
      "Train Epoch: 12 [46080/110534 (42%)]\tClassification Loss: 1.4028\r\n",
      "Train Epoch: 12 [48000/110534 (43%)]\tClassification Loss: 1.6004\r\n",
      "Train Epoch: 12 [49920/110534 (45%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 12 [51840/110534 (47%)]\tClassification Loss: 1.7226\r\n",
      "Train Epoch: 12 [53760/110534 (49%)]\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 12 [55680/110534 (50%)]\tClassification Loss: 1.6823\r\n",
      "Train Epoch: 12 [57600/110534 (52%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 12 [59520/110534 (54%)]\tClassification Loss: 1.8885\r\n",
      "Train Epoch: 12 [61440/110534 (56%)]\tClassification Loss: 1.5745\r\n",
      "Train Epoch: 12 [63360/110534 (57%)]\tClassification Loss: 1.6065\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_1000.pth.tar\r\n",
      "Train Epoch: 12 [65280/110534 (59%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 12 [67200/110534 (61%)]\tClassification Loss: 1.5137\r\n",
      "Train Epoch: 12 [69120/110534 (63%)]\tClassification Loss: 1.6745\r\n",
      "Train Epoch: 12 [71040/110534 (64%)]\tClassification Loss: 1.6477\r\n",
      "Train Epoch: 12 [72960/110534 (66%)]\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 12 [74880/110534 (68%)]\tClassification Loss: 1.4349\r\n",
      "Train Epoch: 12 [76800/110534 (69%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 12 [78720/110534 (71%)]\tClassification Loss: 1.5181\r\n",
      "Train Epoch: 12 [80640/110534 (73%)]\tClassification Loss: 1.7457\r\n",
      "Train Epoch: 12 [82560/110534 (75%)]\tClassification Loss: 1.3805\r\n",
      "Train Epoch: 12 [84480/110534 (76%)]\tClassification Loss: 1.5221\r\n",
      "Train Epoch: 12 [86400/110534 (78%)]\tClassification Loss: 1.5496\r\n",
      "Train Epoch: 12 [88320/110534 (80%)]\tClassification Loss: 1.7867\r\n",
      "Train Epoch: 12 [90240/110534 (82%)]\tClassification Loss: 1.9297\r\n",
      "Train Epoch: 12 [92160/110534 (83%)]\tClassification Loss: 1.8566\r\n",
      "Train Epoch: 12 [94080/110534 (85%)]\tClassification Loss: 1.6881\r\n",
      "Train Epoch: 12 [96000/110534 (87%)]\tClassification Loss: 1.7651\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_1500.pth.tar\r\n",
      "Train Epoch: 12 [97920/110534 (89%)]\tClassification Loss: 1.6528\r\n",
      "Train Epoch: 12 [99840/110534 (90%)]\tClassification Loss: 1.5615\r\n",
      "Train Epoch: 12 [101760/110534 (92%)]\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 12 [103680/110534 (94%)]\tClassification Loss: 1.7209\r\n",
      "Train Epoch: 12 [105600/110534 (96%)]\tClassification Loss: 1.5789\r\n",
      "Train Epoch: 12 [107520/110534 (97%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 12 [109440/110534 (99%)]\tClassification Loss: 1.5758\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_12_final.pth.tar\r\n",
      "Train Epoch: 13 [0/110534 (0%)]\tClassification Loss: 1.5481\r\n",
      "\r\n",
      "Test set: Average loss: 1.4671, Accuracy: 22771/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 13 [1920/110534 (2%)]\tClassification Loss: 1.3480\r\n",
      "Train Epoch: 13 [3840/110534 (3%)]\tClassification Loss: 1.5841\r\n",
      "Train Epoch: 13 [5760/110534 (5%)]\tClassification Loss: 1.5996\r\n",
      "Train Epoch: 13 [7680/110534 (7%)]\tClassification Loss: 1.6026\r\n",
      "Train Epoch: 13 [9600/110534 (9%)]\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 13 [11520/110534 (10%)]\tClassification Loss: 1.7445\r\n",
      "Train Epoch: 13 [13440/110534 (12%)]\tClassification Loss: 1.7061\r\n",
      "Train Epoch: 13 [15360/110534 (14%)]\tClassification Loss: 1.7707\r\n",
      "Train Epoch: 13 [17280/110534 (16%)]\tClassification Loss: 1.7158\r\n",
      "Train Epoch: 13 [19200/110534 (17%)]\tClassification Loss: 2.0507\r\n",
      "Train Epoch: 13 [21120/110534 (19%)]\tClassification Loss: 1.3883\r\n",
      "Train Epoch: 13 [23040/110534 (21%)]\tClassification Loss: 1.3829\r\n",
      "Train Epoch: 13 [24960/110534 (23%)]\tClassification Loss: 1.6204\r\n",
      "Train Epoch: 13 [26880/110534 (24%)]\tClassification Loss: 1.6880\r\n",
      "Train Epoch: 13 [28800/110534 (26%)]\tClassification Loss: 2.0507\r\n",
      "Train Epoch: 13 [30720/110534 (28%)]\tClassification Loss: 1.4934\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_500.pth.tar\r\n",
      "Train Epoch: 13 [32640/110534 (30%)]\tClassification Loss: 1.6667\r\n",
      "Train Epoch: 13 [34560/110534 (31%)]\tClassification Loss: 1.4922\r\n",
      "Train Epoch: 13 [36480/110534 (33%)]\tClassification Loss: 1.5099\r\n",
      "Train Epoch: 13 [38400/110534 (35%)]\tClassification Loss: 1.5378\r\n",
      "Train Epoch: 13 [40320/110534 (36%)]\tClassification Loss: 1.4433\r\n",
      "Train Epoch: 13 [42240/110534 (38%)]\tClassification Loss: 1.3724\r\n",
      "Train Epoch: 13 [44160/110534 (40%)]\tClassification Loss: 1.4358\r\n",
      "Train Epoch: 13 [46080/110534 (42%)]\tClassification Loss: 1.4686\r\n",
      "Train Epoch: 13 [48000/110534 (43%)]\tClassification Loss: 1.7063\r\n",
      "Train Epoch: 13 [49920/110534 (45%)]\tClassification Loss: 1.7141\r\n",
      "Train Epoch: 13 [51840/110534 (47%)]\tClassification Loss: 1.7501\r\n",
      "Train Epoch: 13 [53760/110534 (49%)]\tClassification Loss: 1.4478\r\n",
      "Train Epoch: 13 [55680/110534 (50%)]\tClassification Loss: 1.6502\r\n",
      "Train Epoch: 13 [57600/110534 (52%)]\tClassification Loss: 1.4209\r\n",
      "Train Epoch: 13 [59520/110534 (54%)]\tClassification Loss: 1.8057\r\n",
      "Train Epoch: 13 [61440/110534 (56%)]\tClassification Loss: 1.4342\r\n",
      "Train Epoch: 13 [63360/110534 (57%)]\tClassification Loss: 1.6945\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_1000.pth.tar\r\n",
      "Train Epoch: 13 [65280/110534 (59%)]\tClassification Loss: 1.6716\r\n",
      "Train Epoch: 13 [67200/110534 (61%)]\tClassification Loss: 1.3902\r\n",
      "Train Epoch: 13 [69120/110534 (63%)]\tClassification Loss: 1.5943\r\n",
      "Train Epoch: 13 [71040/110534 (64%)]\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 13 [72960/110534 (66%)]\tClassification Loss: 1.4672\r\n",
      "Train Epoch: 13 [74880/110534 (68%)]\tClassification Loss: 1.4947\r\n",
      "Train Epoch: 13 [76800/110534 (69%)]\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 13 [78720/110534 (71%)]\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 13 [80640/110534 (73%)]\tClassification Loss: 1.6524\r\n",
      "Train Epoch: 13 [82560/110534 (75%)]\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 13 [84480/110534 (76%)]\tClassification Loss: 1.4963\r\n",
      "Train Epoch: 13 [86400/110534 (78%)]\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 13 [88320/110534 (80%)]\tClassification Loss: 1.6375\r\n",
      "Train Epoch: 13 [90240/110534 (82%)]\tClassification Loss: 1.6634\r\n",
      "Train Epoch: 13 [92160/110534 (83%)]\tClassification Loss: 1.7891\r\n",
      "Train Epoch: 13 [94080/110534 (85%)]\tClassification Loss: 1.6743\r\n",
      "Train Epoch: 13 [96000/110534 (87%)]\tClassification Loss: 1.6732\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_1500.pth.tar\r\n",
      "Train Epoch: 13 [97920/110534 (89%)]\tClassification Loss: 1.5834\r\n",
      "Train Epoch: 13 [99840/110534 (90%)]\tClassification Loss: 1.7181\r\n",
      "Train Epoch: 13 [101760/110534 (92%)]\tClassification Loss: 1.4433\r\n",
      "Train Epoch: 13 [103680/110534 (94%)]\tClassification Loss: 1.6484\r\n",
      "Train Epoch: 13 [105600/110534 (96%)]\tClassification Loss: 1.6447\r\n",
      "Train Epoch: 13 [107520/110534 (97%)]\tClassification Loss: 1.5383\r\n",
      "Train Epoch: 13 [109440/110534 (99%)]\tClassification Loss: 1.6224\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_13_final.pth.tar\r\n",
      "Train Epoch: 14 [0/110534 (0%)]\tClassification Loss: 1.7039\r\n",
      "\r\n",
      "Test set: Average loss: 1.4636, Accuracy: 22781/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 14 [1920/110534 (2%)]\tClassification Loss: 1.3985\r\n",
      "Train Epoch: 14 [3840/110534 (3%)]\tClassification Loss: 1.6074\r\n",
      "Train Epoch: 14 [5760/110534 (5%)]\tClassification Loss: 1.7190\r\n",
      "Train Epoch: 14 [7680/110534 (7%)]\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 14 [9600/110534 (9%)]\tClassification Loss: 1.5080\r\n",
      "Train Epoch: 14 [11520/110534 (10%)]\tClassification Loss: 1.6958\r\n",
      "Train Epoch: 14 [13440/110534 (12%)]\tClassification Loss: 1.6147\r\n",
      "Train Epoch: 14 [15360/110534 (14%)]\tClassification Loss: 1.8211\r\n",
      "Train Epoch: 14 [17280/110534 (16%)]\tClassification Loss: 1.8565\r\n",
      "Train Epoch: 14 [19200/110534 (17%)]\tClassification Loss: 1.9132\r\n",
      "Train Epoch: 14 [21120/110534 (19%)]\tClassification Loss: 1.5626\r\n",
      "Train Epoch: 14 [23040/110534 (21%)]\tClassification Loss: 1.4359\r\n",
      "Train Epoch: 14 [24960/110534 (23%)]\tClassification Loss: 1.6245\r\n",
      "Train Epoch: 14 [26880/110534 (24%)]\tClassification Loss: 1.6062\r\n",
      "Train Epoch: 14 [28800/110534 (26%)]\tClassification Loss: 1.8913\r\n",
      "Train Epoch: 14 [30720/110534 (28%)]\tClassification Loss: 1.5351\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_500.pth.tar\r\n",
      "Train Epoch: 14 [32640/110534 (30%)]\tClassification Loss: 1.4333\r\n",
      "Train Epoch: 14 [34560/110534 (31%)]\tClassification Loss: 1.4060\r\n",
      "Train Epoch: 14 [36480/110534 (33%)]\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 14 [38400/110534 (35%)]\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 14 [40320/110534 (36%)]\tClassification Loss: 1.4931\r\n",
      "Train Epoch: 14 [42240/110534 (38%)]\tClassification Loss: 1.5578\r\n",
      "Train Epoch: 14 [44160/110534 (40%)]\tClassification Loss: 1.5239\r\n",
      "Train Epoch: 14 [46080/110534 (42%)]\tClassification Loss: 1.3613\r\n",
      "Train Epoch: 14 [48000/110534 (43%)]\tClassification Loss: 1.5513\r\n",
      "Train Epoch: 14 [49920/110534 (45%)]\tClassification Loss: 1.6118\r\n",
      "Train Epoch: 14 [51840/110534 (47%)]\tClassification Loss: 1.7837\r\n",
      "Train Epoch: 14 [53760/110534 (49%)]\tClassification Loss: 1.5613\r\n",
      "Train Epoch: 14 [55680/110534 (50%)]\tClassification Loss: 1.5477\r\n",
      "Train Epoch: 14 [57600/110534 (52%)]\tClassification Loss: 1.6255\r\n",
      "Train Epoch: 14 [59520/110534 (54%)]\tClassification Loss: 1.7720\r\n",
      "Train Epoch: 14 [61440/110534 (56%)]\tClassification Loss: 1.6008\r\n",
      "Train Epoch: 14 [63360/110534 (57%)]\tClassification Loss: 1.7337\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_1000.pth.tar\r\n",
      "Train Epoch: 14 [65280/110534 (59%)]\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 14 [67200/110534 (61%)]\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 14 [69120/110534 (63%)]\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 14 [71040/110534 (64%)]\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 14 [72960/110534 (66%)]\tClassification Loss: 1.4019\r\n",
      "Train Epoch: 14 [74880/110534 (68%)]\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 14 [76800/110534 (69%)]\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 14 [78720/110534 (71%)]\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 14 [80640/110534 (73%)]\tClassification Loss: 1.6804\r\n",
      "Train Epoch: 14 [82560/110534 (75%)]\tClassification Loss: 1.3230\r\n",
      "Train Epoch: 14 [84480/110534 (76%)]\tClassification Loss: 1.4761\r\n",
      "Train Epoch: 14 [86400/110534 (78%)]\tClassification Loss: 1.4703\r\n",
      "Train Epoch: 14 [88320/110534 (80%)]\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 14 [90240/110534 (82%)]\tClassification Loss: 1.5632\r\n",
      "Train Epoch: 14 [92160/110534 (83%)]\tClassification Loss: 1.8912\r\n",
      "Train Epoch: 14 [94080/110534 (85%)]\tClassification Loss: 1.6779\r\n",
      "Train Epoch: 14 [96000/110534 (87%)]\tClassification Loss: 1.8365\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_1500.pth.tar\r\n",
      "Train Epoch: 14 [97920/110534 (89%)]\tClassification Loss: 1.4100\r\n",
      "Train Epoch: 14 [99840/110534 (90%)]\tClassification Loss: 1.5590\r\n",
      "Train Epoch: 14 [101760/110534 (92%)]\tClassification Loss: 1.4407\r\n",
      "Train Epoch: 14 [103680/110534 (94%)]\tClassification Loss: 1.6462\r\n",
      "Train Epoch: 14 [105600/110534 (96%)]\tClassification Loss: 1.5229\r\n",
      "Train Epoch: 14 [107520/110534 (97%)]\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 14 [109440/110534 (99%)]\tClassification Loss: 1.5187\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_14_final.pth.tar\r\n",
      "Train Epoch: 15 [0/110534 (0%)]\tClassification Loss: 1.6241\r\n",
      "\r\n",
      "Test set: Average loss: 1.4645, Accuracy: 22739/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 15 [1920/110534 (2%)]\tClassification Loss: 1.4647\r\n",
      "Train Epoch: 15 [3840/110534 (3%)]\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 15 [5760/110534 (5%)]\tClassification Loss: 1.6929\r\n",
      "Train Epoch: 15 [7680/110534 (7%)]\tClassification Loss: 1.4810\r\n",
      "Train Epoch: 15 [9600/110534 (9%)]\tClassification Loss: 1.7248\r\n",
      "Train Epoch: 15 [11520/110534 (10%)]\tClassification Loss: 1.7698\r\n",
      "Train Epoch: 15 [13440/110534 (12%)]\tClassification Loss: 1.6018\r\n",
      "Train Epoch: 15 [15360/110534 (14%)]\tClassification Loss: 1.7517\r\n",
      "Train Epoch: 15 [17280/110534 (16%)]\tClassification Loss: 1.8941\r\n",
      "Train Epoch: 15 [19200/110534 (17%)]\tClassification Loss: 1.9131\r\n",
      "Train Epoch: 15 [21120/110534 (19%)]\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 15 [23040/110534 (21%)]\tClassification Loss: 1.3405\r\n",
      "Train Epoch: 15 [24960/110534 (23%)]\tClassification Loss: 1.5918\r\n",
      "Train Epoch: 15 [26880/110534 (24%)]\tClassification Loss: 1.5884\r\n",
      "Train Epoch: 15 [28800/110534 (26%)]\tClassification Loss: 1.9324\r\n",
      "Train Epoch: 15 [30720/110534 (28%)]\tClassification Loss: 1.4195\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_500.pth.tar\r\n",
      "Train Epoch: 15 [32640/110534 (30%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 15 [34560/110534 (31%)]\tClassification Loss: 1.3721\r\n",
      "Train Epoch: 15 [36480/110534 (33%)]\tClassification Loss: 1.3711\r\n",
      "Train Epoch: 15 [38400/110534 (35%)]\tClassification Loss: 1.6328\r\n",
      "Train Epoch: 15 [40320/110534 (36%)]\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 15 [42240/110534 (38%)]\tClassification Loss: 1.6164\r\n",
      "Train Epoch: 15 [44160/110534 (40%)]\tClassification Loss: 1.5001\r\n",
      "Train Epoch: 15 [46080/110534 (42%)]\tClassification Loss: 1.4299\r\n",
      "Train Epoch: 15 [48000/110534 (43%)]\tClassification Loss: 1.6834\r\n",
      "Train Epoch: 15 [49920/110534 (45%)]\tClassification Loss: 1.5716\r\n",
      "Train Epoch: 15 [51840/110534 (47%)]\tClassification Loss: 1.8609\r\n",
      "Train Epoch: 15 [53760/110534 (49%)]\tClassification Loss: 1.5685\r\n",
      "Train Epoch: 15 [55680/110534 (50%)]\tClassification Loss: 1.5050\r\n",
      "Train Epoch: 15 [57600/110534 (52%)]\tClassification Loss: 1.5440\r\n",
      "Train Epoch: 15 [59520/110534 (54%)]\tClassification Loss: 1.9112\r\n",
      "Train Epoch: 15 [61440/110534 (56%)]\tClassification Loss: 1.6866\r\n",
      "Train Epoch: 15 [63360/110534 (57%)]\tClassification Loss: 1.6865\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_1000.pth.tar\r\n",
      "Train Epoch: 15 [65280/110534 (59%)]\tClassification Loss: 1.5776\r\n",
      "Train Epoch: 15 [67200/110534 (61%)]\tClassification Loss: 1.3272\r\n",
      "Train Epoch: 15 [69120/110534 (63%)]\tClassification Loss: 1.4623\r\n",
      "Train Epoch: 15 [71040/110534 (64%)]\tClassification Loss: 1.4967\r\n",
      "Train Epoch: 15 [72960/110534 (66%)]\tClassification Loss: 1.5280\r\n",
      "Train Epoch: 15 [74880/110534 (68%)]\tClassification Loss: 1.5108\r\n",
      "Train Epoch: 15 [76800/110534 (69%)]\tClassification Loss: 1.4281\r\n",
      "Train Epoch: 15 [78720/110534 (71%)]\tClassification Loss: 1.3695\r\n",
      "Train Epoch: 15 [80640/110534 (73%)]\tClassification Loss: 1.7212\r\n",
      "Train Epoch: 15 [82560/110534 (75%)]\tClassification Loss: 1.4917\r\n",
      "Train Epoch: 15 [84480/110534 (76%)]\tClassification Loss: 1.5168\r\n",
      "Train Epoch: 15 [86400/110534 (78%)]\tClassification Loss: 1.4642\r\n",
      "Train Epoch: 15 [88320/110534 (80%)]\tClassification Loss: 1.8294\r\n",
      "Train Epoch: 15 [90240/110534 (82%)]\tClassification Loss: 1.7686\r\n",
      "Train Epoch: 15 [92160/110534 (83%)]\tClassification Loss: 1.9586\r\n",
      "Train Epoch: 15 [94080/110534 (85%)]\tClassification Loss: 1.8045\r\n",
      "Train Epoch: 15 [96000/110534 (87%)]\tClassification Loss: 1.6413\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_1500.pth.tar\r\n",
      "Train Epoch: 15 [97920/110534 (89%)]\tClassification Loss: 1.6307\r\n",
      "Train Epoch: 15 [99840/110534 (90%)]\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 15 [101760/110534 (92%)]\tClassification Loss: 1.3034\r\n",
      "Train Epoch: 15 [103680/110534 (94%)]\tClassification Loss: 1.6753\r\n",
      "Train Epoch: 15 [105600/110534 (96%)]\tClassification Loss: 1.4487\r\n",
      "Train Epoch: 15 [107520/110534 (97%)]\tClassification Loss: 1.6172\r\n",
      "Train Epoch: 15 [109440/110534 (99%)]\tClassification Loss: 1.7772\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_15_final.pth.tar\r\n",
      "Train Epoch: 16 [0/110534 (0%)]\tClassification Loss: 1.5467\r\n",
      "\r\n",
      "Test set: Average loss: 1.4630, Accuracy: 22760/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 16 [1920/110534 (2%)]\tClassification Loss: 1.4927\r\n",
      "Train Epoch: 16 [3840/110534 (3%)]\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 16 [5760/110534 (5%)]\tClassification Loss: 1.8162\r\n",
      "Train Epoch: 16 [7680/110534 (7%)]\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 16 [9600/110534 (9%)]\tClassification Loss: 1.7873\r\n",
      "Train Epoch: 16 [11520/110534 (10%)]\tClassification Loss: 1.7345\r\n",
      "Train Epoch: 16 [13440/110534 (12%)]\tClassification Loss: 1.7046\r\n",
      "Train Epoch: 16 [15360/110534 (14%)]\tClassification Loss: 1.8230\r\n",
      "Train Epoch: 16 [17280/110534 (16%)]\tClassification Loss: 1.8396\r\n",
      "Train Epoch: 16 [19200/110534 (17%)]\tClassification Loss: 1.9378\r\n",
      "Train Epoch: 16 [21120/110534 (19%)]\tClassification Loss: 1.5860\r\n",
      "Train Epoch: 16 [23040/110534 (21%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 16 [24960/110534 (23%)]\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 16 [26880/110534 (24%)]\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 16 [28800/110534 (26%)]\tClassification Loss: 2.0134\r\n",
      "Train Epoch: 16 [30720/110534 (28%)]\tClassification Loss: 1.3874\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_500.pth.tar\r\n",
      "Train Epoch: 16 [32640/110534 (30%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 16 [34560/110534 (31%)]\tClassification Loss: 1.4382\r\n",
      "Train Epoch: 16 [36480/110534 (33%)]\tClassification Loss: 1.4270\r\n",
      "Train Epoch: 16 [38400/110534 (35%)]\tClassification Loss: 1.5808\r\n",
      "Train Epoch: 16 [40320/110534 (36%)]\tClassification Loss: 1.5517\r\n",
      "Train Epoch: 16 [42240/110534 (38%)]\tClassification Loss: 1.4735\r\n",
      "Train Epoch: 16 [44160/110534 (40%)]\tClassification Loss: 1.4878\r\n",
      "Train Epoch: 16 [46080/110534 (42%)]\tClassification Loss: 1.5192\r\n",
      "Train Epoch: 16 [48000/110534 (43%)]\tClassification Loss: 1.6477\r\n",
      "Train Epoch: 16 [49920/110534 (45%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 16 [51840/110534 (47%)]\tClassification Loss: 1.7533\r\n",
      "Train Epoch: 16 [53760/110534 (49%)]\tClassification Loss: 1.3637\r\n",
      "Train Epoch: 16 [55680/110534 (50%)]\tClassification Loss: 1.6864\r\n",
      "Train Epoch: 16 [57600/110534 (52%)]\tClassification Loss: 1.5635\r\n",
      "Train Epoch: 16 [59520/110534 (54%)]\tClassification Loss: 1.9498\r\n",
      "Train Epoch: 16 [61440/110534 (56%)]\tClassification Loss: 1.7598\r\n",
      "Train Epoch: 16 [63360/110534 (57%)]\tClassification Loss: 1.6674\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_1000.pth.tar\r\n",
      "Train Epoch: 16 [65280/110534 (59%)]\tClassification Loss: 1.5729\r\n",
      "Train Epoch: 16 [67200/110534 (61%)]\tClassification Loss: 1.3945\r\n",
      "Train Epoch: 16 [69120/110534 (63%)]\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 16 [71040/110534 (64%)]\tClassification Loss: 1.4715\r\n",
      "Train Epoch: 16 [72960/110534 (66%)]\tClassification Loss: 1.5020\r\n",
      "Train Epoch: 16 [74880/110534 (68%)]\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 16 [76800/110534 (69%)]\tClassification Loss: 1.5636\r\n",
      "Train Epoch: 16 [78720/110534 (71%)]\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 16 [80640/110534 (73%)]\tClassification Loss: 1.5788\r\n",
      "Train Epoch: 16 [82560/110534 (75%)]\tClassification Loss: 1.4622\r\n",
      "Train Epoch: 16 [84480/110534 (76%)]\tClassification Loss: 1.4881\r\n",
      "Train Epoch: 16 [86400/110534 (78%)]\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 16 [88320/110534 (80%)]\tClassification Loss: 1.6782\r\n",
      "Train Epoch: 16 [90240/110534 (82%)]\tClassification Loss: 1.5735\r\n",
      "Train Epoch: 16 [92160/110534 (83%)]\tClassification Loss: 1.8727\r\n",
      "Train Epoch: 16 [94080/110534 (85%)]\tClassification Loss: 1.7638\r\n",
      "Train Epoch: 16 [96000/110534 (87%)]\tClassification Loss: 1.6020\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_1500.pth.tar\r\n",
      "Train Epoch: 16 [97920/110534 (89%)]\tClassification Loss: 1.6380\r\n",
      "Train Epoch: 16 [99840/110534 (90%)]\tClassification Loss: 1.6691\r\n",
      "Train Epoch: 16 [101760/110534 (92%)]\tClassification Loss: 1.4708\r\n",
      "Train Epoch: 16 [103680/110534 (94%)]\tClassification Loss: 1.7367\r\n",
      "Train Epoch: 16 [105600/110534 (96%)]\tClassification Loss: 1.6823\r\n",
      "Train Epoch: 16 [107520/110534 (97%)]\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 16 [109440/110534 (99%)]\tClassification Loss: 1.6284\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_16_final.pth.tar\r\n",
      "Train Epoch: 17 [0/110534 (0%)]\tClassification Loss: 1.6324\r\n",
      "\r\n",
      "Test set: Average loss: 1.4624, Accuracy: 22730/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 17 [1920/110534 (2%)]\tClassification Loss: 1.4938\r\n",
      "Train Epoch: 17 [3840/110534 (3%)]\tClassification Loss: 1.6335\r\n",
      "Train Epoch: 17 [5760/110534 (5%)]\tClassification Loss: 1.8203\r\n",
      "Train Epoch: 17 [7680/110534 (7%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 17 [9600/110534 (9%)]\tClassification Loss: 1.6215\r\n",
      "Train Epoch: 17 [11520/110534 (10%)]\tClassification Loss: 1.7454\r\n",
      "Train Epoch: 17 [13440/110534 (12%)]\tClassification Loss: 1.5851\r\n",
      "Train Epoch: 17 [15360/110534 (14%)]\tClassification Loss: 1.8009\r\n",
      "Train Epoch: 17 [17280/110534 (16%)]\tClassification Loss: 1.8544\r\n",
      "Train Epoch: 17 [19200/110534 (17%)]\tClassification Loss: 1.7913\r\n",
      "Train Epoch: 17 [21120/110534 (19%)]\tClassification Loss: 1.4610\r\n",
      "Train Epoch: 17 [23040/110534 (21%)]\tClassification Loss: 1.5880\r\n",
      "Train Epoch: 17 [24960/110534 (23%)]\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 17 [26880/110534 (24%)]\tClassification Loss: 1.6598\r\n",
      "Train Epoch: 17 [28800/110534 (26%)]\tClassification Loss: 1.9209\r\n",
      "Train Epoch: 17 [30720/110534 (28%)]\tClassification Loss: 1.4200\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_500.pth.tar\r\n",
      "Train Epoch: 17 [32640/110534 (30%)]\tClassification Loss: 1.4680\r\n",
      "Train Epoch: 17 [34560/110534 (31%)]\tClassification Loss: 1.2735\r\n",
      "Train Epoch: 17 [36480/110534 (33%)]\tClassification Loss: 1.5141\r\n",
      "Train Epoch: 17 [38400/110534 (35%)]\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 17 [40320/110534 (36%)]\tClassification Loss: 1.5429\r\n",
      "Train Epoch: 17 [42240/110534 (38%)]\tClassification Loss: 1.5785\r\n",
      "Train Epoch: 17 [44160/110534 (40%)]\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 17 [46080/110534 (42%)]\tClassification Loss: 1.5233\r\n",
      "Train Epoch: 17 [48000/110534 (43%)]\tClassification Loss: 1.6434\r\n",
      "Train Epoch: 17 [49920/110534 (45%)]\tClassification Loss: 1.6943\r\n",
      "Train Epoch: 17 [51840/110534 (47%)]\tClassification Loss: 1.8694\r\n",
      "Train Epoch: 17 [53760/110534 (49%)]\tClassification Loss: 1.4395\r\n",
      "Train Epoch: 17 [55680/110534 (50%)]\tClassification Loss: 1.6507\r\n",
      "Train Epoch: 17 [57600/110534 (52%)]\tClassification Loss: 1.5638\r\n",
      "Train Epoch: 17 [59520/110534 (54%)]\tClassification Loss: 1.7398\r\n",
      "Train Epoch: 17 [61440/110534 (56%)]\tClassification Loss: 1.5142\r\n",
      "Train Epoch: 17 [63360/110534 (57%)]\tClassification Loss: 1.8129\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_1000.pth.tar\r\n",
      "Train Epoch: 17 [65280/110534 (59%)]\tClassification Loss: 1.6559\r\n",
      "Train Epoch: 17 [67200/110534 (61%)]\tClassification Loss: 1.3864\r\n",
      "Train Epoch: 17 [69120/110534 (63%)]\tClassification Loss: 1.5413\r\n",
      "Train Epoch: 17 [71040/110534 (64%)]\tClassification Loss: 1.5882\r\n",
      "Train Epoch: 17 [72960/110534 (66%)]\tClassification Loss: 1.4373\r\n",
      "Train Epoch: 17 [74880/110534 (68%)]\tClassification Loss: 1.4553\r\n",
      "Train Epoch: 17 [76800/110534 (69%)]\tClassification Loss: 1.4671\r\n",
      "Train Epoch: 17 [78720/110534 (71%)]\tClassification Loss: 1.5338\r\n",
      "Train Epoch: 17 [80640/110534 (73%)]\tClassification Loss: 1.6248\r\n",
      "Train Epoch: 17 [82560/110534 (75%)]\tClassification Loss: 1.4376\r\n",
      "Train Epoch: 17 [84480/110534 (76%)]\tClassification Loss: 1.4040\r\n",
      "Train Epoch: 17 [86400/110534 (78%)]\tClassification Loss: 1.4290\r\n",
      "Train Epoch: 17 [88320/110534 (80%)]\tClassification Loss: 1.8999\r\n",
      "Train Epoch: 17 [90240/110534 (82%)]\tClassification Loss: 1.8329\r\n",
      "Train Epoch: 17 [92160/110534 (83%)]\tClassification Loss: 1.6627\r\n",
      "Train Epoch: 17 [94080/110534 (85%)]\tClassification Loss: 1.6389\r\n",
      "Train Epoch: 17 [96000/110534 (87%)]\tClassification Loss: 1.7462\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_1500.pth.tar\r\n",
      "Train Epoch: 17 [97920/110534 (89%)]\tClassification Loss: 1.5138\r\n",
      "Train Epoch: 17 [99840/110534 (90%)]\tClassification Loss: 1.6835\r\n",
      "Train Epoch: 17 [101760/110534 (92%)]\tClassification Loss: 1.3974\r\n",
      "Train Epoch: 17 [103680/110534 (94%)]\tClassification Loss: 1.6820\r\n",
      "Train Epoch: 17 [105600/110534 (96%)]\tClassification Loss: 1.5701\r\n",
      "Train Epoch: 17 [107520/110534 (97%)]\tClassification Loss: 1.5777\r\n",
      "Train Epoch: 17 [109440/110534 (99%)]\tClassification Loss: 1.6331\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_17_final.pth.tar\r\n",
      "Train Epoch: 18 [0/110534 (0%)]\tClassification Loss: 1.5921\r\n",
      "\r\n",
      "Test set: Average loss: 1.4634, Accuracy: 22688/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 18 [1920/110534 (2%)]\tClassification Loss: 1.4582\r\n",
      "Train Epoch: 18 [3840/110534 (3%)]\tClassification Loss: 1.6301\r\n",
      "Train Epoch: 18 [5760/110534 (5%)]\tClassification Loss: 1.7178\r\n",
      "Train Epoch: 18 [7680/110534 (7%)]\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 18 [9600/110534 (9%)]\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 18 [11520/110534 (10%)]\tClassification Loss: 1.7704\r\n",
      "Train Epoch: 18 [13440/110534 (12%)]\tClassification Loss: 1.5542\r\n",
      "Train Epoch: 18 [15360/110534 (14%)]\tClassification Loss: 2.0173\r\n",
      "Train Epoch: 18 [17280/110534 (16%)]\tClassification Loss: 1.7982\r\n",
      "Train Epoch: 18 [19200/110534 (17%)]\tClassification Loss: 1.8938\r\n",
      "Train Epoch: 18 [21120/110534 (19%)]\tClassification Loss: 1.5111\r\n",
      "Train Epoch: 18 [23040/110534 (21%)]\tClassification Loss: 1.5263\r\n",
      "Train Epoch: 18 [24960/110534 (23%)]\tClassification Loss: 1.6551\r\n",
      "Train Epoch: 18 [26880/110534 (24%)]\tClassification Loss: 1.5629\r\n",
      "Train Epoch: 18 [28800/110534 (26%)]\tClassification Loss: 1.9822\r\n",
      "Train Epoch: 18 [30720/110534 (28%)]\tClassification Loss: 1.4136\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_500.pth.tar\r\n",
      "Train Epoch: 18 [32640/110534 (30%)]\tClassification Loss: 1.7459\r\n",
      "Train Epoch: 18 [34560/110534 (31%)]\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 18 [36480/110534 (33%)]\tClassification Loss: 1.4214\r\n",
      "Train Epoch: 18 [38400/110534 (35%)]\tClassification Loss: 1.4893\r\n",
      "Train Epoch: 18 [40320/110534 (36%)]\tClassification Loss: 1.6072\r\n",
      "Train Epoch: 18 [42240/110534 (38%)]\tClassification Loss: 1.6022\r\n",
      "Train Epoch: 18 [44160/110534 (40%)]\tClassification Loss: 1.4124\r\n",
      "Train Epoch: 18 [46080/110534 (42%)]\tClassification Loss: 1.4426\r\n",
      "Train Epoch: 18 [48000/110534 (43%)]\tClassification Loss: 1.6567\r\n",
      "Train Epoch: 18 [49920/110534 (45%)]\tClassification Loss: 1.6445\r\n",
      "Train Epoch: 18 [51840/110534 (47%)]\tClassification Loss: 1.8656\r\n",
      "Train Epoch: 18 [53760/110534 (49%)]\tClassification Loss: 1.3123\r\n",
      "Train Epoch: 18 [55680/110534 (50%)]\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 18 [57600/110534 (52%)]\tClassification Loss: 1.5638\r\n",
      "Train Epoch: 18 [59520/110534 (54%)]\tClassification Loss: 1.8786\r\n",
      "Train Epoch: 18 [61440/110534 (56%)]\tClassification Loss: 1.5943\r\n",
      "Train Epoch: 18 [63360/110534 (57%)]\tClassification Loss: 1.6516\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_1000.pth.tar\r\n",
      "Train Epoch: 18 [65280/110534 (59%)]\tClassification Loss: 1.7530\r\n",
      "Train Epoch: 18 [67200/110534 (61%)]\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 18 [69120/110534 (63%)]\tClassification Loss: 1.6370\r\n",
      "Train Epoch: 18 [71040/110534 (64%)]\tClassification Loss: 1.6269\r\n",
      "Train Epoch: 18 [72960/110534 (66%)]\tClassification Loss: 1.3793\r\n",
      "Train Epoch: 18 [74880/110534 (68%)]\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 18 [76800/110534 (69%)]\tClassification Loss: 1.4541\r\n",
      "Train Epoch: 18 [78720/110534 (71%)]\tClassification Loss: 1.2687\r\n",
      "Train Epoch: 18 [80640/110534 (73%)]\tClassification Loss: 1.6496\r\n",
      "Train Epoch: 18 [82560/110534 (75%)]\tClassification Loss: 1.4488\r\n",
      "Train Epoch: 18 [84480/110534 (76%)]\tClassification Loss: 1.4758\r\n",
      "Train Epoch: 18 [86400/110534 (78%)]\tClassification Loss: 1.3291\r\n",
      "Train Epoch: 18 [88320/110534 (80%)]\tClassification Loss: 1.6777\r\n",
      "Train Epoch: 18 [90240/110534 (82%)]\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 18 [92160/110534 (83%)]\tClassification Loss: 1.8417\r\n",
      "Train Epoch: 18 [94080/110534 (85%)]\tClassification Loss: 1.7957\r\n",
      "Train Epoch: 18 [96000/110534 (87%)]\tClassification Loss: 1.7096\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_1500.pth.tar\r\n",
      "Train Epoch: 18 [97920/110534 (89%)]\tClassification Loss: 1.5501\r\n",
      "Train Epoch: 18 [99840/110534 (90%)]\tClassification Loss: 1.5901\r\n",
      "Train Epoch: 18 [101760/110534 (92%)]\tClassification Loss: 1.3577\r\n",
      "Train Epoch: 18 [103680/110534 (94%)]\tClassification Loss: 1.6047\r\n",
      "Train Epoch: 18 [105600/110534 (96%)]\tClassification Loss: 1.5866\r\n",
      "Train Epoch: 18 [107520/110534 (97%)]\tClassification Loss: 1.4323\r\n",
      "Train Epoch: 18 [109440/110534 (99%)]\tClassification Loss: 1.6022\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_18_final.pth.tar\r\n",
      "Train Epoch: 19 [0/110534 (0%)]\tClassification Loss: 1.5519\r\n",
      "\r\n",
      "Test set: Average loss: 1.4641, Accuracy: 22692/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 19 [1920/110534 (2%)]\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 19 [3840/110534 (3%)]\tClassification Loss: 1.5308\r\n",
      "Train Epoch: 19 [5760/110534 (5%)]\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 19 [7680/110534 (7%)]\tClassification Loss: 1.4827\r\n",
      "Train Epoch: 19 [9600/110534 (9%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 19 [11520/110534 (10%)]\tClassification Loss: 1.8176\r\n",
      "Train Epoch: 19 [13440/110534 (12%)]\tClassification Loss: 1.7580\r\n",
      "Train Epoch: 19 [15360/110534 (14%)]\tClassification Loss: 1.7651\r\n",
      "Train Epoch: 19 [17280/110534 (16%)]\tClassification Loss: 1.8153\r\n",
      "Train Epoch: 19 [19200/110534 (17%)]\tClassification Loss: 1.9298\r\n",
      "Train Epoch: 19 [21120/110534 (19%)]\tClassification Loss: 1.5666\r\n",
      "Train Epoch: 19 [23040/110534 (21%)]\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 19 [24960/110534 (23%)]\tClassification Loss: 1.6293\r\n",
      "Train Epoch: 19 [26880/110534 (24%)]\tClassification Loss: 1.6380\r\n",
      "Train Epoch: 19 [28800/110534 (26%)]\tClassification Loss: 1.9244\r\n",
      "Train Epoch: 19 [30720/110534 (28%)]\tClassification Loss: 1.6653\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_500.pth.tar\r\n",
      "Train Epoch: 19 [32640/110534 (30%)]\tClassification Loss: 1.4837\r\n",
      "Train Epoch: 19 [34560/110534 (31%)]\tClassification Loss: 1.5032\r\n",
      "Train Epoch: 19 [36480/110534 (33%)]\tClassification Loss: 1.4075\r\n",
      "Train Epoch: 19 [38400/110534 (35%)]\tClassification Loss: 1.5815\r\n",
      "Train Epoch: 19 [40320/110534 (36%)]\tClassification Loss: 1.4746\r\n",
      "Train Epoch: 19 [42240/110534 (38%)]\tClassification Loss: 1.3755\r\n",
      "Train Epoch: 19 [44160/110534 (40%)]\tClassification Loss: 1.4178\r\n",
      "Train Epoch: 19 [46080/110534 (42%)]\tClassification Loss: 1.4109\r\n",
      "Train Epoch: 19 [48000/110534 (43%)]\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 19 [49920/110534 (45%)]\tClassification Loss: 1.6277\r\n",
      "Train Epoch: 19 [51840/110534 (47%)]\tClassification Loss: 1.7269\r\n",
      "Train Epoch: 19 [53760/110534 (49%)]\tClassification Loss: 1.3165\r\n",
      "Train Epoch: 19 [55680/110534 (50%)]\tClassification Loss: 1.7109\r\n",
      "Train Epoch: 19 [57600/110534 (52%)]\tClassification Loss: 1.3260\r\n",
      "Train Epoch: 19 [59520/110534 (54%)]\tClassification Loss: 1.8860\r\n",
      "Train Epoch: 19 [61440/110534 (56%)]\tClassification Loss: 1.6508\r\n",
      "Train Epoch: 19 [63360/110534 (57%)]\tClassification Loss: 1.7594\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_1000.pth.tar\r\n",
      "Train Epoch: 19 [65280/110534 (59%)]\tClassification Loss: 1.7354\r\n",
      "Train Epoch: 19 [67200/110534 (61%)]\tClassification Loss: 1.3880\r\n",
      "Train Epoch: 19 [69120/110534 (63%)]\tClassification Loss: 1.4945\r\n",
      "Train Epoch: 19 [71040/110534 (64%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 19 [72960/110534 (66%)]\tClassification Loss: 1.5355\r\n",
      "Train Epoch: 19 [74880/110534 (68%)]\tClassification Loss: 1.4243\r\n",
      "Train Epoch: 19 [76800/110534 (69%)]\tClassification Loss: 1.4935\r\n",
      "Train Epoch: 19 [78720/110534 (71%)]\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 19 [80640/110534 (73%)]\tClassification Loss: 1.6068\r\n",
      "Train Epoch: 19 [82560/110534 (75%)]\tClassification Loss: 1.4141\r\n",
      "Train Epoch: 19 [84480/110534 (76%)]\tClassification Loss: 1.4646\r\n",
      "Train Epoch: 19 [86400/110534 (78%)]\tClassification Loss: 1.4860\r\n",
      "Train Epoch: 19 [88320/110534 (80%)]\tClassification Loss: 1.8087\r\n",
      "Train Epoch: 19 [90240/110534 (82%)]\tClassification Loss: 1.9112\r\n",
      "Train Epoch: 19 [92160/110534 (83%)]\tClassification Loss: 1.8837\r\n",
      "Train Epoch: 19 [94080/110534 (85%)]\tClassification Loss: 1.6379\r\n",
      "Train Epoch: 19 [96000/110534 (87%)]\tClassification Loss: 1.6964\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_1500.pth.tar\r\n",
      "Train Epoch: 19 [97920/110534 (89%)]\tClassification Loss: 1.4965\r\n",
      "Train Epoch: 19 [99840/110534 (90%)]\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 19 [101760/110534 (92%)]\tClassification Loss: 1.4318\r\n",
      "Train Epoch: 19 [103680/110534 (94%)]\tClassification Loss: 1.7572\r\n",
      "Train Epoch: 19 [105600/110534 (96%)]\tClassification Loss: 1.6323\r\n",
      "Train Epoch: 19 [107520/110534 (97%)]\tClassification Loss: 1.4659\r\n",
      "Train Epoch: 19 [109440/110534 (99%)]\tClassification Loss: 1.5923\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_19_final.pth.tar\r\n",
      "Train Epoch: 20 [0/110534 (0%)]\tClassification Loss: 1.5890\r\n",
      "\r\n",
      "Test set: Average loss: 1.4598, Accuracy: 22732/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 20 [1920/110534 (2%)]\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 20 [3840/110534 (3%)]\tClassification Loss: 1.6712\r\n",
      "Train Epoch: 20 [5760/110534 (5%)]\tClassification Loss: 1.7794\r\n",
      "Train Epoch: 20 [7680/110534 (7%)]\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 20 [9600/110534 (9%)]\tClassification Loss: 1.6463\r\n",
      "Train Epoch: 20 [11520/110534 (10%)]\tClassification Loss: 1.7027\r\n",
      "Train Epoch: 20 [13440/110534 (12%)]\tClassification Loss: 1.6230\r\n",
      "Train Epoch: 20 [15360/110534 (14%)]\tClassification Loss: 1.6931\r\n",
      "Train Epoch: 20 [17280/110534 (16%)]\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 20 [19200/110534 (17%)]\tClassification Loss: 1.7301\r\n",
      "Train Epoch: 20 [21120/110534 (19%)]\tClassification Loss: 1.3883\r\n",
      "Train Epoch: 20 [23040/110534 (21%)]\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 20 [24960/110534 (23%)]\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 20 [26880/110534 (24%)]\tClassification Loss: 1.6783\r\n",
      "Train Epoch: 20 [28800/110534 (26%)]\tClassification Loss: 1.9527\r\n",
      "Train Epoch: 20 [30720/110534 (28%)]\tClassification Loss: 1.4925\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_500.pth.tar\r\n",
      "Train Epoch: 20 [32640/110534 (30%)]\tClassification Loss: 1.5506\r\n",
      "Train Epoch: 20 [34560/110534 (31%)]\tClassification Loss: 1.5161\r\n",
      "Train Epoch: 20 [36480/110534 (33%)]\tClassification Loss: 1.4043\r\n",
      "Train Epoch: 20 [38400/110534 (35%)]\tClassification Loss: 1.4533\r\n",
      "Train Epoch: 20 [40320/110534 (36%)]\tClassification Loss: 1.4912\r\n",
      "Train Epoch: 20 [42240/110534 (38%)]\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 20 [44160/110534 (40%)]\tClassification Loss: 1.3849\r\n",
      "Train Epoch: 20 [46080/110534 (42%)]\tClassification Loss: 1.5129\r\n",
      "Train Epoch: 20 [48000/110534 (43%)]\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 20 [49920/110534 (45%)]\tClassification Loss: 1.6807\r\n",
      "Train Epoch: 20 [51840/110534 (47%)]\tClassification Loss: 1.7834\r\n",
      "Train Epoch: 20 [53760/110534 (49%)]\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 20 [55680/110534 (50%)]\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 20 [57600/110534 (52%)]\tClassification Loss: 1.5280\r\n",
      "Train Epoch: 20 [59520/110534 (54%)]\tClassification Loss: 1.8357\r\n",
      "Train Epoch: 20 [61440/110534 (56%)]\tClassification Loss: 1.5382\r\n",
      "Train Epoch: 20 [63360/110534 (57%)]\tClassification Loss: 1.7171\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_1000.pth.tar\r\n",
      "Train Epoch: 20 [65280/110534 (59%)]\tClassification Loss: 1.7156\r\n",
      "Train Epoch: 20 [67200/110534 (61%)]\tClassification Loss: 1.3478\r\n",
      "Train Epoch: 20 [69120/110534 (63%)]\tClassification Loss: 1.7488\r\n",
      "Train Epoch: 20 [71040/110534 (64%)]\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 20 [72960/110534 (66%)]\tClassification Loss: 1.3449\r\n",
      "Train Epoch: 20 [74880/110534 (68%)]\tClassification Loss: 1.5931\r\n",
      "Train Epoch: 20 [76800/110534 (69%)]\tClassification Loss: 1.4100\r\n",
      "Train Epoch: 20 [78720/110534 (71%)]\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 20 [80640/110534 (73%)]\tClassification Loss: 1.6597\r\n",
      "Train Epoch: 20 [82560/110534 (75%)]\tClassification Loss: 1.3303\r\n",
      "Train Epoch: 20 [84480/110534 (76%)]\tClassification Loss: 1.5013\r\n",
      "Train Epoch: 20 [86400/110534 (78%)]\tClassification Loss: 1.3590\r\n",
      "Train Epoch: 20 [88320/110534 (80%)]\tClassification Loss: 1.8078\r\n",
      "Train Epoch: 20 [90240/110534 (82%)]\tClassification Loss: 1.9623\r\n",
      "Train Epoch: 20 [92160/110534 (83%)]\tClassification Loss: 1.8152\r\n",
      "Train Epoch: 20 [94080/110534 (85%)]\tClassification Loss: 1.8370\r\n",
      "Train Epoch: 20 [96000/110534 (87%)]\tClassification Loss: 1.5386\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_1500.pth.tar\r\n",
      "Train Epoch: 20 [97920/110534 (89%)]\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 20 [99840/110534 (90%)]\tClassification Loss: 1.5565\r\n",
      "Train Epoch: 20 [101760/110534 (92%)]\tClassification Loss: 1.4162\r\n",
      "Train Epoch: 20 [103680/110534 (94%)]\tClassification Loss: 1.7025\r\n",
      "Train Epoch: 20 [105600/110534 (96%)]\tClassification Loss: 1.5637\r\n",
      "Train Epoch: 20 [107520/110534 (97%)]\tClassification Loss: 1.6050\r\n",
      "Train Epoch: 20 [109440/110534 (99%)]\tClassification Loss: 1.5851\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_20_final.pth.tar\r\n",
      "Train Epoch: 21 [0/110534 (0%)]\tClassification Loss: 1.6838\r\n",
      "\r\n",
      "Test set: Average loss: 1.4594, Accuracy: 22677/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 21 [1920/110534 (2%)]\tClassification Loss: 1.4152\r\n",
      "Train Epoch: 21 [3840/110534 (3%)]\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 21 [5760/110534 (5%)]\tClassification Loss: 1.6214\r\n",
      "Train Epoch: 21 [7680/110534 (7%)]\tClassification Loss: 1.5620\r\n",
      "Train Epoch: 21 [9600/110534 (9%)]\tClassification Loss: 1.6391\r\n",
      "Train Epoch: 21 [11520/110534 (10%)]\tClassification Loss: 1.7000\r\n",
      "Train Epoch: 21 [13440/110534 (12%)]\tClassification Loss: 1.5756\r\n",
      "Train Epoch: 21 [15360/110534 (14%)]\tClassification Loss: 1.7993\r\n",
      "Train Epoch: 21 [17280/110534 (16%)]\tClassification Loss: 1.8454\r\n",
      "Train Epoch: 21 [19200/110534 (17%)]\tClassification Loss: 1.8716\r\n",
      "Train Epoch: 21 [21120/110534 (19%)]\tClassification Loss: 1.6490\r\n",
      "Train Epoch: 21 [23040/110534 (21%)]\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 21 [24960/110534 (23%)]\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 21 [26880/110534 (24%)]\tClassification Loss: 1.5819\r\n",
      "Train Epoch: 21 [28800/110534 (26%)]\tClassification Loss: 1.9196\r\n",
      "Train Epoch: 21 [30720/110534 (28%)]\tClassification Loss: 1.5616\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_500.pth.tar\r\n",
      "Train Epoch: 21 [32640/110534 (30%)]\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 21 [34560/110534 (31%)]\tClassification Loss: 1.4832\r\n",
      "Train Epoch: 21 [36480/110534 (33%)]\tClassification Loss: 1.4167\r\n",
      "Train Epoch: 21 [38400/110534 (35%)]\tClassification Loss: 1.6286\r\n",
      "Train Epoch: 21 [40320/110534 (36%)]\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 21 [42240/110534 (38%)]\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 21 [44160/110534 (40%)]\tClassification Loss: 1.4278\r\n",
      "Train Epoch: 21 [46080/110534 (42%)]\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 21 [48000/110534 (43%)]\tClassification Loss: 1.5332\r\n",
      "Train Epoch: 21 [49920/110534 (45%)]\tClassification Loss: 1.6414\r\n",
      "Train Epoch: 21 [51840/110534 (47%)]\tClassification Loss: 1.8292\r\n",
      "Train Epoch: 21 [53760/110534 (49%)]\tClassification Loss: 1.4876\r\n",
      "Train Epoch: 21 [55680/110534 (50%)]\tClassification Loss: 1.5820\r\n",
      "Train Epoch: 21 [57600/110534 (52%)]\tClassification Loss: 1.6465\r\n",
      "Train Epoch: 21 [59520/110534 (54%)]\tClassification Loss: 1.9843\r\n",
      "Train Epoch: 21 [61440/110534 (56%)]\tClassification Loss: 1.7302\r\n",
      "Train Epoch: 21 [63360/110534 (57%)]\tClassification Loss: 1.6854\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_1000.pth.tar\r\n",
      "Train Epoch: 21 [65280/110534 (59%)]\tClassification Loss: 1.6510\r\n",
      "Train Epoch: 21 [67200/110534 (61%)]\tClassification Loss: 1.3341\r\n",
      "Train Epoch: 21 [69120/110534 (63%)]\tClassification Loss: 1.6271\r\n",
      "Train Epoch: 21 [71040/110534 (64%)]\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 21 [72960/110534 (66%)]\tClassification Loss: 1.4790\r\n",
      "Train Epoch: 21 [74880/110534 (68%)]\tClassification Loss: 1.4214\r\n",
      "Train Epoch: 21 [76800/110534 (69%)]\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 21 [78720/110534 (71%)]\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 21 [80640/110534 (73%)]\tClassification Loss: 1.6024\r\n",
      "Train Epoch: 21 [82560/110534 (75%)]\tClassification Loss: 1.3218\r\n",
      "Train Epoch: 21 [84480/110534 (76%)]\tClassification Loss: 1.4069\r\n",
      "Train Epoch: 21 [86400/110534 (78%)]\tClassification Loss: 1.4559\r\n",
      "Train Epoch: 21 [88320/110534 (80%)]\tClassification Loss: 1.8895\r\n",
      "Train Epoch: 21 [90240/110534 (82%)]\tClassification Loss: 1.8623\r\n",
      "Train Epoch: 21 [92160/110534 (83%)]\tClassification Loss: 1.8869\r\n",
      "Train Epoch: 21 [94080/110534 (85%)]\tClassification Loss: 1.5831\r\n",
      "Train Epoch: 21 [96000/110534 (87%)]\tClassification Loss: 1.6966\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_1500.pth.tar\r\n",
      "Train Epoch: 21 [97920/110534 (89%)]\tClassification Loss: 1.7217\r\n",
      "Train Epoch: 21 [99840/110534 (90%)]\tClassification Loss: 1.5753\r\n",
      "Train Epoch: 21 [101760/110534 (92%)]\tClassification Loss: 1.5261\r\n",
      "Train Epoch: 21 [103680/110534 (94%)]\tClassification Loss: 1.5875\r\n",
      "Train Epoch: 21 [105600/110534 (96%)]\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 21 [107520/110534 (97%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 21 [109440/110534 (99%)]\tClassification Loss: 1.7491\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_21_final.pth.tar\r\n",
      "Train Epoch: 22 [0/110534 (0%)]\tClassification Loss: 1.5215\r\n",
      "\r\n",
      "Test set: Average loss: 1.4589, Accuracy: 22701/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 22 [1920/110534 (2%)]\tClassification Loss: 1.4565\r\n",
      "Train Epoch: 22 [3840/110534 (3%)]\tClassification Loss: 1.5780\r\n",
      "Train Epoch: 22 [5760/110534 (5%)]\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 22 [7680/110534 (7%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 22 [9600/110534 (9%)]\tClassification Loss: 1.7211\r\n",
      "Train Epoch: 22 [11520/110534 (10%)]\tClassification Loss: 1.7406\r\n",
      "Train Epoch: 22 [13440/110534 (12%)]\tClassification Loss: 1.5233\r\n",
      "Train Epoch: 22 [15360/110534 (14%)]\tClassification Loss: 1.8073\r\n",
      "Train Epoch: 22 [17280/110534 (16%)]\tClassification Loss: 1.7963\r\n",
      "Train Epoch: 22 [19200/110534 (17%)]\tClassification Loss: 2.0872\r\n",
      "Train Epoch: 22 [21120/110534 (19%)]\tClassification Loss: 1.6544\r\n",
      "Train Epoch: 22 [23040/110534 (21%)]\tClassification Loss: 1.3737\r\n",
      "Train Epoch: 22 [24960/110534 (23%)]\tClassification Loss: 1.8409\r\n",
      "Train Epoch: 22 [26880/110534 (24%)]\tClassification Loss: 1.5493\r\n",
      "Train Epoch: 22 [28800/110534 (26%)]\tClassification Loss: 1.9047\r\n",
      "Train Epoch: 22 [30720/110534 (28%)]\tClassification Loss: 1.4662\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_500.pth.tar\r\n",
      "Train Epoch: 22 [32640/110534 (30%)]\tClassification Loss: 1.4882\r\n",
      "Train Epoch: 22 [34560/110534 (31%)]\tClassification Loss: 1.4213\r\n",
      "Train Epoch: 22 [36480/110534 (33%)]\tClassification Loss: 1.4688\r\n",
      "Train Epoch: 22 [38400/110534 (35%)]\tClassification Loss: 1.3973\r\n",
      "Train Epoch: 22 [40320/110534 (36%)]\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 22 [42240/110534 (38%)]\tClassification Loss: 1.4892\r\n",
      "Train Epoch: 22 [44160/110534 (40%)]\tClassification Loss: 1.4528\r\n",
      "Train Epoch: 22 [46080/110534 (42%)]\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 22 [48000/110534 (43%)]\tClassification Loss: 1.7015\r\n",
      "Train Epoch: 22 [49920/110534 (45%)]\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 22 [51840/110534 (47%)]\tClassification Loss: 1.6985\r\n",
      "Train Epoch: 22 [53760/110534 (49%)]\tClassification Loss: 1.4991\r\n",
      "Train Epoch: 22 [55680/110534 (50%)]\tClassification Loss: 1.4753\r\n",
      "Train Epoch: 22 [57600/110534 (52%)]\tClassification Loss: 1.5780\r\n",
      "Train Epoch: 22 [59520/110534 (54%)]\tClassification Loss: 2.1001\r\n",
      "Train Epoch: 22 [61440/110534 (56%)]\tClassification Loss: 1.6751\r\n",
      "Train Epoch: 22 [63360/110534 (57%)]\tClassification Loss: 1.6495\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_1000.pth.tar\r\n",
      "Train Epoch: 22 [65280/110534 (59%)]\tClassification Loss: 1.5789\r\n",
      "Train Epoch: 22 [67200/110534 (61%)]\tClassification Loss: 1.3480\r\n",
      "Train Epoch: 22 [69120/110534 (63%)]\tClassification Loss: 1.5857\r\n",
      "Train Epoch: 22 [71040/110534 (64%)]\tClassification Loss: 1.4335\r\n",
      "Train Epoch: 22 [72960/110534 (66%)]\tClassification Loss: 1.5815\r\n",
      "Train Epoch: 22 [74880/110534 (68%)]\tClassification Loss: 1.5561\r\n",
      "Train Epoch: 22 [76800/110534 (69%)]\tClassification Loss: 1.4126\r\n",
      "Train Epoch: 22 [78720/110534 (71%)]\tClassification Loss: 1.5007\r\n",
      "Train Epoch: 22 [80640/110534 (73%)]\tClassification Loss: 1.6014\r\n",
      "Train Epoch: 22 [82560/110534 (75%)]\tClassification Loss: 1.3831\r\n",
      "Train Epoch: 22 [84480/110534 (76%)]\tClassification Loss: 1.3980\r\n",
      "Train Epoch: 22 [86400/110534 (78%)]\tClassification Loss: 1.4913\r\n",
      "Train Epoch: 22 [88320/110534 (80%)]\tClassification Loss: 1.8510\r\n",
      "Train Epoch: 22 [90240/110534 (82%)]\tClassification Loss: 1.7845\r\n",
      "Train Epoch: 22 [92160/110534 (83%)]\tClassification Loss: 1.8824\r\n",
      "Train Epoch: 22 [94080/110534 (85%)]\tClassification Loss: 1.6067\r\n",
      "Train Epoch: 22 [96000/110534 (87%)]\tClassification Loss: 1.8110\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_1500.pth.tar\r\n",
      "Train Epoch: 22 [97920/110534 (89%)]\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 22 [99840/110534 (90%)]\tClassification Loss: 1.7474\r\n",
      "Train Epoch: 22 [101760/110534 (92%)]\tClassification Loss: 1.4477\r\n",
      "Train Epoch: 22 [103680/110534 (94%)]\tClassification Loss: 1.6494\r\n",
      "Train Epoch: 22 [105600/110534 (96%)]\tClassification Loss: 1.5238\r\n",
      "Train Epoch: 22 [107520/110534 (97%)]\tClassification Loss: 1.3416\r\n",
      "Train Epoch: 22 [109440/110534 (99%)]\tClassification Loss: 1.7414\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_22_final.pth.tar\r\n",
      "Train Epoch: 23 [0/110534 (0%)]\tClassification Loss: 1.4170\r\n",
      "\r\n",
      "Test set: Average loss: 1.4584, Accuracy: 22731/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 23 [1920/110534 (2%)]\tClassification Loss: 1.4636\r\n",
      "Train Epoch: 23 [3840/110534 (3%)]\tClassification Loss: 1.6537\r\n",
      "Train Epoch: 23 [5760/110534 (5%)]\tClassification Loss: 1.5920\r\n",
      "Train Epoch: 23 [7680/110534 (7%)]\tClassification Loss: 1.4397\r\n",
      "Train Epoch: 23 [9600/110534 (9%)]\tClassification Loss: 1.6117\r\n",
      "Train Epoch: 23 [11520/110534 (10%)]\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 23 [13440/110534 (12%)]\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 23 [15360/110534 (14%)]\tClassification Loss: 1.8173\r\n",
      "Train Epoch: 23 [17280/110534 (16%)]\tClassification Loss: 1.8371\r\n",
      "Train Epoch: 23 [19200/110534 (17%)]\tClassification Loss: 1.6893\r\n",
      "Train Epoch: 23 [21120/110534 (19%)]\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 23 [23040/110534 (21%)]\tClassification Loss: 1.3033\r\n",
      "Train Epoch: 23 [24960/110534 (23%)]\tClassification Loss: 1.5735\r\n",
      "Train Epoch: 23 [26880/110534 (24%)]\tClassification Loss: 1.6577\r\n",
      "Train Epoch: 23 [28800/110534 (26%)]\tClassification Loss: 1.8609\r\n",
      "Train Epoch: 23 [30720/110534 (28%)]\tClassification Loss: 1.5365\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_500.pth.tar\r\n",
      "Train Epoch: 23 [32640/110534 (30%)]\tClassification Loss: 1.3164\r\n",
      "Train Epoch: 23 [34560/110534 (31%)]\tClassification Loss: 1.3384\r\n",
      "Train Epoch: 23 [36480/110534 (33%)]\tClassification Loss: 1.4385\r\n",
      "Train Epoch: 23 [38400/110534 (35%)]\tClassification Loss: 1.4877\r\n",
      "Train Epoch: 23 [40320/110534 (36%)]\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 23 [42240/110534 (38%)]\tClassification Loss: 1.4643\r\n",
      "Train Epoch: 23 [44160/110534 (40%)]\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 23 [46080/110534 (42%)]\tClassification Loss: 1.4774\r\n",
      "Train Epoch: 23 [48000/110534 (43%)]\tClassification Loss: 1.6814\r\n",
      "Train Epoch: 23 [49920/110534 (45%)]\tClassification Loss: 1.7230\r\n",
      "Train Epoch: 23 [51840/110534 (47%)]\tClassification Loss: 1.8026\r\n",
      "Train Epoch: 23 [53760/110534 (49%)]\tClassification Loss: 1.4089\r\n",
      "Train Epoch: 23 [55680/110534 (50%)]\tClassification Loss: 1.5994\r\n",
      "Train Epoch: 23 [57600/110534 (52%)]\tClassification Loss: 1.8267\r\n",
      "Train Epoch: 23 [59520/110534 (54%)]\tClassification Loss: 1.9026\r\n",
      "Train Epoch: 23 [61440/110534 (56%)]\tClassification Loss: 1.5311\r\n",
      "Train Epoch: 23 [63360/110534 (57%)]\tClassification Loss: 1.7136\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_1000.pth.tar\r\n",
      "Train Epoch: 23 [65280/110534 (59%)]\tClassification Loss: 1.6789\r\n",
      "Train Epoch: 23 [67200/110534 (61%)]\tClassification Loss: 1.3451\r\n",
      "Train Epoch: 23 [69120/110534 (63%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 23 [71040/110534 (64%)]\tClassification Loss: 1.6257\r\n",
      "Train Epoch: 23 [72960/110534 (66%)]\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 23 [74880/110534 (68%)]\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 23 [76800/110534 (69%)]\tClassification Loss: 1.6486\r\n",
      "Train Epoch: 23 [78720/110534 (71%)]\tClassification Loss: 1.3892\r\n",
      "Train Epoch: 23 [80640/110534 (73%)]\tClassification Loss: 1.6924\r\n",
      "Train Epoch: 23 [82560/110534 (75%)]\tClassification Loss: 1.3777\r\n",
      "Train Epoch: 23 [84480/110534 (76%)]\tClassification Loss: 1.3186\r\n",
      "Train Epoch: 23 [86400/110534 (78%)]\tClassification Loss: 1.5810\r\n",
      "Train Epoch: 23 [88320/110534 (80%)]\tClassification Loss: 1.7812\r\n",
      "Train Epoch: 23 [90240/110534 (82%)]\tClassification Loss: 1.6854\r\n",
      "Train Epoch: 23 [92160/110534 (83%)]\tClassification Loss: 1.9077\r\n",
      "Train Epoch: 23 [94080/110534 (85%)]\tClassification Loss: 1.7815\r\n",
      "Train Epoch: 23 [96000/110534 (87%)]\tClassification Loss: 1.7049\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_1500.pth.tar\r\n",
      "Train Epoch: 23 [97920/110534 (89%)]\tClassification Loss: 1.5710\r\n",
      "Train Epoch: 23 [99840/110534 (90%)]\tClassification Loss: 1.6994\r\n",
      "Train Epoch: 23 [101760/110534 (92%)]\tClassification Loss: 1.4188\r\n",
      "Train Epoch: 23 [103680/110534 (94%)]\tClassification Loss: 1.5976\r\n",
      "Train Epoch: 23 [105600/110534 (96%)]\tClassification Loss: 1.5467\r\n",
      "Train Epoch: 23 [107520/110534 (97%)]\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 23 [109440/110534 (99%)]\tClassification Loss: 1.8475\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_23_final.pth.tar\r\n",
      "Train Epoch: 24 [0/110534 (0%)]\tClassification Loss: 1.7323\r\n",
      "\r\n",
      "Test set: Average loss: 1.4563, Accuracy: 22722/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 24 [1920/110534 (2%)]\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 24 [3840/110534 (3%)]\tClassification Loss: 1.5474\r\n",
      "Train Epoch: 24 [5760/110534 (5%)]\tClassification Loss: 1.7478\r\n",
      "Train Epoch: 24 [7680/110534 (7%)]\tClassification Loss: 1.6212\r\n",
      "Train Epoch: 24 [9600/110534 (9%)]\tClassification Loss: 1.6723\r\n",
      "Train Epoch: 24 [11520/110534 (10%)]\tClassification Loss: 1.6465\r\n",
      "Train Epoch: 24 [13440/110534 (12%)]\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 24 [15360/110534 (14%)]\tClassification Loss: 1.8973\r\n",
      "Train Epoch: 24 [17280/110534 (16%)]\tClassification Loss: 1.9289\r\n",
      "Train Epoch: 24 [19200/110534 (17%)]\tClassification Loss: 1.7776\r\n",
      "Train Epoch: 24 [21120/110534 (19%)]\tClassification Loss: 1.5179\r\n",
      "Train Epoch: 24 [23040/110534 (21%)]\tClassification Loss: 1.5070\r\n",
      "Train Epoch: 24 [24960/110534 (23%)]\tClassification Loss: 1.7026\r\n",
      "Train Epoch: 24 [26880/110534 (24%)]\tClassification Loss: 1.5617\r\n",
      "Train Epoch: 24 [28800/110534 (26%)]\tClassification Loss: 1.9528\r\n",
      "Train Epoch: 24 [30720/110534 (28%)]\tClassification Loss: 1.5284\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_500.pth.tar\r\n",
      "Train Epoch: 24 [32640/110534 (30%)]\tClassification Loss: 1.3235\r\n",
      "Train Epoch: 24 [34560/110534 (31%)]\tClassification Loss: 1.4145\r\n",
      "Train Epoch: 24 [36480/110534 (33%)]\tClassification Loss: 1.3912\r\n",
      "Train Epoch: 24 [38400/110534 (35%)]\tClassification Loss: 1.4544\r\n",
      "Train Epoch: 24 [40320/110534 (36%)]\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 24 [42240/110534 (38%)]\tClassification Loss: 1.4661\r\n",
      "Train Epoch: 24 [44160/110534 (40%)]\tClassification Loss: 1.5447\r\n",
      "Train Epoch: 24 [46080/110534 (42%)]\tClassification Loss: 1.4795\r\n",
      "Train Epoch: 24 [48000/110534 (43%)]\tClassification Loss: 1.6618\r\n",
      "Train Epoch: 24 [49920/110534 (45%)]\tClassification Loss: 1.6826\r\n",
      "Train Epoch: 24 [51840/110534 (47%)]\tClassification Loss: 1.8887\r\n",
      "Train Epoch: 24 [53760/110534 (49%)]\tClassification Loss: 1.5393\r\n",
      "Train Epoch: 24 [55680/110534 (50%)]\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 24 [57600/110534 (52%)]\tClassification Loss: 1.5337\r\n",
      "Train Epoch: 24 [59520/110534 (54%)]\tClassification Loss: 1.7968\r\n",
      "Train Epoch: 24 [61440/110534 (56%)]\tClassification Loss: 1.5560\r\n",
      "Train Epoch: 24 [63360/110534 (57%)]\tClassification Loss: 1.6359\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_1000.pth.tar\r\n",
      "Train Epoch: 24 [65280/110534 (59%)]\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 24 [67200/110534 (61%)]\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 24 [69120/110534 (63%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 24 [71040/110534 (64%)]\tClassification Loss: 1.6491\r\n",
      "Train Epoch: 24 [72960/110534 (66%)]\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 24 [74880/110534 (68%)]\tClassification Loss: 1.5311\r\n",
      "Train Epoch: 24 [76800/110534 (69%)]\tClassification Loss: 1.3254\r\n",
      "Train Epoch: 24 [78720/110534 (71%)]\tClassification Loss: 1.4687\r\n",
      "Train Epoch: 24 [80640/110534 (73%)]\tClassification Loss: 1.6860\r\n",
      "Train Epoch: 24 [82560/110534 (75%)]\tClassification Loss: 1.4041\r\n",
      "Train Epoch: 24 [84480/110534 (76%)]\tClassification Loss: 1.4063\r\n",
      "Train Epoch: 24 [86400/110534 (78%)]\tClassification Loss: 1.4009\r\n",
      "Train Epoch: 24 [88320/110534 (80%)]\tClassification Loss: 1.8868\r\n",
      "Train Epoch: 24 [90240/110534 (82%)]\tClassification Loss: 1.6086\r\n",
      "Train Epoch: 24 [92160/110534 (83%)]\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 24 [94080/110534 (85%)]\tClassification Loss: 1.6438\r\n",
      "Train Epoch: 24 [96000/110534 (87%)]\tClassification Loss: 1.8213\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_1500.pth.tar\r\n",
      "Train Epoch: 24 [97920/110534 (89%)]\tClassification Loss: 1.3926\r\n",
      "Train Epoch: 24 [99840/110534 (90%)]\tClassification Loss: 1.7048\r\n",
      "Train Epoch: 24 [101760/110534 (92%)]\tClassification Loss: 1.5402\r\n",
      "Train Epoch: 24 [103680/110534 (94%)]\tClassification Loss: 1.7330\r\n",
      "Train Epoch: 24 [105600/110534 (96%)]\tClassification Loss: 1.4500\r\n",
      "Train Epoch: 24 [107520/110534 (97%)]\tClassification Loss: 1.5843\r\n",
      "Train Epoch: 24 [109440/110534 (99%)]\tClassification Loss: 1.5581\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_24_final.pth.tar\r\n",
      "Train Epoch: 25 [0/110534 (0%)]\tClassification Loss: 1.4072\r\n",
      "\r\n",
      "Test set: Average loss: 1.4583, Accuracy: 22706/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 25 [1920/110534 (2%)]\tClassification Loss: 1.4010\r\n",
      "Train Epoch: 25 [3840/110534 (3%)]\tClassification Loss: 1.5239\r\n",
      "Train Epoch: 25 [5760/110534 (5%)]\tClassification Loss: 1.6091\r\n",
      "Train Epoch: 25 [7680/110534 (7%)]\tClassification Loss: 1.5588\r\n",
      "Train Epoch: 25 [9600/110534 (9%)]\tClassification Loss: 1.6277\r\n",
      "Train Epoch: 25 [11520/110534 (10%)]\tClassification Loss: 1.8208\r\n",
      "Train Epoch: 25 [13440/110534 (12%)]\tClassification Loss: 1.5082\r\n",
      "Train Epoch: 25 [15360/110534 (14%)]\tClassification Loss: 1.8677\r\n",
      "Train Epoch: 25 [17280/110534 (16%)]\tClassification Loss: 1.7950\r\n",
      "Train Epoch: 25 [19200/110534 (17%)]\tClassification Loss: 2.0857\r\n",
      "Train Epoch: 25 [21120/110534 (19%)]\tClassification Loss: 1.6257\r\n",
      "Train Epoch: 25 [23040/110534 (21%)]\tClassification Loss: 1.3297\r\n",
      "Train Epoch: 25 [24960/110534 (23%)]\tClassification Loss: 1.5596\r\n",
      "Train Epoch: 25 [26880/110534 (24%)]\tClassification Loss: 1.6413\r\n",
      "Train Epoch: 25 [28800/110534 (26%)]\tClassification Loss: 1.9445\r\n",
      "Train Epoch: 25 [30720/110534 (28%)]\tClassification Loss: 1.4709\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_500.pth.tar\r\n",
      "Train Epoch: 25 [32640/110534 (30%)]\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 25 [34560/110534 (31%)]\tClassification Loss: 1.4697\r\n",
      "Train Epoch: 25 [36480/110534 (33%)]\tClassification Loss: 1.2895\r\n",
      "Train Epoch: 25 [38400/110534 (35%)]\tClassification Loss: 1.3921\r\n",
      "Train Epoch: 25 [40320/110534 (36%)]\tClassification Loss: 1.3956\r\n",
      "Train Epoch: 25 [42240/110534 (38%)]\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 25 [44160/110534 (40%)]\tClassification Loss: 1.6429\r\n",
      "Train Epoch: 25 [46080/110534 (42%)]\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 25 [48000/110534 (43%)]\tClassification Loss: 1.6766\r\n",
      "Train Epoch: 25 [49920/110534 (45%)]\tClassification Loss: 1.7345\r\n",
      "Train Epoch: 25 [51840/110534 (47%)]\tClassification Loss: 1.8508\r\n",
      "Train Epoch: 25 [53760/110534 (49%)]\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 25 [55680/110534 (50%)]\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 25 [57600/110534 (52%)]\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 25 [59520/110534 (54%)]\tClassification Loss: 1.7654\r\n",
      "Train Epoch: 25 [61440/110534 (56%)]\tClassification Loss: 1.6016\r\n",
      "Train Epoch: 25 [63360/110534 (57%)]\tClassification Loss: 1.8071\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_1000.pth.tar\r\n",
      "Train Epoch: 25 [65280/110534 (59%)]\tClassification Loss: 1.6725\r\n",
      "Train Epoch: 25 [67200/110534 (61%)]\tClassification Loss: 1.7134\r\n",
      "Train Epoch: 25 [69120/110534 (63%)]\tClassification Loss: 1.6831\r\n",
      "Train Epoch: 25 [71040/110534 (64%)]\tClassification Loss: 1.5068\r\n",
      "Train Epoch: 25 [72960/110534 (66%)]\tClassification Loss: 1.5912\r\n",
      "Train Epoch: 25 [74880/110534 (68%)]\tClassification Loss: 1.5014\r\n",
      "Train Epoch: 25 [76800/110534 (69%)]\tClassification Loss: 1.3506\r\n",
      "Train Epoch: 25 [78720/110534 (71%)]\tClassification Loss: 1.6732\r\n",
      "Train Epoch: 25 [80640/110534 (73%)]\tClassification Loss: 1.5241\r\n",
      "Train Epoch: 25 [82560/110534 (75%)]\tClassification Loss: 1.3798\r\n",
      "Train Epoch: 25 [84480/110534 (76%)]\tClassification Loss: 1.4966\r\n",
      "Train Epoch: 25 [86400/110534 (78%)]\tClassification Loss: 1.4306\r\n",
      "Train Epoch: 25 [88320/110534 (80%)]\tClassification Loss: 1.7824\r\n",
      "Train Epoch: 25 [90240/110534 (82%)]\tClassification Loss: 1.7080\r\n",
      "Train Epoch: 25 [92160/110534 (83%)]\tClassification Loss: 1.8000\r\n",
      "Train Epoch: 25 [94080/110534 (85%)]\tClassification Loss: 1.6471\r\n",
      "Train Epoch: 25 [96000/110534 (87%)]\tClassification Loss: 1.6170\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_1500.pth.tar\r\n",
      "Train Epoch: 25 [97920/110534 (89%)]\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 25 [99840/110534 (90%)]\tClassification Loss: 1.6757\r\n",
      "Train Epoch: 25 [101760/110534 (92%)]\tClassification Loss: 1.4291\r\n",
      "Train Epoch: 25 [103680/110534 (94%)]\tClassification Loss: 1.7189\r\n",
      "Train Epoch: 25 [105600/110534 (96%)]\tClassification Loss: 1.5526\r\n",
      "Train Epoch: 25 [107520/110534 (97%)]\tClassification Loss: 1.4822\r\n",
      "Train Epoch: 25 [109440/110534 (99%)]\tClassification Loss: 1.6743\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_25_final.pth.tar\r\n",
      "Train Epoch: 26 [0/110534 (0%)]\tClassification Loss: 1.6478\r\n",
      "\r\n",
      "Test set: Average loss: 1.4540, Accuracy: 22768/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 26 [1920/110534 (2%)]\tClassification Loss: 1.5861\r\n",
      "Train Epoch: 26 [3840/110534 (3%)]\tClassification Loss: 1.5793\r\n",
      "Train Epoch: 26 [5760/110534 (5%)]\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 26 [7680/110534 (7%)]\tClassification Loss: 1.5385\r\n",
      "Train Epoch: 26 [9600/110534 (9%)]\tClassification Loss: 1.7949\r\n",
      "Train Epoch: 26 [11520/110534 (10%)]\tClassification Loss: 1.8576\r\n",
      "Train Epoch: 26 [13440/110534 (12%)]\tClassification Loss: 1.6467\r\n",
      "Train Epoch: 26 [15360/110534 (14%)]\tClassification Loss: 1.7705\r\n",
      "Train Epoch: 26 [17280/110534 (16%)]\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 26 [19200/110534 (17%)]\tClassification Loss: 1.9156\r\n",
      "Train Epoch: 26 [21120/110534 (19%)]\tClassification Loss: 1.4810\r\n",
      "Train Epoch: 26 [23040/110534 (21%)]\tClassification Loss: 1.6541\r\n",
      "Train Epoch: 26 [24960/110534 (23%)]\tClassification Loss: 1.4568\r\n",
      "Train Epoch: 26 [26880/110534 (24%)]\tClassification Loss: 1.6149\r\n",
      "Train Epoch: 26 [28800/110534 (26%)]\tClassification Loss: 2.0472\r\n",
      "Train Epoch: 26 [30720/110534 (28%)]\tClassification Loss: 1.5697\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_500.pth.tar\r\n",
      "Train Epoch: 26 [32640/110534 (30%)]\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 26 [34560/110534 (31%)]\tClassification Loss: 1.4565\r\n",
      "Train Epoch: 26 [36480/110534 (33%)]\tClassification Loss: 1.2979\r\n",
      "Train Epoch: 26 [38400/110534 (35%)]\tClassification Loss: 1.4884\r\n",
      "Train Epoch: 26 [40320/110534 (36%)]\tClassification Loss: 1.3691\r\n",
      "Train Epoch: 26 [42240/110534 (38%)]\tClassification Loss: 1.4057\r\n",
      "Train Epoch: 26 [44160/110534 (40%)]\tClassification Loss: 1.6198\r\n",
      "Train Epoch: 26 [46080/110534 (42%)]\tClassification Loss: 1.5112\r\n",
      "Train Epoch: 26 [48000/110534 (43%)]\tClassification Loss: 1.5822\r\n",
      "Train Epoch: 26 [49920/110534 (45%)]\tClassification Loss: 1.8089\r\n",
      "Train Epoch: 26 [51840/110534 (47%)]\tClassification Loss: 1.7314\r\n",
      "Train Epoch: 26 [53760/110534 (49%)]\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 26 [55680/110534 (50%)]\tClassification Loss: 1.5657\r\n",
      "Train Epoch: 26 [57600/110534 (52%)]\tClassification Loss: 1.5503\r\n",
      "Train Epoch: 26 [59520/110534 (54%)]\tClassification Loss: 1.7934\r\n",
      "Train Epoch: 26 [61440/110534 (56%)]\tClassification Loss: 1.6222\r\n",
      "Train Epoch: 26 [63360/110534 (57%)]\tClassification Loss: 1.7307\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_1000.pth.tar\r\n",
      "Train Epoch: 26 [65280/110534 (59%)]\tClassification Loss: 1.7064\r\n",
      "Train Epoch: 26 [67200/110534 (61%)]\tClassification Loss: 1.5347\r\n",
      "Train Epoch: 26 [69120/110534 (63%)]\tClassification Loss: 1.5957\r\n",
      "Train Epoch: 26 [71040/110534 (64%)]\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 26 [72960/110534 (66%)]\tClassification Loss: 1.3200\r\n",
      "Train Epoch: 26 [74880/110534 (68%)]\tClassification Loss: 1.4885\r\n",
      "Train Epoch: 26 [76800/110534 (69%)]\tClassification Loss: 1.3604\r\n",
      "Train Epoch: 26 [78720/110534 (71%)]\tClassification Loss: 1.5241\r\n",
      "Train Epoch: 26 [80640/110534 (73%)]\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 26 [82560/110534 (75%)]\tClassification Loss: 1.4687\r\n",
      "Train Epoch: 26 [84480/110534 (76%)]\tClassification Loss: 1.3703\r\n",
      "Train Epoch: 26 [86400/110534 (78%)]\tClassification Loss: 1.6294\r\n",
      "Train Epoch: 26 [88320/110534 (80%)]\tClassification Loss: 1.7894\r\n",
      "Train Epoch: 26 [90240/110534 (82%)]\tClassification Loss: 1.6922\r\n",
      "Train Epoch: 26 [92160/110534 (83%)]\tClassification Loss: 1.7864\r\n",
      "Train Epoch: 26 [94080/110534 (85%)]\tClassification Loss: 1.6972\r\n",
      "Train Epoch: 26 [96000/110534 (87%)]\tClassification Loss: 1.6437\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_1500.pth.tar\r\n",
      "Train Epoch: 26 [97920/110534 (89%)]\tClassification Loss: 1.7434\r\n",
      "Train Epoch: 26 [99840/110534 (90%)]\tClassification Loss: 1.5878\r\n",
      "Train Epoch: 26 [101760/110534 (92%)]\tClassification Loss: 1.5634\r\n",
      "Train Epoch: 26 [103680/110534 (94%)]\tClassification Loss: 1.6366\r\n",
      "Train Epoch: 26 [105600/110534 (96%)]\tClassification Loss: 1.3515\r\n",
      "Train Epoch: 26 [107520/110534 (97%)]\tClassification Loss: 1.5402\r\n",
      "Train Epoch: 26 [109440/110534 (99%)]\tClassification Loss: 1.6715\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_26_final.pth.tar\r\n",
      "Train Epoch: 27 [0/110534 (0%)]\tClassification Loss: 1.4377\r\n",
      "\r\n",
      "Test set: Average loss: 1.4569, Accuracy: 22742/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 27 [1920/110534 (2%)]\tClassification Loss: 1.5254\r\n",
      "Train Epoch: 27 [3840/110534 (3%)]\tClassification Loss: 1.6268\r\n",
      "Train Epoch: 27 [5760/110534 (5%)]\tClassification Loss: 1.8177\r\n",
      "Train Epoch: 27 [7680/110534 (7%)]\tClassification Loss: 1.4158\r\n",
      "Train Epoch: 27 [9600/110534 (9%)]\tClassification Loss: 1.6385\r\n",
      "Train Epoch: 27 [11520/110534 (10%)]\tClassification Loss: 1.7775\r\n",
      "Train Epoch: 27 [13440/110534 (12%)]\tClassification Loss: 1.4592\r\n",
      "Train Epoch: 27 [15360/110534 (14%)]\tClassification Loss: 1.8307\r\n",
      "Train Epoch: 27 [17280/110534 (16%)]\tClassification Loss: 1.7617\r\n",
      "Train Epoch: 27 [19200/110534 (17%)]\tClassification Loss: 1.8573\r\n",
      "Train Epoch: 27 [21120/110534 (19%)]\tClassification Loss: 1.5356\r\n",
      "Train Epoch: 27 [23040/110534 (21%)]\tClassification Loss: 1.5156\r\n",
      "Train Epoch: 27 [24960/110534 (23%)]\tClassification Loss: 1.4243\r\n",
      "Train Epoch: 27 [26880/110534 (24%)]\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 27 [28800/110534 (26%)]\tClassification Loss: 2.0173\r\n",
      "Train Epoch: 27 [30720/110534 (28%)]\tClassification Loss: 1.5766\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_500.pth.tar\r\n",
      "Train Epoch: 27 [32640/110534 (30%)]\tClassification Loss: 1.4574\r\n",
      "Train Epoch: 27 [34560/110534 (31%)]\tClassification Loss: 1.3650\r\n",
      "Train Epoch: 27 [36480/110534 (33%)]\tClassification Loss: 1.3097\r\n",
      "Train Epoch: 27 [38400/110534 (35%)]\tClassification Loss: 1.5335\r\n",
      "Train Epoch: 27 [40320/110534 (36%)]\tClassification Loss: 1.5361\r\n",
      "Train Epoch: 27 [42240/110534 (38%)]\tClassification Loss: 1.3920\r\n",
      "Train Epoch: 27 [44160/110534 (40%)]\tClassification Loss: 1.4795\r\n",
      "Train Epoch: 27 [46080/110534 (42%)]\tClassification Loss: 1.5090\r\n",
      "Train Epoch: 27 [48000/110534 (43%)]\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 27 [49920/110534 (45%)]\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 27 [51840/110534 (47%)]\tClassification Loss: 1.8390\r\n",
      "Train Epoch: 27 [53760/110534 (49%)]\tClassification Loss: 1.6145\r\n",
      "Train Epoch: 27 [55680/110534 (50%)]\tClassification Loss: 1.5251\r\n",
      "Train Epoch: 27 [57600/110534 (52%)]\tClassification Loss: 1.6102\r\n",
      "Train Epoch: 27 [59520/110534 (54%)]\tClassification Loss: 1.9202\r\n",
      "Train Epoch: 27 [61440/110534 (56%)]\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 27 [63360/110534 (57%)]\tClassification Loss: 1.8848\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_1000.pth.tar\r\n",
      "Train Epoch: 27 [65280/110534 (59%)]\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 27 [67200/110534 (61%)]\tClassification Loss: 1.4197\r\n",
      "Train Epoch: 27 [69120/110534 (63%)]\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 27 [71040/110534 (64%)]\tClassification Loss: 1.5927\r\n",
      "Train Epoch: 27 [72960/110534 (66%)]\tClassification Loss: 1.4543\r\n",
      "Train Epoch: 27 [74880/110534 (68%)]\tClassification Loss: 1.5290\r\n",
      "Train Epoch: 27 [76800/110534 (69%)]\tClassification Loss: 1.4593\r\n",
      "Train Epoch: 27 [78720/110534 (71%)]\tClassification Loss: 1.6028\r\n",
      "Train Epoch: 27 [80640/110534 (73%)]\tClassification Loss: 1.6633\r\n",
      "Train Epoch: 27 [82560/110534 (75%)]\tClassification Loss: 1.3439\r\n",
      "Train Epoch: 27 [84480/110534 (76%)]\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 27 [86400/110534 (78%)]\tClassification Loss: 1.5142\r\n",
      "Train Epoch: 27 [88320/110534 (80%)]\tClassification Loss: 1.8217\r\n",
      "Train Epoch: 27 [90240/110534 (82%)]\tClassification Loss: 1.5774\r\n",
      "Train Epoch: 27 [92160/110534 (83%)]\tClassification Loss: 1.9092\r\n",
      "Train Epoch: 27 [94080/110534 (85%)]\tClassification Loss: 1.7215\r\n",
      "Train Epoch: 27 [96000/110534 (87%)]\tClassification Loss: 1.6768\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_1500.pth.tar\r\n",
      "Train Epoch: 27 [97920/110534 (89%)]\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 27 [99840/110534 (90%)]\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 27 [101760/110534 (92%)]\tClassification Loss: 1.5703\r\n",
      "Train Epoch: 27 [103680/110534 (94%)]\tClassification Loss: 1.5249\r\n",
      "Train Epoch: 27 [105600/110534 (96%)]\tClassification Loss: 1.4570\r\n",
      "Train Epoch: 27 [107520/110534 (97%)]\tClassification Loss: 1.4448\r\n",
      "Train Epoch: 27 [109440/110534 (99%)]\tClassification Loss: 1.6150\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_27_final.pth.tar\r\n",
      "Train Epoch: 28 [0/110534 (0%)]\tClassification Loss: 1.5881\r\n",
      "\r\n",
      "Test set: Average loss: 1.4576, Accuracy: 22723/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 28 [1920/110534 (2%)]\tClassification Loss: 1.3401\r\n",
      "Train Epoch: 28 [3840/110534 (3%)]\tClassification Loss: 1.5954\r\n",
      "Train Epoch: 28 [5760/110534 (5%)]\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 28 [7680/110534 (7%)]\tClassification Loss: 1.6356\r\n",
      "Train Epoch: 28 [9600/110534 (9%)]\tClassification Loss: 1.6425\r\n",
      "Train Epoch: 28 [11520/110534 (10%)]\tClassification Loss: 1.8118\r\n",
      "Train Epoch: 28 [13440/110534 (12%)]\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 28 [15360/110534 (14%)]\tClassification Loss: 1.9317\r\n",
      "Train Epoch: 28 [17280/110534 (16%)]\tClassification Loss: 1.8577\r\n",
      "Train Epoch: 28 [19200/110534 (17%)]\tClassification Loss: 2.0244\r\n",
      "Train Epoch: 28 [21120/110534 (19%)]\tClassification Loss: 1.6447\r\n",
      "Train Epoch: 28 [23040/110534 (21%)]\tClassification Loss: 1.4687\r\n",
      "Train Epoch: 28 [24960/110534 (23%)]\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 28 [26880/110534 (24%)]\tClassification Loss: 1.6487\r\n",
      "Train Epoch: 28 [28800/110534 (26%)]\tClassification Loss: 1.8867\r\n",
      "Train Epoch: 28 [30720/110534 (28%)]\tClassification Loss: 1.4762\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_500.pth.tar\r\n",
      "Train Epoch: 28 [32640/110534 (30%)]\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 28 [34560/110534 (31%)]\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 28 [36480/110534 (33%)]\tClassification Loss: 1.3184\r\n",
      "Train Epoch: 28 [38400/110534 (35%)]\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 28 [40320/110534 (36%)]\tClassification Loss: 1.4964\r\n",
      "Train Epoch: 28 [42240/110534 (38%)]\tClassification Loss: 1.4386\r\n",
      "Train Epoch: 28 [44160/110534 (40%)]\tClassification Loss: 1.4180\r\n",
      "Train Epoch: 28 [46080/110534 (42%)]\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 28 [48000/110534 (43%)]\tClassification Loss: 1.5029\r\n",
      "Train Epoch: 28 [49920/110534 (45%)]\tClassification Loss: 1.7713\r\n",
      "Train Epoch: 28 [51840/110534 (47%)]\tClassification Loss: 1.7249\r\n",
      "Train Epoch: 28 [53760/110534 (49%)]\tClassification Loss: 1.4386\r\n",
      "Train Epoch: 28 [55680/110534 (50%)]\tClassification Loss: 1.5383\r\n",
      "Train Epoch: 28 [57600/110534 (52%)]\tClassification Loss: 1.5420\r\n",
      "Train Epoch: 28 [59520/110534 (54%)]\tClassification Loss: 1.7125\r\n",
      "Train Epoch: 28 [61440/110534 (56%)]\tClassification Loss: 1.5453\r\n",
      "Train Epoch: 28 [63360/110534 (57%)]\tClassification Loss: 1.7021\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_1000.pth.tar\r\n",
      "Train Epoch: 28 [65280/110534 (59%)]\tClassification Loss: 1.5491\r\n",
      "Train Epoch: 28 [67200/110534 (61%)]\tClassification Loss: 1.3022\r\n",
      "Train Epoch: 28 [69120/110534 (63%)]\tClassification Loss: 1.5975\r\n",
      "Train Epoch: 28 [71040/110534 (64%)]\tClassification Loss: 1.5419\r\n",
      "Train Epoch: 28 [72960/110534 (66%)]\tClassification Loss: 1.4895\r\n",
      "Train Epoch: 28 [74880/110534 (68%)]\tClassification Loss: 1.5047\r\n",
      "Train Epoch: 28 [76800/110534 (69%)]\tClassification Loss: 1.5155\r\n",
      "Train Epoch: 28 [78720/110534 (71%)]\tClassification Loss: 1.4717\r\n",
      "Train Epoch: 28 [80640/110534 (73%)]\tClassification Loss: 1.4593\r\n",
      "Train Epoch: 28 [82560/110534 (75%)]\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 28 [84480/110534 (76%)]\tClassification Loss: 1.6084\r\n",
      "Train Epoch: 28 [86400/110534 (78%)]\tClassification Loss: 1.6296\r\n",
      "Train Epoch: 28 [88320/110534 (80%)]\tClassification Loss: 1.7071\r\n",
      "Train Epoch: 28 [90240/110534 (82%)]\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 28 [92160/110534 (83%)]\tClassification Loss: 1.8383\r\n",
      "Train Epoch: 28 [94080/110534 (85%)]\tClassification Loss: 1.7201\r\n",
      "Train Epoch: 28 [96000/110534 (87%)]\tClassification Loss: 1.6556\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_1500.pth.tar\r\n",
      "Train Epoch: 28 [97920/110534 (89%)]\tClassification Loss: 1.6739\r\n",
      "Train Epoch: 28 [99840/110534 (90%)]\tClassification Loss: 1.5521\r\n",
      "Train Epoch: 28 [101760/110534 (92%)]\tClassification Loss: 1.4578\r\n",
      "Train Epoch: 28 [103680/110534 (94%)]\tClassification Loss: 1.5710\r\n",
      "Train Epoch: 28 [105600/110534 (96%)]\tClassification Loss: 1.4532\r\n",
      "Train Epoch: 28 [107520/110534 (97%)]\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 28 [109440/110534 (99%)]\tClassification Loss: 1.7687\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_28_final.pth.tar\r\n",
      "Train Epoch: 29 [0/110534 (0%)]\tClassification Loss: 1.6034\r\n",
      "\r\n",
      "Test set: Average loss: 1.4545, Accuracy: 22750/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 29 [1920/110534 (2%)]\tClassification Loss: 1.4235\r\n",
      "Train Epoch: 29 [3840/110534 (3%)]\tClassification Loss: 1.4490\r\n",
      "Train Epoch: 29 [5760/110534 (5%)]\tClassification Loss: 1.6083\r\n",
      "Train Epoch: 29 [7680/110534 (7%)]\tClassification Loss: 1.5136\r\n",
      "Train Epoch: 29 [9600/110534 (9%)]\tClassification Loss: 1.6859\r\n",
      "Train Epoch: 29 [11520/110534 (10%)]\tClassification Loss: 1.6532\r\n",
      "Train Epoch: 29 [13440/110534 (12%)]\tClassification Loss: 1.5118\r\n",
      "Train Epoch: 29 [15360/110534 (14%)]\tClassification Loss: 1.6961\r\n",
      "Train Epoch: 29 [17280/110534 (16%)]\tClassification Loss: 1.8243\r\n",
      "Train Epoch: 29 [19200/110534 (17%)]\tClassification Loss: 1.9465\r\n",
      "Train Epoch: 29 [21120/110534 (19%)]\tClassification Loss: 1.5483\r\n",
      "Train Epoch: 29 [23040/110534 (21%)]\tClassification Loss: 1.3869\r\n",
      "Train Epoch: 29 [24960/110534 (23%)]\tClassification Loss: 1.5315\r\n",
      "Train Epoch: 29 [26880/110534 (24%)]\tClassification Loss: 1.6959\r\n",
      "Train Epoch: 29 [28800/110534 (26%)]\tClassification Loss: 1.9497\r\n",
      "Train Epoch: 29 [30720/110534 (28%)]\tClassification Loss: 1.4114\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_500.pth.tar\r\n",
      "Train Epoch: 29 [32640/110534 (30%)]\tClassification Loss: 1.4369\r\n",
      "Train Epoch: 29 [34560/110534 (31%)]\tClassification Loss: 1.3917\r\n",
      "Train Epoch: 29 [36480/110534 (33%)]\tClassification Loss: 1.3858\r\n",
      "Train Epoch: 29 [38400/110534 (35%)]\tClassification Loss: 1.6869\r\n",
      "Train Epoch: 29 [40320/110534 (36%)]\tClassification Loss: 1.5705\r\n",
      "Train Epoch: 29 [42240/110534 (38%)]\tClassification Loss: 1.5643\r\n",
      "Train Epoch: 29 [44160/110534 (40%)]\tClassification Loss: 1.5630\r\n",
      "Train Epoch: 29 [46080/110534 (42%)]\tClassification Loss: 1.4797\r\n",
      "Train Epoch: 29 [48000/110534 (43%)]\tClassification Loss: 1.6392\r\n",
      "Train Epoch: 29 [49920/110534 (45%)]\tClassification Loss: 1.6454\r\n",
      "Train Epoch: 29 [51840/110534 (47%)]\tClassification Loss: 1.9541\r\n",
      "Train Epoch: 29 [53760/110534 (49%)]\tClassification Loss: 1.5753\r\n",
      "Train Epoch: 29 [55680/110534 (50%)]\tClassification Loss: 1.5335\r\n",
      "Train Epoch: 29 [57600/110534 (52%)]\tClassification Loss: 1.5025\r\n",
      "Train Epoch: 29 [59520/110534 (54%)]\tClassification Loss: 1.8527\r\n",
      "Train Epoch: 29 [61440/110534 (56%)]\tClassification Loss: 1.6543\r\n",
      "Train Epoch: 29 [63360/110534 (57%)]\tClassification Loss: 1.6853\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_1000.pth.tar\r\n",
      "Train Epoch: 29 [65280/110534 (59%)]\tClassification Loss: 1.5723\r\n",
      "Train Epoch: 29 [67200/110534 (61%)]\tClassification Loss: 1.3368\r\n",
      "Train Epoch: 29 [69120/110534 (63%)]\tClassification Loss: 1.6348\r\n",
      "Train Epoch: 29 [71040/110534 (64%)]\tClassification Loss: 1.5781\r\n",
      "Train Epoch: 29 [72960/110534 (66%)]\tClassification Loss: 1.4861\r\n",
      "Train Epoch: 29 [74880/110534 (68%)]\tClassification Loss: 1.4703\r\n",
      "Train Epoch: 29 [76800/110534 (69%)]\tClassification Loss: 1.4137\r\n",
      "Train Epoch: 29 [78720/110534 (71%)]\tClassification Loss: 1.4774\r\n",
      "Train Epoch: 29 [80640/110534 (73%)]\tClassification Loss: 1.4596\r\n",
      "Train Epoch: 29 [82560/110534 (75%)]\tClassification Loss: 1.4177\r\n",
      "Train Epoch: 29 [84480/110534 (76%)]\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 29 [86400/110534 (78%)]\tClassification Loss: 1.5053\r\n",
      "Train Epoch: 29 [88320/110534 (80%)]\tClassification Loss: 1.6694\r\n",
      "Train Epoch: 29 [90240/110534 (82%)]\tClassification Loss: 1.7791\r\n",
      "Train Epoch: 29 [92160/110534 (83%)]\tClassification Loss: 1.8755\r\n",
      "Train Epoch: 29 [94080/110534 (85%)]\tClassification Loss: 1.7293\r\n",
      "Train Epoch: 29 [96000/110534 (87%)]\tClassification Loss: 1.8107\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_1500.pth.tar\r\n",
      "Train Epoch: 29 [97920/110534 (89%)]\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 29 [99840/110534 (90%)]\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 29 [101760/110534 (92%)]\tClassification Loss: 1.3658\r\n",
      "Train Epoch: 29 [103680/110534 (94%)]\tClassification Loss: 1.5920\r\n",
      "Train Epoch: 29 [105600/110534 (96%)]\tClassification Loss: 1.4815\r\n",
      "Train Epoch: 29 [107520/110534 (97%)]\tClassification Loss: 1.5675\r\n",
      "Train Epoch: 29 [109440/110534 (99%)]\tClassification Loss: 1.6800\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_29_final.pth.tar\r\n",
      "Train Epoch: 30 [0/110534 (0%)]\tClassification Loss: 1.5670\r\n",
      "\r\n",
      "Test set: Average loss: 1.4542, Accuracy: 22754/42368 (54%)\r\n",
      "\r\n",
      "Train Epoch: 30 [1920/110534 (2%)]\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 30 [3840/110534 (3%)]\tClassification Loss: 1.5796\r\n",
      "Train Epoch: 30 [5760/110534 (5%)]\tClassification Loss: 1.7190\r\n",
      "Train Epoch: 30 [7680/110534 (7%)]\tClassification Loss: 1.5348\r\n",
      "Train Epoch: 30 [9600/110534 (9%)]\tClassification Loss: 1.5064\r\n",
      "Train Epoch: 30 [11520/110534 (10%)]\tClassification Loss: 1.6245\r\n",
      "Train Epoch: 30 [13440/110534 (12%)]\tClassification Loss: 1.4954\r\n",
      "Train Epoch: 30 [15360/110534 (14%)]\tClassification Loss: 1.6893\r\n",
      "Train Epoch: 30 [17280/110534 (16%)]\tClassification Loss: 1.6778\r\n",
      "Train Epoch: 30 [19200/110534 (17%)]\tClassification Loss: 1.8567\r\n",
      "Train Epoch: 30 [21120/110534 (19%)]\tClassification Loss: 1.5326\r\n",
      "Train Epoch: 30 [23040/110534 (21%)]\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 30 [24960/110534 (23%)]\tClassification Loss: 1.4925\r\n",
      "Train Epoch: 30 [26880/110534 (24%)]\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 30 [28800/110534 (26%)]\tClassification Loss: 1.9139\r\n",
      "Train Epoch: 30 [30720/110534 (28%)]\tClassification Loss: 1.4510\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_500.pth.tar\r\n",
      "Train Epoch: 30 [32640/110534 (30%)]\tClassification Loss: 1.3583\r\n",
      "Train Epoch: 30 [34560/110534 (31%)]\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 30 [36480/110534 (33%)]\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 30 [38400/110534 (35%)]\tClassification Loss: 1.4518\r\n",
      "Train Epoch: 30 [40320/110534 (36%)]\tClassification Loss: 1.5342\r\n",
      "Train Epoch: 30 [42240/110534 (38%)]\tClassification Loss: 1.4899\r\n",
      "Train Epoch: 30 [44160/110534 (40%)]\tClassification Loss: 1.4127\r\n",
      "Train Epoch: 30 [46080/110534 (42%)]\tClassification Loss: 1.4640\r\n",
      "Train Epoch: 30 [48000/110534 (43%)]\tClassification Loss: 1.6059\r\n",
      "Train Epoch: 30 [49920/110534 (45%)]\tClassification Loss: 1.6872\r\n",
      "Train Epoch: 30 [51840/110534 (47%)]\tClassification Loss: 1.6002\r\n",
      "Train Epoch: 30 [53760/110534 (49%)]\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 30 [55680/110534 (50%)]\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 30 [57600/110534 (52%)]\tClassification Loss: 1.6739\r\n",
      "Train Epoch: 30 [59520/110534 (54%)]\tClassification Loss: 1.9049\r\n",
      "Train Epoch: 30 [61440/110534 (56%)]\tClassification Loss: 1.7727\r\n",
      "Train Epoch: 30 [63360/110534 (57%)]\tClassification Loss: 1.5903\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_1000.pth.tar\r\n",
      "Train Epoch: 30 [65280/110534 (59%)]\tClassification Loss: 1.7157\r\n",
      "Train Epoch: 30 [67200/110534 (61%)]\tClassification Loss: 1.4265\r\n",
      "Train Epoch: 30 [69120/110534 (63%)]\tClassification Loss: 1.5607\r\n",
      "Train Epoch: 30 [71040/110534 (64%)]\tClassification Loss: 1.5888\r\n",
      "Train Epoch: 30 [72960/110534 (66%)]\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 30 [74880/110534 (68%)]\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 30 [76800/110534 (69%)]\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 30 [78720/110534 (71%)]\tClassification Loss: 1.4919\r\n",
      "Train Epoch: 30 [80640/110534 (73%)]\tClassification Loss: 1.6838\r\n",
      "Train Epoch: 30 [82560/110534 (75%)]\tClassification Loss: 1.3298\r\n",
      "Train Epoch: 30 [84480/110534 (76%)]\tClassification Loss: 1.4616\r\n",
      "Train Epoch: 30 [86400/110534 (78%)]\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 30 [88320/110534 (80%)]\tClassification Loss: 1.8107\r\n",
      "Train Epoch: 30 [90240/110534 (82%)]\tClassification Loss: 1.8091\r\n",
      "Train Epoch: 30 [92160/110534 (83%)]\tClassification Loss: 1.8121\r\n",
      "Train Epoch: 30 [94080/110534 (85%)]\tClassification Loss: 1.6815\r\n",
      "Train Epoch: 30 [96000/110534 (87%)]\tClassification Loss: 1.7226\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_1500.pth.tar\r\n",
      "Train Epoch: 30 [97920/110534 (89%)]\tClassification Loss: 1.6312\r\n",
      "Train Epoch: 30 [99840/110534 (90%)]\tClassification Loss: 1.5999\r\n",
      "Train Epoch: 30 [101760/110534 (92%)]\tClassification Loss: 1.4895\r\n",
      "Train Epoch: 30 [103680/110534 (94%)]\tClassification Loss: 1.6261\r\n",
      "Train Epoch: 30 [105600/110534 (96%)]\tClassification Loss: 1.5679\r\n",
      "Train Epoch: 30 [107520/110534 (97%)]\tClassification Loss: 1.5724\r\n",
      "Train Epoch: 30 [109440/110534 (99%)]\tClassification Loss: 1.6672\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_30_final.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "# FREEZE = False. LR=0.001. in-shop=False. 30 epochs\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 50 Categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tAll Loss: 7.0639\tTriple Loss(0): 1.5324\tClassification Loss: 3.9991\r\n",
      "train.py:187: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9127\r\n",
      "Top 1 Accuracy: 1393/80000 (2%)\r\n",
      "Top 3 Accuracy: 5045/80000 (6%)\r\n",
      "Top 5 Accuracy: 8674/80000 (11%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.019 sec\r\n",
      "100%|| 14218/14218 [02:19<00:00, 102.25it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:19<00:00, 102.03it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:19<00:00, 101.73it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:19<00:00, 101.60it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tAll Loss: 4.3214\tTriple Loss(1): 0.8653\tClassification Loss: 2.5908\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tAll Loss: 4.3819\tTriple Loss(0): 0.7146\tClassification Loss: 2.9527\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tAll Loss: 3.7524\tTriple Loss(0): 0.4543\tClassification Loss: 2.8438\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tAll Loss: 4.1818\tTriple Loss(1): 0.7572\tClassification Loss: 2.6674\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tAll Loss: 4.3843\tTriple Loss(1): 0.7353\tClassification Loss: 2.9136\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tAll Loss: 4.7259\tTriple Loss(0): 0.9962\tClassification Loss: 2.7335\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/209222 (0%)]\tAll Loss: 4.5350\tTriple Loss(1): 0.6956\tClassification Loss: 3.1438\r\n",
      "\r\n",
      "Test set: Average loss: 2.7592\r\n",
      "Top 1 Accuracy: 20020/80000 (25%)\r\n",
      "Top 3 Accuracy: 37059/80000 (46%)\r\n",
      "Top 5 Accuracy: 46976/80000 (59%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.025 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.24it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:23<00:00, 98.97it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:23<00:00, 98.78it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:24<00:00, 98.37it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 2 [32000/209222 (15%)]\tAll Loss: 4.0815\tTriple Loss(1): 0.7571\tClassification Loss: 2.5673\r\n",
      "Train Epoch: 2 [64000/209222 (31%)]\tAll Loss: 4.2430\tTriple Loss(1): 0.6373\tClassification Loss: 2.9685\r\n",
      "Train Epoch: 2 [96000/209222 (46%)]\tAll Loss: 3.2703\tTriple Loss(0): 0.2384\tClassification Loss: 2.7936\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [128000/209222 (61%)]\tAll Loss: 4.1656\tTriple Loss(1): 0.7461\tClassification Loss: 2.6735\r\n",
      "Train Epoch: 2 [160000/209222 (76%)]\tAll Loss: 3.9200\tTriple Loss(1): 0.5076\tClassification Loss: 2.9047\r\n",
      "Train Epoch: 2 [192000/209222 (92%)]\tAll Loss: 3.8965\tTriple Loss(1): 0.6021\tClassification Loss: 2.6924\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/209222 (0%)]\tAll Loss: 5.6398\tTriple Loss(0): 1.3015\tClassification Loss: 3.0369\r\n",
      "\r\n",
      "Test set: Average loss: 2.7323\r\n",
      "Top 1 Accuracy: 20020/80000 (25%)\r\n",
      "Top 3 Accuracy: 37059/80000 (46%)\r\n",
      "Top 5 Accuracy: 46275/80000 (58%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.024 sec\r\n",
      "100%|| 14218/14218 [02:19<00:00, 101.96it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.72it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.017 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.19it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:22<00:00, 100.04it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 3 [32000/209222 (15%)]\tAll Loss: 2.5528\tTriple Loss(0): 0.0000\tClassification Loss: 2.5528\r\n",
      "Train Epoch: 3 [64000/209222 (31%)]\tAll Loss: 4.1967\tTriple Loss(1): 0.6222\tClassification Loss: 2.9524\r\n",
      "Train Epoch: 3 [96000/209222 (46%)]\tAll Loss: 3.9000\tTriple Loss(1): 0.5676\tClassification Loss: 2.7647\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [128000/209222 (61%)]\tAll Loss: 3.1187\tTriple Loss(1): 0.2521\tClassification Loss: 2.6145\r\n",
      "Train Epoch: 3 [160000/209222 (76%)]\tAll Loss: 3.5058\tTriple Loss(1): 0.3344\tClassification Loss: 2.8369\r\n",
      "Train Epoch: 3 [192000/209222 (92%)]\tAll Loss: 3.4936\tTriple Loss(1): 0.4241\tClassification Loss: 2.6453\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/209222 (0%)]\tAll Loss: 3.9797\tTriple Loss(1): 0.5055\tClassification Loss: 2.9687\r\n",
      "\r\n",
      "Test set: Average loss: 2.6342\r\n",
      "Top 1 Accuracy: 20773/80000 (26%)\r\n",
      "Top 3 Accuracy: 37659/80000 (47%)\r\n",
      "Top 5 Accuracy: 49545/80000 (62%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.024 sec\r\n",
      "100%|| 14218/14218 [02:23<00:00, 98.83it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:24<00:00, 98.13it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:25<00:00, 97.88it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:25<00:00, 97.46it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 4 [32000/209222 (15%)]\tAll Loss: 2.4717\tTriple Loss(0): 0.0000\tClassification Loss: 2.4717\r\n",
      "Train Epoch: 4 [64000/209222 (31%)]\tAll Loss: 3.6532\tTriple Loss(1): 0.3654\tClassification Loss: 2.9223\r\n",
      "Train Epoch: 4 [96000/209222 (46%)]\tAll Loss: 3.4816\tTriple Loss(1): 0.3983\tClassification Loss: 2.6851\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_1500.pth.tar\r\n",
      "Train Epoch: 4 [128000/209222 (61%)]\tAll Loss: 3.4301\tTriple Loss(1): 0.4318\tClassification Loss: 2.5666\r\n",
      "Train Epoch: 4 [160000/209222 (76%)]\tAll Loss: 2.9659\tTriple Loss(1): 0.1038\tClassification Loss: 2.7582\r\n",
      "Train Epoch: 4 [192000/209222 (92%)]\tAll Loss: 3.0979\tTriple Loss(1): 0.3132\tClassification Loss: 2.4714\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_4_final.pth.tar\r\n",
      "Train Epoch: 5 [0/209222 (0%)]\tAll Loss: 3.7272\tTriple Loss(1): 0.4374\tClassification Loss: 2.8523\r\n",
      "\r\n",
      "Test set: Average loss: 2.5326\r\n",
      "Top 1 Accuracy: 23113/80000 (29%)\r\n",
      "Top 3 Accuracy: 39595/80000 (49%)\r\n",
      "Top 5 Accuracy: 52100/80000 (65%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.024 sec\r\n",
      "100%|| 14218/14218 [02:20<00:00, 101.15it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.59it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.37it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.18it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 5 [32000/209222 (15%)]\tAll Loss: 3.7092\tTriple Loss(0): 0.6319\tClassification Loss: 2.4453\r\n",
      "Train Epoch: 5 [64000/209222 (31%)]\tAll Loss: 3.4975\tTriple Loss(1): 0.3075\tClassification Loss: 2.8826\r\n",
      "Train Epoch: 5 [96000/209222 (46%)]\tAll Loss: 3.3216\tTriple Loss(1): 0.3394\tClassification Loss: 2.6429\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_1500.pth.tar\r\n",
      "Train Epoch: 5 [128000/209222 (61%)]\tAll Loss: 4.5308\tTriple Loss(0): 1.0723\tClassification Loss: 2.3862\r\n",
      "Train Epoch: 5 [160000/209222 (76%)]\tAll Loss: 2.9384\tTriple Loss(1): 0.1445\tClassification Loss: 2.6493\r\n",
      "Train Epoch: 5 [192000/209222 (92%)]\tAll Loss: 3.0407\tTriple Loss(1): 0.3012\tClassification Loss: 2.4383\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_5_final.pth.tar\r\n",
      "Train Epoch: 6 [0/209222 (0%)]\tAll Loss: 3.5701\tTriple Loss(1): 0.3599\tClassification Loss: 2.8503\r\n",
      "\r\n",
      "Test set: Average loss: 2.3198\r\n",
      "Top 1 Accuracy: 28717/80000 (36%)\r\n",
      "Top 3 Accuracy: 45913/80000 (57%)\r\n",
      "Top 5 Accuracy: 56681/80000 (71%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.024 sec\r\n",
      "100%|| 14218/14218 [02:20<00:00, 100.96it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.88it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.73it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "Loading in-shop test feature database...\r\n",
      "Loading in-shop test feature database Done. Time: 0.018 sec\r\n",
      "100%|| 14218/14218 [02:23<00:00, 99.28it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 6 [32000/209222 (15%)]\tAll Loss: 2.3078\tTriple Loss(0): 0.0000\tClassification Loss: 2.3078\r\n",
      "Train Epoch: 6 [64000/209222 (31%)]\tAll Loss: 3.2399\tTriple Loss(1): 0.2686\tClassification Loss: 2.7028\r\n",
      "Train Epoch: 6 [96000/209222 (46%)]\tAll Loss: 2.3874\tTriple Loss(0): 0.0000\tClassification Loss: 2.3874\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_6_1500.pth.tar\r\n",
      "Train Epoch: 6 [128000/209222 (61%)]\tAll Loss: 2.8795\tTriple Loss(1): 0.3232\tClassification Loss: 2.2331\r\n"
     ]
    }
   ],
   "source": [
    "# FREEZE = False. LR=0.001. in-shop=True. 30 epochs\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model model_6_2000.pth.tar\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model model_6_2000.pth.tar\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tAll Loss: 3.9578\tTriple Loss(1): 0.6069\tClassification Loss: 2.7440\r\n",
      "train.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 2.6806\r\n",
      "Top 1 Accuracy: 20020/80000 (25%)\r\n",
      "Top 3 Accuracy: 37266/80000 (47%)\r\n",
      "Top 5 Accuracy: 49199/80000 (61%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:20<00:00, 101.04it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.32it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.30it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.90it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tAll Loss: 3.5943\tTriple Loss(1): 0.4393\tClassification Loss: 2.7157\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tAll Loss: 3.3859\tTriple Loss(1): 0.3599\tClassification Loss: 2.6661\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tAll Loss: 2.8750\tTriple Loss(0): 0.0000\tClassification Loss: 2.8750\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tAll Loss: 3.6866\tTriple Loss(1): 0.3495\tClassification Loss: 2.9875\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tAll Loss: 2.7910\tTriple Loss(1): 0.2378\tClassification Loss: 2.3155\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tAll Loss: 5.7401\tTriple Loss(0): 1.6204\tClassification Loss: 2.4993\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/209222 (0%)]\tAll Loss: 3.8300\tTriple Loss(1): 0.5507\tClassification Loss: 2.7285\r\n",
      "\r\n",
      "Test set: Average loss: 2.5922\r\n",
      "Top 1 Accuracy: 20371/80000 (25%)\r\n",
      "Top 3 Accuracy: 38893/80000 (49%)\r\n",
      "Top 5 Accuracy: 50538/80000 (63%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:20<00:00, 101.41it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.36it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "100%|| 14218/14218 [02:20<00:00, 101.49it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.13it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 2 [32000/209222 (15%)]\tAll Loss: 2.7182\tTriple Loss(0): 0.0335\tClassification Loss: 2.6511\r\n",
      "Train Epoch: 2 [64000/209222 (31%)]\tAll Loss: 3.0154\tTriple Loss(1): 0.1978\tClassification Loss: 2.6197\r\n",
      "Train Epoch: 2 [96000/209222 (46%)]\tAll Loss: 3.5326\tTriple Loss(1): 0.3866\tClassification Loss: 2.7595\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [128000/209222 (61%)]\tAll Loss: 3.0781\tTriple Loss(1): 0.1074\tClassification Loss: 2.8633\r\n",
      "Train Epoch: 2 [160000/209222 (76%)]\tAll Loss: 2.9441\tTriple Loss(1): 0.3250\tClassification Loss: 2.2942\r\n",
      "Train Epoch: 2 [192000/209222 (92%)]\tAll Loss: 2.8051\tTriple Loss(1): 0.2068\tClassification Loss: 2.3915\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/209222 (0%)]\tAll Loss: 3.5200\tTriple Loss(1): 0.4260\tClassification Loss: 2.6680\r\n",
      "\r\n",
      "Test set: Average loss: 2.4564\r\n",
      "Top 1 Accuracy: 22785/80000 (28%)\r\n",
      "Top 3 Accuracy: 41755/80000 (52%)\r\n",
      "Top 5 Accuracy: 53000/80000 (66%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:20<00:00, 101.00it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "100%|| 14218/14218 [02:21<00:00, 100.32it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.91it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.86it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 3 [32000/209222 (15%)]\tAll Loss: 3.1631\tTriple Loss(1): 0.2828\tClassification Loss: 2.5976\r\n",
      "Train Epoch: 3 [64000/209222 (31%)]\tAll Loss: 3.1815\tTriple Loss(1): 0.3089\tClassification Loss: 2.5637\r\n",
      "Train Epoch: 3 [96000/209222 (46%)]\tAll Loss: 2.9009\tTriple Loss(1): 0.1388\tClassification Loss: 2.6232\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [128000/209222 (61%)]\tAll Loss: 3.2167\tTriple Loss(1): 0.2418\tClassification Loss: 2.7330\r\n",
      "Train Epoch: 3 [160000/209222 (76%)]\tAll Loss: 3.0881\tTriple Loss(1): 0.4487\tClassification Loss: 2.1908\r\n",
      "Train Epoch: 3 [192000/209222 (92%)]\tAll Loss: 2.8553\tTriple Loss(1): 0.2830\tClassification Loss: 2.2894\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/209222 (0%)]\tAll Loss: 3.3184\tTriple Loss(1): 0.4011\tClassification Loss: 2.5163\r\n",
      "\r\n",
      "Test set: Average loss: 2.2343\r\n",
      "Top 1 Accuracy: 28249/80000 (35%)\r\n",
      "Top 3 Accuracy: 47359/80000 (59%)\r\n",
      "Top 5 Accuracy: 57510/80000 (72%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.47it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "100%|| 14218/14218 [02:23<00:00, 98.90it/s]\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "100%|| 14218/14218 [02:23<00:00, 99.09it/s]\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "100%|| 14218/14218 [02:24<00:00, 98.57it/s]\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 4 [32000/209222 (15%)]\tAll Loss: 2.6745\tTriple Loss(0): 0.1761\tClassification Loss: 2.3223\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing model_6_2000.pth.tar\n",
    "# FREEZE = False. LR=0.001. in-shop=True.\n",
    "# Result is model_3_final.pth.tar. Trained for 9+ epochs.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model model_3_final.pth.tar\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model model_3_final.pth.tar\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tAll Loss: 2.4196\tTriple Loss(0): 0.0000\tClassification Loss: 2.4196\r\n",
      "train.py:191: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 2.2381\r\n",
      "Top 1 Accuracy: 28278/80000 (35%)\r\n",
      "Top 3 Accuracy: 47172/80000 (59%)\r\n",
      "Top 5 Accuracy: 57355/80000 (72%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:23<00:00, 99.16it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tAll Loss: 2.7604\tTriple Loss(1): 0.2631\tClassification Loss: 2.2342\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tAll Loss: 2.9373\tTriple Loss(1): 0.2202\tClassification Loss: 2.4970\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tAll Loss: 3.0321\tTriple Loss(1): 0.3315\tClassification Loss: 2.3691\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tAll Loss: 2.6921\tTriple Loss(1): 0.1971\tClassification Loss: 2.2979\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tAll Loss: 2.3424\tTriple Loss(1): 0.1643\tClassification Loss: 2.0137\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tAll Loss: 2.2741\tTriple Loss(0): 0.0000\tClassification Loss: 2.2741\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/209222 (0%)]\tAll Loss: 3.1489\tTriple Loss(1): 0.4388\tClassification Loss: 2.2713\r\n",
      "\r\n",
      "Test set: Average loss: 1.9718\r\n",
      "Top 1 Accuracy: 34883/80000 (44%)\r\n",
      "Top 3 Accuracy: 54497/80000 (68%)\r\n",
      "Top 5 Accuracy: 63487/80000 (79%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:22<00:00, 100.03it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 2 [32000/209222 (15%)]\tAll Loss: 2.5346\tTriple Loss(1): 0.2154\tClassification Loss: 2.1039\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing model_3_final.pth.tar, which is trained for 9+ epochs.\n",
    "# FREEZE = False. LR=0.001. in-shop=True. 30 epochs\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model model_1_final.pth.tar\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model model_1_final.pth.tar\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tAll Loss: 3.0388\tTriple Loss(1): 0.3443\tClassification Loss: 2.3502\r\n",
      "train.py:192: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.9724\r\n",
      "Top 1 Accuracy: 34889/80000 (44%)\r\n",
      "Top 3 Accuracy: 54489/80000 (68%)\r\n",
      "Top 5 Accuracy: 63483/80000 (79%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:24<00:00, 98.63it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tAll Loss: 2.8499\tTriple Loss(1): 0.2846\tClassification Loss: 2.2808\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tAll Loss: 2.3696\tTriple Loss(0): 0.0000\tClassification Loss: 2.3696\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tAll Loss: 2.2099\tTriple Loss(1): 0.2301\tClassification Loss: 1.7497\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_1500.pth.tar\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tAll Loss: 2.1935\tTriple Loss(0): 0.0000\tClassification Loss: 2.1935\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tAll Loss: 2.0613\tTriple Loss(1): 0.1057\tClassification Loss: 1.8498\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tAll Loss: 2.3959\tTriple Loss(1): 0.0863\tClassification Loss: 2.2232\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_1_final.pth.tar\r\n",
      "Train Epoch: 2 [0/209222 (0%)]\tAll Loss: 5.3615\tTriple Loss(0): 1.4407\tClassification Loss: 2.4801\r\n",
      "\r\n",
      "Test set: Average loss: 1.9545\r\n",
      "Top 1 Accuracy: 34773/80000 (43%)\r\n",
      "Top 3 Accuracy: 53503/80000 (67%)\r\n",
      "Top 5 Accuracy: 62858/80000 (79%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:24<00:00, 98.42it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 2 [32000/209222 (15%)]\tAll Loss: 2.6035\tTriple Loss(1): 0.2918\tClassification Loss: 2.0200\r\n",
      "Train Epoch: 2 [64000/209222 (31%)]\tAll Loss: 2.2079\tTriple Loss(1): 0.2091\tClassification Loss: 1.7897\r\n",
      "Train Epoch: 2 [96000/209222 (46%)]\tAll Loss: 1.9722\tTriple Loss(0): 0.1576\tClassification Loss: 1.6571\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_1500.pth.tar\r\n",
      "Train Epoch: 2 [128000/209222 (61%)]\tAll Loss: 2.4372\tTriple Loss(1): 0.2173\tClassification Loss: 2.0026\r\n",
      "Train Epoch: 2 [160000/209222 (76%)]\tAll Loss: 2.3341\tTriple Loss(1): 0.1917\tClassification Loss: 1.9508\r\n",
      "Train Epoch: 2 [192000/209222 (92%)]\tAll Loss: 2.1387\tTriple Loss(1): 0.0902\tClassification Loss: 1.9584\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_2_final.pth.tar\r\n",
      "Train Epoch: 3 [0/209222 (0%)]\tAll Loss: 2.2551\tTriple Loss(0): 0.0000\tClassification Loss: 2.2551\r\n",
      "\r\n",
      "Test set: Average loss: 1.8277\r\n",
      "Top 1 Accuracy: 37386/80000 (47%)\r\n",
      "Top 3 Accuracy: 57287/80000 (72%)\r\n",
      "Top 5 Accuracy: 65663/80000 (82%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      "100%|| 14218/14218 [02:22<00:00, 99.46it/s]\r\n",
      "n = 1. Accuracy = 22.79%.\r\n",
      "n = 10. Accuracy = 46.01%.\r\n",
      "n = 20. Accuracy = 53.53%.\r\n",
      "n = 50. Accuracy = 63.70%.\r\n",
      "Train Epoch: 3 [32000/209222 (15%)]\tAll Loss: 1.9819\tTriple Loss(1): 0.0855\tClassification Loss: 1.8109\r\n",
      "Train Epoch: 3 [64000/209222 (31%)]\tAll Loss: 2.1006\tTriple Loss(1): 0.1930\tClassification Loss: 1.7145\r\n",
      "Train Epoch: 3 [96000/209222 (46%)]\tAll Loss: 2.2459\tTriple Loss(1): 0.2556\tClassification Loss: 1.7347\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_1500.pth.tar\r\n",
      "Train Epoch: 3 [128000/209222 (61%)]\tAll Loss: 2.3768\tTriple Loss(1): 0.2425\tClassification Loss: 1.8919\r\n",
      "Train Epoch: 3 [160000/209222 (76%)]\tAll Loss: 1.9545\tTriple Loss(1): 0.0970\tClassification Loss: 1.7604\r\n",
      "Train Epoch: 3 [192000/209222 (92%)]\tAll Loss: 3.0874\tTriple Loss(1): 0.3705\tClassification Loss: 2.3463\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_3000.pth.tar\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/model_3_final.pth.tar\r\n",
      "Train Epoch: 4 [0/209222 (0%)]\tAll Loss: 2.9866\tTriple Loss(1): 0.3168\tClassification Loss: 2.3530\r\n",
      "\r\n",
      "Test set: Average loss: 1.8471\r\n",
      "Top 1 Accuracy: 38723/80000 (48%)\r\n",
      "Top 3 Accuracy: 56982/80000 (71%)\r\n",
      "Top 5 Accuracy: 65303/80000 (82%)\r\n",
      "\r\n",
      "Testing in-shop retrieval\r\n",
      " 25%|                           | 3619/14218 [00:35<01:44, 101.09it/s]"
     ]
    }
   ],
   "source": [
    "# Continuing model_1_final.pth.tar, which is trained for 10+ epochs.\n",
    "# FREEZE = False. LR=0.001. in-shop=True. 30 epochs\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/11_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/11_epochs\r\n",
      "start_epoch: 11\r\n",
      "Train Epoch: 11 [0/209222 (0%)]\tAll Loss: 1.9825\tTriple Loss(1): 0.1978\tClassification Loss: 1.5868\r\n",
      "train.py:211: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.5684\r\n",
      "Top 1 Accuracy: 44388/80000 (55%)\r\n",
      "Top 3 Accuracy: 62162/80000 (78%)\r\n",
      "Top 5 Accuracy: 69201/80000 (87%)\r\n",
      " \r\n",
      "Train Epoch: 11 [32000/209222 (15%)]\tAll Loss: 1.7409\tTriple Loss(1): 0.1478\tClassification Loss: 1.4452\r\n",
      "Train Epoch: 11 [64000/209222 (31%)]\tAll Loss: 1.6496\tTriple Loss(1): 0.0472\tClassification Loss: 1.5553\r\n",
      "Train Epoch: 11 [96000/209222 (46%)]\tAll Loss: 2.0770\tTriple Loss(1): 0.0854\tClassification Loss: 1.9063\r\n",
      "Train Epoch: 11 [128000/209222 (61%)]\tAll Loss: 3.2441\tTriple Loss(0): 0.8261\tClassification Loss: 1.5919\r\n",
      "Train Epoch: 11 [160000/209222 (76%)]\tAll Loss: 2.2276\tTriple Loss(1): 0.0534\tClassification Loss: 2.1209\r\n",
      "Train Epoch: 11 [192000/209222 (92%)]\tAll Loss: 2.0469\tTriple Loss(1): 0.0638\tClassification Loss: 1.9193\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/11_epochs\r\n",
      "Train Epoch: 12 [0/209222 (0%)]\tAll Loss: 2.5828\tTriple Loss(1): 0.4799\tClassification Loss: 1.6231\r\n",
      "\r\n",
      "Test set: Average loss: 1.6341\r\n",
      "Top 1 Accuracy: 42322/80000 (53%)\r\n",
      "Top 3 Accuracy: 60813/80000 (76%)\r\n",
      "Top 5 Accuracy: 68279/80000 (85%)\r\n",
      " \r\n",
      "Train Epoch: 12 [32000/209222 (15%)]\tAll Loss: 1.5871\tTriple Loss(0): 0.0000\tClassification Loss: 1.5871\r\n",
      "Train Epoch: 12 [64000/209222 (31%)]\tAll Loss: 2.1089\tTriple Loss(1): 0.1622\tClassification Loss: 1.7845\r\n",
      "Train Epoch: 12 [96000/209222 (46%)]\tAll Loss: 2.0889\tTriple Loss(1): 0.0833\tClassification Loss: 1.9223\r\n",
      "Train Epoch: 12 [128000/209222 (61%)]\tAll Loss: 1.7401\tTriple Loss(1): 0.0259\tClassification Loss: 1.6882\r\n",
      "Train Epoch: 12 [160000/209222 (76%)]\tAll Loss: 1.9743\tTriple Loss(1): 0.0712\tClassification Loss: 1.8319\r\n",
      "Train Epoch: 12 [192000/209222 (92%)]\tAll Loss: 7.4192\tTriple Loss(0): 2.7661\tClassification Loss: 1.8869\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/12_epochs\r\n",
      "Train Epoch: 13 [0/209222 (0%)]\tAll Loss: 2.0225\tTriple Loss(1): 0.1895\tClassification Loss: 1.6435\r\n",
      "\r\n",
      "Test set: Average loss: 1.5708\r\n",
      "Top 1 Accuracy: 43970/80000 (55%)\r\n",
      "Top 3 Accuracy: 62321/80000 (78%)\r\n",
      "Top 5 Accuracy: 69408/80000 (87%)\r\n",
      " \r\n",
      "Train Epoch: 13 [32000/209222 (15%)]\tAll Loss: 2.0393\tTriple Loss(1): 0.1992\tClassification Loss: 1.6409\r\n",
      "Train Epoch: 13 [64000/209222 (31%)]\tAll Loss: 1.5812\tTriple Loss(1): 0.0322\tClassification Loss: 1.5169\r\n",
      "Train Epoch: 13 [96000/209222 (46%)]\tAll Loss: 2.3454\tTriple Loss(1): 0.1642\tClassification Loss: 2.0170\r\n",
      "Train Epoch: 13 [128000/209222 (61%)]\tAll Loss: 1.8618\tTriple Loss(1): 0.1532\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 13 [160000/209222 (76%)]\tAll Loss: 1.8682\tTriple Loss(1): 0.0301\tClassification Loss: 1.8080\r\n",
      "Train Epoch: 13 [192000/209222 (92%)]\tAll Loss: 1.7503\tTriple Loss(0): 0.0000\tClassification Loss: 1.7503\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/13_epochs\r\n",
      "Train Epoch: 14 [0/209222 (0%)]\tAll Loss: 2.1362\tTriple Loss(1): 0.1981\tClassification Loss: 1.7399\r\n",
      "\r\n",
      "Test set: Average loss: 1.4572\r\n",
      "Top 1 Accuracy: 46363/80000 (58%)\r\n",
      "Top 3 Accuracy: 64005/80000 (80%)\r\n",
      "Top 5 Accuracy: 70480/80000 (88%)\r\n",
      " \r\n",
      "Train Epoch: 14 [32000/209222 (15%)]\tAll Loss: 1.7759\tTriple Loss(1): 0.1194\tClassification Loss: 1.5371\r\n",
      "Train Epoch: 14 [64000/209222 (31%)]\tAll Loss: 1.6924\tTriple Loss(0): 0.0000\tClassification Loss: 1.6924\r\n",
      "Train Epoch: 14 [96000/209222 (46%)]\tAll Loss: 2.1997\tTriple Loss(1): 0.1482\tClassification Loss: 1.9032\r\n",
      "Train Epoch: 14 [128000/209222 (61%)]\tAll Loss: 3.4103\tTriple Loss(1): 0.4506\tClassification Loss: 2.5091\r\n",
      "Train Epoch: 14 [160000/209222 (76%)]\tAll Loss: 2.2957\tTriple Loss(1): 0.1729\tClassification Loss: 1.9499\r\n",
      "Train Epoch: 14 [192000/209222 (92%)]\tAll Loss: 1.9018\tTriple Loss(1): 0.0862\tClassification Loss: 1.7294\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/14_epochs\r\n",
      "Train Epoch: 15 [0/209222 (0%)]\tAll Loss: 2.5020\tTriple Loss(1): 0.4037\tClassification Loss: 1.6946\r\n",
      "\r\n",
      "Test set: Average loss: 1.4505\r\n",
      "Top 1 Accuracy: 46769/80000 (58%)\r\n",
      "Top 3 Accuracy: 63894/80000 (80%)\r\n",
      "Top 5 Accuracy: 70497/80000 (88%)\r\n",
      " \r\n",
      "Train Epoch: 15 [32000/209222 (15%)]\tAll Loss: 1.4273\tTriple Loss(0): 0.0000\tClassification Loss: 1.4273\r\n",
      "Train Epoch: 15 [64000/209222 (31%)]\tAll Loss: 1.8094\tTriple Loss(1): 0.0465\tClassification Loss: 1.7164\r\n",
      "Train Epoch: 15 [96000/209222 (46%)]\tAll Loss: 2.2060\tTriple Loss(1): 0.1989\tClassification Loss: 1.8081\r\n",
      "Train Epoch: 15 [128000/209222 (61%)]\tAll Loss: 1.8232\tTriple Loss(1): 0.1164\tClassification Loss: 1.5904\r\n",
      "Train Epoch: 15 [160000/209222 (76%)]\tAll Loss: 1.6860\tTriple Loss(0): 0.0000\tClassification Loss: 1.6860\r\n",
      "Train Epoch: 15 [192000/209222 (92%)]\tAll Loss: 1.7485\tTriple Loss(1): 0.0000\tClassification Loss: 1.7485\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/15_epochs\r\n",
      "Train Epoch: 16 [0/209222 (0%)]\tAll Loss: 1.7352\tTriple Loss(0): 0.0000\tClassification Loss: 1.7352\r\n",
      "\r\n",
      "Test set: Average loss: 1.5067\r\n",
      "Top 1 Accuracy: 45829/80000 (57%)\r\n",
      "Top 3 Accuracy: 63487/80000 (79%)\r\n",
      "Top 5 Accuracy: 70261/80000 (88%)\r\n",
      " \r\n",
      "Train Epoch: 16 [32000/209222 (15%)]\tAll Loss: 1.6277\tTriple Loss(1): 0.0822\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 16 [64000/209222 (31%)]\tAll Loss: 1.8376\tTriple Loss(1): 0.0636\tClassification Loss: 1.7103\r\n",
      "Train Epoch: 16 [96000/209222 (46%)]\tAll Loss: 1.9824\tTriple Loss(1): 0.0391\tClassification Loss: 1.9042\r\n",
      "Train Epoch: 16 [128000/209222 (61%)]\tAll Loss: 1.5033\tTriple Loss(1): 0.0397\tClassification Loss: 1.4239\r\n",
      "Train Epoch: 16 [160000/209222 (76%)]\tAll Loss: 1.9752\tTriple Loss(1): 0.0312\tClassification Loss: 1.9127\r\n",
      "Train Epoch: 16 [192000/209222 (92%)]\tAll Loss: 1.6281\tTriple Loss(0): 0.0000\tClassification Loss: 1.6281\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/16_epochs\r\n",
      "Train Epoch: 17 [0/209222 (0%)]\tAll Loss: 2.5235\tTriple Loss(1): 0.5166\tClassification Loss: 1.4902\r\n",
      "\r\n",
      "Test set: Average loss: 1.4225\r\n",
      "Top 1 Accuracy: 47344/80000 (59%)\r\n",
      "Top 3 Accuracy: 64593/80000 (81%)\r\n",
      "Top 5 Accuracy: 70958/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 17 [32000/209222 (15%)]\tAll Loss: 1.4384\tTriple Loss(1): 0.0819\tClassification Loss: 1.2747\r\n",
      "Train Epoch: 17 [64000/209222 (31%)]\tAll Loss: 1.6601\tTriple Loss(1): 0.0089\tClassification Loss: 1.6422\r\n",
      "Train Epoch: 17 [96000/209222 (46%)]\tAll Loss: 1.9003\tTriple Loss(1): 0.0734\tClassification Loss: 1.7534\r\n",
      "Train Epoch: 17 [128000/209222 (61%)]\tAll Loss: 1.7292\tTriple Loss(1): 0.0813\tClassification Loss: 1.5667\r\n",
      "Train Epoch: 17 [160000/209222 (76%)]\tAll Loss: 2.0210\tTriple Loss(1): 0.0641\tClassification Loss: 1.8929\r\n",
      "Train Epoch: 17 [192000/209222 (92%)]\tAll Loss: 3.0409\tTriple Loss(0): 0.6585\tClassification Loss: 1.7239\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/17_epochs\r\n",
      "Train Epoch: 18 [0/209222 (0%)]\tAll Loss: 2.3724\tTriple Loss(1): 0.3807\tClassification Loss: 1.6110\r\n",
      "\r\n",
      "Test set: Average loss: 1.4283\r\n",
      "Top 1 Accuracy: 47167/80000 (59%)\r\n",
      "Top 3 Accuracy: 64451/80000 (81%)\r\n",
      "Top 5 Accuracy: 71003/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 18 [32000/209222 (15%)]\tAll Loss: 1.4481\tTriple Loss(1): 0.0682\tClassification Loss: 1.3117\r\n",
      "Train Epoch: 18 [64000/209222 (31%)]\tAll Loss: 1.4975\tTriple Loss(0): 0.0000\tClassification Loss: 1.4975\r\n",
      "Train Epoch: 18 [96000/209222 (46%)]\tAll Loss: 1.9644\tTriple Loss(1): 0.0447\tClassification Loss: 1.8750\r\n",
      "Train Epoch: 18 [128000/209222 (61%)]\tAll Loss: 1.6314\tTriple Loss(1): 0.0310\tClassification Loss: 1.5695\r\n",
      "Train Epoch: 18 [160000/209222 (76%)]\tAll Loss: 1.8234\tTriple Loss(1): 0.0367\tClassification Loss: 1.7500\r\n",
      "Train Epoch: 18 [192000/209222 (92%)]\tAll Loss: 1.7534\tTriple Loss(0): 0.0000\tClassification Loss: 1.7534\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/18_epochs\r\n",
      "Train Epoch: 19 [0/209222 (0%)]\tAll Loss: 2.5086\tTriple Loss(1): 0.3875\tClassification Loss: 1.7336\r\n",
      "\r\n",
      "Test set: Average loss: 1.4006\r\n",
      "Top 1 Accuracy: 47861/80000 (60%)\r\n",
      "Top 3 Accuracy: 64872/80000 (81%)\r\n",
      "Top 5 Accuracy: 71136/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 19 [32000/209222 (15%)]\tAll Loss: 1.4543\tTriple Loss(1): 0.0526\tClassification Loss: 1.3491\r\n",
      "Train Epoch: 19 [64000/209222 (31%)]\tAll Loss: 1.6963\tTriple Loss(1): 0.1109\tClassification Loss: 1.4745\r\n",
      "Train Epoch: 19 [96000/209222 (46%)]\tAll Loss: 1.7528\tTriple Loss(1): 0.0196\tClassification Loss: 1.7135\r\n",
      "Train Epoch: 19 [128000/209222 (61%)]\tAll Loss: 1.6705\tTriple Loss(1): 0.0409\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 19 [160000/209222 (76%)]\tAll Loss: 1.7634\tTriple Loss(1): 0.0447\tClassification Loss: 1.6740\r\n",
      "Train Epoch: 19 [192000/209222 (92%)]\tAll Loss: 1.5846\tTriple Loss(1): 0.0380\tClassification Loss: 1.5086\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/19_epochs\r\n",
      "Train Epoch: 20 [0/209222 (0%)]\tAll Loss: 2.2465\tTriple Loss(1): 0.2811\tClassification Loss: 1.6843\r\n",
      "\r\n",
      "Test set: Average loss: 1.4298\r\n",
      "Top 1 Accuracy: 46994/80000 (59%)\r\n",
      "Top 3 Accuracy: 64693/80000 (81%)\r\n",
      "Top 5 Accuracy: 71045/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 20 [32000/209222 (15%)]\tAll Loss: 1.4383\tTriple Loss(1): 0.0576\tClassification Loss: 1.3230\r\n",
      "Train Epoch: 20 [64000/209222 (31%)]\tAll Loss: 1.6447\tTriple Loss(1): 0.0855\tClassification Loss: 1.4737\r\n",
      "Train Epoch: 20 [96000/209222 (46%)]\tAll Loss: 1.9280\tTriple Loss(1): 0.0566\tClassification Loss: 1.8148\r\n",
      "Train Epoch: 20 [128000/209222 (61%)]\tAll Loss: 1.4013\tTriple Loss(0): 0.0000\tClassification Loss: 1.4013\r\n",
      "Train Epoch: 20 [160000/209222 (76%)]\tAll Loss: 1.7028\tTriple Loss(1): 0.0018\tClassification Loss: 1.6992\r\n",
      "Train Epoch: 20 [192000/209222 (92%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.0605\tClassification Loss: 1.7327\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/20_epochs\r\n",
      "Train Epoch: 21 [0/209222 (0%)]\tAll Loss: 1.6257\tTriple Loss(0): 0.0000\tClassification Loss: 1.6257\r\n",
      "\r\n",
      "Test set: Average loss: 1.3536\r\n",
      "Top 1 Accuracy: 48735/80000 (61%)\r\n",
      "Top 3 Accuracy: 65814/80000 (82%)\r\n",
      "Top 5 Accuracy: 71804/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 21 [32000/209222 (15%)]\tAll Loss: 1.4744\tTriple Loss(1): 0.0359\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 21 [64000/209222 (31%)]\tAll Loss: 1.5124\tTriple Loss(1): 0.0000\tClassification Loss: 1.5124\r\n",
      "Train Epoch: 21 [96000/209222 (46%)]\tAll Loss: 1.6106\tTriple Loss(0): 0.0000\tClassification Loss: 1.6106\r\n",
      "Train Epoch: 21 [128000/209222 (61%)]\tAll Loss: 1.4774\tTriple Loss(1): 0.0716\tClassification Loss: 1.3343\r\n",
      "Train Epoch: 21 [160000/209222 (76%)]\tAll Loss: 1.7422\tTriple Loss(0): 0.0000\tClassification Loss: 1.7422\r\n",
      "Train Epoch: 21 [192000/209222 (92%)]\tAll Loss: 1.7055\tTriple Loss(1): 0.0281\tClassification Loss: 1.6494\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/21_epochs\r\n",
      "Train Epoch: 22 [0/209222 (0%)]\tAll Loss: 1.6546\tTriple Loss(0): 0.0000\tClassification Loss: 1.6546\r\n",
      "\r\n",
      "Test set: Average loss: 1.7936\r\n",
      "Top 1 Accuracy: 40934/80000 (51%)\r\n",
      "Top 3 Accuracy: 57945/80000 (72%)\r\n",
      "Top 5 Accuracy: 65883/80000 (82%)\r\n",
      " \r\n",
      "Train Epoch: 22 [32000/209222 (15%)]\tAll Loss: 1.2523\tTriple Loss(0): 0.0000\tClassification Loss: 1.2523\r\n",
      "Train Epoch: 22 [64000/209222 (31%)]\tAll Loss: 1.9774\tTriple Loss(0): 0.2041\tClassification Loss: 1.5691\r\n",
      "Train Epoch: 22 [96000/209222 (46%)]\tAll Loss: 1.9325\tTriple Loss(1): 0.1074\tClassification Loss: 1.7176\r\n",
      "Train Epoch: 22 [128000/209222 (61%)]\tAll Loss: 1.6778\tTriple Loss(1): 0.1412\tClassification Loss: 1.3953\r\n",
      "Train Epoch: 22 [160000/209222 (76%)]\tAll Loss: 1.9088\tTriple Loss(1): 0.0852\tClassification Loss: 1.7385\r\n",
      "Train Epoch: 22 [192000/209222 (92%)]\tAll Loss: 1.7768\tTriple Loss(1): 0.0949\tClassification Loss: 1.5871\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/22_epochs\r\n",
      "Train Epoch: 23 [0/209222 (0%)]\tAll Loss: 1.8723\tTriple Loss(1): 0.1439\tClassification Loss: 1.5845\r\n",
      "\r\n",
      "Test set: Average loss: 1.3904\r\n",
      "Top 1 Accuracy: 47514/80000 (59%)\r\n",
      "Top 3 Accuracy: 64663/80000 (81%)\r\n",
      "Top 5 Accuracy: 71093/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 23 [32000/209222 (15%)]\tAll Loss: 1.1479\tTriple Loss(1): 0.0042\tClassification Loss: 1.1394\r\n",
      "Train Epoch: 23 [64000/209222 (31%)]\tAll Loss: 1.7254\tTriple Loss(1): 0.0929\tClassification Loss: 1.5396\r\n",
      "Train Epoch: 23 [96000/209222 (46%)]\tAll Loss: 1.8694\tTriple Loss(1): 0.0875\tClassification Loss: 1.6943\r\n",
      "Train Epoch: 23 [128000/209222 (61%)]\tAll Loss: 1.4735\tTriple Loss(1): 0.0196\tClassification Loss: 1.4344\r\n",
      "Train Epoch: 23 [160000/209222 (76%)]\tAll Loss: 1.9167\tTriple Loss(1): 0.0350\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 23 [192000/209222 (92%)]\tAll Loss: 1.7338\tTriple Loss(1): 0.0111\tClassification Loss: 1.7115\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/23_epochs\r\n",
      "Train Epoch: 24 [0/209222 (0%)]\tAll Loss: 2.0700\tTriple Loss(1): 0.2788\tClassification Loss: 1.5124\r\n",
      "\r\n",
      "Test set: Average loss: 1.3003\r\n",
      "Top 1 Accuracy: 50013/80000 (63%)\r\n",
      "Top 3 Accuracy: 66677/80000 (83%)\r\n",
      "Top 5 Accuracy: 72392/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 24 [32000/209222 (15%)]\tAll Loss: 1.3412\tTriple Loss(1): 0.0361\tClassification Loss: 1.2691\r\n",
      "Train Epoch: 24 [64000/209222 (31%)]\tAll Loss: 1.5444\tTriple Loss(0): 0.0000\tClassification Loss: 1.5444\r\n",
      "Train Epoch: 24 [96000/209222 (46%)]\tAll Loss: 1.9712\tTriple Loss(1): 0.1087\tClassification Loss: 1.7538\r\n",
      "Train Epoch: 24 [128000/209222 (61%)]\tAll Loss: 1.4051\tTriple Loss(0): 0.0000\tClassification Loss: 1.4051\r\n",
      "Train Epoch: 24 [160000/209222 (76%)]\tAll Loss: 1.7177\tTriple Loss(1): 0.0886\tClassification Loss: 1.5406\r\n",
      "Train Epoch: 24 [192000/209222 (92%)]\tAll Loss: 4.3862\tTriple Loss(0): 1.4267\tClassification Loss: 1.5328\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/24_epochs\r\n",
      "Train Epoch: 25 [0/209222 (0%)]\tAll Loss: 2.1497\tTriple Loss(1): 0.2569\tClassification Loss: 1.6360\r\n",
      "\r\n",
      "Test set: Average loss: 1.2812\r\n",
      "Top 1 Accuracy: 50193/80000 (63%)\r\n",
      "Top 3 Accuracy: 66912/80000 (84%)\r\n",
      "Top 5 Accuracy: 72507/80000 (91%)\r\n",
      " \r\n",
      "Train Epoch: 25 [32000/209222 (15%)]\tAll Loss: 1.0959\tTriple Loss(0): 0.0000\tClassification Loss: 1.0959\r\n",
      "Train Epoch: 25 [64000/209222 (31%)]\tAll Loss: 1.5261\tTriple Loss(1): 0.0239\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 25 [96000/209222 (46%)]\tAll Loss: 1.6870\tTriple Loss(1): 0.0068\tClassification Loss: 1.6733\r\n",
      "Train Epoch: 25 [128000/209222 (61%)]\tAll Loss: 1.7412\tTriple Loss(1): 0.0733\tClassification Loss: 1.5946\r\n",
      "Train Epoch: 25 [160000/209222 (76%)]\tAll Loss: 1.8310\tTriple Loss(0): 0.0000\tClassification Loss: 1.8310\r\n",
      "Train Epoch: 25 [192000/209222 (92%)]\tAll Loss: 1.8355\tTriple Loss(1): 0.0924\tClassification Loss: 1.6508\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/25_epochs\r\n",
      "Train Epoch: 26 [0/209222 (0%)]\tAll Loss: 7.1886\tTriple Loss(0): 2.6894\tClassification Loss: 1.8099\r\n",
      "\r\n",
      "Test set: Average loss: 1.5254\r\n",
      "Top 1 Accuracy: 44866/80000 (56%)\r\n",
      "Top 3 Accuracy: 62580/80000 (78%)\r\n",
      "Top 5 Accuracy: 69362/80000 (87%)\r\n",
      " \r\n",
      "Train Epoch: 26 [32000/209222 (15%)]\tAll Loss: 1.3380\tTriple Loss(1): 0.0163\tClassification Loss: 1.3053\r\n",
      "Train Epoch: 26 [64000/209222 (31%)]\tAll Loss: 1.6408\tTriple Loss(0): 0.0000\tClassification Loss: 1.6408\r\n",
      "Train Epoch: 26 [96000/209222 (46%)]\tAll Loss: 1.6642\tTriple Loss(1): 0.0242\tClassification Loss: 1.6158\r\n",
      "Train Epoch: 26 [128000/209222 (61%)]\tAll Loss: 1.5501\tTriple Loss(1): 0.0178\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 26 [160000/209222 (76%)]\tAll Loss: 1.6068\tTriple Loss(0): 0.0000\tClassification Loss: 1.6068\r\n",
      "Train Epoch: 26 [192000/209222 (92%)]\tAll Loss: 1.6974\tTriple Loss(1): 0.0206\tClassification Loss: 1.6561\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/26_epochs\r\n",
      "Train Epoch: 27 [0/209222 (0%)]\tAll Loss: 1.5481\tTriple Loss(0): 0.0000\tClassification Loss: 1.5481\r\n",
      "\r\n",
      "Test set: Average loss: 1.2997\r\n",
      "Top 1 Accuracy: 49851/80000 (62%)\r\n",
      "Top 3 Accuracy: 66599/80000 (83%)\r\n",
      "Top 5 Accuracy: 72348/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 27 [32000/209222 (15%)]\tAll Loss: 1.6282\tTriple Loss(1): 0.2078\tClassification Loss: 1.2125\r\n",
      "Train Epoch: 27 [64000/209222 (31%)]\tAll Loss: 1.5413\tTriple Loss(1): 0.0440\tClassification Loss: 1.4532\r\n",
      "Train Epoch: 27 [96000/209222 (46%)]\tAll Loss: 1.7877\tTriple Loss(1): 0.0464\tClassification Loss: 1.6949\r\n",
      "Train Epoch: 27 [128000/209222 (61%)]\tAll Loss: 1.6147\tTriple Loss(1): 0.1025\tClassification Loss: 1.4098\r\n",
      "Train Epoch: 27 [160000/209222 (76%)]\tAll Loss: 1.7927\tTriple Loss(1): 0.1370\tClassification Loss: 1.5188\r\n",
      "Train Epoch: 27 [192000/209222 (92%)]\tAll Loss: 1.5316\tTriple Loss(0): 0.0000\tClassification Loss: 1.5316\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/27_epochs\r\n",
      "Train Epoch: 28 [0/209222 (0%)]\tAll Loss: 1.9278\tTriple Loss(1): 0.2067\tClassification Loss: 1.5144\r\n",
      "\r\n",
      "Test set: Average loss: 1.3041\r\n",
      "Top 1 Accuracy: 49574/80000 (62%)\r\n",
      "Top 3 Accuracy: 66468/80000 (83%)\r\n",
      "Top 5 Accuracy: 72317/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 28 [32000/209222 (15%)]\tAll Loss: 1.3260\tTriple Loss(1): 0.0497\tClassification Loss: 1.2266\r\n",
      "Train Epoch: 28 [64000/209222 (31%)]\tAll Loss: 1.4815\tTriple Loss(0): 0.0000\tClassification Loss: 1.4815\r\n",
      "Train Epoch: 28 [96000/209222 (46%)]\tAll Loss: 1.4764\tTriple Loss(0): 0.0000\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 28 [128000/209222 (61%)]\tAll Loss: 1.6899\tTriple Loss(1): 0.1273\tClassification Loss: 1.4354\r\n",
      "Train Epoch: 28 [160000/209222 (76%)]\tAll Loss: 1.5621\tTriple Loss(1): 0.0116\tClassification Loss: 1.5389\r\n",
      "Train Epoch: 28 [192000/209222 (92%)]\tAll Loss: 1.8435\tTriple Loss(1): 0.0420\tClassification Loss: 1.7596\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/28_epochs\r\n",
      "Train Epoch: 29 [0/209222 (0%)]\tAll Loss: 1.4295\tTriple Loss(0): 0.0000\tClassification Loss: 1.4295\r\n",
      "\r\n",
      "Test set: Average loss: 1.2937\r\n",
      "Top 1 Accuracy: 49979/80000 (62%)\r\n",
      "Top 3 Accuracy: 66603/80000 (83%)\r\n",
      "Top 5 Accuracy: 72370/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 29 [32000/209222 (15%)]\tAll Loss: 1.4815\tTriple Loss(1): 0.0485\tClassification Loss: 1.3844\r\n",
      "Train Epoch: 29 [64000/209222 (31%)]\tAll Loss: 1.5651\tTriple Loss(1): 0.0294\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 29 [96000/209222 (46%)]\tAll Loss: 1.9207\tTriple Loss(0): 0.0000\tClassification Loss: 1.9207\r\n",
      "Train Epoch: 29 [128000/209222 (61%)]\tAll Loss: 1.3447\tTriple Loss(0): 0.0000\tClassification Loss: 1.3447\r\n",
      "Train Epoch: 29 [160000/209222 (76%)]\tAll Loss: 1.6548\tTriple Loss(1): 0.0273\tClassification Loss: 1.6003\r\n",
      "Train Epoch: 29 [192000/209222 (92%)]\tAll Loss: 1.5000\tTriple Loss(0): 0.0000\tClassification Loss: 1.5000\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/29_epochs\r\n",
      "Train Epoch: 30 [0/209222 (0%)]\tAll Loss: 2.1086\tTriple Loss(1): 0.2358\tClassification Loss: 1.6371\r\n",
      "\r\n",
      "Test set: Average loss: 1.2710\r\n",
      "Top 1 Accuracy: 50515/80000 (63%)\r\n",
      "Top 3 Accuracy: 67118/80000 (84%)\r\n",
      "Top 5 Accuracy: 72726/80000 (91%)\r\n",
      " \r\n",
      "Train Epoch: 30 [32000/209222 (15%)]\tAll Loss: 1.4715\tTriple Loss(1): 0.1115\tClassification Loss: 1.2485\r\n",
      "Train Epoch: 30 [64000/209222 (31%)]\tAll Loss: 1.5765\tTriple Loss(1): 0.0686\tClassification Loss: 1.4393\r\n",
      "Train Epoch: 30 [96000/209222 (46%)]\tAll Loss: 1.6756\tTriple Loss(0): 0.0000\tClassification Loss: 1.6756\r\n",
      "Train Epoch: 30 [128000/209222 (61%)]\tAll Loss: 1.6649\tTriple Loss(1): 0.1467\tClassification Loss: 1.3716\r\n",
      "Train Epoch: 30 [160000/209222 (76%)]\tAll Loss: 1.6042\tTriple Loss(1): 0.0397\tClassification Loss: 1.5247\r\n",
      "Train Epoch: 30 [192000/209222 (92%)]\tAll Loss: 1.9966\tTriple Loss(1): 0.1483\tClassification Loss: 1.7000\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/30_epochs\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing \"models/freeze=False/lr=0.001/11_epochs\"\n",
    "# FREEZE = False. LR=0.001. in-shop=True.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/30_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/30_epochs\r\n",
      "Train Epoch: 30 [0/209222 (0%)]\tAll Loss: 2.6941\tTriple Loss(1): 0.3309\tClassification Loss: 2.0323\r\n",
      "train.py:211: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.3078\r\n",
      "Top 1 Accuracy: 49601/80000 (62%)\r\n",
      "Top 3 Accuracy: 66543/80000 (83%)\r\n",
      "Top 5 Accuracy: 72373/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 30 [32000/209222 (15%)]\tAll Loss: 1.9181\tTriple Loss(1): 0.1279\tClassification Loss: 1.6623\r\n",
      "Train Epoch: 30 [64000/209222 (31%)]\tAll Loss: 2.1724\tTriple Loss(1): 0.0974\tClassification Loss: 1.9775\r\n",
      "Train Epoch: 30 [96000/209222 (46%)]\tAll Loss: 1.7544\tTriple Loss(1): 0.1008\tClassification Loss: 1.5529\r\n",
      "Train Epoch: 30 [128000/209222 (61%)]\tAll Loss: 1.7980\tTriple Loss(1): 0.0503\tClassification Loss: 1.6974\r\n",
      "Train Epoch: 30 [160000/209222 (76%)]\tAll Loss: 1.9695\tTriple Loss(1): 0.0478\tClassification Loss: 1.8738\r\n",
      "Train Epoch: 30 [192000/209222 (92%)]\tAll Loss: 1.8118\tTriple Loss(1): 0.0053\tClassification Loss: 1.8012\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/30_epochs\r\n",
      "Train Epoch: 31 [0/209222 (0%)]\tAll Loss: 2.4121\tTriple Loss(1): 0.3101\tClassification Loss: 1.7919\r\n",
      "\r\n",
      "Test set: Average loss: 1.3253\r\n",
      "Top 1 Accuracy: 49392/80000 (62%)\r\n",
      "Top 3 Accuracy: 66083/80000 (83%)\r\n",
      "Top 5 Accuracy: 71978/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 31 [32000/209222 (15%)]\tAll Loss: 1.4103\tTriple Loss(0): 0.0000\tClassification Loss: 1.4103\r\n",
      "Train Epoch: 31 [64000/209222 (31%)]\tAll Loss: 2.0936\tTriple Loss(1): 0.0764\tClassification Loss: 1.9407\r\n",
      "Train Epoch: 31 [96000/209222 (46%)]\tAll Loss: 1.5586\tTriple Loss(1): 0.1145\tClassification Loss: 1.3297\r\n",
      "Train Epoch: 31 [128000/209222 (61%)]\tAll Loss: 1.5351\tTriple Loss(1): 0.0807\tClassification Loss: 1.3738\r\n",
      "Train Epoch: 31 [160000/209222 (76%)]\tAll Loss: 1.8872\tTriple Loss(1): 0.0491\tClassification Loss: 1.7891\r\n",
      "Train Epoch: 31 [192000/209222 (92%)]\tAll Loss: 2.0250\tTriple Loss(1): 0.1465\tClassification Loss: 1.7321\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/31_epochs\r\n",
      "Train Epoch: 32 [0/209222 (0%)]\tAll Loss: 2.4566\tTriple Loss(1): 0.2200\tClassification Loss: 2.0165\r\n",
      "\r\n",
      "Test set: Average loss: 1.4565\r\n",
      "Top 1 Accuracy: 46328/80000 (58%)\r\n",
      "Top 3 Accuracy: 63723/80000 (80%)\r\n",
      "Top 5 Accuracy: 70313/80000 (88%)\r\n",
      " \r\n",
      "Train Epoch: 32 [32000/209222 (15%)]\tAll Loss: 1.6245\tTriple Loss(1): 0.0908\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 32 [64000/209222 (31%)]\tAll Loss: 1.7534\tTriple Loss(0): 0.0000\tClassification Loss: 1.7534\r\n",
      "Train Epoch: 32 [96000/209222 (46%)]\tAll Loss: 1.3518\tTriple Loss(1): 0.0683\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 32 [128000/209222 (61%)]\tAll Loss: 3.1919\tTriple Loss(0): 0.8905\tClassification Loss: 1.4110\r\n",
      "Train Epoch: 32 [160000/209222 (76%)]\tAll Loss: 1.8273\tTriple Loss(1): 0.0443\tClassification Loss: 1.7387\r\n",
      "Train Epoch: 32 [192000/209222 (92%)]\tAll Loss: 1.9723\tTriple Loss(1): 0.0303\tClassification Loss: 1.9116\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/32_epochs\r\n",
      "Train Epoch: 33 [0/209222 (0%)]\tAll Loss: 1.9818\tTriple Loss(1): 0.1218\tClassification Loss: 1.7383\r\n",
      "\r\n",
      "Test set: Average loss: 1.3886\r\n",
      "Top 1 Accuracy: 48223/80000 (60%)\r\n",
      "Top 3 Accuracy: 64773/80000 (81%)\r\n",
      "Top 5 Accuracy: 70915/80000 (89%)\r\n",
      " \r\n",
      "Train Epoch: 33 [32000/209222 (15%)]\tAll Loss: 1.4909\tTriple Loss(1): 0.0731\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 33 [64000/209222 (31%)]\tAll Loss: 1.8093\tTriple Loss(1): 0.0685\tClassification Loss: 1.6722\r\n",
      "Train Epoch: 33 [96000/209222 (46%)]\tAll Loss: 1.4731\tTriple Loss(1): 0.0453\tClassification Loss: 1.3825\r\n",
      "Train Epoch: 33 [128000/209222 (61%)]\tAll Loss: 1.7903\tTriple Loss(1): 0.1824\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 33 [160000/209222 (76%)]\tAll Loss: 1.7331\tTriple Loss(0): 0.0000\tClassification Loss: 1.7331\r\n",
      "Train Epoch: 33 [192000/209222 (92%)]\tAll Loss: 2.1235\tTriple Loss(1): 0.0839\tClassification Loss: 1.9558\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/33_epochs\r\n",
      "Train Epoch: 34 [0/209222 (0%)]\tAll Loss: 1.9937\tTriple Loss(0): 0.0000\tClassification Loss: 1.9937\r\n",
      "\r\n",
      "Test set: Average loss: 1.3396\r\n",
      "Top 1 Accuracy: 48963/80000 (61%)\r\n",
      "Top 3 Accuracy: 65876/80000 (82%)\r\n",
      "Top 5 Accuracy: 71858/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 34 [32000/209222 (15%)]\tAll Loss: 1.6978\tTriple Loss(0): 0.1823\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 34 [64000/209222 (31%)]\tAll Loss: 1.9933\tTriple Loss(1): 0.0683\tClassification Loss: 1.8568\r\n",
      "Train Epoch: 34 [96000/209222 (46%)]\tAll Loss: 1.5360\tTriple Loss(0): 0.0000\tClassification Loss: 1.5360\r\n",
      "Train Epoch: 34 [128000/209222 (61%)]\tAll Loss: 1.4894\tTriple Loss(0): 0.0000\tClassification Loss: 1.4894\r\n",
      "Train Epoch: 34 [160000/209222 (76%)]\tAll Loss: 1.8964\tTriple Loss(1): 0.0000\tClassification Loss: 1.8964\r\n",
      "Train Epoch: 34 [192000/209222 (92%)]\tAll Loss: 1.8835\tTriple Loss(0): 0.0000\tClassification Loss: 1.8835\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/34_epochs\r\n",
      "Train Epoch: 35 [0/209222 (0%)]\tAll Loss: 2.3856\tTriple Loss(1): 0.2452\tClassification Loss: 1.8952\r\n",
      "\r\n",
      "Test set: Average loss: 1.3285\r\n",
      "Top 1 Accuracy: 49442/80000 (62%)\r\n",
      "Top 3 Accuracy: 66087/80000 (83%)\r\n",
      "Top 5 Accuracy: 72014/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 35 [32000/209222 (15%)]\tAll Loss: 1.5216\tTriple Loss(1): 0.0940\tClassification Loss: 1.3335\r\n",
      "Train Epoch: 35 [64000/209222 (31%)]\tAll Loss: 1.9986\tTriple Loss(1): 0.0519\tClassification Loss: 1.8949\r\n",
      "Train Epoch: 35 [96000/209222 (46%)]\tAll Loss: 1.3708\tTriple Loss(1): 0.0054\tClassification Loss: 1.3600\r\n",
      "Train Epoch: 35 [128000/209222 (61%)]\tAll Loss: 1.4992\tTriple Loss(1): 0.0593\tClassification Loss: 1.3806\r\n",
      "Train Epoch: 35 [160000/209222 (76%)]\tAll Loss: 2.1249\tTriple Loss(1): 0.1636\tClassification Loss: 1.7977\r\n",
      "Train Epoch: 35 [192000/209222 (92%)]\tAll Loss: 2.9075\tTriple Loss(0): 0.5775\tClassification Loss: 1.7526\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/35_epochs\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing \"models/freeze=False/lr=0.001/30_epochs\"\n",
    "# FREEZE = False. LR=0.001. in-shop=True.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/35_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/35_epochs\r\n",
      "Train Epoch: 36 [0/209222 (0%)]\tAll Loss: 2.7008\tTriple Loss(1): 0.4123\tClassification Loss: 1.8762\r\n",
      "train.py:211: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.3492\r\n",
      "Top 1 Accuracy: 48885/80000 (61%)\r\n",
      "Top 3 Accuracy: 65530/80000 (82%)\r\n",
      "Top 5 Accuracy: 71684/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 36 [32000/209222 (15%)]\tAll Loss: 2.0340\tTriple Loss(1): 0.0123\tClassification Loss: 2.0094\r\n",
      "Train Epoch: 36 [64000/209222 (31%)]\tAll Loss: 1.2264\tTriple Loss(0): 0.0000\tClassification Loss: 1.2264\r\n",
      "Train Epoch: 36 [96000/209222 (46%)]\tAll Loss: 1.6258\tTriple Loss(1): 0.0920\tClassification Loss: 1.4417\r\n",
      "Train Epoch: 36 [128000/209222 (61%)]\tAll Loss: 2.0134\tTriple Loss(1): 0.1649\tClassification Loss: 1.6836\r\n",
      "Train Epoch: 36 [160000/209222 (76%)]\tAll Loss: 1.6589\tTriple Loss(0): 0.0000\tClassification Loss: 1.6589\r\n",
      "Train Epoch: 36 [192000/209222 (92%)]\tAll Loss: 1.7822\tTriple Loss(1): 0.0701\tClassification Loss: 1.6420\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/36_epochs\r\n",
      "Train Epoch: 37 [0/209222 (0%)]\tAll Loss: 2.1207\tTriple Loss(1): 0.2610\tClassification Loss: 1.5986\r\n",
      "\r\n",
      "Test set: Average loss: 1.2983\r\n",
      "Top 1 Accuracy: 49855/80000 (62%)\r\n",
      "Top 3 Accuracy: 66437/80000 (83%)\r\n",
      "Top 5 Accuracy: 72260/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 37 [32000/209222 (15%)]\tAll Loss: 1.7590\tTriple Loss(1): 0.1068\tClassification Loss: 1.5453\r\n",
      "Train Epoch: 37 [64000/209222 (31%)]\tAll Loss: 1.2852\tTriple Loss(0): 0.0000\tClassification Loss: 1.2852\r\n",
      "Train Epoch: 37 [96000/209222 (46%)]\tAll Loss: 1.3542\tTriple Loss(1): 0.0414\tClassification Loss: 1.2715\r\n",
      "Train Epoch: 37 [128000/209222 (61%)]\tAll Loss: 1.5621\tTriple Loss(0): 0.0000\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 37 [160000/209222 (76%)]\tAll Loss: 1.7482\tTriple Loss(1): 0.0335\tClassification Loss: 1.6811\r\n",
      "Train Epoch: 37 [192000/209222 (92%)]\tAll Loss: 17.3824\tTriple Loss(0): 7.7851\tClassification Loss: 1.8122\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/37_epochs\r\n",
      "Train Epoch: 38 [0/209222 (0%)]\tAll Loss: 2.0768\tTriple Loss(1): 0.1873\tClassification Loss: 1.7023\r\n",
      "\r\n",
      "Test set: Average loss: 1.3189\r\n",
      "Top 1 Accuracy: 49730/80000 (62%)\r\n",
      "Top 3 Accuracy: 66210/80000 (83%)\r\n",
      "Top 5 Accuracy: 72021/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 38 [32000/209222 (15%)]\tAll Loss: 2.0088\tTriple Loss(1): 0.1682\tClassification Loss: 1.6724\r\n",
      "Train Epoch: 38 [64000/209222 (31%)]\tAll Loss: 1.3004\tTriple Loss(1): 0.0386\tClassification Loss: 1.2231\r\n",
      "Train Epoch: 38 [96000/209222 (46%)]\tAll Loss: 1.4203\tTriple Loss(1): 0.0726\tClassification Loss: 1.2752\r\n",
      "Train Epoch: 38 [128000/209222 (61%)]\tAll Loss: 1.7374\tTriple Loss(1): 0.0952\tClassification Loss: 1.5470\r\n",
      "Train Epoch: 38 [160000/209222 (76%)]\tAll Loss: 7.2645\tTriple Loss(0): 2.7245\tClassification Loss: 1.8154\r\n",
      "Train Epoch: 38 [192000/209222 (92%)]\tAll Loss: 2.1441\tTriple Loss(1): 0.1865\tClassification Loss: 1.7711\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/38_epochs\r\n",
      "Train Epoch: 39 [0/209222 (0%)]\tAll Loss: 9.1340\tTriple Loss(0): 3.6938\tClassification Loss: 1.7464\r\n",
      "\r\n",
      "Test set: Average loss: 1.2934\r\n",
      "Top 1 Accuracy: 50029/80000 (63%)\r\n",
      "Top 3 Accuracy: 66518/80000 (83%)\r\n",
      "Top 5 Accuracy: 72283/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 39 [32000/209222 (15%)]\tAll Loss: 1.5645\tTriple Loss(1): 0.0056\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 39 [64000/209222 (31%)]\tAll Loss: 1.4052\tTriple Loss(1): 0.0407\tClassification Loss: 1.3238\r\n",
      "Train Epoch: 39 [96000/209222 (46%)]\tAll Loss: 1.5595\tTriple Loss(1): 0.1294\tClassification Loss: 1.3006\r\n",
      "Train Epoch: 39 [128000/209222 (61%)]\tAll Loss: 1.5319\tTriple Loss(1): 0.0071\tClassification Loss: 1.5177\r\n",
      "Train Epoch: 39 [160000/209222 (76%)]\tAll Loss: 1.7899\tTriple Loss(0): 0.0000\tClassification Loss: 1.7899\r\n",
      "Train Epoch: 39 [192000/209222 (92%)]\tAll Loss: 1.8665\tTriple Loss(1): 0.0635\tClassification Loss: 1.7394\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/39_epochs\r\n",
      "Train Epoch: 40 [0/209222 (0%)]\tAll Loss: 2.2265\tTriple Loss(1): 0.3530\tClassification Loss: 1.5206\r\n",
      "\r\n",
      "Test set: Average loss: 1.3117\r\n",
      "Top 1 Accuracy: 49714/80000 (62%)\r\n",
      "Top 3 Accuracy: 66284/80000 (83%)\r\n",
      "Top 5 Accuracy: 72079/80000 (90%)\r\n",
      " \r\n",
      "Train Epoch: 40 [32000/209222 (15%)]\tAll Loss: 1.7171\tTriple Loss(1): 0.0337\tClassification Loss: 1.6497\r\n",
      "Train Epoch: 40 [64000/209222 (31%)]\tAll Loss: 1.2704\tTriple Loss(1): 0.0164\tClassification Loss: 1.2376\r\n",
      "Train Epoch: 40 [96000/209222 (46%)]\tAll Loss: 1.6456\tTriple Loss(1): 0.0939\tClassification Loss: 1.4578\r\n",
      "Train Epoch: 40 [128000/209222 (61%)]\tAll Loss: 1.5449\tTriple Loss(0): 0.0000\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 40 [160000/209222 (76%)]\tAll Loss: 1.8320\tTriple Loss(1): 0.0698\tClassification Loss: 1.6924\r\n",
      "Train Epoch: 40 [192000/209222 (92%)]\tAll Loss: 1.8431\tTriple Loss(1): 0.0400\tClassification Loss: 1.7631\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.001/40_epochs\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing \"models/freeze=False/lr=0.001/35_epochs\"\n",
    "# FREEZE = False. LR=0.001. in-shop=True.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "100%|| 14218/14218 [04:24<00:00, 53.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model freeze=False/lr=0.001/29_epochs\n",
      "Extracting features of in-shop test images.\n",
      "0 / 12612\n",
      "0 / 14218\n",
      "n = 1. Accuracy = 61.50%.\n",
      "n = 10. Accuracy = 87.33%.\n",
      "n = 20. Accuracy = 91.51%.\n",
      "n = 50. Accuracy = 95.20%.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{1: 61.499507666338445,\n 10: 87.32592488394992,\n 20: 91.510761007174,\n 50: 95.20326346884231}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing best model (29_epochs) on in-shop retrieval\n",
    "from in_shop_eval import eval\n",
    "eval()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209222 (0%)]\tAll Loss: 5.0796\tTriple Loss(1): 0.5479\tClassification Loss: 3.9838\r\n",
      "train.py:211: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9415\r\n",
      "Top 1 Accuracy: 549/80000 (1%)\r\n",
      "Top 3 Accuracy: 2017/80000 (3%)\r\n",
      "Top 5 Accuracy: 3580/80000 (4%)\r\n",
      " \r\n",
      "Train Epoch: 1 [32000/209222 (15%)]\tAll Loss: 4.5420\tTriple Loss(1): 0.7815\tClassification Loss: 2.9790\r\n",
      "Train Epoch: 1 [64000/209222 (31%)]\tAll Loss: 4.3550\tTriple Loss(1): 0.7060\tClassification Loss: 2.9430\r\n",
      "Train Epoch: 1 [96000/209222 (46%)]\tAll Loss: 4.0740\tTriple Loss(1): 0.6593\tClassification Loss: 2.7553\r\n",
      "Train Epoch: 1 [128000/209222 (61%)]\tAll Loss: 6.4166\tTriple Loss(0): 1.8121\tClassification Loss: 2.7924\r\n",
      "Train Epoch: 1 [160000/209222 (76%)]\tAll Loss: 4.4957\tTriple Loss(1): 0.8889\tClassification Loss: 2.7180\r\n",
      "Train Epoch: 1 [192000/209222 (92%)]\tAll Loss: 3.9002\tTriple Loss(1): 0.6539\tClassification Loss: 2.5924\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/1_epochs\r\n",
      "Train Epoch: 2 [0/209222 (0%)]\tAll Loss: 4.4130\tTriple Loss(1): 0.6332\tClassification Loss: 3.1465\r\n",
      "\r\n",
      "Test set: Average loss: 2.7041\r\n",
      "Top 1 Accuracy: 20025/80000 (25%)\r\n",
      "Top 3 Accuracy: 37059/80000 (46%)\r\n",
      "Top 5 Accuracy: 47001/80000 (59%)\r\n",
      " \r\n",
      "Train Epoch: 2 [32000/209222 (15%)]\tAll Loss: 4.8207\tTriple Loss(1): 0.9327\tClassification Loss: 2.9553\r\n",
      "Train Epoch: 2 [64000/209222 (31%)]\tAll Loss: 4.3064\tTriple Loss(1): 0.6685\tClassification Loss: 2.9695\r\n",
      "Train Epoch: 2 [96000/209222 (46%)]\tAll Loss: 2.7490\tTriple Loss(0): 0.0000\tClassification Loss: 2.7490\r\n",
      "Train Epoch: 2 [128000/209222 (61%)]\tAll Loss: 4.1369\tTriple Loss(1): 0.6999\tClassification Loss: 2.7370\r\n",
      "Train Epoch: 2 [160000/209222 (76%)]\tAll Loss: 3.6858\tTriple Loss(1): 0.5161\tClassification Loss: 2.6535\r\n",
      "Train Epoch: 2 [192000/209222 (92%)]\tAll Loss: 4.1885\tTriple Loss(1): 0.8172\tClassification Loss: 2.5541\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/2_epochs\r\n",
      "Train Epoch: 3 [0/209222 (0%)]\tAll Loss: 4.4271\tTriple Loss(0): 0.8358\tClassification Loss: 2.7554\r\n",
      "\r\n",
      "Test set: Average loss: 2.6745\r\n",
      "Top 1 Accuracy: 20020/80000 (25%)\r\n",
      "Top 3 Accuracy: 37323/80000 (47%)\r\n",
      "Top 5 Accuracy: 48740/80000 (61%)\r\n",
      " \r\n",
      "Train Epoch: 3 [32000/209222 (15%)]\tAll Loss: 3.9146\tTriple Loss(1): 0.5233\tClassification Loss: 2.8681\r\n",
      "Train Epoch: 3 [64000/209222 (31%)]\tAll Loss: 3.7915\tTriple Loss(1): 0.4801\tClassification Loss: 2.8314\r\n",
      "Train Epoch: 3 [96000/209222 (46%)]\tAll Loss: 5.2415\tTriple Loss(0): 1.2655\tClassification Loss: 2.7105\r\n",
      "Train Epoch: 3 [128000/209222 (61%)]\tAll Loss: 3.8056\tTriple Loss(0): 0.6193\tClassification Loss: 2.5669\r\n",
      "Train Epoch: 3 [160000/209222 (76%)]\tAll Loss: 2.6280\tTriple Loss(0): 0.0000\tClassification Loss: 2.6280\r\n",
      "Train Epoch: 3 [192000/209222 (92%)]\tAll Loss: 3.1811\tTriple Loss(1): 0.4027\tClassification Loss: 2.3757\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/3_epochs\r\n",
      "Train Epoch: 4 [0/209222 (0%)]\tAll Loss: 3.6598\tTriple Loss(1): 0.5295\tClassification Loss: 2.6009\r\n",
      "\r\n",
      "Test set: Average loss: 2.5321\r\n",
      "Top 1 Accuracy: 22531/80000 (28%)\r\n",
      "Top 3 Accuracy: 38668/80000 (48%)\r\n",
      "Top 5 Accuracy: 50285/80000 (63%)\r\n",
      " \r\n",
      "Train Epoch: 4 [32000/209222 (15%)]\tAll Loss: 3.5825\tTriple Loss(1): 0.4111\tClassification Loss: 2.7603\r\n",
      "Train Epoch: 4 [64000/209222 (31%)]\tAll Loss: 2.7531\tTriple Loss(0): 0.0000\tClassification Loss: 2.7531\r\n",
      "Train Epoch: 4 [96000/209222 (46%)]\tAll Loss: 3.8305\tTriple Loss(1): 0.5597\tClassification Loss: 2.7110\r\n",
      "Train Epoch: 4 [128000/209222 (61%)]\tAll Loss: 3.4494\tTriple Loss(1): 0.4094\tClassification Loss: 2.6306\r\n",
      "Train Epoch: 4 [160000/209222 (76%)]\tAll Loss: 3.6620\tTriple Loss(1): 0.5093\tClassification Loss: 2.6433\r\n",
      "Train Epoch: 4 [192000/209222 (92%)]\tAll Loss: 3.0877\tTriple Loss(1): 0.4046\tClassification Loss: 2.2785\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/4_epochs\r\n",
      "Train Epoch: 5 [0/209222 (0%)]\tAll Loss: 3.1631\tTriple Loss(1): 0.4043\tClassification Loss: 2.3545\r\n",
      "\r\n",
      "Test set: Average loss: 2.4083\r\n",
      "Top 1 Accuracy: 24492/80000 (31%)\r\n",
      "Top 3 Accuracy: 42869/80000 (54%)\r\n",
      "Top 5 Accuracy: 53151/80000 (66%)\r\n",
      " \r\n",
      "Train Epoch: 5 [32000/209222 (15%)]\tAll Loss: 2.9665\tTriple Loss(1): 0.2095\tClassification Loss: 2.5476\r\n",
      "Train Epoch: 5 [64000/209222 (31%)]\tAll Loss: 2.8068\tTriple Loss(1): 0.1326\tClassification Loss: 2.5416\r\n",
      "Train Epoch: 5 [96000/209222 (46%)]\tAll Loss: 3.1306\tTriple Loss(1): 0.3389\tClassification Loss: 2.4528\r\n",
      "Train Epoch: 5 [128000/209222 (61%)]\tAll Loss: 2.2665\tTriple Loss(0): 0.0000\tClassification Loss: 2.2665\r\n",
      "Train Epoch: 5 [160000/209222 (76%)]\tAll Loss: 2.4218\tTriple Loss(0): 0.0000\tClassification Loss: 2.4218\r\n",
      "Train Epoch: 5 [192000/209222 (92%)]\tAll Loss: 3.4224\tTriple Loss(1): 0.5864\tClassification Loss: 2.2497\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/5_epochs\r\n",
      "Train Epoch: 6 [0/209222 (0%)]\tAll Loss: 2.9984\tTriple Loss(1): 0.3696\tClassification Loss: 2.2592\r\n",
      "\r\n",
      "Test set: Average loss: 2.2573\r\n",
      "Top 1 Accuracy: 27814/80000 (35%)\r\n",
      "Top 3 Accuracy: 46004/80000 (58%)\r\n",
      "Top 5 Accuracy: 57267/80000 (72%)\r\n",
      " \r\n",
      "Train Epoch: 6 [32000/209222 (15%)]\tAll Loss: 3.0034\tTriple Loss(1): 0.2264\tClassification Loss: 2.5506\r\n",
      "Train Epoch: 6 [64000/209222 (31%)]\tAll Loss: 2.7003\tTriple Loss(1): 0.1533\tClassification Loss: 2.3936\r\n",
      "Train Epoch: 6 [96000/209222 (46%)]\tAll Loss: 2.5595\tTriple Loss(0): 0.0000\tClassification Loss: 2.5595\r\n",
      "Train Epoch: 6 [128000/209222 (61%)]\tAll Loss: 2.2385\tTriple Loss(0): 0.0000\tClassification Loss: 2.2385\r\n",
      "Train Epoch: 6 [160000/209222 (76%)]\tAll Loss: 2.2207\tTriple Loss(0): 0.0000\tClassification Loss: 2.2207\r\n",
      "Train Epoch: 6 [192000/209222 (92%)]\tAll Loss: 2.2400\tTriple Loss(1): 0.2072\tClassification Loss: 1.8256\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/6_epochs\r\n",
      "Train Epoch: 7 [0/209222 (0%)]\tAll Loss: 2.9010\tTriple Loss(1): 0.3194\tClassification Loss: 2.2622\r\n",
      "\r\n",
      "Test set: Average loss: 1.9848\r\n",
      "Top 1 Accuracy: 33807/80000 (42%)\r\n",
      "Top 3 Accuracy: 53175/80000 (66%)\r\n",
      "Top 5 Accuracy: 62434/80000 (78%)\r\n",
      " \r\n",
      "Train Epoch: 7 [32000/209222 (15%)]\tAll Loss: 2.5146\tTriple Loss(1): 0.0923\tClassification Loss: 2.3299\r\n",
      "Train Epoch: 7 [64000/209222 (31%)]\tAll Loss: 2.3419\tTriple Loss(0): 0.0000\tClassification Loss: 2.3419\r\n",
      "Train Epoch: 7 [96000/209222 (46%)]\tAll Loss: 2.9821\tTriple Loss(1): 0.2599\tClassification Loss: 2.4623\r\n",
      "Train Epoch: 7 [128000/209222 (61%)]\tAll Loss: 2.5718\tTriple Loss(1): 0.1442\tClassification Loss: 2.2834\r\n",
      "Train Epoch: 7 [160000/209222 (76%)]\tAll Loss: 2.8403\tTriple Loss(1): 0.3291\tClassification Loss: 2.1821\r\n",
      "Train Epoch: 7 [192000/209222 (92%)]\tAll Loss: 2.5501\tTriple Loss(1): 0.3376\tClassification Loss: 1.8748\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/7_epochs\r\n",
      "Train Epoch: 8 [0/209222 (0%)]\tAll Loss: 2.5025\tTriple Loss(1): 0.2078\tClassification Loss: 2.0869\r\n",
      "\r\n",
      "Test set: Average loss: 1.9855\r\n",
      "Top 1 Accuracy: 34636/80000 (43%)\r\n",
      "Top 3 Accuracy: 53317/80000 (67%)\r\n",
      "Top 5 Accuracy: 62636/80000 (78%)\r\n",
      " \r\n",
      "Train Epoch: 8 [32000/209222 (15%)]\tAll Loss: 3.1020\tTriple Loss(1): 0.3837\tClassification Loss: 2.3347\r\n",
      "Train Epoch: 8 [64000/209222 (31%)]\tAll Loss: 3.3249\tTriple Loss(1): 0.4485\tClassification Loss: 2.4279\r\n",
      "Train Epoch: 8 [96000/209222 (46%)]\tAll Loss: 2.8052\tTriple Loss(1): 0.2225\tClassification Loss: 2.3602\r\n",
      "Train Epoch: 8 [128000/209222 (61%)]\tAll Loss: 2.0906\tTriple Loss(0): 0.0000\tClassification Loss: 2.0906\r\n",
      "Train Epoch: 8 [160000/209222 (76%)]\tAll Loss: 2.3391\tTriple Loss(1): 0.1759\tClassification Loss: 1.9872\r\n",
      "Train Epoch: 8 [192000/209222 (92%)]\tAll Loss: 2.3739\tTriple Loss(1): 0.2234\tClassification Loss: 1.9271\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/8_epochs\r\n",
      "Train Epoch: 9 [0/209222 (0%)]\tAll Loss: 2.7106\tTriple Loss(1): 0.3028\tClassification Loss: 2.1050\r\n",
      "\r\n",
      "Test set: Average loss: 1.8669\r\n",
      "Top 1 Accuracy: 36557/80000 (46%)\r\n",
      "Top 3 Accuracy: 55758/80000 (70%)\r\n",
      "Top 5 Accuracy: 64577/80000 (81%)\r\n",
      " \r\n",
      "Train Epoch: 9 [32000/209222 (15%)]\tAll Loss: 2.1567\tTriple Loss(1): 0.0728\tClassification Loss: 2.0111\r\n",
      "Train Epoch: 9 [64000/209222 (31%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.1497\tClassification Loss: 2.0299\r\n",
      "Train Epoch: 9 [96000/209222 (46%)]\tAll Loss: 2.4505\tTriple Loss(0): 0.0000\tClassification Loss: 2.4505\r\n",
      "Train Epoch: 9 [128000/209222 (61%)]\tAll Loss: 2.1909\tTriple Loss(1): 0.0784\tClassification Loss: 2.0341\r\n",
      "Train Epoch: 9 [160000/209222 (76%)]\tAll Loss: 2.4484\tTriple Loss(1): 0.1624\tClassification Loss: 2.1236\r\n",
      "Train Epoch: 9 [192000/209222 (92%)]\tAll Loss: 1.9262\tTriple Loss(0): 0.0000\tClassification Loss: 1.9262\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/9_epochs\r\n",
      "Train Epoch: 10 [0/209222 (0%)]\tAll Loss: 2.3872\tTriple Loss(1): 0.2258\tClassification Loss: 1.9357\r\n",
      "\r\n",
      "Test set: Average loss: 1.7371\r\n",
      "Top 1 Accuracy: 40157/80000 (50%)\r\n",
      "Top 3 Accuracy: 58581/80000 (73%)\r\n",
      "Top 5 Accuracy: 66650/80000 (83%)\r\n",
      " \r\n",
      "Train Epoch: 10 [32000/209222 (15%)]\tAll Loss: 2.3762\tTriple Loss(1): 0.1683\tClassification Loss: 2.0397\r\n",
      "Train Epoch: 10 [64000/209222 (31%)]\tAll Loss: 2.5377\tTriple Loss(1): 0.2136\tClassification Loss: 2.1106\r\n",
      "Train Epoch: 10 [96000/209222 (46%)]\tAll Loss: 2.2234\tTriple Loss(0): 0.0000\tClassification Loss: 2.2234\r\n"
     ]
    }
   ],
   "source": [
    "# FREEZE = False. LR=0.002. in-shop=True.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/9_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/9_epochs\r\n",
      "Train Epoch: 10 [0/209222 (0%)]\tAll Loss: 2.8286\tTriple Loss(1): 0.2787\tClassification Loss: 2.2713\r\n",
      "train.py:211: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.7347\r\n",
      "Top 1 Accuracy: 40172/80000 (50%)\r\n",
      "Top 3 Accuracy: 58634/80000 (73%)\r\n",
      "Top 5 Accuracy: 66638/80000 (83%)\r\n",
      " \r\n",
      "Train Epoch: 10 [32000/209222 (15%)]\tAll Loss: 2.2351\tTriple Loss(1): 0.1249\tClassification Loss: 1.9853\r\n",
      "Train Epoch: 10 [64000/209222 (31%)]\tAll Loss: 2.7991\tTriple Loss(1): 0.2963\tClassification Loss: 2.2064\r\n",
      "Train Epoch: 10 [96000/209222 (46%)]\tAll Loss: 1.9463\tTriple Loss(1): 0.0882\tClassification Loss: 1.7699\r\n",
      "Train Epoch: 10 [128000/209222 (61%)]\tAll Loss: 1.8923\tTriple Loss(1): 0.1096\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 10 [160000/209222 (76%)]\tAll Loss: 1.6912\tTriple Loss(1): 0.0840\tClassification Loss: 1.5232\r\n",
      "Train Epoch: 10 [192000/209222 (92%)]\tAll Loss: 1.9212\tTriple Loss(1): 0.1253\tClassification Loss: 1.6706\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=False/lr=0.002/10_epochs\r\n",
      "Train Epoch: 11 [0/209222 (0%)]\tAll Loss: 2.9691\tTriple Loss(1): 0.4475\tClassification Loss: 2.0741\r\n",
      "\r\n",
      "Test set: Average loss: 1.6604\r\n",
      "Top 1 Accuracy: 41942/80000 (52%)\r\n",
      "Top 3 Accuracy: 60147/80000 (75%)\r\n",
      "Top 5 Accuracy: 67700/80000 (85%)\r\n",
      " \r\n",
      "Train Epoch: 11 [32000/209222 (15%)]\tAll Loss: 2.1874\tTriple Loss(1): 0.1945\tClassification Loss: 1.7985\r\n",
      "Train Epoch: 11 [64000/209222 (31%)]\tAll Loss: 2.3540\tTriple Loss(1): 0.2192\tClassification Loss: 1.9156\r\n",
      "Train Epoch: 11 [96000/209222 (46%)]\tAll Loss: 2.7238\tTriple Loss(1): 0.2337\tClassification Loss: 2.2563\r\n",
      "Train Epoch: 11 [128000/209222 (61%)]\tAll Loss: 2.1069\tTriple Loss(1): 0.1014\tClassification Loss: 1.9041\r\n",
      "Train Epoch: 11 [160000/209222 (76%)]\tAll Loss: 2.0217\tTriple Loss(1): 0.2179\tClassification Loss: 1.5858\r\n"
     ]
    }
   ],
   "source": [
    "# Continuing \"models/freeze=False/lr=0.002/9_epochs\"\n",
    "# FREEZE = False. LR=0.002. in-shop=True.\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/29_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/29_epochs\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Faded_High-Waisted_Denim_Shorts/img_00000005.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [0/209222 (0%)]\tAll Loss: 2.8755\tTriple Loss(1): 0.4070\tClassification Loss: 2.0614\r\n",
      "train.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.2719\r\n",
      "Top 1 Accuracy: 50498/80000 (63%)\r\n",
      "Top 3 Accuracy: 67081/80000 (84%)\r\n",
      "Top 5 Accuracy: 72765/80000 (91%)\r\n",
      " \r\n",
      "Train Epoch: 30 [64/209222 (0%)]\tAll Loss: 1.5520\tTriple Loss(0): 0.0000\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 30 [128/209222 (0%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0095\tClassification Loss: 1.5716\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Denim_Dolphin_Hem_Shorts/img_00000027.jpg\r\n",
      "img_n: img/Ribbed_Knit_Turtleneck/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [192/209222 (0%)]\tAll Loss: 1.6233\tTriple Loss(0): 0.0000\tClassification Loss: 1.6233\r\n",
      "Train Epoch: 30 [256/209222 (0%)]\tAll Loss: 1.3639\tTriple Loss(1): 0.0109\tClassification Loss: 1.3422\r\n",
      "Train Epoch: 30 [320/209222 (0%)]\tAll Loss: 1.3075\tTriple Loss(1): 0.0228\tClassification Loss: 1.2620\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Colorblock_Performance_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Faux_Shearling_Hooded_Parka/img_00000046.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [384/209222 (0%)]\tAll Loss: 1.7084\tTriple Loss(0): 0.0000\tClassification Loss: 1.7084\r\n",
      "Train Epoch: 30 [448/209222 (0%)]\tAll Loss: 1.6508\tTriple Loss(1): 0.0157\tClassification Loss: 1.6193\r\n",
      "Train Epoch: 30 [512/209222 (0%)]\tAll Loss: 1.5994\tTriple Loss(1): 0.0633\tClassification Loss: 1.4727\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Flat_Front_Denim_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Slub_Knit_Buttoned-Back_Blouse/img_00000029.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [576/209222 (0%)]\tAll Loss: 1.4131\tTriple Loss(0): 0.0000\tClassification Loss: 1.4131\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Smocked_Ruffle-Trim_Shorts/img_00000024.jpg\r\n",
      "img_n: img/High-Slit_Longline_Flannel/img_00000029.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [640/209222 (0%)]\tAll Loss: 1.3842\tTriple Loss(0): 0.0000\tClassification Loss: 1.3842\r\n",
      "Train Epoch: 30 [704/209222 (0%)]\tAll Loss: 2.1007\tTriple Loss(1): 0.0855\tClassification Loss: 1.9296\r\n",
      "Train Epoch: 30 [768/209222 (0%)]\tAll Loss: 1.7341\tTriple Loss(1): 0.0925\tClassification Loss: 1.5492\r\n",
      "Train Epoch: 30 [832/209222 (0%)]\tAll Loss: 1.8348\tTriple Loss(1): 0.0333\tClassification Loss: 1.7681\r\n",
      "Train Epoch: 30 [896/209222 (0%)]\tAll Loss: 1.4477\tTriple Loss(1): 0.1188\tClassification Loss: 1.2101\r\n",
      "Train Epoch: 30 [960/209222 (0%)]\tAll Loss: 1.4365\tTriple Loss(1): 0.0038\tClassification Loss: 1.4288\r\n",
      "Train Epoch: 30 [1024/209222 (0%)]\tAll Loss: 1.4007\tTriple Loss(1): 0.0036\tClassification Loss: 1.3935\r\n",
      "Train Epoch: 30 [1088/209222 (1%)]\tAll Loss: 1.4375\tTriple Loss(1): 0.0290\tClassification Loss: 1.3795\r\n",
      "Train Epoch: 30 [1152/209222 (1%)]\tAll Loss: 1.3938\tTriple Loss(1): 0.1068\tClassification Loss: 1.1803\r\n",
      "Train Epoch: 30 [1216/209222 (1%)]\tAll Loss: 1.5387\tTriple Loss(1): 0.1765\tClassification Loss: 1.1857\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pleated_Abstract_Print_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Striped_Sweater-Knit_Hoodie/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [1280/209222 (1%)]\tAll Loss: 1.6846\tTriple Loss(0): 0.0000\tClassification Loss: 1.6846\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Drawstring_Floral_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Tartan_Plaid_Button-Down/img_00000065.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [1344/209222 (1%)]\tAll Loss: 1.4585\tTriple Loss(0): 0.0000\tClassification Loss: 1.4585\r\n",
      "Train Epoch: 30 [1408/209222 (1%)]\tAll Loss: 1.6671\tTriple Loss(1): 0.0337\tClassification Loss: 1.5997\r\n",
      "Train Epoch: 30 [1472/209222 (1%)]\tAll Loss: 1.3473\tTriple Loss(1): 0.0664\tClassification Loss: 1.2145\r\n",
      "Train Epoch: 30 [1536/209222 (1%)]\tAll Loss: 1.6345\tTriple Loss(1): 0.0077\tClassification Loss: 1.6192\r\n",
      "Train Epoch: 30 [1600/209222 (1%)]\tAll Loss: 1.5762\tTriple Loss(1): 0.0479\tClassification Loss: 1.4803\r\n",
      "Train Epoch: 30 [1664/209222 (1%)]\tAll Loss: 1.3902\tTriple Loss(1): 0.0220\tClassification Loss: 1.3462\r\n",
      "Train Epoch: 30 [1728/209222 (1%)]\tAll Loss: 1.4400\tTriple Loss(1): 0.0392\tClassification Loss: 1.3617\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Painted_Floral_Denim_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Brooklyn_Nets_Graphic_Tank/img_00000050.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [1792/209222 (1%)]\tAll Loss: 1.8291\tTriple Loss(0): 0.1720\tClassification Loss: 1.4851\r\n",
      "Train Epoch: 30 [1856/209222 (1%)]\tAll Loss: 1.5700\tTriple Loss(1): 0.0555\tClassification Loss: 1.4590\r\n",
      "Train Epoch: 30 [1920/209222 (1%)]\tAll Loss: 1.4613\tTriple Loss(1): 0.0718\tClassification Loss: 1.3177\r\n",
      "Train Epoch: 30 [1984/209222 (1%)]\tAll Loss: 1.3102\tTriple Loss(1): 0.0223\tClassification Loss: 1.2657\r\n",
      "Train Epoch: 30 [2048/209222 (1%)]\tAll Loss: 1.3638\tTriple Loss(1): 0.0137\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 30 [2112/209222 (1%)]\tAll Loss: 1.6189\tTriple Loss(1): 0.0104\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 30 [2176/209222 (1%)]\tAll Loss: 1.3530\tTriple Loss(1): 0.0347\tClassification Loss: 1.2837\r\n",
      "Train Epoch: 30 [2240/209222 (1%)]\tAll Loss: 1.5472\tTriple Loss(1): 0.1140\tClassification Loss: 1.3191\r\n",
      "Train Epoch: 30 [2304/209222 (1%)]\tAll Loss: 1.6473\tTriple Loss(1): 0.0000\tClassification Loss: 1.6473\r\n",
      "Train Epoch: 30 [2368/209222 (1%)]\tAll Loss: 1.6232\tTriple Loss(1): 0.0688\tClassification Loss: 1.4855\r\n",
      "Train Epoch: 30 [2432/209222 (1%)]\tAll Loss: 1.6038\tTriple Loss(1): 0.0848\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 30 [2496/209222 (1%)]\tAll Loss: 1.5655\tTriple Loss(1): 0.0499\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 30 [2560/209222 (1%)]\tAll Loss: 1.6431\tTriple Loss(1): 0.0000\tClassification Loss: 1.6431\r\n",
      "Train Epoch: 30 [2624/209222 (1%)]\tAll Loss: 1.3648\tTriple Loss(1): 0.0000\tClassification Loss: 1.3648\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tribal_Print_Dolphin_Shorts/img_00000015.jpg\r\n",
      "img_n: img/Ribbed_Knit_Turtleneck/img_00000011.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [2688/209222 (1%)]\tAll Loss: 1.5029\tTriple Loss(0): 0.0000\tClassification Loss: 1.5029\r\n",
      "Train Epoch: 30 [2752/209222 (1%)]\tAll Loss: 1.5658\tTriple Loss(1): 0.0156\tClassification Loss: 1.5346\r\n",
      "Train Epoch: 30 [2816/209222 (1%)]\tAll Loss: 1.5672\tTriple Loss(1): 0.0180\tClassification Loss: 1.5312\r\n",
      "Train Epoch: 30 [2880/209222 (1%)]\tAll Loss: 1.4501\tTriple Loss(1): 0.0159\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 30 [2944/209222 (1%)]\tAll Loss: 1.5365\tTriple Loss(1): 0.0045\tClassification Loss: 1.5274\r\n",
      "Train Epoch: 30 [3008/209222 (1%)]\tAll Loss: 1.7461\tTriple Loss(1): 0.1133\tClassification Loss: 1.5195\r\n",
      "Train Epoch: 30 [3072/209222 (1%)]\tAll Loss: 1.8596\tTriple Loss(1): 0.0157\tClassification Loss: 1.8282\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Lemon_Print_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Marled_Dolman-Sleeve_Hoodie/img_00000023.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [3136/209222 (1%)]\tAll Loss: 1.9522\tTriple Loss(0): 0.0000\tClassification Loss: 1.9522\r\n",
      "Train Epoch: 30 [3200/209222 (2%)]\tAll Loss: 1.8330\tTriple Loss(1): 0.0760\tClassification Loss: 1.6810\r\n",
      "Train Epoch: 30 [3264/209222 (2%)]\tAll Loss: 1.6213\tTriple Loss(1): 0.0711\tClassification Loss: 1.4791\r\n",
      "Train Epoch: 30 [3328/209222 (2%)]\tAll Loss: 1.2036\tTriple Loss(1): 0.0000\tClassification Loss: 1.2036\r\n",
      "Train Epoch: 30 [3392/209222 (2%)]\tAll Loss: 1.6490\tTriple Loss(1): 0.0723\tClassification Loss: 1.5045\r\n",
      "Train Epoch: 30 [3456/209222 (2%)]\tAll Loss: 1.8679\tTriple Loss(1): 0.0108\tClassification Loss: 1.8462\r\n",
      "Train Epoch: 30 [3520/209222 (2%)]\tAll Loss: 2.0012\tTriple Loss(1): 0.1248\tClassification Loss: 1.7515\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tropical_Print_Chino_Shorts/img_00000021.jpg\r\n",
      "img_n: img/Tie-Dye_Hoodie/img_00000022.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [3584/209222 (2%)]\tAll Loss: 1.5352\tTriple Loss(0): 0.0000\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 30 [3648/209222 (2%)]\tAll Loss: 1.4136\tTriple Loss(1): 0.0018\tClassification Loss: 1.4099\r\n",
      "Train Epoch: 30 [3712/209222 (2%)]\tAll Loss: 1.7060\tTriple Loss(1): 0.1961\tClassification Loss: 1.3139\r\n",
      "Train Epoch: 30 [3776/209222 (2%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0424\tClassification Loss: 1.5058\r\n",
      "Train Epoch: 30 [3840/209222 (2%)]\tAll Loss: 1.7493\tTriple Loss(1): 0.0243\tClassification Loss: 1.7007\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral_Print_PJ_Shorts/img_00000050.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000002.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [3904/209222 (2%)]\tAll Loss: 1.4868\tTriple Loss(0): 0.0969\tClassification Loss: 1.2929\r\n",
      "Train Epoch: 30 [3968/209222 (2%)]\tAll Loss: 1.4978\tTriple Loss(1): 0.0913\tClassification Loss: 1.3152\r\n",
      "Train Epoch: 30 [4032/209222 (2%)]\tAll Loss: 1.5569\tTriple Loss(1): 0.0488\tClassification Loss: 1.4593\r\n",
      "Train Epoch: 30 [4096/209222 (2%)]\tAll Loss: 1.3094\tTriple Loss(1): 0.0263\tClassification Loss: 1.2569\r\n",
      "Train Epoch: 30 [4160/209222 (2%)]\tAll Loss: 1.4111\tTriple Loss(1): 0.0474\tClassification Loss: 1.3164\r\n",
      "Train Epoch: 30 [4224/209222 (2%)]\tAll Loss: 1.7138\tTriple Loss(1): 0.0626\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 30 [4288/209222 (2%)]\tAll Loss: 1.5874\tTriple Loss(1): 0.0609\tClassification Loss: 1.4656\r\n",
      "Train Epoch: 30 [4352/209222 (2%)]\tAll Loss: 1.6329\tTriple Loss(1): 0.0772\tClassification Loss: 1.4786\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Matelot_Denim_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Quilted_Faux_Leather_Bomber/img_00000159.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [4416/209222 (2%)]\tAll Loss: 1.5315\tTriple Loss(0): 0.0000\tClassification Loss: 1.5315\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Rolled-Up_Distressed_Shorts/img_00000051.jpg\r\n",
      "img_n: img/Cropped_Ribbed_Knit_Turtleneck/img_00000047.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [4480/209222 (2%)]\tAll Loss: 4.6388\tTriple Loss(0): 1.5296\tClassification Loss: 1.5796\r\n",
      "Train Epoch: 30 [4544/209222 (2%)]\tAll Loss: 1.6191\tTriple Loss(1): 0.0197\tClassification Loss: 1.5797\r\n",
      "Train Epoch: 30 [4608/209222 (2%)]\tAll Loss: 1.3308\tTriple Loss(1): 0.1016\tClassification Loss: 1.1276\r\n",
      "Train Epoch: 30 [4672/209222 (2%)]\tAll Loss: 1.7923\tTriple Loss(1): 0.1508\tClassification Loss: 1.4907\r\n",
      "Train Epoch: 30 [4736/209222 (2%)]\tAll Loss: 1.6667\tTriple Loss(1): 0.0726\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 30 [4800/209222 (2%)]\tAll Loss: 1.7607\tTriple Loss(1): 0.0676\tClassification Loss: 1.6255\r\n",
      "Train Epoch: 30 [4864/209222 (2%)]\tAll Loss: 1.4679\tTriple Loss(1): 0.0000\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 30 [4928/209222 (2%)]\tAll Loss: 1.1649\tTriple Loss(1): 0.0324\tClassification Loss: 1.1002\r\n",
      "Train Epoch: 30 [4992/209222 (2%)]\tAll Loss: 1.5298\tTriple Loss(1): 0.0656\tClassification Loss: 1.3986\r\n",
      "Train Epoch: 30 [5056/209222 (2%)]\tAll Loss: 1.5675\tTriple Loss(1): 0.0576\tClassification Loss: 1.4523\r\n",
      "Train Epoch: 30 [5120/209222 (2%)]\tAll Loss: 1.6107\tTriple Loss(1): 0.0000\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 30 [5184/209222 (2%)]\tAll Loss: 1.5999\tTriple Loss(1): 0.0000\tClassification Loss: 1.5999\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tribal_Pattern_Sequin_Shorts/img_00000031.jpg\r\n",
      "img_n: img/Ruffle-Trimmed_Blouse/img_00000023.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [5248/209222 (3%)]\tAll Loss: 8.7103\tTriple Loss(0): 3.6696\tClassification Loss: 1.3711\r\n",
      "Train Epoch: 30 [5312/209222 (3%)]\tAll Loss: 1.3453\tTriple Loss(1): 0.0300\tClassification Loss: 1.2854\r\n",
      "Train Epoch: 30 [5376/209222 (3%)]\tAll Loss: 1.6232\tTriple Loss(1): 0.0031\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 30 [5440/209222 (3%)]\tAll Loss: 1.5094\tTriple Loss(1): 0.1164\tClassification Loss: 1.2765\r\n",
      "Train Epoch: 30 [5504/209222 (3%)]\tAll Loss: 1.5104\tTriple Loss(1): 0.0408\tClassification Loss: 1.4287\r\n",
      "Train Epoch: 30 [5568/209222 (3%)]\tAll Loss: 1.9681\tTriple Loss(1): 0.0831\tClassification Loss: 1.8018\r\n",
      "Train Epoch: 30 [5632/209222 (3%)]\tAll Loss: 1.6721\tTriple Loss(1): 0.0264\tClassification Loss: 1.6193\r\n",
      "Train Epoch: 30 [5696/209222 (3%)]\tAll Loss: 1.6225\tTriple Loss(1): 0.0322\tClassification Loss: 1.5580\r\n",
      "Train Epoch: 30 [5760/209222 (3%)]\tAll Loss: 1.5881\tTriple Loss(1): 0.1275\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 30 [5824/209222 (3%)]\tAll Loss: 1.6383\tTriple Loss(1): 0.1375\tClassification Loss: 1.3633\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Drawstring_Crepe_Shorts/img_00000007.jpg\r\n",
      "img_n: img/Hooded_Purl_Knit_Sweater/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [5888/209222 (3%)]\tAll Loss: 1.2087\tTriple Loss(0): 0.0000\tClassification Loss: 1.2087\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Geo-Printed_Flat_Front_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Collarless_One-Button_Blazer/img_00000086.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [5952/209222 (3%)]\tAll Loss: 1.7148\tTriple Loss(0): 0.0000\tClassification Loss: 1.7148\r\n",
      "Train Epoch: 30 [6016/209222 (3%)]\tAll Loss: 1.5429\tTriple Loss(1): 0.0102\tClassification Loss: 1.5226\r\n",
      "Train Epoch: 30 [6080/209222 (3%)]\tAll Loss: 1.7095\tTriple Loss(1): 0.0590\tClassification Loss: 1.5916\r\n",
      "Train Epoch: 30 [6144/209222 (3%)]\tAll Loss: 1.3181\tTriple Loss(1): 0.0250\tClassification Loss: 1.2681\r\n",
      "Train Epoch: 30 [6208/209222 (3%)]\tAll Loss: 1.4022\tTriple Loss(1): 0.0232\tClassification Loss: 1.3557\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Flat-Front_Shorts/img_00000009.jpg\r\n",
      "img_n: img/Hooded_Puff_Parka/img_00000008.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [6272/209222 (3%)]\tAll Loss: 1.3182\tTriple Loss(0): 0.0000\tClassification Loss: 1.3182\r\n",
      "Train Epoch: 30 [6336/209222 (3%)]\tAll Loss: 1.5940\tTriple Loss(1): 0.0174\tClassification Loss: 1.5592\r\n",
      "Train Epoch: 30 [6400/209222 (3%)]\tAll Loss: 1.5497\tTriple Loss(1): 0.0230\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 30 [6464/209222 (3%)]\tAll Loss: 1.5703\tTriple Loss(1): 0.0266\tClassification Loss: 1.5171\r\n",
      "Train Epoch: 30 [6528/209222 (3%)]\tAll Loss: 1.7267\tTriple Loss(1): 0.1118\tClassification Loss: 1.5031\r\n",
      "Train Epoch: 30 [6592/209222 (3%)]\tAll Loss: 1.4353\tTriple Loss(1): 0.1467\tClassification Loss: 1.1419\r\n",
      "Train Epoch: 30 [6656/209222 (3%)]\tAll Loss: 1.5000\tTriple Loss(1): 0.0376\tClassification Loss: 1.4248\r\n",
      "Train Epoch: 30 [6720/209222 (3%)]\tAll Loss: 1.7079\tTriple Loss(1): 0.0577\tClassification Loss: 1.5925\r\n",
      "Train Epoch: 30 [6784/209222 (3%)]\tAll Loss: 1.7673\tTriple Loss(1): 0.0843\tClassification Loss: 1.5986\r\n",
      "Train Epoch: 30 [6848/209222 (3%)]\tAll Loss: 1.7079\tTriple Loss(1): 0.0572\tClassification Loss: 1.5935\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral_Tribal_Print_Shorts/img_00000019.jpg\r\n",
      "img_n: img/Faux_Suede_Fringe_Jacket/img_00000061.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [6912/209222 (3%)]\tAll Loss: 6.2837\tTriple Loss(0): 2.5623\tClassification Loss: 1.1591\r\n",
      "Train Epoch: 30 [6976/209222 (3%)]\tAll Loss: 1.6698\tTriple Loss(1): 0.0973\tClassification Loss: 1.4753\r\n",
      "Train Epoch: 30 [7040/209222 (3%)]\tAll Loss: 1.5575\tTriple Loss(1): 0.0032\tClassification Loss: 1.5510\r\n",
      "Train Epoch: 30 [7104/209222 (3%)]\tAll Loss: 1.6220\tTriple Loss(1): 0.0018\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 30 [7168/209222 (3%)]\tAll Loss: 1.1720\tTriple Loss(1): 0.0294\tClassification Loss: 1.1133\r\n",
      "Train Epoch: 30 [7232/209222 (3%)]\tAll Loss: 1.4811\tTriple Loss(1): 0.0100\tClassification Loss: 1.4610\r\n",
      "Train Epoch: 30 [7296/209222 (3%)]\tAll Loss: 1.4425\tTriple Loss(1): 0.0224\tClassification Loss: 1.3977\r\n",
      "Train Epoch: 30 [7360/209222 (4%)]\tAll Loss: 1.5580\tTriple Loss(1): 0.0440\tClassification Loss: 1.4700\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Classic_Bike_Shorts/img_00000028.jpg\r\n",
      "img_n: img/Striped_Henley/img_00000069.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [7424/209222 (4%)]\tAll Loss: 1.6148\tTriple Loss(0): 0.0000\tClassification Loss: 1.6148\r\n",
      "Train Epoch: 30 [7488/209222 (4%)]\tAll Loss: 1.7565\tTriple Loss(1): 0.0330\tClassification Loss: 1.6906\r\n",
      "Train Epoch: 30 [7552/209222 (4%)]\tAll Loss: 1.4871\tTriple Loss(1): 0.0356\tClassification Loss: 1.4158\r\n",
      "Train Epoch: 30 [7616/209222 (4%)]\tAll Loss: 1.5630\tTriple Loss(1): 0.0305\tClassification Loss: 1.5020\r\n",
      "Train Epoch: 30 [7680/209222 (4%)]\tAll Loss: 1.0562\tTriple Loss(1): 0.0059\tClassification Loss: 1.0443\r\n",
      "Train Epoch: 30 [7744/209222 (4%)]\tAll Loss: 1.7588\tTriple Loss(1): 0.0553\tClassification Loss: 1.6482\r\n",
      "Train Epoch: 30 [7808/209222 (4%)]\tAll Loss: 1.5361\tTriple Loss(1): 0.0004\tClassification Loss: 1.5353\r\n",
      "Train Epoch: 30 [7872/209222 (4%)]\tAll Loss: 1.1999\tTriple Loss(1): 0.0000\tClassification Loss: 1.1999\r\n",
      "Train Epoch: 30 [7936/209222 (4%)]\tAll Loss: 1.7958\tTriple Loss(1): 0.0744\tClassification Loss: 1.6469\r\n",
      "Train Epoch: 30 [8000/209222 (4%)]\tAll Loss: 1.4482\tTriple Loss(1): 0.0037\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 30 [8064/209222 (4%)]\tAll Loss: 1.8483\tTriple Loss(1): 0.0501\tClassification Loss: 1.7481\r\n",
      "Train Epoch: 30 [8128/209222 (4%)]\tAll Loss: 1.9258\tTriple Loss(1): 0.1359\tClassification Loss: 1.6539\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet-Hemmed_Denim_Shorts/img_00000033.jpg\r\n",
      "img_n: img/Heathered_Knit_Cardigan/img_00000042.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [8192/209222 (4%)]\tAll Loss: 1.4265\tTriple Loss(0): 0.0000\tClassification Loss: 1.4265\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral-Embroidered_Mesh_Shorts/img_00000010.jpg\r\n",
      "img_n: img/Abstract_Geo_Print_Crop_Top/img_00000012.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [8256/209222 (4%)]\tAll Loss: 4.8206\tTriple Loss(0): 1.7495\tClassification Loss: 1.3216\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_High-Waist_Shorts/img_00000025.jpg\r\n",
      "img_n: img/Drape-Front_Cardigan/img_00000070.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [8320/209222 (4%)]\tAll Loss: 1.5450\tTriple Loss(0): 0.0000\tClassification Loss: 1.5450\r\n",
      "Train Epoch: 30 [8384/209222 (4%)]\tAll Loss: 1.5196\tTriple Loss(1): 0.0445\tClassification Loss: 1.4306\r\n",
      "Train Epoch: 30 [8448/209222 (4%)]\tAll Loss: 1.7379\tTriple Loss(1): 0.0444\tClassification Loss: 1.6490\r\n",
      "Train Epoch: 30 [8512/209222 (4%)]\tAll Loss: 1.8184\tTriple Loss(1): 0.1306\tClassification Loss: 1.5572\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Colorblock_Heathered_Yoga_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Faux-Fur_Trimmed_Puffer_Parka/img_00000069.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [8576/209222 (4%)]\tAll Loss: 1.3397\tTriple Loss(0): 0.0000\tClassification Loss: 1.3397\r\n",
      "Train Epoch: 30 [8640/209222 (4%)]\tAll Loss: 1.3833\tTriple Loss(1): 0.0125\tClassification Loss: 1.3582\r\n",
      "Train Epoch: 30 [8704/209222 (4%)]\tAll Loss: 1.5569\tTriple Loss(1): 0.0248\tClassification Loss: 1.5074\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pop_Art_Floral_Shorts/img_00000015.jpg\r\n",
      "img_n: img/Heathered_Zip_Hoodie/img_00000025.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [8768/209222 (4%)]\tAll Loss: 1.5825\tTriple Loss(0): 0.0000\tClassification Loss: 1.5825\r\n",
      "Train Epoch: 30 [8832/209222 (4%)]\tAll Loss: 1.5283\tTriple Loss(1): 0.0137\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 30 [8896/209222 (4%)]\tAll Loss: 1.5713\tTriple Loss(1): 0.0642\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 30 [8960/209222 (4%)]\tAll Loss: 1.5670\tTriple Loss(1): 0.0158\tClassification Loss: 1.5353\r\n",
      "Train Epoch: 30 [9024/209222 (4%)]\tAll Loss: 1.5585\tTriple Loss(1): 0.0966\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 30 [9088/209222 (4%)]\tAll Loss: 1.5943\tTriple Loss(1): 0.0347\tClassification Loss: 1.5249\r\n",
      "Train Epoch: 30 [9152/209222 (4%)]\tAll Loss: 1.5887\tTriple Loss(1): 0.0537\tClassification Loss: 1.4813\r\n",
      "Train Epoch: 30 [9216/209222 (4%)]\tAll Loss: 1.7846\tTriple Loss(1): 0.0235\tClassification Loss: 1.7376\r\n",
      "Train Epoch: 30 [9280/209222 (4%)]\tAll Loss: 1.8119\tTriple Loss(1): 0.0623\tClassification Loss: 1.6873\r\n",
      "Train Epoch: 30 [9344/209222 (4%)]\tAll Loss: 1.4982\tTriple Loss(1): 0.0508\tClassification Loss: 1.3966\r\n",
      "Train Epoch: 30 [9408/209222 (4%)]\tAll Loss: 1.3918\tTriple Loss(1): 0.0623\tClassification Loss: 1.2672\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Linen-Blend_Drawstring_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Popcorn_Knit_Cardigan/img_00000026.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [9472/209222 (5%)]\tAll Loss: 1.3736\tTriple Loss(0): 0.0000\tClassification Loss: 1.3736\r\n",
      "Train Epoch: 30 [9536/209222 (5%)]\tAll Loss: 1.4483\tTriple Loss(1): 0.1517\tClassification Loss: 1.1449\r\n",
      "Train Epoch: 30 [9600/209222 (5%)]\tAll Loss: 1.7419\tTriple Loss(1): 0.0832\tClassification Loss: 1.5754\r\n",
      "Train Epoch: 30 [9664/209222 (5%)]\tAll Loss: 1.5747\tTriple Loss(1): 0.0000\tClassification Loss: 1.5747\r\n",
      "Train Epoch: 30 [9728/209222 (5%)]\tAll Loss: 1.8150\tTriple Loss(1): 0.0896\tClassification Loss: 1.6358\r\n",
      "Train Epoch: 30 [9792/209222 (5%)]\tAll Loss: 2.0373\tTriple Loss(1): 0.2022\tClassification Loss: 1.6329\r\n",
      "Train Epoch: 30 [9856/209222 (5%)]\tAll Loss: 1.4540\tTriple Loss(1): 0.0000\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 30 [9920/209222 (5%)]\tAll Loss: 1.7020\tTriple Loss(1): 0.0060\tClassification Loss: 1.6900\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Trouser_Shorts/img_00000063.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000090.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [9984/209222 (5%)]\tAll Loss: 1.6425\tTriple Loss(0): 0.0000\tClassification Loss: 1.6425\r\n",
      "Train Epoch: 30 [10048/209222 (5%)]\tAll Loss: 1.5516\tTriple Loss(1): 0.0744\tClassification Loss: 1.4028\r\n",
      "Train Epoch: 30 [10112/209222 (5%)]\tAll Loss: 1.6019\tTriple Loss(1): 0.0016\tClassification Loss: 1.5987\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_Denim_Bermuda_Shorts/img_00000055.jpg\r\n",
      "img_n: img/Tartan_Plaid_Chiffon_Blouse/img_00000023.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [10176/209222 (5%)]\tAll Loss: 1.6877\tTriple Loss(0): 0.0000\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 30 [10240/209222 (5%)]\tAll Loss: 1.3057\tTriple Loss(1): 0.0593\tClassification Loss: 1.1872\r\n",
      "Train Epoch: 30 [10304/209222 (5%)]\tAll Loss: 1.8162\tTriple Loss(1): 0.0065\tClassification Loss: 1.8031\r\n",
      "Train Epoch: 30 [10368/209222 (5%)]\tAll Loss: 1.5985\tTriple Loss(1): 0.0000\tClassification Loss: 1.5985\r\n",
      "Train Epoch: 30 [10432/209222 (5%)]\tAll Loss: 1.6732\tTriple Loss(1): 0.0814\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 30 [10496/209222 (5%)]\tAll Loss: 1.4033\tTriple Loss(1): 0.0124\tClassification Loss: 1.3784\r\n",
      "Train Epoch: 30 [10560/209222 (5%)]\tAll Loss: 1.4873\tTriple Loss(1): 0.0726\tClassification Loss: 1.3420\r\n",
      "Train Epoch: 30 [10624/209222 (5%)]\tAll Loss: 1.6077\tTriple Loss(1): 0.0410\tClassification Loss: 1.5256\r\n",
      "Train Epoch: 30 [10688/209222 (5%)]\tAll Loss: 1.7154\tTriple Loss(1): 0.0607\tClassification Loss: 1.5939\r\n",
      "Train Epoch: 30 [10752/209222 (5%)]\tAll Loss: 1.2906\tTriple Loss(1): 0.0519\tClassification Loss: 1.1868\r\n",
      "Train Epoch: 30 [10816/209222 (5%)]\tAll Loss: 1.7623\tTriple Loss(1): 0.0918\tClassification Loss: 1.5787\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Betty_Boop_PJ_Shorts/img_00000001.jpg\r\n",
      "img_n: img/Faux_Shearling_Parka/img_00000019.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [10880/209222 (5%)]\tAll Loss: 1.5823\tTriple Loss(0): 0.0000\tClassification Loss: 1.5823\r\n",
      "Train Epoch: 30 [10944/209222 (5%)]\tAll Loss: 1.4203\tTriple Loss(1): 0.0049\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 30 [11008/209222 (5%)]\tAll Loss: 1.6323\tTriple Loss(1): 0.0450\tClassification Loss: 1.5423\r\n",
      "Train Epoch: 30 [11072/209222 (5%)]\tAll Loss: 1.5540\tTriple Loss(1): 0.0321\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 30 [11136/209222 (5%)]\tAll Loss: 1.6202\tTriple Loss(1): 0.0993\tClassification Loss: 1.4215\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Mickey_Mouse_PJ_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Boxy_Chiffon_Top/img_00000040.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [11200/209222 (5%)]\tAll Loss: 1.4492\tTriple Loss(0): 0.0000\tClassification Loss: 1.4492\r\n",
      "Train Epoch: 30 [11264/209222 (5%)]\tAll Loss: 1.7529\tTriple Loss(1): 0.0792\tClassification Loss: 1.5944\r\n",
      "Train Epoch: 30 [11328/209222 (5%)]\tAll Loss: 1.4524\tTriple Loss(1): 0.0242\tClassification Loss: 1.4039\r\n",
      "Train Epoch: 30 [11392/209222 (5%)]\tAll Loss: 1.3577\tTriple Loss(1): 0.0405\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 30 [11456/209222 (5%)]\tAll Loss: 1.3911\tTriple Loss(1): 0.0537\tClassification Loss: 1.2838\r\n",
      "Train Epoch: 30 [11520/209222 (6%)]\tAll Loss: 1.6172\tTriple Loss(1): 0.0227\tClassification Loss: 1.5718\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral_Crochet_Shorts/img_00000015.jpg\r\n",
      "img_n: img/Double-Breasted_Blazer/img_00000115.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [11584/209222 (6%)]\tAll Loss: 1.5645\tTriple Loss(0): 0.0000\tClassification Loss: 1.5645\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Striped_Linen_Belted_Shorts/img_00000028.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [11648/209222 (6%)]\tAll Loss: 1.4360\tTriple Loss(0): 0.0000\tClassification Loss: 1.4360\r\n",
      "Train Epoch: 30 [11712/209222 (6%)]\tAll Loss: 1.3634\tTriple Loss(1): 0.0550\tClassification Loss: 1.2534\r\n",
      "Train Epoch: 30 [11776/209222 (6%)]\tAll Loss: 1.8104\tTriple Loss(1): 0.0734\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 30 [11840/209222 (6%)]\tAll Loss: 1.5402\tTriple Loss(1): 0.0284\tClassification Loss: 1.4834\r\n",
      "Train Epoch: 30 [11904/209222 (6%)]\tAll Loss: 1.2505\tTriple Loss(1): 0.0000\tClassification Loss: 1.2505\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Terry_Knit_Dolphin_Hem_Shorts/img_00000001.jpg\r\n",
      "img_n: img/Boucl&eacute;_Knit_Cardigan/img_00000023.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [11968/209222 (6%)]\tAll Loss: 1.3895\tTriple Loss(0): 0.0000\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 30 [12032/209222 (6%)]\tAll Loss: 1.3790\tTriple Loss(1): 0.0727\tClassification Loss: 1.2336\r\n",
      "Train Epoch: 30 [12096/209222 (6%)]\tAll Loss: 1.8284\tTriple Loss(1): 0.0699\tClassification Loss: 1.6886\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pintucked_High-Waisted_Shorts/img_00000038.jpg\r\n",
      "img_n: img/American_Flag_Sweater/img_00000018.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [12160/209222 (6%)]\tAll Loss: 3.0443\tTriple Loss(0): 0.8517\tClassification Loss: 1.3408\r\n",
      "Train Epoch: 30 [12224/209222 (6%)]\tAll Loss: 1.5086\tTriple Loss(1): 0.0490\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 30 [12288/209222 (6%)]\tAll Loss: 1.4150\tTriple Loss(1): 0.0574\tClassification Loss: 1.3002\r\n",
      "Train Epoch: 30 [12352/209222 (6%)]\tAll Loss: 1.2834\tTriple Loss(1): 0.0451\tClassification Loss: 1.1932\r\n",
      "Train Epoch: 30 [12416/209222 (6%)]\tAll Loss: 1.6859\tTriple Loss(1): 0.0162\tClassification Loss: 1.6534\r\n",
      "Train Epoch: 30 [12480/209222 (6%)]\tAll Loss: 1.8264\tTriple Loss(1): 0.0512\tClassification Loss: 1.7239\r\n",
      "Train Epoch: 30 [12544/209222 (6%)]\tAll Loss: 1.7331\tTriple Loss(1): 0.0269\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 30 [12608/209222 (6%)]\tAll Loss: 1.5963\tTriple Loss(1): 0.0172\tClassification Loss: 1.5619\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pleated_Chambray_Shorts/img_00000001.jpg\r\n",
      "img_n: img/Striped_Open-Front_Fringed_Poncho/img_00000007.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [12672/209222 (6%)]\tAll Loss: 1.4475\tTriple Loss(0): 0.0000\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 30 [12736/209222 (6%)]\tAll Loss: 1.5336\tTriple Loss(1): 0.0211\tClassification Loss: 1.4915\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Geo-Printed_Flat_Front_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Roman_Numeral_Graphic_Tee/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [12800/209222 (6%)]\tAll Loss: 1.3263\tTriple Loss(0): 0.0000\tClassification Loss: 1.3263\r\n",
      "Train Epoch: 30 [12864/209222 (6%)]\tAll Loss: 1.2432\tTriple Loss(1): 0.0000\tClassification Loss: 1.2432\r\n",
      "Train Epoch: 30 [12928/209222 (6%)]\tAll Loss: 1.9437\tTriple Loss(1): 0.0985\tClassification Loss: 1.7466\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Lace-Hem_Floral_PJ_Shorts/img_00000037.jpg\r\n",
      "img_n: img/Peter_Pan_Collar_Sweater/img_00000064.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [12992/209222 (6%)]\tAll Loss: 1.2730\tTriple Loss(0): 0.0000\tClassification Loss: 1.2730\r\n",
      "Train Epoch: 30 [13056/209222 (6%)]\tAll Loss: 1.5727\tTriple Loss(1): 0.0326\tClassification Loss: 1.5074\r\n",
      "Train Epoch: 30 [13120/209222 (6%)]\tAll Loss: 1.5426\tTriple Loss(1): 0.0115\tClassification Loss: 1.5196\r\n",
      "Train Epoch: 30 [13184/209222 (6%)]\tAll Loss: 1.7945\tTriple Loss(1): 0.0415\tClassification Loss: 1.7115\r\n",
      "Train Epoch: 30 [13248/209222 (6%)]\tAll Loss: 2.0076\tTriple Loss(1): 0.1116\tClassification Loss: 1.7843\r\n",
      "Train Epoch: 30 [13312/209222 (6%)]\tAll Loss: 1.6220\tTriple Loss(1): 0.0506\tClassification Loss: 1.5207\r\n",
      "Train Epoch: 30 [13376/209222 (6%)]\tAll Loss: 2.0470\tTriple Loss(1): 0.1419\tClassification Loss: 1.7631\r\n",
      "Train Epoch: 30 [13440/209222 (6%)]\tAll Loss: 1.7825\tTriple Loss(1): 0.0853\tClassification Loss: 1.6119\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Trouser_Shorts/img_00000027.jpg\r\n",
      "img_n: img/Classic_Baseball_Jersey/img_00000085.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [13504/209222 (6%)]\tAll Loss: 1.4330\tTriple Loss(0): 0.0000\tClassification Loss: 1.4330\r\n",
      "Train Epoch: 30 [13568/209222 (6%)]\tAll Loss: 1.5818\tTriple Loss(1): 0.0272\tClassification Loss: 1.5274\r\n",
      "Train Epoch: 30 [13632/209222 (7%)]\tAll Loss: 1.4585\tTriple Loss(1): 0.0165\tClassification Loss: 1.4255\r\n",
      "Train Epoch: 30 [13696/209222 (7%)]\tAll Loss: 1.5988\tTriple Loss(1): 0.0336\tClassification Loss: 1.5317\r\n",
      "Train Epoch: 30 [13760/209222 (7%)]\tAll Loss: 1.8652\tTriple Loss(1): 0.0947\tClassification Loss: 1.6758\r\n",
      "Train Epoch: 30 [13824/209222 (7%)]\tAll Loss: 1.9264\tTriple Loss(1): 0.0756\tClassification Loss: 1.7752\r\n",
      "Train Epoch: 30 [13888/209222 (7%)]\tAll Loss: 1.6124\tTriple Loss(1): 0.0629\tClassification Loss: 1.4867\r\n",
      "Train Epoch: 30 [13952/209222 (7%)]\tAll Loss: 1.5662\tTriple Loss(1): 0.0340\tClassification Loss: 1.4982\r\n",
      "Train Epoch: 30 [14016/209222 (7%)]\tAll Loss: 1.8448\tTriple Loss(1): 0.0810\tClassification Loss: 1.6827\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Polka_Dot_Denim_Shorts/img_00000048.jpg\r\n",
      "img_n: img/Classic_Halter_Top/img_00000051.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [14080/209222 (7%)]\tAll Loss: 1.0380\tTriple Loss(0): 0.0000\tClassification Loss: 1.0380\r\n",
      "Train Epoch: 30 [14144/209222 (7%)]\tAll Loss: 1.5851\tTriple Loss(1): 0.0813\tClassification Loss: 1.4225\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Rose_Print_Dolphin_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Striped_Muscle_Tee/img_00000011.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [14208/209222 (7%)]\tAll Loss: 1.1593\tTriple Loss(0): 0.0000\tClassification Loss: 1.1593\r\n",
      "Train Epoch: 30 [14272/209222 (7%)]\tAll Loss: 1.6786\tTriple Loss(1): 0.0747\tClassification Loss: 1.5292\r\n",
      "Train Epoch: 30 [14336/209222 (7%)]\tAll Loss: 1.4246\tTriple Loss(1): 0.1010\tClassification Loss: 1.2226\r\n",
      "Train Epoch: 30 [14400/209222 (7%)]\tAll Loss: 1.6666\tTriple Loss(1): 0.0401\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 30 [14464/209222 (7%)]\tAll Loss: 1.9032\tTriple Loss(1): 0.0107\tClassification Loss: 1.8818\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Luxe_Voyager_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Faux_Fur_Trimmed_Moto_Jacket/img_00000028.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [14528/209222 (7%)]\tAll Loss: 3.5274\tTriple Loss(0): 1.0412\tClassification Loss: 1.4450\r\n",
      "Train Epoch: 30 [14592/209222 (7%)]\tAll Loss: 1.6556\tTriple Loss(1): 0.0714\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 30 [14656/209222 (7%)]\tAll Loss: 1.9681\tTriple Loss(1): 0.2854\tClassification Loss: 1.3973\r\n",
      "Train Epoch: 30 [14720/209222 (7%)]\tAll Loss: 1.5245\tTriple Loss(1): 0.0000\tClassification Loss: 1.5245\r\n",
      "Train Epoch: 30 [14784/209222 (7%)]\tAll Loss: 1.5368\tTriple Loss(1): 0.0624\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 30 [14848/209222 (7%)]\tAll Loss: 1.6759\tTriple Loss(1): 0.0520\tClassification Loss: 1.5718\r\n",
      "Train Epoch: 30 [14912/209222 (7%)]\tAll Loss: 1.2376\tTriple Loss(1): 0.0166\tClassification Loss: 1.2045\r\n",
      "Train Epoch: 30 [14976/209222 (7%)]\tAll Loss: 1.6959\tTriple Loss(1): 0.0723\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 30 [15040/209222 (7%)]\tAll Loss: 1.6473\tTriple Loss(1): 0.1271\tClassification Loss: 1.3931\r\n",
      "Train Epoch: 30 [15104/209222 (7%)]\tAll Loss: 1.8046\tTriple Loss(1): 0.0438\tClassification Loss: 1.7170\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/French_Terry_Drawstring_Shorts/img_00000029.jpg\r\n",
      "img_n: img/Colorblocked_Hooded_Knit_Henley/img_00000043.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [15168/209222 (7%)]\tAll Loss: 1.4916\tTriple Loss(0): 0.0000\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 30 [15232/209222 (7%)]\tAll Loss: 1.7139\tTriple Loss(1): 0.1120\tClassification Loss: 1.4900\r\n",
      "Train Epoch: 30 [15296/209222 (7%)]\tAll Loss: 1.4547\tTriple Loss(1): 0.1094\tClassification Loss: 1.2360\r\n",
      "Train Epoch: 30 [15360/209222 (7%)]\tAll Loss: 1.3715\tTriple Loss(1): 0.0541\tClassification Loss: 1.2632\r\n",
      "Train Epoch: 30 [15424/209222 (7%)]\tAll Loss: 1.5725\tTriple Loss(1): 0.0943\tClassification Loss: 1.3839\r\n",
      "Train Epoch: 30 [15488/209222 (7%)]\tAll Loss: 1.5361\tTriple Loss(1): 0.0787\tClassification Loss: 1.3787\r\n",
      "Train Epoch: 30 [15552/209222 (7%)]\tAll Loss: 1.8755\tTriple Loss(1): 0.0351\tClassification Loss: 1.8052\r\n",
      "Train Epoch: 30 [15616/209222 (7%)]\tAll Loss: 1.8894\tTriple Loss(1): 0.1048\tClassification Loss: 1.6798\r\n",
      "Train Epoch: 30 [15680/209222 (7%)]\tAll Loss: 1.3881\tTriple Loss(1): 0.0843\tClassification Loss: 1.2196\r\n",
      "Train Epoch: 30 [15744/209222 (8%)]\tAll Loss: 1.6213\tTriple Loss(1): 0.1073\tClassification Loss: 1.4066\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Southwestern_Print_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Hooded_Slub_Knit_Henley/img_00000007.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [15808/209222 (8%)]\tAll Loss: 1.4313\tTriple Loss(0): 0.0000\tClassification Loss: 1.4313\r\n",
      "Train Epoch: 30 [15872/209222 (8%)]\tAll Loss: 1.0928\tTriple Loss(1): 0.0960\tClassification Loss: 0.9008\r\n",
      "Train Epoch: 30 [15936/209222 (8%)]\tAll Loss: 1.4226\tTriple Loss(1): 0.0294\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 30 [16000/209222 (8%)]\tAll Loss: 1.5558\tTriple Loss(1): 0.0383\tClassification Loss: 1.4792\r\n",
      "Train Epoch: 30 [16064/209222 (8%)]\tAll Loss: 1.1848\tTriple Loss(1): 0.0321\tClassification Loss: 1.1206\r\n",
      "Train Epoch: 30 [16128/209222 (8%)]\tAll Loss: 1.5912\tTriple Loss(1): 0.0875\tClassification Loss: 1.4162\r\n",
      "Train Epoch: 30 [16192/209222 (8%)]\tAll Loss: 1.3488\tTriple Loss(1): 0.0093\tClassification Loss: 1.3302\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Print_Pleated_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000083.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [16256/209222 (8%)]\tAll Loss: 1.3374\tTriple Loss(0): 0.0000\tClassification Loss: 1.3374\r\n",
      "Train Epoch: 30 [16320/209222 (8%)]\tAll Loss: 1.6650\tTriple Loss(1): 0.1574\tClassification Loss: 1.3502\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Control_Sector_Reversible_Drawstring_Shorts/img_00000007.jpg\r\n",
      "img_n: img/BAE_Graphic_Tee/img_00000036.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [16384/209222 (8%)]\tAll Loss: 1.2556\tTriple Loss(0): 0.0000\tClassification Loss: 1.2556\r\n",
      "Train Epoch: 30 [16448/209222 (8%)]\tAll Loss: 1.4453\tTriple Loss(1): 0.0000\tClassification Loss: 1.4453\r\n",
      "Train Epoch: 30 [16512/209222 (8%)]\tAll Loss: 1.4052\tTriple Loss(1): 0.0705\tClassification Loss: 1.2642\r\n",
      "Train Epoch: 30 [16576/209222 (8%)]\tAll Loss: 1.3783\tTriple Loss(1): 0.0709\tClassification Loss: 1.2365\r\n",
      "Train Epoch: 30 [16640/209222 (8%)]\tAll Loss: 1.5355\tTriple Loss(1): 0.0983\tClassification Loss: 1.3389\r\n",
      "Train Epoch: 30 [16704/209222 (8%)]\tAll Loss: 1.8512\tTriple Loss(1): 0.1918\tClassification Loss: 1.4675\r\n",
      "Train Epoch: 30 [16768/209222 (8%)]\tAll Loss: 1.3484\tTriple Loss(1): 0.0033\tClassification Loss: 1.3418\r\n",
      "Train Epoch: 30 [16832/209222 (8%)]\tAll Loss: 1.5797\tTriple Loss(1): 0.1200\tClassification Loss: 1.3398\r\n",
      "Train Epoch: 30 [16896/209222 (8%)]\tAll Loss: 1.8428\tTriple Loss(1): 0.0528\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 30 [16960/209222 (8%)]\tAll Loss: 1.8122\tTriple Loss(1): 0.1915\tClassification Loss: 1.4292\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Colorblocked_Workout_Shorts/img_00000009.jpg\r\n",
      "img_n: img/Varsity-Striped_Crew_Neck_Sweater/img_00000034.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [17024/209222 (8%)]\tAll Loss: 1.5448\tTriple Loss(0): 0.0000\tClassification Loss: 1.5448\r\n",
      "Train Epoch: 30 [17088/209222 (8%)]\tAll Loss: 1.6193\tTriple Loss(1): 0.0615\tClassification Loss: 1.4963\r\n",
      "Train Epoch: 30 [17152/209222 (8%)]\tAll Loss: 1.5256\tTriple Loss(1): 0.0396\tClassification Loss: 1.4465\r\n",
      "Train Epoch: 30 [17216/209222 (8%)]\tAll Loss: 1.7160\tTriple Loss(1): 0.0562\tClassification Loss: 1.6036\r\n",
      "Train Epoch: 30 [17280/209222 (8%)]\tAll Loss: 1.2012\tTriple Loss(1): 0.0427\tClassification Loss: 1.1157\r\n",
      "Train Epoch: 30 [17344/209222 (8%)]\tAll Loss: 1.6540\tTriple Loss(1): 0.0633\tClassification Loss: 1.5273\r\n",
      "Train Epoch: 30 [17408/209222 (8%)]\tAll Loss: 1.2618\tTriple Loss(1): 0.0013\tClassification Loss: 1.2591\r\n",
      "Train Epoch: 30 [17472/209222 (8%)]\tAll Loss: 1.6656\tTriple Loss(1): 0.0631\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 30 [17536/209222 (8%)]\tAll Loss: 1.1990\tTriple Loss(1): 0.0814\tClassification Loss: 1.0362\r\n",
      "Train Epoch: 30 [17600/209222 (8%)]\tAll Loss: 1.7383\tTriple Loss(1): 0.0804\tClassification Loss: 1.5776\r\n",
      "Train Epoch: 30 [17664/209222 (8%)]\tAll Loss: 1.9324\tTriple Loss(1): 0.0210\tClassification Loss: 1.8903\r\n",
      "Train Epoch: 30 [17728/209222 (8%)]\tAll Loss: 1.1674\tTriple Loss(1): 0.0198\tClassification Loss: 1.1277\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tropical_Hibiscus_Drawstring_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Essential_Raglan_Tee/img_00000005.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [17792/209222 (9%)]\tAll Loss: 1.3025\tTriple Loss(0): 0.0000\tClassification Loss: 1.3025\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Voyager_Shorts/img_00000045.jpg\r\n",
      "img_n: img/Corduroy_Hooded_Parka/img_00000048.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [17856/209222 (9%)]\tAll Loss: 1.2929\tTriple Loss(0): 0.0000\tClassification Loss: 1.2929\r\n",
      "Train Epoch: 30 [17920/209222 (9%)]\tAll Loss: 1.5869\tTriple Loss(1): 0.0753\tClassification Loss: 1.4364\r\n",
      "Train Epoch: 30 [17984/209222 (9%)]\tAll Loss: 1.7192\tTriple Loss(1): 0.0141\tClassification Loss: 1.6911\r\n",
      "Train Epoch: 30 [18048/209222 (9%)]\tAll Loss: 1.4981\tTriple Loss(1): 0.0625\tClassification Loss: 1.3732\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Flat-Front_Woven_Shorts/img_00000031.jpg\r\n",
      "img_n: img/Cropped_Ribbed_Knit_Turtleneck/img_00000003.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [18112/209222 (9%)]\tAll Loss: 1.0027\tTriple Loss(0): 0.0000\tClassification Loss: 1.0027\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Life_in_Progress_Chambray_Dolphin_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Lace-Up_Striped_Baja_Hoodie/img_00000012.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [18176/209222 (9%)]\tAll Loss: 1.5833\tTriple Loss(0): 0.0000\tClassification Loss: 1.5833\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Zippered_Faux_Leather_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Fringed_Stripe_Poncho/img_00000018.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [18240/209222 (9%)]\tAll Loss: 1.6828\tTriple Loss(0): 0.0000\tClassification Loss: 1.6828\r\n",
      "Train Epoch: 30 [18304/209222 (9%)]\tAll Loss: 1.7022\tTriple Loss(1): 0.0708\tClassification Loss: 1.5606\r\n",
      "Train Epoch: 30 [18368/209222 (9%)]\tAll Loss: 1.6297\tTriple Loss(1): 0.0703\tClassification Loss: 1.4892\r\n",
      "Train Epoch: 30 [18432/209222 (9%)]\tAll Loss: 1.5290\tTriple Loss(1): 0.0027\tClassification Loss: 1.5237\r\n",
      "Train Epoch: 30 [18496/209222 (9%)]\tAll Loss: 1.5547\tTriple Loss(1): 0.0000\tClassification Loss: 1.5547\r\n",
      "Train Epoch: 30 [18560/209222 (9%)]\tAll Loss: 1.5458\tTriple Loss(1): 0.0066\tClassification Loss: 1.5326\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Safari_Animal_Print_Shorts/img_00000036.jpg\r\n",
      "img_n: img/Abstract_Geo_Chiffon_Tank/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [18624/209222 (9%)]\tAll Loss: 1.6928\tTriple Loss(0): 0.0000\tClassification Loss: 1.6928\r\n",
      "Train Epoch: 30 [18688/209222 (9%)]\tAll Loss: 1.5909\tTriple Loss(1): 0.0207\tClassification Loss: 1.5495\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Button_Fly_Denim_Shorts/img_00000039.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000127.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [18752/209222 (9%)]\tAll Loss: 4.5848\tTriple Loss(0): 1.6176\tClassification Loss: 1.3497\r\n",
      "Train Epoch: 30 [18816/209222 (9%)]\tAll Loss: 1.8440\tTriple Loss(1): 0.0497\tClassification Loss: 1.7446\r\n",
      "Train Epoch: 30 [18880/209222 (9%)]\tAll Loss: 1.5992\tTriple Loss(1): 0.0685\tClassification Loss: 1.4621\r\n",
      "Train Epoch: 30 [18944/209222 (9%)]\tAll Loss: 1.7524\tTriple Loss(1): 0.0857\tClassification Loss: 1.5809\r\n",
      "Train Epoch: 30 [19008/209222 (9%)]\tAll Loss: 1.1923\tTriple Loss(1): 0.0337\tClassification Loss: 1.1249\r\n",
      "Train Epoch: 30 [19072/209222 (9%)]\tAll Loss: 1.5258\tTriple Loss(1): 0.0600\tClassification Loss: 1.4057\r\n",
      "Train Epoch: 30 [19136/209222 (9%)]\tAll Loss: 1.5214\tTriple Loss(1): 0.0329\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 30 [19200/209222 (9%)]\tAll Loss: 1.3665\tTriple Loss(1): 0.0270\tClassification Loss: 1.3125\r\n",
      "Train Epoch: 30 [19264/209222 (9%)]\tAll Loss: 1.3759\tTriple Loss(1): 0.0695\tClassification Loss: 1.2368\r\n",
      "Train Epoch: 30 [19328/209222 (9%)]\tAll Loss: 1.5251\tTriple Loss(1): 0.0876\tClassification Loss: 1.3500\r\n",
      "Train Epoch: 30 [19392/209222 (9%)]\tAll Loss: 1.4453\tTriple Loss(1): 0.0434\tClassification Loss: 1.3584\r\n",
      "Train Epoch: 30 [19456/209222 (9%)]\tAll Loss: 1.7679\tTriple Loss(1): 0.0000\tClassification Loss: 1.7679\r\n",
      "Train Epoch: 30 [19520/209222 (9%)]\tAll Loss: 1.6357\tTriple Loss(1): 0.1082\tClassification Loss: 1.4193\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/High-Waist_Denim_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Open-Front_Shawl_Cardigan/img_00000052.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [19584/209222 (9%)]\tAll Loss: 1.9030\tTriple Loss(0): 0.0000\tClassification Loss: 1.9030\r\n",
      "Train Epoch: 30 [19648/209222 (9%)]\tAll Loss: 1.5686\tTriple Loss(1): 0.0353\tClassification Loss: 1.4979\r\n",
      "Train Epoch: 30 [19712/209222 (9%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0274\tClassification Loss: 1.5357\r\n",
      "Train Epoch: 30 [19776/209222 (9%)]\tAll Loss: 1.6647\tTriple Loss(1): 0.0128\tClassification Loss: 1.6391\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/High-Waisted_Tweed_Shorts/img_00000052.jpg\r\n",
      "img_n: img/Classic_Collared_Button-Down/img_00000092.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [19840/209222 (9%)]\tAll Loss: 1.4498\tTriple Loss(0): 0.0000\tClassification Loss: 1.4498\r\n",
      "Train Epoch: 30 [19904/209222 (10%)]\tAll Loss: 1.4122\tTriple Loss(1): 0.0725\tClassification Loss: 1.2672\r\n",
      "Train Epoch: 30 [19968/209222 (10%)]\tAll Loss: 1.9443\tTriple Loss(1): 0.1036\tClassification Loss: 1.7372\r\n",
      "Train Epoch: 30 [20032/209222 (10%)]\tAll Loss: 1.5125\tTriple Loss(1): 0.0759\tClassification Loss: 1.3608\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Heart_Print_Denim_Shorts/img_00000059.jpg\r\n",
      "img_n: img/Zip_Collar_Bomber/img_00000034.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [20096/209222 (10%)]\tAll Loss: 1.4610\tTriple Loss(0): 0.0000\tClassification Loss: 1.4610\r\n",
      "Train Epoch: 30 [20160/209222 (10%)]\tAll Loss: 1.5575\tTriple Loss(1): 0.0305\tClassification Loss: 1.4965\r\n",
      "Train Epoch: 30 [20224/209222 (10%)]\tAll Loss: 1.4901\tTriple Loss(1): 0.0569\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 30 [20288/209222 (10%)]\tAll Loss: 1.2549\tTriple Loss(1): 0.0232\tClassification Loss: 1.2085\r\n",
      "Train Epoch: 30 [20352/209222 (10%)]\tAll Loss: 1.3831\tTriple Loss(1): 0.0459\tClassification Loss: 1.2914\r\n",
      "Train Epoch: 30 [20416/209222 (10%)]\tAll Loss: 1.9474\tTriple Loss(1): 0.0351\tClassification Loss: 1.8772\r\n",
      "Train Epoch: 30 [20480/209222 (10%)]\tAll Loss: 1.7795\tTriple Loss(1): 0.0000\tClassification Loss: 1.7795\r\n",
      "Train Epoch: 30 [20544/209222 (10%)]\tAll Loss: 1.1060\tTriple Loss(1): 0.0492\tClassification Loss: 1.0077\r\n",
      "Train Epoch: 30 [20608/209222 (10%)]\tAll Loss: 1.8319\tTriple Loss(1): 0.1048\tClassification Loss: 1.6223\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Two-Tone_Denim_Shorts/img_00000038.jpg\r\n",
      "img_n: img/Cami_Peplum_Top/img_00000042.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [20672/209222 (10%)]\tAll Loss: 1.2176\tTriple Loss(0): 0.0000\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 30 [20736/209222 (10%)]\tAll Loss: 1.4790\tTriple Loss(1): 0.0203\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 30 [20800/209222 (10%)]\tAll Loss: 1.5051\tTriple Loss(1): 0.0355\tClassification Loss: 1.4341\r\n",
      "Train Epoch: 30 [20864/209222 (10%)]\tAll Loss: 1.6403\tTriple Loss(1): 0.0243\tClassification Loss: 1.5916\r\n",
      "Train Epoch: 30 [20928/209222 (10%)]\tAll Loss: 1.5781\tTriple Loss(1): 0.1166\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 30 [20992/209222 (10%)]\tAll Loss: 1.2840\tTriple Loss(1): 0.0241\tClassification Loss: 1.2359\r\n",
      "Train Epoch: 30 [21056/209222 (10%)]\tAll Loss: 1.3781\tTriple Loss(1): 0.0100\tClassification Loss: 1.3581\r\n",
      "Train Epoch: 30 [21120/209222 (10%)]\tAll Loss: 1.6091\tTriple Loss(1): 0.0753\tClassification Loss: 1.4585\r\n",
      "Train Epoch: 30 [21184/209222 (10%)]\tAll Loss: 1.7837\tTriple Loss(1): 0.0560\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 30 [21248/209222 (10%)]\tAll Loss: 1.5979\tTriple Loss(1): 0.0945\tClassification Loss: 1.4089\r\n",
      "Train Epoch: 30 [21312/209222 (10%)]\tAll Loss: 1.6147\tTriple Loss(1): 0.0149\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 30 [21376/209222 (10%)]\tAll Loss: 1.6144\tTriple Loss(1): 0.0046\tClassification Loss: 1.6053\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral_Print_Tulip-Hem_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Simba_Graphic_Hoodie/img_00000009.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [21440/209222 (10%)]\tAll Loss: 1.4211\tTriple Loss(0): 0.0000\tClassification Loss: 1.4211\r\n",
      "Train Epoch: 30 [21504/209222 (10%)]\tAll Loss: 1.5534\tTriple Loss(1): 0.0736\tClassification Loss: 1.4063\r\n",
      "Train Epoch: 30 [21568/209222 (10%)]\tAll Loss: 1.8239\tTriple Loss(1): 0.0893\tClassification Loss: 1.6452\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Anchor_Print_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Tailored_Woven_Blazer/img_00000062.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [21632/209222 (10%)]\tAll Loss: 1.5144\tTriple Loss(0): 0.0000\tClassification Loss: 1.5144\r\n",
      "Train Epoch: 30 [21696/209222 (10%)]\tAll Loss: 1.3991\tTriple Loss(1): 0.0000\tClassification Loss: 1.3991\r\n",
      "Train Epoch: 30 [21760/209222 (10%)]\tAll Loss: 1.7302\tTriple Loss(1): 0.0742\tClassification Loss: 1.5818\r\n",
      "Train Epoch: 30 [21824/209222 (10%)]\tAll Loss: 1.6629\tTriple Loss(1): 0.0488\tClassification Loss: 1.5653\r\n",
      "Train Epoch: 30 [21888/209222 (10%)]\tAll Loss: 1.4184\tTriple Loss(1): 0.0178\tClassification Loss: 1.3829\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Marled_French_Terry_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Boxy_Plaid_Flannel/img_00000028.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [21952/209222 (10%)]\tAll Loss: 5.9911\tTriple Loss(0): 2.2383\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 30 [22016/209222 (11%)]\tAll Loss: 1.5731\tTriple Loss(1): 0.0513\tClassification Loss: 1.4705\r\n",
      "Train Epoch: 30 [22080/209222 (11%)]\tAll Loss: 1.6006\tTriple Loss(1): 0.0636\tClassification Loss: 1.4734\r\n",
      "Train Epoch: 30 [22144/209222 (11%)]\tAll Loss: 1.5571\tTriple Loss(1): 0.0780\tClassification Loss: 1.4012\r\n",
      "Train Epoch: 30 [22208/209222 (11%)]\tAll Loss: 1.8059\tTriple Loss(1): 0.1173\tClassification Loss: 1.5713\r\n",
      "Train Epoch: 30 [22272/209222 (11%)]\tAll Loss: 1.5678\tTriple Loss(1): 0.0143\tClassification Loss: 1.5392\r\n",
      "Train Epoch: 30 [22336/209222 (11%)]\tAll Loss: 1.4517\tTriple Loss(1): 0.0445\tClassification Loss: 1.3628\r\n",
      "Train Epoch: 30 [22400/209222 (11%)]\tAll Loss: 1.6087\tTriple Loss(1): 0.0177\tClassification Loss: 1.5733\r\n",
      "Train Epoch: 30 [22464/209222 (11%)]\tAll Loss: 1.6205\tTriple Loss(1): 0.0347\tClassification Loss: 1.5511\r\n",
      "Train Epoch: 30 [22528/209222 (11%)]\tAll Loss: 1.5493\tTriple Loss(1): 0.0193\tClassification Loss: 1.5108\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Sequined_Drawstring_Shorts/img_00000052.jpg\r\n",
      "img_n: img/Quilted_Faux_Leather_Bomber/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [22592/209222 (11%)]\tAll Loss: 1.3392\tTriple Loss(0): 0.0000\tClassification Loss: 1.3392\r\n",
      "Train Epoch: 30 [22656/209222 (11%)]\tAll Loss: 1.2756\tTriple Loss(1): 0.0817\tClassification Loss: 1.1121\r\n",
      "Train Epoch: 30 [22720/209222 (11%)]\tAll Loss: 1.2930\tTriple Loss(1): 0.0255\tClassification Loss: 1.2420\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet-Hemmed_Denim_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Classic_Knit_Turtleneck/img_00000012.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [22784/209222 (11%)]\tAll Loss: 1.3928\tTriple Loss(0): 0.0000\tClassification Loss: 1.3928\r\n",
      "Train Epoch: 30 [22848/209222 (11%)]\tAll Loss: 1.2382\tTriple Loss(1): 0.0768\tClassification Loss: 1.0846\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Smocked_Ruffle-Trim_Shorts/img_00000039.jpg\r\n",
      "img_n: img/Classic_Cropped_Sweater/img_00000034.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [22912/209222 (11%)]\tAll Loss: 8.2323\tTriple Loss(0): 3.4575\tClassification Loss: 1.3173\r\n",
      "Train Epoch: 30 [22976/209222 (11%)]\tAll Loss: 1.6574\tTriple Loss(1): 0.1437\tClassification Loss: 1.3700\r\n",
      "Train Epoch: 30 [23040/209222 (11%)]\tAll Loss: 1.6519\tTriple Loss(1): 0.0175\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 30 [23104/209222 (11%)]\tAll Loss: 1.4657\tTriple Loss(1): 0.0457\tClassification Loss: 1.3744\r\n",
      "Train Epoch: 30 [23168/209222 (11%)]\tAll Loss: 1.5617\tTriple Loss(1): 0.0642\tClassification Loss: 1.4333\r\n",
      "Train Epoch: 30 [23232/209222 (11%)]\tAll Loss: 1.7312\tTriple Loss(1): 0.0797\tClassification Loss: 1.5718\r\n",
      "Train Epoch: 30 [23296/209222 (11%)]\tAll Loss: 1.6103\tTriple Loss(1): 0.0579\tClassification Loss: 1.4945\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Eyelash_Lace-Trimmed_Shorts/img_00000041.jpg\r\n",
      "img_n: img/Striped_Knit_Pullover_Hoodie/img_00000054.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [23360/209222 (11%)]\tAll Loss: 1.3581\tTriple Loss(0): 0.0000\tClassification Loss: 1.3581\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Button_Cuffed_Denim_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Classic_Denim_Blazer/img_00000066.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [23424/209222 (11%)]\tAll Loss: 2.5301\tTriple Loss(0): 0.4934\tClassification Loss: 1.5433\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Nautical_Print_Shorts/img_00000037.jpg\r\n",
      "img_n: img/Asymmetrical_Dolman_Sweater/img_00000001.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [23488/209222 (11%)]\tAll Loss: 1.6218\tTriple Loss(0): 0.0000\tClassification Loss: 1.6218\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Kangaroo_Pocket_Heathered_Shorts/img_00000037.jpg\r\n",
      "img_n: img/Southwestern_Print_Henley_Hoodie/img_00000038.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [23552/209222 (11%)]\tAll Loss: 1.6769\tTriple Loss(0): 0.0000\tClassification Loss: 1.6769\r\n",
      "Train Epoch: 30 [23616/209222 (11%)]\tAll Loss: 1.3218\tTriple Loss(1): 0.0227\tClassification Loss: 1.2763\r\n",
      "Train Epoch: 30 [23680/209222 (11%)]\tAll Loss: 1.4630\tTriple Loss(1): 0.0452\tClassification Loss: 1.3727\r\n",
      "Train Epoch: 30 [23744/209222 (11%)]\tAll Loss: 1.7498\tTriple Loss(1): 0.0546\tClassification Loss: 1.6405\r\n",
      "Train Epoch: 30 [23808/209222 (11%)]\tAll Loss: 1.3015\tTriple Loss(1): 0.0702\tClassification Loss: 1.1612\r\n",
      "Train Epoch: 30 [23872/209222 (11%)]\tAll Loss: 1.4533\tTriple Loss(1): 0.0543\tClassification Loss: 1.3446\r\n",
      "Train Epoch: 30 [23936/209222 (11%)]\tAll Loss: 1.6053\tTriple Loss(1): 0.0839\tClassification Loss: 1.4375\r\n",
      "Train Epoch: 30 [24000/209222 (11%)]\tAll Loss: 1.4930\tTriple Loss(1): 0.0618\tClassification Loss: 1.3694\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet-Hem_Shorts/img_00000016.jpg\r\n",
      "img_n: img/Classic_Heathered_Sweater/img_00000024.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [24064/209222 (12%)]\tAll Loss: 1.5985\tTriple Loss(0): 0.0000\tClassification Loss: 1.5985\r\n",
      "Train Epoch: 30 [24128/209222 (12%)]\tAll Loss: 1.6904\tTriple Loss(1): 0.0685\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 30 [24192/209222 (12%)]\tAll Loss: 1.6264\tTriple Loss(1): 0.0272\tClassification Loss: 1.5719\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Genuine_Suede_Shorts/img_00000043.jpg\r\n",
      "img_n: img/Angel-Sleeved_Top/img_00000009.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [24256/209222 (12%)]\tAll Loss: 1.6921\tTriple Loss(0): 0.0000\tClassification Loss: 1.6921\r\n",
      "Train Epoch: 30 [24320/209222 (12%)]\tAll Loss: 1.6564\tTriple Loss(1): 0.0335\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 30 [24384/209222 (12%)]\tAll Loss: 1.7250\tTriple Loss(1): 0.0914\tClassification Loss: 1.5422\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tribal_Pattern_Sequin_Shorts/img_00000043.jpg\r\n",
      "img_n: img/Double-Breasted_Peacoat/img_00000064.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [24448/209222 (12%)]\tAll Loss: 1.4526\tTriple Loss(0): 0.0000\tClassification Loss: 1.4526\r\n",
      "Train Epoch: 30 [24512/209222 (12%)]\tAll Loss: 1.6120\tTriple Loss(1): 0.0643\tClassification Loss: 1.4835\r\n",
      "Train Epoch: 30 [24576/209222 (12%)]\tAll Loss: 1.6207\tTriple Loss(1): 0.0000\tClassification Loss: 1.6207\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Floral_Print_Drawstring_Shorts/img_00000007.jpg\r\n",
      "img_n: img/Tartan_Plaid_Button-Down/img_00000083.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [24640/209222 (12%)]\tAll Loss: 1.6418\tTriple Loss(0): 0.0000\tClassification Loss: 1.6418\r\n",
      "Train Epoch: 30 [24704/209222 (12%)]\tAll Loss: 1.8439\tTriple Loss(1): 0.0809\tClassification Loss: 1.6821\r\n",
      "Train Epoch: 30 [24768/209222 (12%)]\tAll Loss: 1.2881\tTriple Loss(1): 0.0191\tClassification Loss: 1.2498\r\n",
      "Train Epoch: 30 [24832/209222 (12%)]\tAll Loss: 1.7535\tTriple Loss(1): 0.1098\tClassification Loss: 1.5340\r\n",
      "Train Epoch: 30 [24896/209222 (12%)]\tAll Loss: 1.4511\tTriple Loss(1): 0.0439\tClassification Loss: 1.3634\r\n",
      "Train Epoch: 30 [24960/209222 (12%)]\tAll Loss: 1.6509\tTriple Loss(1): 0.0529\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 30 [25024/209222 (12%)]\tAll Loss: 1.7039\tTriple Loss(1): 0.0154\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 30 [25088/209222 (12%)]\tAll Loss: 1.3138\tTriple Loss(1): 0.0247\tClassification Loss: 1.2644\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet_Crepe_Woven_Shorts/img_00000021.jpg\r\n",
      "img_n: img/Double-Breasted_Drawstring_Parka/img_00000050.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [25152/209222 (12%)]\tAll Loss: 1.6997\tTriple Loss(0): 0.0000\tClassification Loss: 1.6997\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Medium_Wash_Distressed_Denim_Shorts/img_00000059.jpg\r\n",
      "img_n: img/Marled_Colorblock_Raglan_Hoodie/img_00000011.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [25216/209222 (12%)]\tAll Loss: 1.3007\tTriple Loss(0): 0.0000\tClassification Loss: 1.3007\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Metallic_Tribal-Inspired_Shorts/img_00000006.jpg\r\n",
      "img_n: img/Boxy_Plaid_Flannel/img_00000012.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [25280/209222 (12%)]\tAll Loss: 1.4399\tTriple Loss(0): 0.0000\tClassification Loss: 1.4399\r\n",
      "Train Epoch: 30 [25344/209222 (12%)]\tAll Loss: 1.6610\tTriple Loss(1): 0.0562\tClassification Loss: 1.5486\r\n",
      "Train Epoch: 30 [25408/209222 (12%)]\tAll Loss: 1.4024\tTriple Loss(1): 0.0231\tClassification Loss: 1.3562\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/High-Waisted_Tweed_Shorts/img_00000014.jpg\r\n",
      "img_n: img/Tartan_Plaid_Button-Down/img_00000036.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [25472/209222 (12%)]\tAll Loss: 1.3837\tTriple Loss(0): 0.0000\tClassification Loss: 1.3837\r\n",
      "Train Epoch: 30 [25536/209222 (12%)]\tAll Loss: 1.7361\tTriple Loss(1): 0.0123\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 30 [25600/209222 (12%)]\tAll Loss: 1.5224\tTriple Loss(1): 0.0132\tClassification Loss: 1.4960\r\n",
      "Train Epoch: 30 [25664/209222 (12%)]\tAll Loss: 1.7803\tTriple Loss(1): 0.0164\tClassification Loss: 1.7476\r\n",
      "Train Epoch: 30 [25728/209222 (12%)]\tAll Loss: 1.5442\tTriple Loss(1): 0.0154\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 30 [25792/209222 (12%)]\tAll Loss: 1.6296\tTriple Loss(1): 0.0158\tClassification Loss: 1.5980\r\n",
      "Train Epoch: 30 [25856/209222 (12%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.1310\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 30 [25920/209222 (12%)]\tAll Loss: 1.6116\tTriple Loss(1): 0.0077\tClassification Loss: 1.5963\r\n",
      "Train Epoch: 30 [25984/209222 (12%)]\tAll Loss: 1.8363\tTriple Loss(1): 0.0364\tClassification Loss: 1.7636\r\n",
      "Train Epoch: 30 [26048/209222 (12%)]\tAll Loss: 1.4579\tTriple Loss(1): 0.0317\tClassification Loss: 1.3944\r\n",
      "Train Epoch: 30 [26112/209222 (12%)]\tAll Loss: 1.6545\tTriple Loss(1): 0.0668\tClassification Loss: 1.5210\r\n",
      "Train Epoch: 30 [26176/209222 (13%)]\tAll Loss: 1.4330\tTriple Loss(1): 0.0582\tClassification Loss: 1.3167\r\n",
      "Train Epoch: 30 [26240/209222 (13%)]\tAll Loss: 1.5789\tTriple Loss(1): 0.0228\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 30 [26304/209222 (13%)]\tAll Loss: 1.4969\tTriple Loss(1): 0.0445\tClassification Loss: 1.4079\r\n",
      "Train Epoch: 30 [26368/209222 (13%)]\tAll Loss: 1.6279\tTriple Loss(1): 0.0356\tClassification Loss: 1.5566\r\n",
      "Train Epoch: 30 [26432/209222 (13%)]\tAll Loss: 1.5673\tTriple Loss(1): 0.0460\tClassification Loss: 1.4753\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crinkled_Denim_Shorts/img_00000035.jpg\r\n",
      "img_n: img/Double-Breasted_Peacoat/img_00000095.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [26496/209222 (13%)]\tAll Loss: 1.5686\tTriple Loss(0): 0.0000\tClassification Loss: 1.5686\r\n",
      "Train Epoch: 30 [26560/209222 (13%)]\tAll Loss: 1.9062\tTriple Loss(1): 0.0068\tClassification Loss: 1.8925\r\n",
      "Train Epoch: 30 [26624/209222 (13%)]\tAll Loss: 1.4850\tTriple Loss(1): 0.0411\tClassification Loss: 1.4028\r\n",
      "Train Epoch: 30 [26688/209222 (13%)]\tAll Loss: 1.7003\tTriple Loss(1): 0.0452\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 30 [26752/209222 (13%)]\tAll Loss: 1.4692\tTriple Loss(1): 0.0489\tClassification Loss: 1.3714\r\n",
      "Train Epoch: 30 [26816/209222 (13%)]\tAll Loss: 1.7017\tTriple Loss(1): 0.0052\tClassification Loss: 1.6914\r\n",
      "Train Epoch: 30 [26880/209222 (13%)]\tAll Loss: 1.6036\tTriple Loss(1): 0.0799\tClassification Loss: 1.4438\r\n",
      "Train Epoch: 30 [26944/209222 (13%)]\tAll Loss: 1.5164\tTriple Loss(1): 0.0402\tClassification Loss: 1.4361\r\n",
      "Train Epoch: 30 [27008/209222 (13%)]\tAll Loss: 1.9308\tTriple Loss(1): 0.1097\tClassification Loss: 1.7113\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Flat-Front_Shorts/img_00000043.jpg\r\n",
      "img_n: img/Boxy_Plaid_Flannel/img_00000008.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [27072/209222 (13%)]\tAll Loss: 1.3074\tTriple Loss(0): 0.0000\tClassification Loss: 1.3074\r\n",
      "Train Epoch: 30 [27136/209222 (13%)]\tAll Loss: 1.8055\tTriple Loss(1): 0.0128\tClassification Loss: 1.7800\r\n",
      "Train Epoch: 30 [27200/209222 (13%)]\tAll Loss: 1.7297\tTriple Loss(1): 0.0370\tClassification Loss: 1.6557\r\n",
      "Train Epoch: 30 [27264/209222 (13%)]\tAll Loss: 1.5863\tTriple Loss(1): 0.0550\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 30 [27328/209222 (13%)]\tAll Loss: 1.6291\tTriple Loss(1): 0.0615\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 30 [27392/209222 (13%)]\tAll Loss: 1.3954\tTriple Loss(1): 0.0210\tClassification Loss: 1.3535\r\n",
      "Train Epoch: 30 [27456/209222 (13%)]\tAll Loss: 1.4861\tTriple Loss(1): 0.0204\tClassification Loss: 1.4454\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_Linen-Blend_Shorts/img_00000026.jpg\r\n",
      "img_n: img/Classic_Baseball_Jersey/img_00000035.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [27520/209222 (13%)]\tAll Loss: 1.5749\tTriple Loss(0): 0.0000\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 30 [27584/209222 (13%)]\tAll Loss: 1.4490\tTriple Loss(1): 0.0796\tClassification Loss: 1.2897\r\n",
      "Train Epoch: 30 [27648/209222 (13%)]\tAll Loss: 1.6984\tTriple Loss(1): 0.0770\tClassification Loss: 1.5445\r\n",
      "Train Epoch: 30 [27712/209222 (13%)]\tAll Loss: 1.3051\tTriple Loss(1): 0.0643\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 30 [27776/209222 (13%)]\tAll Loss: 1.5116\tTriple Loss(1): 0.0158\tClassification Loss: 1.4800\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Print_Drawstring_Shorts/img_00000017.jpg\r\n",
      "img_n: img/Faux_Leather_Jersey/img_00000002.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [27840/209222 (13%)]\tAll Loss: 1.4961\tTriple Loss(0): 0.0000\tClassification Loss: 1.4961\r\n",
      "Train Epoch: 30 [27904/209222 (13%)]\tAll Loss: 1.5141\tTriple Loss(1): 0.0177\tClassification Loss: 1.4787\r\n",
      "Train Epoch: 30 [27968/209222 (13%)]\tAll Loss: 1.1853\tTriple Loss(1): 0.0091\tClassification Loss: 1.1672\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Chambray_Drawstring_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000008.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [28032/209222 (13%)]\tAll Loss: 1.5504\tTriple Loss(0): 0.0000\tClassification Loss: 1.5504\r\n",
      "Train Epoch: 30 [28096/209222 (13%)]\tAll Loss: 1.4528\tTriple Loss(1): 0.0875\tClassification Loss: 1.2778\r\n",
      "Train Epoch: 30 [28160/209222 (13%)]\tAll Loss: 1.5210\tTriple Loss(1): 0.0524\tClassification Loss: 1.4161\r\n",
      "Train Epoch: 30 [28224/209222 (13%)]\tAll Loss: 2.1190\tTriple Loss(1): 0.0799\tClassification Loss: 1.9592\r\n",
      "Train Epoch: 30 [28288/209222 (14%)]\tAll Loss: 1.5701\tTriple Loss(1): 0.0518\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 30 [28352/209222 (14%)]\tAll Loss: 1.5943\tTriple Loss(1): 0.0191\tClassification Loss: 1.5561\r\n",
      "Train Epoch: 30 [28416/209222 (14%)]\tAll Loss: 1.2906\tTriple Loss(1): 0.0111\tClassification Loss: 1.2685\r\n",
      "Train Epoch: 30 [28480/209222 (14%)]\tAll Loss: 1.4156\tTriple Loss(1): 0.0024\tClassification Loss: 1.4108\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Sequined_Drawstring_Shorts/img_00000052.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000103.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [28544/209222 (14%)]\tAll Loss: 3.6614\tTriple Loss(0): 1.0263\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 30 [28608/209222 (14%)]\tAll Loss: 1.2943\tTriple Loss(1): 0.0423\tClassification Loss: 1.2097\r\n",
      "Train Epoch: 30 [28672/209222 (14%)]\tAll Loss: 1.7898\tTriple Loss(1): 0.0018\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 30 [28736/209222 (14%)]\tAll Loss: 1.4914\tTriple Loss(1): 0.0834\tClassification Loss: 1.3245\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Southwestern_Print_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Cropped_Ribbed_Knit_Turtleneck/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [28800/209222 (14%)]\tAll Loss: 1.6079\tTriple Loss(0): 0.0000\tClassification Loss: 1.6079\r\n",
      "Train Epoch: 30 [28864/209222 (14%)]\tAll Loss: 1.3056\tTriple Loss(1): 0.0816\tClassification Loss: 1.1424\r\n",
      "Train Epoch: 30 [28928/209222 (14%)]\tAll Loss: 1.8501\tTriple Loss(1): 0.0640\tClassification Loss: 1.7222\r\n",
      "Train Epoch: 30 [28992/209222 (14%)]\tAll Loss: 1.2017\tTriple Loss(1): 0.0000\tClassification Loss: 1.2017\r\n",
      "Train Epoch: 30 [29056/209222 (14%)]\tAll Loss: 1.5063\tTriple Loss(1): 0.1029\tClassification Loss: 1.3005\r\n",
      "Train Epoch: 30 [29120/209222 (14%)]\tAll Loss: 1.4215\tTriple Loss(1): 0.0177\tClassification Loss: 1.3861\r\n",
      "Train Epoch: 30 [29184/209222 (14%)]\tAll Loss: 1.3348\tTriple Loss(1): 0.0255\tClassification Loss: 1.2838\r\n",
      "Train Epoch: 30 [29248/209222 (14%)]\tAll Loss: 1.4087\tTriple Loss(1): 0.0500\tClassification Loss: 1.3087\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Mesh_Drawstring_Shorts/img_00000055.jpg\r\n",
      "img_n: img/Open-Front_Zippered_Blazer/img_00000045.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [29312/209222 (14%)]\tAll Loss: 1.7703\tTriple Loss(0): 0.0000\tClassification Loss: 1.7703\r\n",
      "Train Epoch: 30 [29376/209222 (14%)]\tAll Loss: 1.4326\tTriple Loss(1): 0.0516\tClassification Loss: 1.3294\r\n",
      "Train Epoch: 30 [29440/209222 (14%)]\tAll Loss: 1.6043\tTriple Loss(1): 0.1449\tClassification Loss: 1.3146\r\n",
      "Train Epoch: 30 [29504/209222 (14%)]\tAll Loss: 1.5490\tTriple Loss(1): 0.1059\tClassification Loss: 1.3371\r\n",
      "Train Epoch: 30 [29568/209222 (14%)]\tAll Loss: 1.6453\tTriple Loss(1): 0.0136\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 30 [29632/209222 (14%)]\tAll Loss: 1.5428\tTriple Loss(1): 0.0141\tClassification Loss: 1.5146\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Nautical_Print_Shorts/img_00000038.jpg\r\n",
      "img_n: img/Woven_Open-Front_Blazer/img_00000044.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [29696/209222 (14%)]\tAll Loss: 1.5645\tTriple Loss(0): 0.0000\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 30 [29760/209222 (14%)]\tAll Loss: 1.9275\tTriple Loss(1): 0.0912\tClassification Loss: 1.7451\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet_Trim_Tribal_Print_Shorts/img_00000014.jpg\r\n",
      "img_n: img/L.A.T.H.C._Legend_Jersey/img_00000032.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [29824/209222 (14%)]\tAll Loss: 1.7509\tTriple Loss(0): 0.0000\tClassification Loss: 1.7509\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Distressed_Cuffed_Denim_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000017.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [29888/209222 (14%)]\tAll Loss: 1.8390\tTriple Loss(0): 0.0000\tClassification Loss: 1.8390\r\n",
      "Train Epoch: 30 [29952/209222 (14%)]\tAll Loss: 1.3429\tTriple Loss(1): 0.0171\tClassification Loss: 1.3087\r\n",
      "Train Epoch: 30 [30016/209222 (14%)]\tAll Loss: 1.8960\tTriple Loss(1): 0.0884\tClassification Loss: 1.7193\r\n",
      "Train Epoch: 30 [30080/209222 (14%)]\tAll Loss: 1.4483\tTriple Loss(1): 0.0648\tClassification Loss: 1.3187\r\n",
      "Train Epoch: 30 [30144/209222 (14%)]\tAll Loss: 1.4825\tTriple Loss(1): 0.0308\tClassification Loss: 1.4209\r\n",
      "Train Epoch: 30 [30208/209222 (14%)]\tAll Loss: 1.2897\tTriple Loss(1): 0.0125\tClassification Loss: 1.2647\r\n",
      "Train Epoch: 30 [30272/209222 (14%)]\tAll Loss: 1.8542\tTriple Loss(1): 0.0000\tClassification Loss: 1.8542\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tropical_Print_Shorts/img_00000046.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000001.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [30336/209222 (14%)]\tAll Loss: 1.5545\tTriple Loss(0): 0.0000\tClassification Loss: 1.5545\r\n",
      "Train Epoch: 30 [30400/209222 (15%)]\tAll Loss: 1.4212\tTriple Loss(1): 0.0341\tClassification Loss: 1.3530\r\n",
      "Train Epoch: 30 [30464/209222 (15%)]\tAll Loss: 1.6737\tTriple Loss(1): 0.0663\tClassification Loss: 1.5412\r\n",
      "Train Epoch: 30 [30528/209222 (15%)]\tAll Loss: 1.3574\tTriple Loss(1): 0.0463\tClassification Loss: 1.2648\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Self-Tie_Ruffled_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Marled_Knit_Zippered_Hoodie/img_00000031.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [30592/209222 (15%)]\tAll Loss: 1.6837\tTriple Loss(0): 0.0000\tClassification Loss: 1.6837\r\n",
      "Train Epoch: 30 [30656/209222 (15%)]\tAll Loss: 1.6446\tTriple Loss(1): 0.0746\tClassification Loss: 1.4954\r\n",
      "Train Epoch: 30 [30720/209222 (15%)]\tAll Loss: 1.6214\tTriple Loss(1): 0.0705\tClassification Loss: 1.4803\r\n",
      "Train Epoch: 30 [30784/209222 (15%)]\tAll Loss: 1.4021\tTriple Loss(1): 0.0028\tClassification Loss: 1.3966\r\n",
      "Train Epoch: 30 [30848/209222 (15%)]\tAll Loss: 1.6205\tTriple Loss(1): 0.0088\tClassification Loss: 1.6029\r\n",
      "Train Epoch: 30 [30912/209222 (15%)]\tAll Loss: 1.4607\tTriple Loss(1): 0.0521\tClassification Loss: 1.3565\r\n",
      "Train Epoch: 30 [30976/209222 (15%)]\tAll Loss: 2.0679\tTriple Loss(1): 0.1548\tClassification Loss: 1.7583\r\n",
      "Train Epoch: 30 [31040/209222 (15%)]\tAll Loss: 1.5399\tTriple Loss(1): 0.0765\tClassification Loss: 1.3868\r\n",
      "Train Epoch: 30 [31104/209222 (15%)]\tAll Loss: 1.5007\tTriple Loss(1): 0.0408\tClassification Loss: 1.4191\r\n",
      "Train Epoch: 30 [31168/209222 (15%)]\tAll Loss: 1.7293\tTriple Loss(1): 0.0248\tClassification Loss: 1.6797\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet_Trim_Tribal_Print_Shorts/img_00000027.jpg\r\n",
      "img_n: img/Longline_Raw-Cut_Hoodie/img_00000035.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [31232/209222 (15%)]\tAll Loss: 2.7087\tTriple Loss(0): 0.7156\tClassification Loss: 1.2776\r\n",
      "Train Epoch: 30 [31296/209222 (15%)]\tAll Loss: 1.5732\tTriple Loss(1): 0.1133\tClassification Loss: 1.3466\r\n",
      "Train Epoch: 30 [31360/209222 (15%)]\tAll Loss: 1.8124\tTriple Loss(1): 0.0358\tClassification Loss: 1.7408\r\n",
      "Train Epoch: 30 [31424/209222 (15%)]\tAll Loss: 1.2831\tTriple Loss(1): 0.0497\tClassification Loss: 1.1837\r\n",
      "Train Epoch: 30 [31488/209222 (15%)]\tAll Loss: 1.3081\tTriple Loss(1): 0.0635\tClassification Loss: 1.1811\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Painted_Floral_Denim_Shorts/img_00000019.jpg\r\n",
      "img_n: img/Cable_Knit_Mock-Neck_Sweater/img_00000003.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [31552/209222 (15%)]\tAll Loss: 1.4176\tTriple Loss(0): 0.0000\tClassification Loss: 1.4176\r\n",
      "Train Epoch: 30 [31616/209222 (15%)]\tAll Loss: 1.5062\tTriple Loss(1): 0.0803\tClassification Loss: 1.3456\r\n",
      "Train Epoch: 30 [31680/209222 (15%)]\tAll Loss: 1.2733\tTriple Loss(1): 0.0300\tClassification Loss: 1.2134\r\n",
      "Train Epoch: 30 [31744/209222 (15%)]\tAll Loss: 1.5565\tTriple Loss(1): 0.0817\tClassification Loss: 1.3930\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Space_Dye_Waist_Running_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Classic_Baseball_Jersey/img_00000067.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [31808/209222 (15%)]\tAll Loss: 1.5265\tTriple Loss(0): 0.0000\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 30 [31872/209222 (15%)]\tAll Loss: 1.6903\tTriple Loss(1): 0.0460\tClassification Loss: 1.5982\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Geo-Embroidered_Shorts/img_00000015.jpg\r\n",
      "img_n: img/Draped_Asymmetrical_Front_Jacket/img_00000048.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [31936/209222 (15%)]\tAll Loss: 1.5094\tTriple Loss(0): 0.0000\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 30 [32000/209222 (15%)]\tAll Loss: 1.5906\tTriple Loss(1): 0.0689\tClassification Loss: 1.4528\r\n",
      "Train Epoch: 30 [32064/209222 (15%)]\tAll Loss: 1.6561\tTriple Loss(1): 0.0413\tClassification Loss: 1.5736\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Distressed_Denim_Bermuda_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Abstract_Print_Halter_Crop_Top/img_00000022.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [32128/209222 (15%)]\tAll Loss: 1.5363\tTriple Loss(0): 0.0000\tClassification Loss: 1.5363\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tribal_Print_Drawstring_Shorts/img_00000005.jpg\r\n",
      "img_n: img/Hooded_Chenille_Cardigan/img_00000051.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [32192/209222 (15%)]\tAll Loss: 1.7307\tTriple Loss(0): 0.0000\tClassification Loss: 1.7307\r\n",
      "Train Epoch: 30 [32256/209222 (15%)]\tAll Loss: 1.5317\tTriple Loss(1): 0.0000\tClassification Loss: 1.5317\r\n",
      "Train Epoch: 30 [32320/209222 (15%)]\tAll Loss: 1.5867\tTriple Loss(1): 0.0445\tClassification Loss: 1.4978\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/High-Waisted_Knit_Shorts/img_00000042.jpg\r\n",
      "img_n: img/Favorite_Parka/img_00000053.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [32384/209222 (15%)]\tAll Loss: 1.4720\tTriple Loss(0): 0.0000\tClassification Loss: 1.4720\r\n",
      "Train Epoch: 30 [32448/209222 (16%)]\tAll Loss: 1.7316\tTriple Loss(1): 0.0388\tClassification Loss: 1.6540\r\n",
      "Train Epoch: 30 [32512/209222 (16%)]\tAll Loss: 1.7492\tTriple Loss(1): 0.0537\tClassification Loss: 1.6418\r\n",
      "Train Epoch: 30 [32576/209222 (16%)]\tAll Loss: 1.5331\tTriple Loss(1): 0.0929\tClassification Loss: 1.3473\r\n",
      "Train Epoch: 30 [32640/209222 (16%)]\tAll Loss: 1.6638\tTriple Loss(1): 0.0128\tClassification Loss: 1.6382\r\n",
      "Train Epoch: 30 [32704/209222 (16%)]\tAll Loss: 1.6640\tTriple Loss(1): 0.1465\tClassification Loss: 1.3710\r\n",
      "Train Epoch: 30 [32768/209222 (16%)]\tAll Loss: 1.5194\tTriple Loss(1): 0.0334\tClassification Loss: 1.4526\r\n",
      "Train Epoch: 30 [32832/209222 (16%)]\tAll Loss: 1.9069\tTriple Loss(1): 0.0373\tClassification Loss: 1.8323\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_Button-Front_Denim_Shorts/img_00000023.jpg\r\n",
      "img_n: img/Vegan_Leather-Sleeved_Bomber_Jacket/img_00000003.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [32896/209222 (16%)]\tAll Loss: 1.1643\tTriple Loss(0): 0.0000\tClassification Loss: 1.1643\r\n",
      "Train Epoch: 30 [32960/209222 (16%)]\tAll Loss: 1.8078\tTriple Loss(1): 0.0807\tClassification Loss: 1.6463\r\n",
      "Train Epoch: 30 [33024/209222 (16%)]\tAll Loss: 1.7129\tTriple Loss(1): 0.0340\tClassification Loss: 1.6449\r\n",
      "Train Epoch: 30 [33088/209222 (16%)]\tAll Loss: 1.4140\tTriple Loss(1): 0.0093\tClassification Loss: 1.3954\r\n",
      "Train Epoch: 30 [33152/209222 (16%)]\tAll Loss: 1.6079\tTriple Loss(1): 0.0741\tClassification Loss: 1.4598\r\n",
      "Train Epoch: 30 [33216/209222 (16%)]\tAll Loss: 2.1708\tTriple Loss(1): 0.0770\tClassification Loss: 2.0169\r\n",
      "Train Epoch: 30 [33280/209222 (16%)]\tAll Loss: 1.4617\tTriple Loss(1): 0.0762\tClassification Loss: 1.3093\r\n",
      "Train Epoch: 30 [33344/209222 (16%)]\tAll Loss: 1.7034\tTriple Loss(1): 0.0328\tClassification Loss: 1.6379\r\n",
      "Train Epoch: 30 [33408/209222 (16%)]\tAll Loss: 1.6048\tTriple Loss(1): 0.1026\tClassification Loss: 1.3996\r\n",
      "Train Epoch: 30 [33472/209222 (16%)]\tAll Loss: 1.4316\tTriple Loss(1): 0.0275\tClassification Loss: 1.3766\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Print_Running_Shorts/img_00000014.jpg\r\n",
      "img_n: img/Boat_Neck_Striped_Top/img_00000065.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [33536/209222 (16%)]\tAll Loss: 1.4497\tTriple Loss(0): 0.0000\tClassification Loss: 1.4497\r\n",
      "Train Epoch: 30 [33600/209222 (16%)]\tAll Loss: 1.5321\tTriple Loss(1): 0.0164\tClassification Loss: 1.4992\r\n",
      "Train Epoch: 30 [33664/209222 (16%)]\tAll Loss: 1.5453\tTriple Loss(1): 0.0767\tClassification Loss: 1.3919\r\n",
      "Train Epoch: 30 [33728/209222 (16%)]\tAll Loss: 1.6978\tTriple Loss(1): 0.0960\tClassification Loss: 1.5057\r\n",
      "Train Epoch: 30 [33792/209222 (16%)]\tAll Loss: 1.8504\tTriple Loss(1): 0.0604\tClassification Loss: 1.7296\r\n",
      "Train Epoch: 30 [33856/209222 (16%)]\tAll Loss: 1.3762\tTriple Loss(1): 0.0643\tClassification Loss: 1.2476\r\n",
      "Train Epoch: 30 [33920/209222 (16%)]\tAll Loss: 1.4489\tTriple Loss(1): 0.0063\tClassification Loss: 1.4363\r\n",
      "Train Epoch: 30 [33984/209222 (16%)]\tAll Loss: 1.6369\tTriple Loss(1): 0.0232\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 30 [34048/209222 (16%)]\tAll Loss: 1.4850\tTriple Loss(1): 0.0228\tClassification Loss: 1.4393\r\n",
      "Train Epoch: 30 [34112/209222 (16%)]\tAll Loss: 1.4655\tTriple Loss(1): 0.0334\tClassification Loss: 1.3988\r\n",
      "Train Epoch: 30 [34176/209222 (16%)]\tAll Loss: 1.7512\tTriple Loss(1): 0.0511\tClassification Loss: 1.6490\r\n",
      "Train Epoch: 30 [34240/209222 (16%)]\tAll Loss: 1.7817\tTriple Loss(1): 0.0651\tClassification Loss: 1.6514\r\n",
      "Train Epoch: 30 [34304/209222 (16%)]\tAll Loss: 1.4887\tTriple Loss(1): 0.1023\tClassification Loss: 1.2841\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Rainbow_Print_Running_Shorts/img_00000010.jpg\r\n",
      "img_n: img/Suede_Fringe_Jacket/img_00000037.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [34368/209222 (16%)]\tAll Loss: 1.6238\tTriple Loss(0): 0.0000\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 30 [34432/209222 (16%)]\tAll Loss: 1.4405\tTriple Loss(1): 0.0000\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 30 [34496/209222 (16%)]\tAll Loss: 1.4732\tTriple Loss(1): 0.0828\tClassification Loss: 1.3077\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pleated_Linen-Blend_Shorts/img_00000031.jpg\r\n",
      "img_n: img/Slub_Knit_Henley/img_00000011.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [34560/209222 (17%)]\tAll Loss: 1.3941\tTriple Loss(0): 0.0000\tClassification Loss: 1.3941\r\n",
      "Train Epoch: 30 [34624/209222 (17%)]\tAll Loss: 1.5446\tTriple Loss(1): 0.0988\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 30 [34688/209222 (17%)]\tAll Loss: 1.7685\tTriple Loss(1): 0.1150\tClassification Loss: 1.5385\r\n",
      "Train Epoch: 30 [34752/209222 (17%)]\tAll Loss: 1.2679\tTriple Loss(1): 0.0144\tClassification Loss: 1.2390\r\n",
      "Train Epoch: 30 [34816/209222 (17%)]\tAll Loss: 1.5969\tTriple Loss(1): 0.0563\tClassification Loss: 1.4843\r\n",
      "Train Epoch: 30 [34880/209222 (17%)]\tAll Loss: 1.8128\tTriple Loss(1): 0.0420\tClassification Loss: 1.7288\r\n",
      "Train Epoch: 30 [34944/209222 (17%)]\tAll Loss: 1.5577\tTriple Loss(1): 0.0158\tClassification Loss: 1.5260\r\n",
      "Train Epoch: 30 [35008/209222 (17%)]\tAll Loss: 1.4631\tTriple Loss(1): 0.0000\tClassification Loss: 1.4631\r\n",
      "Train Epoch: 30 [35072/209222 (17%)]\tAll Loss: 1.6069\tTriple Loss(1): 0.1074\tClassification Loss: 1.3920\r\n",
      "Train Epoch: 30 [35136/209222 (17%)]\tAll Loss: 1.6740\tTriple Loss(1): 0.0286\tClassification Loss: 1.6167\r\n",
      "Train Epoch: 30 [35200/209222 (17%)]\tAll Loss: 1.8140\tTriple Loss(1): 0.0162\tClassification Loss: 1.7816\r\n",
      "Train Epoch: 30 [35264/209222 (17%)]\tAll Loss: 1.3258\tTriple Loss(1): 0.0530\tClassification Loss: 1.2199\r\n",
      "Train Epoch: 30 [35328/209222 (17%)]\tAll Loss: 2.0734\tTriple Loss(1): 0.0708\tClassification Loss: 1.9317\r\n",
      "Train Epoch: 30 [35392/209222 (17%)]\tAll Loss: 1.6083\tTriple Loss(1): 0.0432\tClassification Loss: 1.5219\r\n",
      "Train Epoch: 30 [35456/209222 (17%)]\tAll Loss: 1.4614\tTriple Loss(1): 0.0572\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 30 [35520/209222 (17%)]\tAll Loss: 1.5031\tTriple Loss(1): 0.0281\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 30 [35584/209222 (17%)]\tAll Loss: 1.5057\tTriple Loss(1): 0.0852\tClassification Loss: 1.3354\r\n",
      "Train Epoch: 30 [35648/209222 (17%)]\tAll Loss: 1.5536\tTriple Loss(1): 0.0844\tClassification Loss: 1.3848\r\n",
      "Train Epoch: 30 [35712/209222 (17%)]\tAll Loss: 1.5824\tTriple Loss(1): 0.0667\tClassification Loss: 1.4491\r\n",
      "Train Epoch: 30 [35776/209222 (17%)]\tAll Loss: 1.6469\tTriple Loss(1): 0.0587\tClassification Loss: 1.5294\r\n",
      "Train Epoch: 30 [35840/209222 (17%)]\tAll Loss: 1.5195\tTriple Loss(1): 0.0681\tClassification Loss: 1.3833\r\n",
      "Train Epoch: 30 [35904/209222 (17%)]\tAll Loss: 1.2193\tTriple Loss(1): 0.0000\tClassification Loss: 1.2193\r\n",
      "Train Epoch: 30 [35968/209222 (17%)]\tAll Loss: 1.4812\tTriple Loss(1): 0.0395\tClassification Loss: 1.4023\r\n",
      "Train Epoch: 30 [36032/209222 (17%)]\tAll Loss: 1.4885\tTriple Loss(1): 0.1001\tClassification Loss: 1.2882\r\n",
      "Train Epoch: 30 [36096/209222 (17%)]\tAll Loss: 1.2407\tTriple Loss(1): 0.0058\tClassification Loss: 1.2292\r\n",
      "Train Epoch: 30 [36160/209222 (17%)]\tAll Loss: 1.7523\tTriple Loss(1): 0.0965\tClassification Loss: 1.5592\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Rainbow_Print_Running_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000004.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [36224/209222 (17%)]\tAll Loss: 1.6092\tTriple Loss(0): 0.0000\tClassification Loss: 1.6092\r\n",
      "Train Epoch: 30 [36288/209222 (17%)]\tAll Loss: 1.5830\tTriple Loss(1): 0.0261\tClassification Loss: 1.5308\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Stretch-Knit_Shorts/img_00000019.jpg\r\n",
      "img_n: img/Striped_Knit_Pullover_Hoodie/img_00000024.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [36352/209222 (17%)]\tAll Loss: 1.9350\tTriple Loss(0): 0.0000\tClassification Loss: 1.9350\r\n",
      "Train Epoch: 30 [36416/209222 (17%)]\tAll Loss: 1.5418\tTriple Loss(1): 0.0228\tClassification Loss: 1.4961\r\n",
      "Train Epoch: 30 [36480/209222 (17%)]\tAll Loss: 1.3778\tTriple Loss(1): 0.0201\tClassification Loss: 1.3376\r\n",
      "Train Epoch: 30 [36544/209222 (17%)]\tAll Loss: 1.8621\tTriple Loss(1): 0.1082\tClassification Loss: 1.6456\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet_Trim_Tribal_Print_Shorts/img_00000033.jpg\r\n",
      "img_n: img/Classic_Knit_Turtleneck/img_00000016.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [36608/209222 (17%)]\tAll Loss: 1.3529\tTriple Loss(0): 0.0000\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 30 [36672/209222 (18%)]\tAll Loss: 1.5123\tTriple Loss(1): 0.0364\tClassification Loss: 1.4394\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Distressed_Paint-Flecked_Denim_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Dotted_Bow_Blouse/img_00000005.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [36736/209222 (18%)]\tAll Loss: 1.4631\tTriple Loss(0): 0.0000\tClassification Loss: 1.4631\r\n",
      "Train Epoch: 30 [36800/209222 (18%)]\tAll Loss: 1.5910\tTriple Loss(1): 0.0706\tClassification Loss: 1.4498\r\n",
      "Train Epoch: 30 [36864/209222 (18%)]\tAll Loss: 1.7080\tTriple Loss(1): 0.1157\tClassification Loss: 1.4767\r\n",
      "Train Epoch: 30 [36928/209222 (18%)]\tAll Loss: 1.5324\tTriple Loss(1): 0.0429\tClassification Loss: 1.4466\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Betty_Boop_PJ_Shorts/img_00000025.jpg\r\n",
      "img_n: img/Reason_Tropical_Print_Baseball_Jersey/img_00000038.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [36992/209222 (18%)]\tAll Loss: 1.9939\tTriple Loss(0): 0.2318\tClassification Loss: 1.5304\r\n",
      "Train Epoch: 30 [37056/209222 (18%)]\tAll Loss: 1.6455\tTriple Loss(1): 0.0107\tClassification Loss: 1.6240\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_High-Waisted_Denim_Shorts/img_00000021.jpg\r\n",
      "img_n: img/Mirrored_Palm_Print_Tee/img_00000025.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [37120/209222 (18%)]\tAll Loss: 1.1381\tTriple Loss(0): 0.0000\tClassification Loss: 1.1381\r\n",
      "Train Epoch: 30 [37184/209222 (18%)]\tAll Loss: 1.5170\tTriple Loss(1): 0.0934\tClassification Loss: 1.3302\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Denim_Dolphin_Hem_Shorts/img_00000047.jpg\r\n",
      "img_n: img/Embroidered_Lace-Up_Poncho/img_00000002.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [37248/209222 (18%)]\tAll Loss: 1.5658\tTriple Loss(0): 0.0000\tClassification Loss: 1.5658\r\n",
      "Train Epoch: 30 [37312/209222 (18%)]\tAll Loss: 1.3631\tTriple Loss(1): 0.0744\tClassification Loss: 1.2144\r\n",
      "Train Epoch: 30 [37376/209222 (18%)]\tAll Loss: 1.3253\tTriple Loss(1): 0.0378\tClassification Loss: 1.2497\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Pleated_Baroque_Print_Shorts/img_00000008.jpg\r\n",
      "img_n: img/Double-Breasted_Peacoat/img_00000014.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [37440/209222 (18%)]\tAll Loss: 1.4389\tTriple Loss(0): 0.0000\tClassification Loss: 1.4389\r\n",
      "Train Epoch: 30 [37504/209222 (18%)]\tAll Loss: 1.5584\tTriple Loss(1): 0.0651\tClassification Loss: 1.4282\r\n",
      "Train Epoch: 30 [37568/209222 (18%)]\tAll Loss: 1.5642\tTriple Loss(1): 0.0353\tClassification Loss: 1.4935\r\n",
      "Train Epoch: 30 [37632/209222 (18%)]\tAll Loss: 1.5020\tTriple Loss(1): 0.0160\tClassification Loss: 1.4700\r\n",
      "Train Epoch: 30 [37696/209222 (18%)]\tAll Loss: 1.5037\tTriple Loss(1): 0.0335\tClassification Loss: 1.4368\r\n",
      "Train Epoch: 30 [37760/209222 (18%)]\tAll Loss: 1.4572\tTriple Loss(1): 0.0000\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 30 [37824/209222 (18%)]\tAll Loss: 1.7107\tTriple Loss(1): 0.0963\tClassification Loss: 1.5181\r\n",
      "Train Epoch: 30 [37888/209222 (18%)]\tAll Loss: 1.4602\tTriple Loss(1): 0.0883\tClassification Loss: 1.2836\r\n",
      "Train Epoch: 30 [37952/209222 (18%)]\tAll Loss: 1.6909\tTriple Loss(1): 0.0884\tClassification Loss: 1.5141\r\n",
      "Train Epoch: 30 [38016/209222 (18%)]\tAll Loss: 1.5131\tTriple Loss(1): 0.0287\tClassification Loss: 1.4556\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Sequined_Drawstring_Shorts/img_00000055.jpg\r\n",
      "img_n: img/Mixed_Knit_Cowl_Neck_Sweater/img_00000015.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [38080/209222 (18%)]\tAll Loss: 1.6752\tTriple Loss(0): 0.0000\tClassification Loss: 1.6752\r\n",
      "Train Epoch: 30 [38144/209222 (18%)]\tAll Loss: 1.2423\tTriple Loss(1): 0.0440\tClassification Loss: 1.1542\r\n",
      "Train Epoch: 30 [38208/209222 (18%)]\tAll Loss: 1.5158\tTriple Loss(1): 0.0270\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 30 [38272/209222 (18%)]\tAll Loss: 1.7008\tTriple Loss(1): 0.0155\tClassification Loss: 1.6698\r\n",
      "Train Epoch: 30 [38336/209222 (18%)]\tAll Loss: 1.5994\tTriple Loss(1): 0.0293\tClassification Loss: 1.5407\r\n",
      "Train Epoch: 30 [38400/209222 (18%)]\tAll Loss: 1.3971\tTriple Loss(1): 0.0609\tClassification Loss: 1.2754\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Baroque_Print_Shorts/img_00000016.jpg\r\n",
      "img_n: img/Hooded_Stripe_Poncho/img_00000017.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [38464/209222 (18%)]\tAll Loss: 1.5226\tTriple Loss(0): 0.0000\tClassification Loss: 1.5226\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Clear_Cut_High-Waisted_Shorts/img_00000001.jpg\r\n",
      "img_n: img/Tiered_Crochet_Tank/img_00000025.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [38528/209222 (18%)]\tAll Loss: 1.4580\tTriple Loss(0): 0.0000\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 30 [38592/209222 (18%)]\tAll Loss: 1.3647\tTriple Loss(1): 0.0182\tClassification Loss: 1.3284\r\n",
      "Train Epoch: 30 [38656/209222 (18%)]\tAll Loss: 1.6835\tTriple Loss(1): 0.0498\tClassification Loss: 1.5839\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Embroidered_Faux_Leather_Shorts/img_00000031.jpg\r\n",
      "img_n: img/Colorblocked_Open-Front_Cardigan/img_00000022.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [38720/209222 (19%)]\tAll Loss: 1.4229\tTriple Loss(0): 0.0000\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 30 [38784/209222 (19%)]\tAll Loss: 1.4980\tTriple Loss(1): 0.0723\tClassification Loss: 1.3534\r\n",
      "Train Epoch: 30 [38848/209222 (19%)]\tAll Loss: 1.3907\tTriple Loss(1): 0.0176\tClassification Loss: 1.3554\r\n",
      "Train Epoch: 30 [38912/209222 (19%)]\tAll Loss: 1.3597\tTriple Loss(1): 0.0295\tClassification Loss: 1.3008\r\n",
      "Train Epoch: 30 [38976/209222 (19%)]\tAll Loss: 1.4270\tTriple Loss(1): 0.0460\tClassification Loss: 1.3350\r\n",
      "Train Epoch: 30 [39040/209222 (19%)]\tAll Loss: 1.5335\tTriple Loss(1): 0.0699\tClassification Loss: 1.3937\r\n",
      "Train Epoch: 30 [39104/209222 (19%)]\tAll Loss: 1.7601\tTriple Loss(1): 0.0171\tClassification Loss: 1.7258\r\n",
      "Train Epoch: 30 [39168/209222 (19%)]\tAll Loss: 1.6343\tTriple Loss(1): 0.0359\tClassification Loss: 1.5625\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Textured_Polka_Dot_Shorts/img_00000001.jpg\r\n",
      "img_n: img/Striped_Chambray-Pocket_Henley/img_00000068.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [39232/209222 (19%)]\tAll Loss: 1.6316\tTriple Loss(0): 0.0000\tClassification Loss: 1.6316\r\n",
      "Train Epoch: 30 [39296/209222 (19%)]\tAll Loss: 1.7511\tTriple Loss(1): 0.0543\tClassification Loss: 1.6425\r\n",
      "Train Epoch: 30 [39360/209222 (19%)]\tAll Loss: 1.3977\tTriple Loss(1): 0.0000\tClassification Loss: 1.3977\r\n",
      "Train Epoch: 30 [39424/209222 (19%)]\tAll Loss: 1.6768\tTriple Loss(1): 0.0808\tClassification Loss: 1.5152\r\n",
      "Train Epoch: 30 [39488/209222 (19%)]\tAll Loss: 1.6870\tTriple Loss(1): 0.0398\tClassification Loss: 1.6074\r\n",
      "Train Epoch: 30 [39552/209222 (19%)]\tAll Loss: 1.8610\tTriple Loss(1): 0.0086\tClassification Loss: 1.8438\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Ruffled_Floral_Print_Shorts/img_00000037.jpg\r\n",
      "img_n: img/Slub_Knit_Henley/img_00000051.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [39616/209222 (19%)]\tAll Loss: 6.0080\tTriple Loss(0): 2.3509\tClassification Loss: 1.3062\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Antiqued_Floral_Denim_Shorts/img_00000018.jpg\r\n",
      "img_n: img/Raw-Cut_Tank/img_00000038.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [39680/209222 (19%)]\tAll Loss: 1.5614\tTriple Loss(0): 0.0000\tClassification Loss: 1.5614\r\n",
      "Train Epoch: 30 [39744/209222 (19%)]\tAll Loss: 1.4782\tTriple Loss(1): 0.0000\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 30 [39808/209222 (19%)]\tAll Loss: 1.4618\tTriple Loss(1): 0.0231\tClassification Loss: 1.4157\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Belted_Stripe_Print_Shorts/img_00000021.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000012.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [39872/209222 (19%)]\tAll Loss: 1.3419\tTriple Loss(0): 0.0000\tClassification Loss: 1.3419\r\n",
      "Train Epoch: 30 [39936/209222 (19%)]\tAll Loss: 1.4775\tTriple Loss(1): 0.0566\tClassification Loss: 1.3643\r\n",
      "Train Epoch: 30 [40000/209222 (19%)]\tAll Loss: 1.6609\tTriple Loss(1): 0.1068\tClassification Loss: 1.4474\r\n",
      "Train Epoch: 30 [40064/209222 (19%)]\tAll Loss: 1.6027\tTriple Loss(1): 0.0370\tClassification Loss: 1.5288\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Lace-Trimmed_Floral_Shorts/img_00000039.jpg\r\n",
      "img_n: img/Floral_Lace_Bomber_Jacket/img_00000001.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [40128/209222 (19%)]\tAll Loss: 1.1413\tTriple Loss(0): 0.0000\tClassification Loss: 1.1413\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Lacy_Pintucked_PJ_Shorts/img_00000012.jpg\r\n",
      "img_n: img/Drape-Front_Cardigan/img_00000066.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [40192/209222 (19%)]\tAll Loss: 1.2499\tTriple Loss(0): 0.0000\tClassification Loss: 1.2499\r\n",
      "Train Epoch: 30 [40256/209222 (19%)]\tAll Loss: 1.3804\tTriple Loss(1): 0.0201\tClassification Loss: 1.3402\r\n",
      "Train Epoch: 30 [40320/209222 (19%)]\tAll Loss: 1.7448\tTriple Loss(1): 0.0329\tClassification Loss: 1.6790\r\n",
      "Train Epoch: 30 [40384/209222 (19%)]\tAll Loss: 1.7510\tTriple Loss(1): 0.0379\tClassification Loss: 1.6752\r\n",
      "Train Epoch: 30 [40448/209222 (19%)]\tAll Loss: 1.4544\tTriple Loss(1): 0.0475\tClassification Loss: 1.3593\r\n",
      "Train Epoch: 30 [40512/209222 (19%)]\tAll Loss: 1.8964\tTriple Loss(1): 0.0258\tClassification Loss: 1.8449\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet-Trimmed_Shorts/img_00000031.jpg\r\n",
      "img_n: img/Floral_Eyelet_Tank/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [40576/209222 (19%)]\tAll Loss: 1.6339\tTriple Loss(0): 0.0000\tClassification Loss: 1.6339\r\n",
      "Train Epoch: 30 [40640/209222 (19%)]\tAll Loss: 1.4856\tTriple Loss(1): 0.0245\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 30 [40704/209222 (19%)]\tAll Loss: 1.6115\tTriple Loss(1): 0.0708\tClassification Loss: 1.4699\r\n",
      "Train Epoch: 30 [40768/209222 (19%)]\tAll Loss: 1.1444\tTriple Loss(1): 0.0533\tClassification Loss: 1.0379\r\n",
      "Train Epoch: 30 [40832/209222 (20%)]\tAll Loss: 1.4579\tTriple Loss(1): 0.0755\tClassification Loss: 1.3069\r\n",
      "Train Epoch: 30 [40896/209222 (20%)]\tAll Loss: 1.2571\tTriple Loss(1): 0.0283\tClassification Loss: 1.2005\r\n",
      "Train Epoch: 30 [40960/209222 (20%)]\tAll Loss: 1.4815\tTriple Loss(1): 0.0325\tClassification Loss: 1.4165\r\n",
      "Train Epoch: 30 [41024/209222 (20%)]\tAll Loss: 1.8777\tTriple Loss(1): 0.0522\tClassification Loss: 1.7733\r\n",
      "Train Epoch: 30 [41088/209222 (20%)]\tAll Loss: 1.8606\tTriple Loss(1): 0.1657\tClassification Loss: 1.5292\r\n",
      "Train Epoch: 30 [41152/209222 (20%)]\tAll Loss: 1.0592\tTriple Loss(1): 0.0254\tClassification Loss: 1.0083\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Control_Sector_Striped_Cotton-Blend_Shorts/img_00000023.jpg\r\n",
      "img_n: img/Double-Breasted_Peacoat/img_00000077.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [41216/209222 (20%)]\tAll Loss: 1.6345\tTriple Loss(0): 0.0000\tClassification Loss: 1.6345\r\n",
      "Train Epoch: 30 [41280/209222 (20%)]\tAll Loss: 1.4363\tTriple Loss(1): 0.0527\tClassification Loss: 1.3310\r\n",
      "Train Epoch: 30 [41344/209222 (20%)]\tAll Loss: 1.6224\tTriple Loss(1): 0.0655\tClassification Loss: 1.4915\r\n",
      "Train Epoch: 30 [41408/209222 (20%)]\tAll Loss: 1.6563\tTriple Loss(1): 0.0304\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 30 [41472/209222 (20%)]\tAll Loss: 1.5087\tTriple Loss(1): 0.0574\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 30 [41536/209222 (20%)]\tAll Loss: 1.2204\tTriple Loss(1): 0.0625\tClassification Loss: 1.0955\r\n",
      "Train Epoch: 30 [41600/209222 (20%)]\tAll Loss: 1.4320\tTriple Loss(1): 0.0000\tClassification Loss: 1.4320\r\n",
      "Train Epoch: 30 [41664/209222 (20%)]\tAll Loss: 1.9277\tTriple Loss(1): 0.0727\tClassification Loss: 1.7823\r\n",
      "Train Epoch: 30 [41728/209222 (20%)]\tAll Loss: 1.7883\tTriple Loss(1): 0.0325\tClassification Loss: 1.7233\r\n",
      "Train Epoch: 30 [41792/209222 (20%)]\tAll Loss: 1.4961\tTriple Loss(1): 0.0940\tClassification Loss: 1.3081\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_High-Waisted_Denim_Shorts/img_00000011.jpg\r\n",
      "img_n: img/Paisley_Print_Strappy_Halter/img_00000003.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [41856/209222 (20%)]\tAll Loss: 1.4312\tTriple Loss(0): 0.0000\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 30 [41920/209222 (20%)]\tAll Loss: 1.5841\tTriple Loss(1): 0.0636\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 30 [41984/209222 (20%)]\tAll Loss: 1.8631\tTriple Loss(1): 0.0733\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 30 [42048/209222 (20%)]\tAll Loss: 1.9971\tTriple Loss(1): 0.1106\tClassification Loss: 1.7759\r\n",
      "Train Epoch: 30 [42112/209222 (20%)]\tAll Loss: 1.5830\tTriple Loss(1): 0.0728\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 30 [42176/209222 (20%)]\tAll Loss: 1.4672\tTriple Loss(1): 0.0220\tClassification Loss: 1.4233\r\n",
      "Train Epoch: 30 [42240/209222 (20%)]\tAll Loss: 1.5264\tTriple Loss(1): 0.0452\tClassification Loss: 1.4360\r\n",
      "Train Epoch: 30 [42304/209222 (20%)]\tAll Loss: 1.3012\tTriple Loss(1): 0.0753\tClassification Loss: 1.1506\r\n",
      "Train Epoch: 30 [42368/209222 (20%)]\tAll Loss: 1.9207\tTriple Loss(1): 0.1784\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 30 [42432/209222 (20%)]\tAll Loss: 1.5968\tTriple Loss(1): 0.0690\tClassification Loss: 1.4588\r\n",
      "Train Epoch: 30 [42496/209222 (20%)]\tAll Loss: 1.2358\tTriple Loss(1): 0.0337\tClassification Loss: 1.1684\r\n",
      "Train Epoch: 30 [42560/209222 (20%)]\tAll Loss: 1.5971\tTriple Loss(1): 0.0738\tClassification Loss: 1.4495\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Geo_Girl_Dolphin_Shorts/img_00000019.jpg\r\n",
      "img_n: img/Rustic_Plaid_Flannel/img_00000004.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [42624/209222 (20%)]\tAll Loss: 1.4665\tTriple Loss(0): 0.0000\tClassification Loss: 1.4665\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Refined_Twill_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Striped_Open-Front_Fringed_Poncho/img_00000029.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [42688/209222 (20%)]\tAll Loss: 1.5691\tTriple Loss(0): 0.0000\tClassification Loss: 1.5691\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Cuffed_Drawstring_Denim_Shorts/img_00000017.jpg\r\n",
      "img_n: img/Reason_Rose_Paneled_Jersey/img_00000027.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [42752/209222 (20%)]\tAll Loss: 1.4094\tTriple Loss(0): 0.0000\tClassification Loss: 1.4094\r\n",
      "Train Epoch: 30 [42816/209222 (20%)]\tAll Loss: 1.4446\tTriple Loss(1): 0.0318\tClassification Loss: 1.3810\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Utility_Pocket_Shorts/img_00000027.jpg\r\n",
      "img_n: img/Faux_Fur-Lined_Parka/img_00000073.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [42880/209222 (20%)]\tAll Loss: 1.5111\tTriple Loss(0): 0.0000\tClassification Loss: 1.5111\r\n",
      "Train Epoch: 30 [42944/209222 (21%)]\tAll Loss: 1.8422\tTriple Loss(1): 0.0123\tClassification Loss: 1.8177\r\n",
      "Train Epoch: 30 [43008/209222 (21%)]\tAll Loss: 1.5152\tTriple Loss(1): 0.0001\tClassification Loss: 1.5150\r\n",
      "Train Epoch: 30 [43072/209222 (21%)]\tAll Loss: 1.5251\tTriple Loss(1): 0.0395\tClassification Loss: 1.4461\r\n",
      "Train Epoch: 30 [43136/209222 (21%)]\tAll Loss: 1.3407\tTriple Loss(1): 0.0111\tClassification Loss: 1.3185\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tropical_Print_Chino_Shorts/img_00000032.jpg\r\n",
      "img_n: img/Colorblocked_Hooded_Knit_Henley/img_00000020.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [43200/209222 (21%)]\tAll Loss: 3.1398\tTriple Loss(0): 0.6868\tClassification Loss: 1.7662\r\n",
      "Train Epoch: 30 [43264/209222 (21%)]\tAll Loss: 1.4249\tTriple Loss(1): 0.0124\tClassification Loss: 1.4001\r\n",
      "Train Epoch: 30 [43328/209222 (21%)]\tAll Loss: 1.6453\tTriple Loss(1): 0.0630\tClassification Loss: 1.5194\r\n",
      "Train Epoch: 30 [43392/209222 (21%)]\tAll Loss: 1.7500\tTriple Loss(1): 0.1286\tClassification Loss: 1.4928\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/High-Waisted_Workout_Shorts/img_00000050.jpg\r\n",
      "img_n: img/Scuba_Knit_Moto_Jacket/img_00000023.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [43456/209222 (21%)]\tAll Loss: 1.6134\tTriple Loss(0): 0.0000\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 30 [43520/209222 (21%)]\tAll Loss: 1.4037\tTriple Loss(1): 0.0081\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 30 [43584/209222 (21%)]\tAll Loss: 1.4054\tTriple Loss(1): 0.0293\tClassification Loss: 1.3468\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Kangaroo_Pocket_Heathered_Shorts/img_00000044.jpg\r\n",
      "img_n: img/Hooded_Bomber_Jacket/img_00000074.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [43648/209222 (21%)]\tAll Loss: 1.4953\tTriple Loss(0): 0.0000\tClassification Loss: 1.4953\r\n",
      "Train Epoch: 30 [43712/209222 (21%)]\tAll Loss: 1.7147\tTriple Loss(1): 0.0293\tClassification Loss: 1.6562\r\n",
      "Train Epoch: 30 [43776/209222 (21%)]\tAll Loss: 1.6431\tTriple Loss(1): 0.0581\tClassification Loss: 1.5270\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Tribal-Inspired_Chino_Shorts/img_00000013.jpg\r\n",
      "img_n: img/Classic_Open-Front_Blazer/img_00000033.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [43840/209222 (21%)]\tAll Loss: 1.3436\tTriple Loss(0): 0.0000\tClassification Loss: 1.3436\r\n",
      "Train Epoch: 30 [43904/209222 (21%)]\tAll Loss: 1.9349\tTriple Loss(1): 0.0028\tClassification Loss: 1.9292\r\n",
      "Train Epoch: 30 [43968/209222 (21%)]\tAll Loss: 1.6724\tTriple Loss(1): 0.0284\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 30 [44032/209222 (21%)]\tAll Loss: 1.7006\tTriple Loss(1): 0.0587\tClassification Loss: 1.5832\r\n",
      "Train Epoch: 30 [44096/209222 (21%)]\tAll Loss: 1.5362\tTriple Loss(1): 0.0000\tClassification Loss: 1.5362\r\n",
      "Train Epoch: 30 [44160/209222 (21%)]\tAll Loss: 1.8194\tTriple Loss(1): 0.1182\tClassification Loss: 1.5829\r\n",
      "Train Epoch: 30 [44224/209222 (21%)]\tAll Loss: 1.6691\tTriple Loss(1): 0.0522\tClassification Loss: 1.5646\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Drawstring_Palm_Print_Shorts/img_00000008.jpg\r\n",
      "img_n: img/Ruffled_Floral_Blouse/img_00000035.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [44288/209222 (21%)]\tAll Loss: 1.4884\tTriple Loss(0): 0.0000\tClassification Loss: 1.4884\r\n",
      "Train Epoch: 30 [44352/209222 (21%)]\tAll Loss: 1.2847\tTriple Loss(1): 0.0442\tClassification Loss: 1.1962\r\n",
      "Train Epoch: 30 [44416/209222 (21%)]\tAll Loss: 1.8720\tTriple Loss(1): 0.0218\tClassification Loss: 1.8283\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Mesh_Overlay_Shorts/img_00000004.jpg\r\n",
      "img_n: img/Faux_Leather-Sleeve_Marled_Jacket/img_00000013.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [44480/209222 (21%)]\tAll Loss: 1.3326\tTriple Loss(0): 0.0000\tClassification Loss: 1.3326\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Giraffe_Print_Shorts/img_00000044.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000084.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [44544/209222 (21%)]\tAll Loss: 1.5679\tTriple Loss(0): 0.0000\tClassification Loss: 1.5679\r\n",
      "Train Epoch: 30 [44608/209222 (21%)]\tAll Loss: 1.5257\tTriple Loss(1): 0.0031\tClassification Loss: 1.5195\r\n",
      "Train Epoch: 30 [44672/209222 (21%)]\tAll Loss: 1.6726\tTriple Loss(1): 0.0426\tClassification Loss: 1.5873\r\n",
      "Train Epoch: 30 [44736/209222 (21%)]\tAll Loss: 1.3686\tTriple Loss(1): 0.0242\tClassification Loss: 1.3202\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Mid-Rise_Cuffed_Denim_Shorts/img_00000024.jpg\r\n",
      "img_n: img/Quilted_Bomber_Jacket/img_00000059.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [44800/209222 (21%)]\tAll Loss: 1.4771\tTriple Loss(0): 0.0000\tClassification Loss: 1.4771\r\n",
      "Train Epoch: 30 [44864/209222 (21%)]\tAll Loss: 1.5085\tTriple Loss(1): 0.0602\tClassification Loss: 1.3881\r\n",
      "Train Epoch: 30 [44928/209222 (21%)]\tAll Loss: 1.6778\tTriple Loss(1): 0.0601\tClassification Loss: 1.5576\r\n",
      "Train Epoch: 30 [44992/209222 (22%)]\tAll Loss: 1.4689\tTriple Loss(1): 0.0152\tClassification Loss: 1.4386\r\n",
      "Train Epoch: 30 [45056/209222 (22%)]\tAll Loss: 1.4621\tTriple Loss(1): 0.0606\tClassification Loss: 1.3409\r\n",
      "Train Epoch: 30 [45120/209222 (22%)]\tAll Loss: 1.4637\tTriple Loss(1): 0.0247\tClassification Loss: 1.4142\r\n",
      "Train Epoch: 30 [45184/209222 (22%)]\tAll Loss: 1.1890\tTriple Loss(1): 0.0252\tClassification Loss: 1.1386\r\n",
      "Train Epoch: 30 [45248/209222 (22%)]\tAll Loss: 1.3314\tTriple Loss(1): 0.0226\tClassification Loss: 1.2862\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Ikat_Print_Shorts/img_00000018.jpg\r\n",
      "img_n: img/Classic_Baseball_Jersey/img_00000047.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [45312/209222 (22%)]\tAll Loss: 1.5473\tTriple Loss(0): 0.0000\tClassification Loss: 1.5473\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Quilted_Dolphin_Shorts/img_00000008.jpg\r\n",
      "img_n: img/Classic_Knit_Turtleneck/img_00000016.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [45376/209222 (22%)]\tAll Loss: 6.8052\tTriple Loss(0): 2.7835\tClassification Loss: 1.2382\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Faded_High-Waisted_Denim_Shorts/img_00000046.jpg\r\n",
      "img_n: img/Venice_Beach_Muscle_Tee/img_00000026.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [45440/209222 (22%)]\tAll Loss: 2.4134\tTriple Loss(0): 0.3507\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 30 [45504/209222 (22%)]\tAll Loss: 1.6652\tTriple Loss(1): 0.0675\tClassification Loss: 1.5303\r\n",
      "Train Epoch: 30 [45568/209222 (22%)]\tAll Loss: 1.7869\tTriple Loss(1): 0.0974\tClassification Loss: 1.5921\r\n",
      "Train Epoch: 30 [45632/209222 (22%)]\tAll Loss: 1.7037\tTriple Loss(1): 0.0676\tClassification Loss: 1.5685\r\n",
      "Train Epoch: 30 [45696/209222 (22%)]\tAll Loss: 1.6100\tTriple Loss(1): 0.0554\tClassification Loss: 1.4993\r\n",
      "Train Epoch: 30 [45760/209222 (22%)]\tAll Loss: 1.3917\tTriple Loss(1): 0.1150\tClassification Loss: 1.1617\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Scalloped_Lace_Shorts/img_00000002.jpg\r\n",
      "img_n: img/Two-Pocket_Cardigan/img_00000047.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [45824/209222 (22%)]\tAll Loss: 1.3004\tTriple Loss(0): 0.0000\tClassification Loss: 1.3004\r\n",
      "Train Epoch: 30 [45888/209222 (22%)]\tAll Loss: 1.7011\tTriple Loss(1): 0.0894\tClassification Loss: 1.5223\r\n",
      "Train Epoch: 30 [45952/209222 (22%)]\tAll Loss: 1.4438\tTriple Loss(1): 0.0148\tClassification Loss: 1.4142\r\n",
      "Train Epoch: 30 [46016/209222 (22%)]\tAll Loss: 1.6062\tTriple Loss(1): 0.0288\tClassification Loss: 1.5485\r\n",
      "Train Epoch: 30 [46080/209222 (22%)]\tAll Loss: 1.5850\tTriple Loss(1): 0.0439\tClassification Loss: 1.4972\r\n",
      "Train Epoch: 30 [46144/209222 (22%)]\tAll Loss: 1.4802\tTriple Loss(1): 0.0753\tClassification Loss: 1.3296\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Heathered_Scuba_Knit_Shorts/img_00000007.jpg\r\n",
      "img_n: img/Zippered_Tartan_Plaid_Flannel/img_00000022.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [46208/209222 (22%)]\tAll Loss: 1.6145\tTriple Loss(0): 0.0000\tClassification Loss: 1.6145\r\n",
      "Train Epoch: 30 [46272/209222 (22%)]\tAll Loss: 1.4908\tTriple Loss(1): 0.0471\tClassification Loss: 1.3965\r\n",
      "Train Epoch: 30 [46336/209222 (22%)]\tAll Loss: 1.7198\tTriple Loss(1): 0.0728\tClassification Loss: 1.5743\r\n",
      "Train Epoch: 30 [46400/209222 (22%)]\tAll Loss: 1.5130\tTriple Loss(1): 0.0839\tClassification Loss: 1.3452\r\n",
      "Train Epoch: 30 [46464/209222 (22%)]\tAll Loss: 1.8428\tTriple Loss(1): 0.0684\tClassification Loss: 1.7060\r\n",
      "Train Epoch: 30 [46528/209222 (22%)]\tAll Loss: 1.8252\tTriple Loss(1): 0.0526\tClassification Loss: 1.7200\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Stripe_Overall_Shorts/img_00000008.jpg\r\n",
      "img_n: img/Ribbed_Open-Front_Cardigan/img_00000052.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [46592/209222 (22%)]\tAll Loss: 1.4757\tTriple Loss(0): 0.0000\tClassification Loss: 1.4757\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Crochet-Hem_Shorts/img_00000043.jpg\r\n",
      "img_n: img/Flutter-Sleeve_Blouse/img_00000001.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [46656/209222 (22%)]\tAll Loss: 1.4579\tTriple Loss(0): 0.0000\tClassification Loss: 1.4579\r\n",
      "Train Epoch: 30 [46720/209222 (22%)]\tAll Loss: 1.2225\tTriple Loss(1): 0.0120\tClassification Loss: 1.1985\r\n",
      "Train Epoch: 30 [46784/209222 (22%)]\tAll Loss: 1.5635\tTriple Loss(1): 0.0992\tClassification Loss: 1.3651\r\n",
      "Train Epoch: 30 [46848/209222 (22%)]\tAll Loss: 1.5279\tTriple Loss(1): 0.0420\tClassification Loss: 1.4438\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Easy_Chino_Shorts/img_00000066.jpg\r\n",
      "img_n: img/Stretchy_Button-Front_Denim_Jacket/img_00000043.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [46912/209222 (22%)]\tAll Loss: 1.2803\tTriple Loss(0): 0.0000\tClassification Loss: 1.2803\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Abstract_Ikat_Print_Shorts/img_00000023.jpg\r\n",
      "img_n: img/Cropped_Ribbed_Knit_Turtleneck/img_00000011.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [46976/209222 (22%)]\tAll Loss: 1.5262\tTriple Loss(0): 0.0000\tClassification Loss: 1.5262\r\n",
      "Train Epoch: 30 [47040/209222 (22%)]\tAll Loss: 1.7721\tTriple Loss(1): 0.0474\tClassification Loss: 1.6772\r\n",
      "Train Epoch: 30 [47104/209222 (23%)]\tAll Loss: 1.7598\tTriple Loss(1): 0.0621\tClassification Loss: 1.6356\r\n",
      "Train Epoch: 30 [47168/209222 (23%)]\tAll Loss: 1.3274\tTriple Loss(1): 0.0439\tClassification Loss: 1.2396\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Colorblocked_Workout_Shorts/img_00000030.jpg\r\n",
      "img_n: img/Hooded_Cotton_Canvas_Anorak/img_00000036.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [47232/209222 (23%)]\tAll Loss: 1.4609\tTriple Loss(0): 0.0000\tClassification Loss: 1.4609\r\n",
      "index: 0\r\n",
      "img_path: img/Cutoff_Sweat_Shorts/img_00000035.jpg\r\n",
      "img_p: img/Life_in_Progress_Linen-Blend_Drawstring_Shorts/img_00000003.jpg\r\n",
      "img_n: img/Abstract_Geo_Print_Cardigan/img_00000004.jpg\r\n",
      "\r\n",
      "Train Epoch: 30 [47296/209222 (23%)]\tAll Loss: 1.7955\tTriple Loss(0): 0.0000\tClassification Loss: 1.7955\r\n",
      "Train Epoch: 30 [47360/209222 (23%)]\tAll Loss: 1.5804\tTriple Loss(1): 0.0563\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 30 [47424/209222 (23%)]\tAll Loss: 1.4658\tTriple Loss(1): 0.1064\tClassification Loss: 1.2530\r\n",
      "Train Epoch: 30 [47488/209222 (23%)]\tAll Loss: 1.2294\tTriple Loss(1): 0.0334\tClassification Loss: 1.1625\r\n",
      "Train Epoch: 30 [47552/209222 (23%)]\tAll Loss: 1.5734\tTriple Loss(1): 0.0327\tClassification Loss: 1.5080\r\n",
      "Train Epoch: 30 [47616/209222 (23%)]\tAll Loss: 1.5580\tTriple Loss(1): 0.1330\tClassification Loss: 1.2920\r\n",
      "Train Epoch: 30 [47680/209222 (23%)]\tAll Loss: 1.9917\tTriple Loss(1): 0.0693\tClassification Loss: 1.8530\r\n",
      "Train Epoch: 30 [47744/209222 (23%)]\tAll Loss: 1.3329\tTriple Loss(1): 0.0501\tClassification Loss: 1.2327\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 330, in <module>\r\n",
      "    train(epoch)\r\n",
      "  File \"train.py\", line 151, in train\r\n",
      "    running_clf_loss += classification_loss.data.item() * TRAIN_BATCH_SIZE\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209371 (0%)]\tAll Loss: 5.2095\tTriple Loss(1): 0.5935\tClassification Loss: 4.0224\r\n",
      "train.py:213: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9274, Accuracy: 851/80128 (1%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/209371 (0%)]\tAll Loss: 5.0185\tTriple Loss(1): 0.7047\tClassification Loss: 3.6092\r\n",
      "Train Epoch: 1 [1280/209371 (1%)]\tAll Loss: 4.3144\tTriple Loss(1): 0.4194\tClassification Loss: 3.4756\r\n",
      "Train Epoch: 1 [1920/209371 (1%)]\tAll Loss: 4.0834\tTriple Loss(1): 0.4603\tClassification Loss: 3.1629\r\n",
      "Train Epoch: 1 [2560/209371 (1%)]\tAll Loss: 5.1133\tTriple Loss(0): 0.9764\tClassification Loss: 3.1606\r\n",
      "Train Epoch: 1 [3200/209371 (2%)]\tAll Loss: 3.9337\tTriple Loss(1): 0.4589\tClassification Loss: 3.0158\r\n",
      "Train Epoch: 1 [3840/209371 (2%)]\tAll Loss: 3.6295\tTriple Loss(1): 0.3989\tClassification Loss: 2.8318\r\n",
      "Train Epoch: 1 [4480/209371 (2%)]\tAll Loss: 3.8969\tTriple Loss(1): 0.4387\tClassification Loss: 3.0195\r\n",
      "Train Epoch: 1 [5120/209371 (2%)]\tAll Loss: 3.7451\tTriple Loss(1): 0.4626\tClassification Loss: 2.8199\r\n",
      "Train Epoch: 1 [5760/209371 (3%)]\tAll Loss: 4.1766\tTriple Loss(1): 0.5809\tClassification Loss: 3.0148\r\n",
      "Train Epoch: 1 [6400/209371 (3%)]\tAll Loss: 3.9227\tTriple Loss(1): 0.5255\tClassification Loss: 2.8718\r\n",
      "Train Epoch: 1 [7040/209371 (3%)]\tAll Loss: 3.6694\tTriple Loss(1): 0.3873\tClassification Loss: 2.8949\r\n",
      "Train Epoch: 1 [7680/209371 (4%)]\tAll Loss: 3.8515\tTriple Loss(1): 0.3950\tClassification Loss: 3.0616\r\n",
      "Train Epoch: 1 [8320/209371 (4%)]\tAll Loss: 3.5754\tTriple Loss(1): 0.4621\tClassification Loss: 2.6513\r\n",
      "Train Epoch: 1 [8960/209371 (4%)]\tAll Loss: 4.4710\tTriple Loss(0): 0.8910\tClassification Loss: 2.6890\r\n",
      "Train Epoch: 1 [9600/209371 (5%)]\tAll Loss: 3.7895\tTriple Loss(1): 0.5297\tClassification Loss: 2.7301\r\n",
      "Train Epoch: 1 [10240/209371 (5%)]\tAll Loss: 3.6857\tTriple Loss(1): 0.5037\tClassification Loss: 2.6784\r\n",
      "Train Epoch: 1 [10880/209371 (5%)]\tAll Loss: 3.5249\tTriple Loss(1): 0.4564\tClassification Loss: 2.6122\r\n",
      "Train Epoch: 1 [11520/209371 (6%)]\tAll Loss: 3.7859\tTriple Loss(1): 0.3872\tClassification Loss: 3.0115\r\n",
      "Train Epoch: 1 [12160/209371 (6%)]\tAll Loss: 3.5550\tTriple Loss(1): 0.4194\tClassification Loss: 2.7162\r\n",
      "Train Epoch: 1 [12800/209371 (6%)]\tAll Loss: 3.4229\tTriple Loss(1): 0.3470\tClassification Loss: 2.7289\r\n",
      "Train Epoch: 1 [13440/209371 (6%)]\tAll Loss: 4.2091\tTriple Loss(0): 0.7969\tClassification Loss: 2.6154\r\n",
      "Train Epoch: 1 [14080/209371 (7%)]\tAll Loss: 3.1739\tTriple Loss(1): 0.3379\tClassification Loss: 2.4980\r\n",
      "Train Epoch: 1 [14720/209371 (7%)]\tAll Loss: 3.4741\tTriple Loss(1): 0.4273\tClassification Loss: 2.6196\r\n",
      "Train Epoch: 1 [15360/209371 (7%)]\tAll Loss: 4.1308\tTriple Loss(0): 0.8629\tClassification Loss: 2.4050\r\n",
      "Train Epoch: 1 [16000/209371 (8%)]\tAll Loss: 3.7445\tTriple Loss(1): 0.5493\tClassification Loss: 2.6459\r\n",
      "Train Epoch: 1 [16640/209371 (8%)]\tAll Loss: 3.2532\tTriple Loss(1): 0.3553\tClassification Loss: 2.5426\r\n",
      "Train Epoch: 1 [17280/209371 (8%)]\tAll Loss: 3.3400\tTriple Loss(1): 0.4175\tClassification Loss: 2.5051\r\n",
      "Train Epoch: 1 [17920/209371 (9%)]\tAll Loss: 3.4614\tTriple Loss(1): 0.3326\tClassification Loss: 2.7962\r\n",
      "Train Epoch: 1 [18560/209371 (9%)]\tAll Loss: 3.8965\tTriple Loss(1): 0.6264\tClassification Loss: 2.6437\r\n",
      "Train Epoch: 1 [19200/209371 (9%)]\tAll Loss: 3.0203\tTriple Loss(1): 0.2486\tClassification Loss: 2.5231\r\n",
      "Train Epoch: 1 [19840/209371 (9%)]\tAll Loss: 3.7962\tTriple Loss(0): 0.7517\tClassification Loss: 2.2927\r\n",
      "Train Epoch: 1 [20480/209371 (10%)]\tAll Loss: 4.2020\tTriple Loss(0): 0.7730\tClassification Loss: 2.6559\r\n",
      "Train Epoch: 1 [21120/209371 (10%)]\tAll Loss: 3.6977\tTriple Loss(1): 0.5033\tClassification Loss: 2.6911\r\n",
      "Train Epoch: 1 [21760/209371 (10%)]\tAll Loss: 3.5456\tTriple Loss(1): 0.3641\tClassification Loss: 2.8174\r\n",
      "Train Epoch: 1 [22400/209371 (11%)]\tAll Loss: 3.4152\tTriple Loss(1): 0.4507\tClassification Loss: 2.5137\r\n",
      "Train Epoch: 1 [23040/209371 (11%)]\tAll Loss: 3.2586\tTriple Loss(1): 0.3957\tClassification Loss: 2.4672\r\n",
      "Train Epoch: 1 [23680/209371 (11%)]\tAll Loss: 3.0807\tTriple Loss(1): 0.2967\tClassification Loss: 2.4873\r\n",
      "Train Epoch: 1 [24320/209371 (12%)]\tAll Loss: 3.3144\tTriple Loss(1): 0.3241\tClassification Loss: 2.6661\r\n",
      "Train Epoch: 1 [24960/209371 (12%)]\tAll Loss: 4.0657\tTriple Loss(0): 0.8557\tClassification Loss: 2.3543\r\n",
      "Train Epoch: 1 [25600/209371 (12%)]\tAll Loss: 3.3409\tTriple Loss(1): 0.4437\tClassification Loss: 2.4535\r\n",
      "Train Epoch: 1 [26240/209371 (13%)]\tAll Loss: 3.5281\tTriple Loss(1): 0.4773\tClassification Loss: 2.5735\r\n",
      "Train Epoch: 1 [26880/209371 (13%)]\tAll Loss: 3.1688\tTriple Loss(1): 0.3390\tClassification Loss: 2.4909\r\n",
      "Train Epoch: 1 [27520/209371 (13%)]\tAll Loss: 3.2779\tTriple Loss(1): 0.4109\tClassification Loss: 2.4560\r\n",
      "Train Epoch: 1 [28160/209371 (13%)]\tAll Loss: 2.9982\tTriple Loss(1): 0.2570\tClassification Loss: 2.4842\r\n",
      "Train Epoch: 1 [28800/209371 (14%)]\tAll Loss: 3.6118\tTriple Loss(0): 0.6494\tClassification Loss: 2.3130\r\n",
      "Train Epoch: 1 [29440/209371 (14%)]\tAll Loss: 3.3625\tTriple Loss(1): 0.3981\tClassification Loss: 2.5663\r\n",
      "Train Epoch: 1 [30080/209371 (14%)]\tAll Loss: 2.5840\tTriple Loss(1): 0.1929\tClassification Loss: 2.1982\r\n",
      "Train Epoch: 1 [30720/209371 (15%)]\tAll Loss: 3.3426\tTriple Loss(1): 0.4218\tClassification Loss: 2.4989\r\n",
      "Train Epoch: 1 [31360/209371 (15%)]\tAll Loss: 3.4590\tTriple Loss(1): 0.4735\tClassification Loss: 2.5120\r\n",
      "Train Epoch: 1 [32000/209371 (15%)]\tAll Loss: 3.0019\tTriple Loss(1): 0.3588\tClassification Loss: 2.2844\r\n",
      "Train Epoch: 1 [32640/209371 (16%)]\tAll Loss: 3.0545\tTriple Loss(1): 0.4171\tClassification Loss: 2.2203\r\n",
      "Train Epoch: 1 [33280/209371 (16%)]\tAll Loss: 3.4857\tTriple Loss(1): 0.4677\tClassification Loss: 2.5504\r\n",
      "Train Epoch: 1 [33920/209371 (16%)]\tAll Loss: 3.2726\tTriple Loss(1): 0.3638\tClassification Loss: 2.5450\r\n",
      "Train Epoch: 1 [34560/209371 (17%)]\tAll Loss: 2.8337\tTriple Loss(1): 0.3165\tClassification Loss: 2.2007\r\n",
      "Train Epoch: 1 [35200/209371 (17%)]\tAll Loss: 2.9979\tTriple Loss(1): 0.3627\tClassification Loss: 2.2724\r\n",
      "Train Epoch: 1 [35840/209371 (17%)]\tAll Loss: 3.1128\tTriple Loss(1): 0.3310\tClassification Loss: 2.4508\r\n",
      "Train Epoch: 1 [36480/209371 (17%)]\tAll Loss: 3.0509\tTriple Loss(1): 0.3688\tClassification Loss: 2.3133\r\n",
      "Train Epoch: 1 [37120/209371 (18%)]\tAll Loss: 3.0233\tTriple Loss(1): 0.3073\tClassification Loss: 2.4087\r\n",
      "Train Epoch: 1 [37760/209371 (18%)]\tAll Loss: 2.8550\tTriple Loss(1): 0.1989\tClassification Loss: 2.4572\r\n",
      "Train Epoch: 1 [38400/209371 (18%)]\tAll Loss: 3.3230\tTriple Loss(1): 0.3477\tClassification Loss: 2.6276\r\n",
      "Train Epoch: 1 [39040/209371 (19%)]\tAll Loss: 2.8596\tTriple Loss(1): 0.2794\tClassification Loss: 2.3008\r\n",
      "Train Epoch: 1 [39680/209371 (19%)]\tAll Loss: 2.7201\tTriple Loss(1): 0.2658\tClassification Loss: 2.1885\r\n",
      "Train Epoch: 1 [40320/209371 (19%)]\tAll Loss: 3.1833\tTriple Loss(1): 0.3036\tClassification Loss: 2.5761\r\n",
      "Train Epoch: 1 [40960/209371 (20%)]\tAll Loss: 3.1805\tTriple Loss(1): 0.4117\tClassification Loss: 2.3570\r\n",
      "Train Epoch: 1 [41600/209371 (20%)]\tAll Loss: 3.1696\tTriple Loss(1): 0.3473\tClassification Loss: 2.4751\r\n",
      "Train Epoch: 1 [42240/209371 (20%)]\tAll Loss: 3.2614\tTriple Loss(1): 0.3636\tClassification Loss: 2.5341\r\n",
      "Train Epoch: 1 [42880/209371 (20%)]\tAll Loss: 3.0947\tTriple Loss(1): 0.4840\tClassification Loss: 2.1267\r\n",
      "Train Epoch: 1 [43520/209371 (21%)]\tAll Loss: 2.8043\tTriple Loss(1): 0.1894\tClassification Loss: 2.4255\r\n",
      "Train Epoch: 1 [44160/209371 (21%)]\tAll Loss: 3.2583\tTriple Loss(1): 0.5049\tClassification Loss: 2.2484\r\n",
      "Train Epoch: 1 [44800/209371 (21%)]\tAll Loss: 3.2206\tTriple Loss(1): 0.3607\tClassification Loss: 2.4991\r\n",
      "Train Epoch: 1 [45440/209371 (22%)]\tAll Loss: 4.1208\tTriple Loss(0): 0.8299\tClassification Loss: 2.4611\r\n",
      "Train Epoch: 1 [46080/209371 (22%)]\tAll Loss: 3.2040\tTriple Loss(1): 0.3326\tClassification Loss: 2.5387\r\n",
      "Train Epoch: 1 [46720/209371 (22%)]\tAll Loss: 3.0252\tTriple Loss(1): 0.3765\tClassification Loss: 2.2721\r\n",
      "Train Epoch: 1 [47360/209371 (23%)]\tAll Loss: 3.8265\tTriple Loss(0): 0.7428\tClassification Loss: 2.3409\r\n",
      "Train Epoch: 1 [48000/209371 (23%)]\tAll Loss: 3.4180\tTriple Loss(1): 0.3848\tClassification Loss: 2.6484\r\n",
      "Train Epoch: 1 [48640/209371 (23%)]\tAll Loss: 2.9204\tTriple Loss(1): 0.3553\tClassification Loss: 2.2098\r\n",
      "Train Epoch: 1 [49280/209371 (24%)]\tAll Loss: 2.8815\tTriple Loss(1): 0.3752\tClassification Loss: 2.1311\r\n",
      "Train Epoch: 1 [49920/209371 (24%)]\tAll Loss: 3.6952\tTriple Loss(0): 0.7626\tClassification Loss: 2.1699\r\n",
      "Train Epoch: 1 [50560/209371 (24%)]\tAll Loss: 2.9098\tTriple Loss(1): 0.3716\tClassification Loss: 2.1666\r\n",
      "Train Epoch: 1 [51200/209371 (24%)]\tAll Loss: 2.8769\tTriple Loss(1): 0.2802\tClassification Loss: 2.3164\r\n",
      "Train Epoch: 1 [51840/209371 (25%)]\tAll Loss: 2.6191\tTriple Loss(1): 0.3210\tClassification Loss: 1.9771\r\n",
      "Train Epoch: 1 [52480/209371 (25%)]\tAll Loss: 3.6103\tTriple Loss(0): 0.6467\tClassification Loss: 2.3168\r\n",
      "Train Epoch: 1 [53120/209371 (25%)]\tAll Loss: 2.8839\tTriple Loss(1): 0.2792\tClassification Loss: 2.3254\r\n",
      "Train Epoch: 1 [53760/209371 (26%)]\tAll Loss: 2.6065\tTriple Loss(1): 0.2245\tClassification Loss: 2.1575\r\n",
      "Train Epoch: 1 [54400/209371 (26%)]\tAll Loss: 3.8800\tTriple Loss(0): 0.8058\tClassification Loss: 2.2684\r\n",
      "Train Epoch: 1 [55040/209371 (26%)]\tAll Loss: 2.9349\tTriple Loss(1): 0.2930\tClassification Loss: 2.3489\r\n",
      "Train Epoch: 1 [55680/209371 (27%)]\tAll Loss: 2.7047\tTriple Loss(1): 0.2429\tClassification Loss: 2.2190\r\n",
      "Train Epoch: 1 [56320/209371 (27%)]\tAll Loss: 3.1788\tTriple Loss(1): 0.3937\tClassification Loss: 2.3914\r\n",
      "Train Epoch: 1 [56960/209371 (27%)]\tAll Loss: 2.5496\tTriple Loss(1): 0.2012\tClassification Loss: 2.1472\r\n",
      "Train Epoch: 1 [57600/209371 (28%)]\tAll Loss: 3.9452\tTriple Loss(0): 0.8234\tClassification Loss: 2.2984\r\n",
      "Train Epoch: 1 [58240/209371 (28%)]\tAll Loss: 3.6884\tTriple Loss(0): 0.6907\tClassification Loss: 2.3070\r\n",
      "Train Epoch: 1 [58880/209371 (28%)]\tAll Loss: 2.8466\tTriple Loss(1): 0.3213\tClassification Loss: 2.2041\r\n",
      "Train Epoch: 1 [59520/209371 (28%)]\tAll Loss: 3.6365\tTriple Loss(0): 0.7219\tClassification Loss: 2.1926\r\n",
      "Train Epoch: 1 [60160/209371 (29%)]\tAll Loss: 2.8829\tTriple Loss(1): 0.3342\tClassification Loss: 2.2146\r\n",
      "Train Epoch: 1 [60800/209371 (29%)]\tAll Loss: 2.8554\tTriple Loss(1): 0.2383\tClassification Loss: 2.3789\r\n",
      "Train Epoch: 1 [61440/209371 (29%)]\tAll Loss: 3.4215\tTriple Loss(0): 0.5334\tClassification Loss: 2.3547\r\n",
      "Train Epoch: 1 [62080/209371 (30%)]\tAll Loss: 2.7856\tTriple Loss(1): 0.3084\tClassification Loss: 2.1689\r\n",
      "Train Epoch: 1 [62720/209371 (30%)]\tAll Loss: 3.5657\tTriple Loss(0): 0.7056\tClassification Loss: 2.1545\r\n",
      "Train Epoch: 1 [63360/209371 (30%)]\tAll Loss: 3.6486\tTriple Loss(0): 0.6248\tClassification Loss: 2.3989\r\n",
      "Train Epoch: 1 [64000/209371 (31%)]\tAll Loss: 3.7543\tTriple Loss(0): 0.7451\tClassification Loss: 2.2641\r\n",
      "Train Epoch: 1 [64640/209371 (31%)]\tAll Loss: 2.6515\tTriple Loss(1): 0.2710\tClassification Loss: 2.1095\r\n",
      "Train Epoch: 1 [65280/209371 (31%)]\tAll Loss: 3.9597\tTriple Loss(0): 0.9178\tClassification Loss: 2.1240\r\n",
      "Train Epoch: 1 [65920/209371 (31%)]\tAll Loss: 2.6602\tTriple Loss(1): 0.2017\tClassification Loss: 2.2567\r\n",
      "Train Epoch: 1 [66560/209371 (32%)]\tAll Loss: 2.9766\tTriple Loss(1): 0.3643\tClassification Loss: 2.2479\r\n",
      "Train Epoch: 1 [67200/209371 (32%)]\tAll Loss: 2.8345\tTriple Loss(1): 0.2377\tClassification Loss: 2.3590\r\n",
      "Train Epoch: 1 [67840/209371 (32%)]\tAll Loss: 2.9937\tTriple Loss(1): 0.3423\tClassification Loss: 2.3091\r\n",
      "Train Epoch: 1 [68480/209371 (33%)]\tAll Loss: 2.6598\tTriple Loss(1): 0.2069\tClassification Loss: 2.2460\r\n",
      "Train Epoch: 1 [69120/209371 (33%)]\tAll Loss: 3.1424\tTriple Loss(1): 0.3818\tClassification Loss: 2.3788\r\n",
      "Train Epoch: 1 [69760/209371 (33%)]\tAll Loss: 2.8206\tTriple Loss(1): 0.3621\tClassification Loss: 2.0964\r\n",
      "Train Epoch: 1 [70400/209371 (34%)]\tAll Loss: 2.6157\tTriple Loss(1): 0.2262\tClassification Loss: 2.1633\r\n",
      "Train Epoch: 1 [71040/209371 (34%)]\tAll Loss: 2.6307\tTriple Loss(1): 0.2279\tClassification Loss: 2.1748\r\n",
      "Train Epoch: 1 [71680/209371 (34%)]\tAll Loss: 4.1532\tTriple Loss(0): 0.8449\tClassification Loss: 2.4633\r\n",
      "Train Epoch: 1 [72320/209371 (35%)]\tAll Loss: 3.9791\tTriple Loss(0): 0.8224\tClassification Loss: 2.3344\r\n",
      "Train Epoch: 1 [72960/209371 (35%)]\tAll Loss: 2.6495\tTriple Loss(1): 0.2501\tClassification Loss: 2.1493\r\n",
      "Train Epoch: 1 [73600/209371 (35%)]\tAll Loss: 2.9034\tTriple Loss(1): 0.3670\tClassification Loss: 2.1693\r\n",
      "Train Epoch: 1 [74240/209371 (35%)]\tAll Loss: 2.9044\tTriple Loss(1): 0.4387\tClassification Loss: 2.0269\r\n",
      "Train Epoch: 1 [74880/209371 (36%)]\tAll Loss: 2.8447\tTriple Loss(1): 0.4194\tClassification Loss: 2.0060\r\n",
      "Train Epoch: 1 [75520/209371 (36%)]\tAll Loss: 3.9710\tTriple Loss(0): 0.7797\tClassification Loss: 2.4116\r\n",
      "Train Epoch: 1 [76160/209371 (36%)]\tAll Loss: 2.6064\tTriple Loss(1): 0.2750\tClassification Loss: 2.0564\r\n",
      "Train Epoch: 1 [76800/209371 (37%)]\tAll Loss: 3.9933\tTriple Loss(0): 0.8292\tClassification Loss: 2.3349\r\n",
      "Train Epoch: 1 [77440/209371 (37%)]\tAll Loss: 2.7924\tTriple Loss(1): 0.2405\tClassification Loss: 2.3114\r\n",
      "Train Epoch: 1 [78080/209371 (37%)]\tAll Loss: 2.5090\tTriple Loss(1): 0.2865\tClassification Loss: 1.9360\r\n",
      "Train Epoch: 1 [78720/209371 (38%)]\tAll Loss: 3.0239\tTriple Loss(1): 0.3755\tClassification Loss: 2.2729\r\n",
      "Train Epoch: 1 [79360/209371 (38%)]\tAll Loss: 3.4482\tTriple Loss(0): 0.7027\tClassification Loss: 2.0428\r\n",
      "Train Epoch: 1 [80000/209371 (38%)]\tAll Loss: 2.8941\tTriple Loss(1): 0.2620\tClassification Loss: 2.3701\r\n",
      "Train Epoch: 1 [80640/209371 (39%)]\tAll Loss: 2.5909\tTriple Loss(1): 0.1900\tClassification Loss: 2.2108\r\n",
      "Train Epoch: 1 [81280/209371 (39%)]\tAll Loss: 2.8593\tTriple Loss(1): 0.4155\tClassification Loss: 2.0284\r\n",
      "Train Epoch: 1 [81920/209371 (39%)]\tAll Loss: 3.5026\tTriple Loss(0): 0.6900\tClassification Loss: 2.1225\r\n",
      "Train Epoch: 1 [82560/209371 (39%)]\tAll Loss: 3.0084\tTriple Loss(0): 0.5459\tClassification Loss: 1.9167\r\n",
      "Train Epoch: 1 [83200/209371 (40%)]\tAll Loss: 3.1460\tTriple Loss(1): 0.3224\tClassification Loss: 2.5012\r\n",
      "Train Epoch: 1 [83840/209371 (40%)]\tAll Loss: 3.5784\tTriple Loss(0): 0.6994\tClassification Loss: 2.1797\r\n",
      "Train Epoch: 1 [84480/209371 (40%)]\tAll Loss: 3.9075\tTriple Loss(0): 0.7545\tClassification Loss: 2.3986\r\n",
      "Train Epoch: 1 [85120/209371 (41%)]\tAll Loss: 2.9754\tTriple Loss(1): 0.2954\tClassification Loss: 2.3845\r\n",
      "Train Epoch: 1 [85760/209371 (41%)]\tAll Loss: 2.9299\tTriple Loss(1): 0.2700\tClassification Loss: 2.3899\r\n",
      "Train Epoch: 1 [86400/209371 (41%)]\tAll Loss: 3.0733\tTriple Loss(1): 0.3961\tClassification Loss: 2.2810\r\n",
      "Train Epoch: 1 [87040/209371 (42%)]\tAll Loss: 2.8806\tTriple Loss(1): 0.2932\tClassification Loss: 2.2942\r\n",
      "Train Epoch: 1 [87680/209371 (42%)]\tAll Loss: 3.1252\tTriple Loss(1): 0.3743\tClassification Loss: 2.3766\r\n",
      "Train Epoch: 1 [88320/209371 (42%)]\tAll Loss: 2.5295\tTriple Loss(1): 0.3154\tClassification Loss: 1.8987\r\n",
      "Train Epoch: 1 [88960/209371 (42%)]\tAll Loss: 2.4639\tTriple Loss(1): 0.1591\tClassification Loss: 2.1457\r\n",
      "Train Epoch: 1 [89600/209371 (43%)]\tAll Loss: 2.7617\tTriple Loss(1): 0.2496\tClassification Loss: 2.2624\r\n",
      "Train Epoch: 1 [90240/209371 (43%)]\tAll Loss: 2.4964\tTriple Loss(1): 0.2583\tClassification Loss: 1.9798\r\n",
      "Train Epoch: 1 [90880/209371 (43%)]\tAll Loss: 3.5832\tTriple Loss(1): 0.4009\tClassification Loss: 2.7813\r\n",
      "Train Epoch: 1 [91520/209371 (44%)]\tAll Loss: 3.2262\tTriple Loss(1): 0.4645\tClassification Loss: 2.2973\r\n",
      "Train Epoch: 1 [92160/209371 (44%)]\tAll Loss: 3.8726\tTriple Loss(0): 0.8636\tClassification Loss: 2.1455\r\n",
      "Train Epoch: 1 [92800/209371 (44%)]\tAll Loss: 2.6668\tTriple Loss(1): 0.2645\tClassification Loss: 2.1377\r\n",
      "Train Epoch: 1 [93440/209371 (45%)]\tAll Loss: 2.8316\tTriple Loss(1): 0.2440\tClassification Loss: 2.3436\r\n",
      "Train Epoch: 1 [94080/209371 (45%)]\tAll Loss: 3.7263\tTriple Loss(0): 0.7778\tClassification Loss: 2.1708\r\n",
      "Train Epoch: 1 [94720/209371 (45%)]\tAll Loss: 2.8994\tTriple Loss(1): 0.3081\tClassification Loss: 2.2832\r\n",
      "Train Epoch: 1 [95360/209371 (46%)]\tAll Loss: 4.1946\tTriple Loss(0): 0.9228\tClassification Loss: 2.3491\r\n",
      "Train Epoch: 1 [96000/209371 (46%)]\tAll Loss: 2.8057\tTriple Loss(1): 0.2934\tClassification Loss: 2.2188\r\n",
      "Train Epoch: 1 [96640/209371 (46%)]\tAll Loss: 2.7014\tTriple Loss(1): 0.1770\tClassification Loss: 2.3475\r\n",
      "Train Epoch: 1 [97280/209371 (46%)]\tAll Loss: 2.9776\tTriple Loss(1): 0.3228\tClassification Loss: 2.3321\r\n",
      "Train Epoch: 1 [97920/209371 (47%)]\tAll Loss: 2.3982\tTriple Loss(1): 0.2505\tClassification Loss: 1.8972\r\n",
      "Train Epoch: 1 [98560/209371 (47%)]\tAll Loss: 2.6178\tTriple Loss(1): 0.2948\tClassification Loss: 2.0282\r\n",
      "Train Epoch: 1 [99200/209371 (47%)]\tAll Loss: 2.3413\tTriple Loss(1): 0.1681\tClassification Loss: 2.0052\r\n",
      "Train Epoch: 1 [99840/209371 (48%)]\tAll Loss: 2.9553\tTriple Loss(1): 0.3524\tClassification Loss: 2.2505\r\n",
      "Train Epoch: 1 [100480/209371 (48%)]\tAll Loss: 2.7561\tTriple Loss(1): 0.1997\tClassification Loss: 2.3567\r\n",
      "Train Epoch: 1 [101120/209371 (48%)]\tAll Loss: 4.2298\tTriple Loss(0): 0.9827\tClassification Loss: 2.2644\r\n",
      "Train Epoch: 1 [101760/209371 (49%)]\tAll Loss: 2.7711\tTriple Loss(1): 0.3087\tClassification Loss: 2.1538\r\n",
      "Train Epoch: 1 [102400/209371 (49%)]\tAll Loss: 3.4891\tTriple Loss(0): 0.7074\tClassification Loss: 2.0743\r\n",
      "Train Epoch: 1 [103040/209371 (49%)]\tAll Loss: 3.0746\tTriple Loss(1): 0.3740\tClassification Loss: 2.3265\r\n",
      "Train Epoch: 1 [103680/209371 (50%)]\tAll Loss: 2.5410\tTriple Loss(1): 0.1343\tClassification Loss: 2.2725\r\n",
      "Train Epoch: 1 [104320/209371 (50%)]\tAll Loss: 2.7121\tTriple Loss(1): 0.4060\tClassification Loss: 1.9001\r\n",
      "Train Epoch: 1 [104960/209371 (50%)]\tAll Loss: 3.1744\tTriple Loss(1): 0.4765\tClassification Loss: 2.2214\r\n",
      "Train Epoch: 1 [105600/209371 (50%)]\tAll Loss: 2.8105\tTriple Loss(1): 0.3289\tClassification Loss: 2.1527\r\n",
      "Train Epoch: 1 [106240/209371 (51%)]\tAll Loss: 3.9483\tTriple Loss(0): 0.8334\tClassification Loss: 2.2814\r\n",
      "Train Epoch: 1 [106880/209371 (51%)]\tAll Loss: 2.3139\tTriple Loss(1): 0.1828\tClassification Loss: 1.9483\r\n",
      "Train Epoch: 1 [107520/209371 (51%)]\tAll Loss: 2.6512\tTriple Loss(1): 0.3090\tClassification Loss: 2.0332\r\n",
      "Train Epoch: 1 [108160/209371 (52%)]\tAll Loss: 2.6067\tTriple Loss(1): 0.3180\tClassification Loss: 1.9707\r\n",
      "Train Epoch: 1 [108800/209371 (52%)]\tAll Loss: 2.3611\tTriple Loss(1): 0.2145\tClassification Loss: 1.9320\r\n",
      "Train Epoch: 1 [109440/209371 (52%)]\tAll Loss: 3.1577\tTriple Loss(0): 0.5838\tClassification Loss: 1.9902\r\n",
      "Train Epoch: 1 [110080/209371 (53%)]\tAll Loss: 2.6422\tTriple Loss(1): 0.3135\tClassification Loss: 2.0153\r\n",
      "Train Epoch: 1 [110720/209371 (53%)]\tAll Loss: 2.7521\tTriple Loss(1): 0.2476\tClassification Loss: 2.2568\r\n",
      "Train Epoch: 1 [111360/209371 (53%)]\tAll Loss: 2.6665\tTriple Loss(1): 0.3910\tClassification Loss: 1.8846\r\n",
      "Train Epoch: 1 [112000/209371 (54%)]\tAll Loss: 2.8783\tTriple Loss(1): 0.3737\tClassification Loss: 2.1309\r\n",
      "Train Epoch: 1 [112640/209371 (54%)]\tAll Loss: 2.6917\tTriple Loss(1): 0.3629\tClassification Loss: 1.9660\r\n",
      "Train Epoch: 1 [113280/209371 (54%)]\tAll Loss: 2.6939\tTriple Loss(1): 0.3977\tClassification Loss: 1.8984\r\n",
      "Train Epoch: 1 [113920/209371 (54%)]\tAll Loss: 2.3066\tTriple Loss(1): 0.1876\tClassification Loss: 1.9314\r\n",
      "Train Epoch: 1 [114560/209371 (55%)]\tAll Loss: 2.7934\tTriple Loss(1): 0.2308\tClassification Loss: 2.3318\r\n",
      "Train Epoch: 1 [115200/209371 (55%)]\tAll Loss: 2.8886\tTriple Loss(1): 0.3103\tClassification Loss: 2.2679\r\n",
      "Train Epoch: 1 [115840/209371 (55%)]\tAll Loss: 2.3813\tTriple Loss(1): 0.2366\tClassification Loss: 1.9080\r\n",
      "Train Epoch: 1 [116480/209371 (56%)]\tAll Loss: 2.9683\tTriple Loss(1): 0.4758\tClassification Loss: 2.0166\r\n",
      "Train Epoch: 1 [117120/209371 (56%)]\tAll Loss: 2.7295\tTriple Loss(1): 0.2469\tClassification Loss: 2.2357\r\n",
      "Train Epoch: 1 [117760/209371 (56%)]\tAll Loss: 2.7290\tTriple Loss(1): 0.2680\tClassification Loss: 2.1931\r\n",
      "Train Epoch: 1 [118400/209371 (57%)]\tAll Loss: 3.2424\tTriple Loss(1): 0.4129\tClassification Loss: 2.4167\r\n",
      "Train Epoch: 1 [119040/209371 (57%)]\tAll Loss: 3.8469\tTriple Loss(0): 0.8529\tClassification Loss: 2.1412\r\n",
      "Train Epoch: 1 [119680/209371 (57%)]\tAll Loss: 2.7722\tTriple Loss(1): 0.3760\tClassification Loss: 2.0202\r\n",
      "Train Epoch: 1 [120320/209371 (57%)]\tAll Loss: 2.5816\tTriple Loss(1): 0.2046\tClassification Loss: 2.1723\r\n",
      "Train Epoch: 1 [120960/209371 (58%)]\tAll Loss: 2.8016\tTriple Loss(1): 0.4561\tClassification Loss: 1.8893\r\n",
      "Train Epoch: 1 [121600/209371 (58%)]\tAll Loss: 3.8881\tTriple Loss(0): 0.8616\tClassification Loss: 2.1649\r\n",
      "Train Epoch: 1 [122240/209371 (58%)]\tAll Loss: 3.3079\tTriple Loss(0): 0.7235\tClassification Loss: 1.8610\r\n",
      "Train Epoch: 1 [122880/209371 (59%)]\tAll Loss: 2.9607\tTriple Loss(1): 0.2397\tClassification Loss: 2.4813\r\n",
      "Train Epoch: 1 [123520/209371 (59%)]\tAll Loss: 2.6476\tTriple Loss(1): 0.4145\tClassification Loss: 1.8185\r\n",
      "Train Epoch: 1 [124160/209371 (59%)]\tAll Loss: 2.6254\tTriple Loss(1): 0.2682\tClassification Loss: 2.0891\r\n",
      "Train Epoch: 1 [124800/209371 (60%)]\tAll Loss: 2.3476\tTriple Loss(1): 0.2728\tClassification Loss: 1.8019\r\n",
      "Train Epoch: 1 [125440/209371 (60%)]\tAll Loss: 2.8343\tTriple Loss(1): 0.3499\tClassification Loss: 2.1344\r\n",
      "Train Epoch: 1 [126080/209371 (60%)]\tAll Loss: 3.6218\tTriple Loss(0): 0.6872\tClassification Loss: 2.2474\r\n",
      "Train Epoch: 1 [126720/209371 (61%)]\tAll Loss: 2.3652\tTriple Loss(1): 0.1806\tClassification Loss: 2.0040\r\n",
      "Train Epoch: 1 [127360/209371 (61%)]\tAll Loss: 2.4813\tTriple Loss(1): 0.3522\tClassification Loss: 1.7769\r\n",
      "Train Epoch: 1 [128000/209371 (61%)]\tAll Loss: 2.7961\tTriple Loss(1): 0.4052\tClassification Loss: 1.9857\r\n",
      "Train Epoch: 1 [128640/209371 (61%)]\tAll Loss: 2.9116\tTriple Loss(1): 0.2590\tClassification Loss: 2.3937\r\n",
      "Train Epoch: 1 [129280/209371 (62%)]\tAll Loss: 2.5057\tTriple Loss(1): 0.2539\tClassification Loss: 1.9980\r\n",
      "Train Epoch: 1 [129920/209371 (62%)]\tAll Loss: 2.7511\tTriple Loss(1): 0.3259\tClassification Loss: 2.0993\r\n",
      "Train Epoch: 1 [130560/209371 (62%)]\tAll Loss: 2.4909\tTriple Loss(1): 0.1657\tClassification Loss: 2.1595\r\n",
      "Train Epoch: 1 [131200/209371 (63%)]\tAll Loss: 2.5896\tTriple Loss(1): 0.3740\tClassification Loss: 1.8417\r\n",
      "Train Epoch: 1 [131840/209371 (63%)]\tAll Loss: 2.7609\tTriple Loss(1): 0.3518\tClassification Loss: 2.0573\r\n",
      "Train Epoch: 1 [132480/209371 (63%)]\tAll Loss: 3.0802\tTriple Loss(1): 0.4352\tClassification Loss: 2.2098\r\n",
      "Train Epoch: 1 [133120/209371 (64%)]\tAll Loss: 2.9357\tTriple Loss(1): 0.4012\tClassification Loss: 2.1332\r\n",
      "Train Epoch: 1 [133760/209371 (64%)]\tAll Loss: 3.4313\tTriple Loss(0): 0.6687\tClassification Loss: 2.0938\r\n",
      "Train Epoch: 1 [134400/209371 (64%)]\tAll Loss: 2.2980\tTriple Loss(1): 0.2312\tClassification Loss: 1.8356\r\n",
      "Train Epoch: 1 [135040/209371 (65%)]\tAll Loss: 2.8818\tTriple Loss(1): 0.3922\tClassification Loss: 2.0973\r\n",
      "Train Epoch: 1 [135680/209371 (65%)]\tAll Loss: 2.5853\tTriple Loss(1): 0.2898\tClassification Loss: 2.0058\r\n",
      "Train Epoch: 1 [136320/209371 (65%)]\tAll Loss: 2.7738\tTriple Loss(1): 0.3520\tClassification Loss: 2.0698\r\n",
      "Train Epoch: 1 [136960/209371 (65%)]\tAll Loss: 2.5721\tTriple Loss(1): 0.2125\tClassification Loss: 2.1471\r\n",
      "Train Epoch: 1 [137600/209371 (66%)]\tAll Loss: 2.6355\tTriple Loss(1): 0.2516\tClassification Loss: 2.1323\r\n",
      "Train Epoch: 1 [138240/209371 (66%)]\tAll Loss: 2.4053\tTriple Loss(1): 0.2093\tClassification Loss: 1.9867\r\n",
      "Train Epoch: 1 [138880/209371 (66%)]\tAll Loss: 2.8937\tTriple Loss(1): 0.3979\tClassification Loss: 2.0978\r\n",
      "Train Epoch: 1 [139520/209371 (67%)]\tAll Loss: 2.6122\tTriple Loss(1): 0.1643\tClassification Loss: 2.2836\r\n",
      "Train Epoch: 1 [140160/209371 (67%)]\tAll Loss: 3.7716\tTriple Loss(0): 0.8639\tClassification Loss: 2.0438\r\n",
      "Train Epoch: 1 [140800/209371 (67%)]\tAll Loss: 2.5115\tTriple Loss(1): 0.2684\tClassification Loss: 1.9747\r\n",
      "Train Epoch: 1 [141440/209371 (68%)]\tAll Loss: 2.5789\tTriple Loss(1): 0.1543\tClassification Loss: 2.2703\r\n",
      "Train Epoch: 1 [142080/209371 (68%)]\tAll Loss: 2.7011\tTriple Loss(1): 0.2912\tClassification Loss: 2.1187\r\n",
      "Train Epoch: 1 [142720/209371 (68%)]\tAll Loss: 2.7502\tTriple Loss(1): 0.2830\tClassification Loss: 2.1842\r\n",
      "Train Epoch: 1 [143360/209371 (68%)]\tAll Loss: 2.5057\tTriple Loss(1): 0.2132\tClassification Loss: 2.0794\r\n",
      "Train Epoch: 1 [144000/209371 (69%)]\tAll Loss: 2.9576\tTriple Loss(1): 0.4001\tClassification Loss: 2.1574\r\n",
      "Train Epoch: 1 [144640/209371 (69%)]\tAll Loss: 2.4906\tTriple Loss(1): 0.3304\tClassification Loss: 1.8297\r\n",
      "Train Epoch: 1 [145280/209371 (69%)]\tAll Loss: 2.7463\tTriple Loss(1): 0.3104\tClassification Loss: 2.1256\r\n",
      "Train Epoch: 1 [145920/209371 (70%)]\tAll Loss: 2.5022\tTriple Loss(1): 0.2466\tClassification Loss: 2.0090\r\n",
      "Train Epoch: 1 [146560/209371 (70%)]\tAll Loss: 2.8134\tTriple Loss(1): 0.2707\tClassification Loss: 2.2720\r\n",
      "Train Epoch: 1 [147200/209371 (70%)]\tAll Loss: 2.7912\tTriple Loss(1): 0.2569\tClassification Loss: 2.2774\r\n",
      "Train Epoch: 1 [147840/209371 (71%)]\tAll Loss: 2.6754\tTriple Loss(1): 0.2626\tClassification Loss: 2.1501\r\n",
      "Train Epoch: 1 [148480/209371 (71%)]\tAll Loss: 2.5790\tTriple Loss(1): 0.2871\tClassification Loss: 2.0049\r\n",
      "Train Epoch: 1 [149120/209371 (71%)]\tAll Loss: 3.4164\tTriple Loss(0): 0.7173\tClassification Loss: 1.9818\r\n",
      "Train Epoch: 1 [149760/209371 (72%)]\tAll Loss: 2.6313\tTriple Loss(1): 0.2356\tClassification Loss: 2.1602\r\n",
      "Train Epoch: 1 [150400/209371 (72%)]\tAll Loss: 2.5597\tTriple Loss(1): 0.3273\tClassification Loss: 1.9052\r\n",
      "Train Epoch: 1 [151040/209371 (72%)]\tAll Loss: 2.3938\tTriple Loss(1): 0.2602\tClassification Loss: 1.8734\r\n",
      "Train Epoch: 1 [151680/209371 (72%)]\tAll Loss: 2.4239\tTriple Loss(1): 0.2161\tClassification Loss: 1.9918\r\n",
      "Train Epoch: 1 [152320/209371 (73%)]\tAll Loss: 2.4925\tTriple Loss(1): 0.2442\tClassification Loss: 2.0041\r\n",
      "Train Epoch: 1 [152960/209371 (73%)]\tAll Loss: 3.5205\tTriple Loss(0): 0.8241\tClassification Loss: 1.8724\r\n",
      "Train Epoch: 1 [153600/209371 (73%)]\tAll Loss: 2.5453\tTriple Loss(1): 0.2377\tClassification Loss: 2.0700\r\n",
      "Train Epoch: 1 [154240/209371 (74%)]\tAll Loss: 2.5003\tTriple Loss(1): 0.3715\tClassification Loss: 1.7572\r\n",
      "Train Epoch: 1 [154880/209371 (74%)]\tAll Loss: 2.6326\tTriple Loss(1): 0.3261\tClassification Loss: 1.9805\r\n",
      "Train Epoch: 1 [155520/209371 (74%)]\tAll Loss: 2.9322\tTriple Loss(1): 0.3955\tClassification Loss: 2.1413\r\n",
      "Train Epoch: 1 [156160/209371 (75%)]\tAll Loss: 2.3794\tTriple Loss(1): 0.2246\tClassification Loss: 1.9302\r\n",
      "Train Epoch: 1 [156800/209371 (75%)]\tAll Loss: 2.7741\tTriple Loss(1): 0.2784\tClassification Loss: 2.2173\r\n",
      "Train Epoch: 1 [157440/209371 (75%)]\tAll Loss: 2.2697\tTriple Loss(1): 0.2021\tClassification Loss: 1.8656\r\n",
      "Train Epoch: 1 [158080/209371 (76%)]\tAll Loss: 3.6245\tTriple Loss(0): 0.8415\tClassification Loss: 1.9414\r\n",
      "Train Epoch: 1 [158720/209371 (76%)]\tAll Loss: 2.7639\tTriple Loss(1): 0.4139\tClassification Loss: 1.9361\r\n",
      "Train Epoch: 1 [159360/209371 (76%)]\tAll Loss: 2.2552\tTriple Loss(1): 0.2103\tClassification Loss: 1.8345\r\n",
      "Train Epoch: 1 [160000/209371 (76%)]\tAll Loss: 3.1813\tTriple Loss(1): 0.4321\tClassification Loss: 2.3171\r\n",
      "Train Epoch: 1 [160640/209371 (77%)]\tAll Loss: 2.3215\tTriple Loss(1): 0.1120\tClassification Loss: 2.0974\r\n",
      "Train Epoch: 1 [161280/209371 (77%)]\tAll Loss: 3.0168\tTriple Loss(1): 0.3876\tClassification Loss: 2.2416\r\n",
      "Train Epoch: 1 [161920/209371 (77%)]\tAll Loss: 2.5519\tTriple Loss(1): 0.1850\tClassification Loss: 2.1819\r\n",
      "Train Epoch: 1 [162560/209371 (78%)]\tAll Loss: 2.8799\tTriple Loss(1): 0.3780\tClassification Loss: 2.1240\r\n",
      "Train Epoch: 1 [163200/209371 (78%)]\tAll Loss: 2.6161\tTriple Loss(1): 0.2931\tClassification Loss: 2.0299\r\n",
      "Train Epoch: 1 [163840/209371 (78%)]\tAll Loss: 3.4454\tTriple Loss(0): 0.6465\tClassification Loss: 2.1524\r\n",
      "Train Epoch: 1 [164480/209371 (79%)]\tAll Loss: 2.6955\tTriple Loss(1): 0.3361\tClassification Loss: 2.0233\r\n",
      "Train Epoch: 1 [165120/209371 (79%)]\tAll Loss: 2.0977\tTriple Loss(1): 0.1091\tClassification Loss: 1.8795\r\n",
      "Train Epoch: 1 [165760/209371 (79%)]\tAll Loss: 2.6614\tTriple Loss(1): 0.2807\tClassification Loss: 2.1001\r\n",
      "Train Epoch: 1 [166400/209371 (79%)]\tAll Loss: 2.5878\tTriple Loss(1): 0.2782\tClassification Loss: 2.0314\r\n",
      "Train Epoch: 1 [167040/209371 (80%)]\tAll Loss: 2.0954\tTriple Loss(1): 0.0663\tClassification Loss: 1.9628\r\n",
      "Train Epoch: 1 [167680/209371 (80%)]\tAll Loss: 2.5869\tTriple Loss(1): 0.1881\tClassification Loss: 2.2107\r\n",
      "Train Epoch: 1 [168320/209371 (80%)]\tAll Loss: 2.7006\tTriple Loss(1): 0.2336\tClassification Loss: 2.2334\r\n",
      "Train Epoch: 1 [168960/209371 (81%)]\tAll Loss: 2.4122\tTriple Loss(1): 0.2079\tClassification Loss: 1.9964\r\n",
      "Train Epoch: 1 [169600/209371 (81%)]\tAll Loss: 2.2187\tTriple Loss(1): 0.1981\tClassification Loss: 1.8225\r\n",
      "Train Epoch: 1 [170240/209371 (81%)]\tAll Loss: 2.3476\tTriple Loss(1): 0.2222\tClassification Loss: 1.9032\r\n",
      "Train Epoch: 1 [170880/209371 (82%)]\tAll Loss: 2.7115\tTriple Loss(1): 0.3018\tClassification Loss: 2.1080\r\n",
      "Train Epoch: 1 [171520/209371 (82%)]\tAll Loss: 3.6820\tTriple Loss(0): 0.6823\tClassification Loss: 2.3174\r\n",
      "Train Epoch: 1 [172160/209371 (82%)]\tAll Loss: 2.6906\tTriple Loss(1): 0.2176\tClassification Loss: 2.2554\r\n",
      "Train Epoch: 1 [172800/209371 (83%)]\tAll Loss: 2.6662\tTriple Loss(1): 0.2569\tClassification Loss: 2.1524\r\n",
      "Train Epoch: 1 [173440/209371 (83%)]\tAll Loss: 2.5384\tTriple Loss(1): 0.2219\tClassification Loss: 2.0945\r\n",
      "Train Epoch: 1 [174080/209371 (83%)]\tAll Loss: 2.8225\tTriple Loss(1): 0.3564\tClassification Loss: 2.1098\r\n",
      "Train Epoch: 1 [174720/209371 (83%)]\tAll Loss: 3.8075\tTriple Loss(0): 0.7200\tClassification Loss: 2.3674\r\n",
      "Train Epoch: 1 [175360/209371 (84%)]\tAll Loss: 2.1490\tTriple Loss(1): 0.1561\tClassification Loss: 1.8368\r\n",
      "Train Epoch: 1 [176000/209371 (84%)]\tAll Loss: 2.4394\tTriple Loss(1): 0.2685\tClassification Loss: 1.9025\r\n",
      "Train Epoch: 1 [176640/209371 (84%)]\tAll Loss: 2.6656\tTriple Loss(1): 0.2199\tClassification Loss: 2.2258\r\n",
      "Train Epoch: 1 [177280/209371 (85%)]\tAll Loss: 2.8302\tTriple Loss(1): 0.3432\tClassification Loss: 2.1439\r\n",
      "Train Epoch: 1 [177920/209371 (85%)]\tAll Loss: 2.6935\tTriple Loss(1): 0.4231\tClassification Loss: 1.8474\r\n",
      "Train Epoch: 1 [178560/209371 (85%)]\tAll Loss: 2.4369\tTriple Loss(1): 0.1450\tClassification Loss: 2.1470\r\n",
      "Train Epoch: 1 [179200/209371 (86%)]\tAll Loss: 2.9374\tTriple Loss(1): 0.4196\tClassification Loss: 2.0982\r\n",
      "Train Epoch: 1 [179840/209371 (86%)]\tAll Loss: 4.5092\tTriple Loss(0): 1.0693\tClassification Loss: 2.3706\r\n",
      "Train Epoch: 1 [180480/209371 (86%)]\tAll Loss: 3.0784\tTriple Loss(0): 0.5205\tClassification Loss: 2.0374\r\n",
      "Train Epoch: 1 [181120/209371 (87%)]\tAll Loss: 2.6010\tTriple Loss(1): 0.1601\tClassification Loss: 2.2807\r\n",
      "Train Epoch: 1 [181760/209371 (87%)]\tAll Loss: 3.8050\tTriple Loss(0): 0.5941\tClassification Loss: 2.6168\r\n",
      "Train Epoch: 1 [182400/209371 (87%)]\tAll Loss: 2.7550\tTriple Loss(1): 0.2939\tClassification Loss: 2.1671\r\n",
      "Train Epoch: 1 [183040/209371 (87%)]\tAll Loss: 2.7307\tTriple Loss(1): 0.3090\tClassification Loss: 2.1128\r\n",
      "Train Epoch: 1 [183680/209371 (88%)]\tAll Loss: 2.5572\tTriple Loss(1): 0.2373\tClassification Loss: 2.0825\r\n",
      "Train Epoch: 1 [184320/209371 (88%)]\tAll Loss: 2.5758\tTriple Loss(1): 0.3481\tClassification Loss: 1.8796\r\n",
      "Train Epoch: 1 [184960/209371 (88%)]\tAll Loss: 2.4469\tTriple Loss(1): 0.2399\tClassification Loss: 1.9672\r\n",
      "Train Epoch: 1 [185600/209371 (89%)]\tAll Loss: 3.2142\tTriple Loss(0): 0.6116\tClassification Loss: 1.9911\r\n",
      "Train Epoch: 1 [186240/209371 (89%)]\tAll Loss: 2.6892\tTriple Loss(1): 0.2764\tClassification Loss: 2.1364\r\n",
      "Train Epoch: 1 [186880/209371 (89%)]\tAll Loss: 2.7446\tTriple Loss(1): 0.2837\tClassification Loss: 2.1772\r\n",
      "Train Epoch: 1 [187520/209371 (90%)]\tAll Loss: 2.5497\tTriple Loss(1): 0.2237\tClassification Loss: 2.1023\r\n",
      "Train Epoch: 1 [188160/209371 (90%)]\tAll Loss: 2.6980\tTriple Loss(1): 0.2502\tClassification Loss: 2.1976\r\n",
      "Train Epoch: 1 [188800/209371 (90%)]\tAll Loss: 2.6285\tTriple Loss(1): 0.3579\tClassification Loss: 1.9127\r\n",
      "Train Epoch: 1 [189440/209371 (90%)]\tAll Loss: 2.3510\tTriple Loss(1): 0.2988\tClassification Loss: 1.7534\r\n",
      "Train Epoch: 1 [190080/209371 (91%)]\tAll Loss: 3.3117\tTriple Loss(0): 0.6703\tClassification Loss: 1.9711\r\n",
      "Train Epoch: 1 [190720/209371 (91%)]\tAll Loss: 2.1308\tTriple Loss(1): 0.1930\tClassification Loss: 1.7447\r\n",
      "Train Epoch: 1 [191360/209371 (91%)]\tAll Loss: 3.0333\tTriple Loss(1): 0.3728\tClassification Loss: 2.2878\r\n",
      "Train Epoch: 1 [192000/209371 (92%)]\tAll Loss: 2.4059\tTriple Loss(1): 0.2209\tClassification Loss: 1.9642\r\n",
      "Train Epoch: 1 [192640/209371 (92%)]\tAll Loss: 2.6325\tTriple Loss(1): 0.2797\tClassification Loss: 2.0731\r\n",
      "Train Epoch: 1 [193280/209371 (92%)]\tAll Loss: 2.5777\tTriple Loss(1): 0.2425\tClassification Loss: 2.0927\r\n",
      "Train Epoch: 1 [193920/209371 (93%)]\tAll Loss: 3.5670\tTriple Loss(0): 0.8461\tClassification Loss: 1.8748\r\n",
      "Train Epoch: 1 [194560/209371 (93%)]\tAll Loss: 3.0001\tTriple Loss(0): 0.5162\tClassification Loss: 1.9677\r\n",
      "Train Epoch: 1 [195200/209371 (93%)]\tAll Loss: 2.6764\tTriple Loss(1): 0.2705\tClassification Loss: 2.1353\r\n",
      "Train Epoch: 1 [195840/209371 (94%)]\tAll Loss: 2.5986\tTriple Loss(1): 0.3646\tClassification Loss: 1.8695\r\n",
      "Train Epoch: 1 [196480/209371 (94%)]\tAll Loss: 2.4697\tTriple Loss(1): 0.1941\tClassification Loss: 2.0815\r\n",
      "Train Epoch: 1 [197120/209371 (94%)]\tAll Loss: 2.8376\tTriple Loss(0): 0.5755\tClassification Loss: 1.6867\r\n",
      "Train Epoch: 1 [197760/209371 (94%)]\tAll Loss: 2.3830\tTriple Loss(1): 0.2094\tClassification Loss: 1.9643\r\n",
      "Train Epoch: 1 [198400/209371 (95%)]\tAll Loss: 2.4698\tTriple Loss(1): 0.2934\tClassification Loss: 1.8830\r\n",
      "Train Epoch: 1 [199040/209371 (95%)]\tAll Loss: 2.4177\tTriple Loss(1): 0.2516\tClassification Loss: 1.9145\r\n",
      "Train Epoch: 1 [199680/209371 (95%)]\tAll Loss: 2.2522\tTriple Loss(1): 0.1587\tClassification Loss: 1.9348\r\n",
      "Train Epoch: 1 [200320/209371 (96%)]\tAll Loss: 2.7542\tTriple Loss(1): 0.3322\tClassification Loss: 2.0897\r\n",
      "Train Epoch: 1 [200960/209371 (96%)]\tAll Loss: 2.2401\tTriple Loss(1): 0.2053\tClassification Loss: 1.8295\r\n",
      "Train Epoch: 1 [201600/209371 (96%)]\tAll Loss: 2.7120\tTriple Loss(1): 0.3483\tClassification Loss: 2.0153\r\n",
      "Train Epoch: 1 [202240/209371 (97%)]\tAll Loss: 2.4103\tTriple Loss(1): 0.3403\tClassification Loss: 1.7298\r\n",
      "Train Epoch: 1 [202880/209371 (97%)]\tAll Loss: 2.8151\tTriple Loss(0): 0.6650\tClassification Loss: 1.4852\r\n",
      "Train Epoch: 1 [203520/209371 (97%)]\tAll Loss: 2.7858\tTriple Loss(1): 0.2407\tClassification Loss: 2.3043\r\n",
      "Train Epoch: 1 [204160/209371 (98%)]\tAll Loss: 2.5229\tTriple Loss(1): 0.3739\tClassification Loss: 1.7751\r\n",
      "Train Epoch: 1 [204800/209371 (98%)]\tAll Loss: 2.7816\tTriple Loss(1): 0.2037\tClassification Loss: 2.3742\r\n",
      "Train Epoch: 1 [205440/209371 (98%)]\tAll Loss: 2.6195\tTriple Loss(1): 0.3368\tClassification Loss: 1.9459\r\n",
      "Train Epoch: 1 [206080/209371 (98%)]\tAll Loss: 2.2654\tTriple Loss(1): 0.2050\tClassification Loss: 1.8555\r\n",
      "Train Epoch: 1 [206720/209371 (99%)]\tAll Loss: 3.9679\tTriple Loss(0): 0.9025\tClassification Loss: 2.1629\r\n",
      "Train Epoch: 1 [207360/209371 (99%)]\tAll Loss: 2.3809\tTriple Loss(1): 0.2616\tClassification Loss: 1.8576\r\n",
      "Train Epoch: 1 [208000/209371 (99%)]\tAll Loss: 2.6603\tTriple Loss(1): 0.1751\tClassification Loss: 2.3100\r\n",
      "Train Epoch: 1 [208640/209371 (100%)]\tAll Loss: 2.5549\tTriple Loss(1): 0.2291\tClassification Loss: 2.0968\r\n",
      "Train Epoch: 1 [209280/209371 (100%)]\tAll Loss: 3.1473\tTriple Loss(1): 0.4899\tClassification Loss: 2.1674\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.001/1_epochs\r\n",
      "Train Epoch: 2 [0/209371 (0%)]\tAll Loss: 4.0811\tTriple Loss(0): 0.9240\tClassification Loss: 2.2331\r\n",
      "\r\n",
      "Test set: Average loss: 1.8939, Accuracy: 37944/80128 (47%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/209371 (0%)]\tAll Loss: 2.3531\tTriple Loss(1): 0.1419\tClassification Loss: 2.0694\r\n",
      "Train Epoch: 2 [1280/209371 (1%)]\tAll Loss: 3.3441\tTriple Loss(0): 0.6329\tClassification Loss: 2.0784\r\n",
      "Train Epoch: 2 [1920/209371 (1%)]\tAll Loss: 2.5827\tTriple Loss(1): 0.3542\tClassification Loss: 1.8743\r\n",
      "Train Epoch: 2 [2560/209371 (1%)]\tAll Loss: 2.3472\tTriple Loss(1): 0.1998\tClassification Loss: 1.9475\r\n",
      "Train Epoch: 2 [3200/209371 (2%)]\tAll Loss: 2.2595\tTriple Loss(1): 0.2139\tClassification Loss: 1.8318\r\n",
      "Train Epoch: 2 [3840/209371 (2%)]\tAll Loss: 2.3839\tTriple Loss(1): 0.2241\tClassification Loss: 1.9357\r\n",
      "Train Epoch: 2 [4480/209371 (2%)]\tAll Loss: 3.2030\tTriple Loss(0): 0.5708\tClassification Loss: 2.0613\r\n",
      "Train Epoch: 2 [5120/209371 (2%)]\tAll Loss: 2.4064\tTriple Loss(1): 0.2772\tClassification Loss: 1.8520\r\n",
      "Train Epoch: 2 [5760/209371 (3%)]\tAll Loss: 2.6648\tTriple Loss(1): 0.2814\tClassification Loss: 2.1020\r\n",
      "Train Epoch: 2 [6400/209371 (3%)]\tAll Loss: 2.5814\tTriple Loss(1): 0.2165\tClassification Loss: 2.1485\r\n",
      "Train Epoch: 2 [7040/209371 (3%)]\tAll Loss: 3.0825\tTriple Loss(0): 0.6550\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 2 [7680/209371 (4%)]\tAll Loss: 4.5579\tTriple Loss(0): 1.1451\tClassification Loss: 2.2677\r\n",
      "Train Epoch: 2 [8320/209371 (4%)]\tAll Loss: 2.8719\tTriple Loss(0): 0.5719\tClassification Loss: 1.7281\r\n",
      "Train Epoch: 2 [8960/209371 (4%)]\tAll Loss: 2.3650\tTriple Loss(1): 0.1721\tClassification Loss: 2.0208\r\n",
      "Train Epoch: 2 [9600/209371 (5%)]\tAll Loss: 2.1796\tTriple Loss(1): 0.2379\tClassification Loss: 1.7039\r\n",
      "Train Epoch: 2 [10240/209371 (5%)]\tAll Loss: 2.5497\tTriple Loss(1): 0.2700\tClassification Loss: 2.0097\r\n",
      "Train Epoch: 2 [10880/209371 (5%)]\tAll Loss: 2.3639\tTriple Loss(1): 0.2286\tClassification Loss: 1.9068\r\n",
      "Train Epoch: 2 [11520/209371 (6%)]\tAll Loss: 3.0536\tTriple Loss(1): 0.3482\tClassification Loss: 2.3572\r\n",
      "Train Epoch: 2 [12160/209371 (6%)]\tAll Loss: 2.2897\tTriple Loss(1): 0.2203\tClassification Loss: 1.8491\r\n",
      "Train Epoch: 2 [12800/209371 (6%)]\tAll Loss: 2.4371\tTriple Loss(1): 0.1764\tClassification Loss: 2.0842\r\n",
      "Train Epoch: 2 [13440/209371 (6%)]\tAll Loss: 2.1968\tTriple Loss(1): 0.1248\tClassification Loss: 1.9471\r\n",
      "Train Epoch: 2 [14080/209371 (7%)]\tAll Loss: 2.3158\tTriple Loss(1): 0.2051\tClassification Loss: 1.9056\r\n",
      "Train Epoch: 2 [14720/209371 (7%)]\tAll Loss: 2.4693\tTriple Loss(1): 0.1936\tClassification Loss: 2.0822\r\n",
      "Train Epoch: 2 [15360/209371 (7%)]\tAll Loss: 2.5542\tTriple Loss(1): 0.3698\tClassification Loss: 1.8146\r\n",
      "Train Epoch: 2 [16000/209371 (8%)]\tAll Loss: 2.7771\tTriple Loss(1): 0.3103\tClassification Loss: 2.1564\r\n",
      "Train Epoch: 2 [16640/209371 (8%)]\tAll Loss: 3.2100\tTriple Loss(0): 0.5639\tClassification Loss: 2.0821\r\n",
      "Train Epoch: 2 [17280/209371 (8%)]\tAll Loss: 2.5002\tTriple Loss(1): 0.2331\tClassification Loss: 2.0340\r\n",
      "Train Epoch: 2 [17920/209371 (9%)]\tAll Loss: 2.7050\tTriple Loss(1): 0.2588\tClassification Loss: 2.1874\r\n",
      "Train Epoch: 2 [18560/209371 (9%)]\tAll Loss: 2.5911\tTriple Loss(1): 0.2292\tClassification Loss: 2.1327\r\n",
      "Train Epoch: 2 [19200/209371 (9%)]\tAll Loss: 2.5440\tTriple Loss(1): 0.2581\tClassification Loss: 2.0278\r\n",
      "Train Epoch: 2 [19840/209371 (9%)]\tAll Loss: 2.5603\tTriple Loss(1): 0.3184\tClassification Loss: 1.9236\r\n",
      "Train Epoch: 2 [20480/209371 (10%)]\tAll Loss: 3.6714\tTriple Loss(0): 0.8559\tClassification Loss: 1.9597\r\n",
      "Train Epoch: 2 [21120/209371 (10%)]\tAll Loss: 2.4896\tTriple Loss(1): 0.2112\tClassification Loss: 2.0672\r\n",
      "Train Epoch: 2 [21760/209371 (10%)]\tAll Loss: 2.6176\tTriple Loss(1): 0.2056\tClassification Loss: 2.2064\r\n",
      "Train Epoch: 2 [22400/209371 (11%)]\tAll Loss: 2.4120\tTriple Loss(1): 0.1894\tClassification Loss: 2.0332\r\n",
      "Train Epoch: 2 [23040/209371 (11%)]\tAll Loss: 2.5998\tTriple Loss(1): 0.2601\tClassification Loss: 2.0795\r\n",
      "Train Epoch: 2 [23680/209371 (11%)]\tAll Loss: 2.2947\tTriple Loss(1): 0.2041\tClassification Loss: 1.8865\r\n",
      "Train Epoch: 2 [24320/209371 (12%)]\tAll Loss: 3.2542\tTriple Loss(0): 0.5874\tClassification Loss: 2.0794\r\n",
      "Train Epoch: 2 [24960/209371 (12%)]\tAll Loss: 2.1861\tTriple Loss(1): 0.1161\tClassification Loss: 1.9540\r\n",
      "Train Epoch: 2 [25600/209371 (12%)]\tAll Loss: 3.5007\tTriple Loss(0): 0.7385\tClassification Loss: 2.0238\r\n",
      "Train Epoch: 2 [26240/209371 (13%)]\tAll Loss: 3.4254\tTriple Loss(0): 0.7670\tClassification Loss: 1.8914\r\n",
      "Train Epoch: 2 [26880/209371 (13%)]\tAll Loss: 2.7855\tTriple Loss(1): 0.3927\tClassification Loss: 2.0001\r\n",
      "Train Epoch: 2 [27520/209371 (13%)]\tAll Loss: 2.2832\tTriple Loss(1): 0.1761\tClassification Loss: 1.9309\r\n",
      "Train Epoch: 2 [28160/209371 (13%)]\tAll Loss: 2.7258\tTriple Loss(1): 0.3412\tClassification Loss: 2.0433\r\n",
      "Train Epoch: 2 [28800/209371 (14%)]\tAll Loss: 2.1917\tTriple Loss(1): 0.1937\tClassification Loss: 1.8043\r\n",
      "Train Epoch: 2 [29440/209371 (14%)]\tAll Loss: 3.6847\tTriple Loss(0): 0.8380\tClassification Loss: 2.0087\r\n",
      "Train Epoch: 2 [30080/209371 (14%)]\tAll Loss: 2.7961\tTriple Loss(0): 0.5515\tClassification Loss: 1.6930\r\n",
      "Train Epoch: 2 [30720/209371 (15%)]\tAll Loss: 2.2290\tTriple Loss(1): 0.1335\tClassification Loss: 1.9620\r\n",
      "Train Epoch: 2 [31360/209371 (15%)]\tAll Loss: 2.8208\tTriple Loss(1): 0.3125\tClassification Loss: 2.1959\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209539 (0%)]\tAll Loss: 5.1293\tTriple Loss(1): 0.5911\tClassification Loss: 3.9470\r\n",
      "train.py:213: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9235, Accuracy: 1911/80128 (2%)\r\n",
      "\r\n",
      "Train Epoch: 1 [640/209539 (0%)]\tAll Loss: 4.2651\tTriple Loss(1): 0.3743\tClassification Loss: 3.5166\r\n",
      "Train Epoch: 1 [1280/209539 (1%)]\tAll Loss: 4.1336\tTriple Loss(1): 0.3813\tClassification Loss: 3.3710\r\n",
      "Train Epoch: 1 [1920/209539 (1%)]\tAll Loss: 4.8955\tTriple Loss(0): 1.0321\tClassification Loss: 2.8312\r\n",
      "Train Epoch: 1 [2560/209539 (1%)]\tAll Loss: 3.8840\tTriple Loss(1): 0.4737\tClassification Loss: 2.9367\r\n",
      "Train Epoch: 1 [3200/209539 (2%)]\tAll Loss: 3.6880\tTriple Loss(1): 0.4213\tClassification Loss: 2.8455\r\n",
      "Train Epoch: 1 [3840/209539 (2%)]\tAll Loss: 3.8061\tTriple Loss(1): 0.5001\tClassification Loss: 2.8060\r\n",
      "Train Epoch: 1 [4480/209539 (2%)]\tAll Loss: 3.3799\tTriple Loss(1): 0.3202\tClassification Loss: 2.7395\r\n",
      "Train Epoch: 1 [5120/209539 (2%)]\tAll Loss: 3.6034\tTriple Loss(1): 0.3755\tClassification Loss: 2.8523\r\n",
      "Train Epoch: 1 [5760/209539 (3%)]\tAll Loss: 3.2706\tTriple Loss(1): 0.3566\tClassification Loss: 2.5574\r\n",
      "Train Epoch: 1 [6400/209539 (3%)]\tAll Loss: 3.6432\tTriple Loss(1): 0.4546\tClassification Loss: 2.7339\r\n",
      "Train Epoch: 1 [7040/209539 (3%)]\tAll Loss: 3.2652\tTriple Loss(1): 0.2454\tClassification Loss: 2.7744\r\n",
      "Train Epoch: 1 [7680/209539 (4%)]\tAll Loss: 3.3352\tTriple Loss(1): 0.3485\tClassification Loss: 2.6382\r\n",
      "Train Epoch: 1 [8320/209539 (4%)]\tAll Loss: 4.0496\tTriple Loss(0): 0.7744\tClassification Loss: 2.5008\r\n",
      "Train Epoch: 1 [8960/209539 (4%)]\tAll Loss: 3.0531\tTriple Loss(1): 0.2791\tClassification Loss: 2.4949\r\n",
      "Train Epoch: 1 [9600/209539 (5%)]\tAll Loss: 3.4186\tTriple Loss(1): 0.4388\tClassification Loss: 2.5411\r\n",
      "Train Epoch: 1 [10240/209539 (5%)]\tAll Loss: 3.0186\tTriple Loss(1): 0.3436\tClassification Loss: 2.3314\r\n",
      "Train Epoch: 1 [10880/209539 (5%)]\tAll Loss: 3.5787\tTriple Loss(1): 0.4273\tClassification Loss: 2.7240\r\n",
      "Train Epoch: 1 [11520/209539 (5%)]\tAll Loss: 3.0208\tTriple Loss(1): 0.3246\tClassification Loss: 2.3716\r\n",
      "Train Epoch: 1 [12160/209539 (6%)]\tAll Loss: 3.2198\tTriple Loss(1): 0.3453\tClassification Loss: 2.5292\r\n",
      "Train Epoch: 1 [12800/209539 (6%)]\tAll Loss: 4.0564\tTriple Loss(0): 0.7964\tClassification Loss: 2.4637\r\n",
      "Train Epoch: 1 [13440/209539 (6%)]\tAll Loss: 3.3409\tTriple Loss(1): 0.4080\tClassification Loss: 2.5249\r\n",
      "Train Epoch: 1 [14080/209539 (7%)]\tAll Loss: 3.3916\tTriple Loss(1): 0.4803\tClassification Loss: 2.4309\r\n",
      "Train Epoch: 1 [14720/209539 (7%)]\tAll Loss: 3.3121\tTriple Loss(1): 0.3511\tClassification Loss: 2.6100\r\n",
      "Train Epoch: 1 [15360/209539 (7%)]\tAll Loss: 3.3034\tTriple Loss(1): 0.4223\tClassification Loss: 2.4588\r\n",
      "Train Epoch: 1 [16000/209539 (8%)]\tAll Loss: 3.5912\tTriple Loss(0): 0.6291\tClassification Loss: 2.3331\r\n",
      "Train Epoch: 1 [16640/209539 (8%)]\tAll Loss: 2.7769\tTriple Loss(1): 0.3009\tClassification Loss: 2.1751\r\n",
      "Train Epoch: 1 [17280/209539 (8%)]\tAll Loss: 3.9192\tTriple Loss(0): 0.7928\tClassification Loss: 2.3336\r\n",
      "Train Epoch: 1 [17920/209539 (9%)]\tAll Loss: 3.0007\tTriple Loss(1): 0.3694\tClassification Loss: 2.2618\r\n",
      "Train Epoch: 1 [18560/209539 (9%)]\tAll Loss: 3.0900\tTriple Loss(1): 0.2810\tClassification Loss: 2.5279\r\n",
      "Train Epoch: 1 [19200/209539 (9%)]\tAll Loss: 3.0358\tTriple Loss(1): 0.3464\tClassification Loss: 2.3431\r\n",
      "Train Epoch: 1 [19840/209539 (9%)]\tAll Loss: 3.9499\tTriple Loss(0): 0.7791\tClassification Loss: 2.3918\r\n",
      "Train Epoch: 1 [20480/209539 (10%)]\tAll Loss: 3.7786\tTriple Loss(1): 0.5094\tClassification Loss: 2.7599\r\n",
      "Train Epoch: 1 [21120/209539 (10%)]\tAll Loss: 2.6475\tTriple Loss(1): 0.2425\tClassification Loss: 2.1626\r\n",
      "Train Epoch: 1 [21760/209539 (10%)]\tAll Loss: 2.9704\tTriple Loss(1): 0.2009\tClassification Loss: 2.5686\r\n",
      "Train Epoch: 1 [22400/209539 (11%)]\tAll Loss: 3.0068\tTriple Loss(1): 0.4201\tClassification Loss: 2.1666\r\n",
      "Train Epoch: 1 [23040/209539 (11%)]\tAll Loss: 3.2618\tTriple Loss(1): 0.4139\tClassification Loss: 2.4339\r\n",
      "Train Epoch: 1 [23680/209539 (11%)]\tAll Loss: 3.6639\tTriple Loss(0): 0.6888\tClassification Loss: 2.2864\r\n",
      "Train Epoch: 1 [24320/209539 (12%)]\tAll Loss: 2.8215\tTriple Loss(1): 0.3153\tClassification Loss: 2.1908\r\n",
      "Train Epoch: 1 [24960/209539 (12%)]\tAll Loss: 2.8439\tTriple Loss(1): 0.3202\tClassification Loss: 2.2036\r\n",
      "Train Epoch: 1 [25600/209539 (12%)]\tAll Loss: 3.6770\tTriple Loss(0): 0.7701\tClassification Loss: 2.1368\r\n",
      "Train Epoch: 1 [26240/209539 (13%)]\tAll Loss: 2.8694\tTriple Loss(1): 0.2628\tClassification Loss: 2.3439\r\n",
      "Train Epoch: 1 [26880/209539 (13%)]\tAll Loss: 2.8602\tTriple Loss(1): 0.2421\tClassification Loss: 2.3761\r\n",
      "Train Epoch: 1 [27520/209539 (13%)]\tAll Loss: 2.7620\tTriple Loss(1): 0.2820\tClassification Loss: 2.1979\r\n",
      "Train Epoch: 1 [28160/209539 (13%)]\tAll Loss: 2.9216\tTriple Loss(1): 0.3503\tClassification Loss: 2.2210\r\n",
      "Train Epoch: 1 [28800/209539 (14%)]\tAll Loss: 3.0141\tTriple Loss(1): 0.3453\tClassification Loss: 2.3236\r\n",
      "Train Epoch: 1 [29440/209539 (14%)]\tAll Loss: 2.8605\tTriple Loss(1): 0.2344\tClassification Loss: 2.3916\r\n",
      "Train Epoch: 1 [30080/209539 (14%)]\tAll Loss: 2.6606\tTriple Loss(1): 0.2497\tClassification Loss: 2.1611\r\n",
      "Train Epoch: 1 [30720/209539 (15%)]\tAll Loss: 3.1809\tTriple Loss(1): 0.3584\tClassification Loss: 2.4641\r\n",
      "Train Epoch: 1 [31360/209539 (15%)]\tAll Loss: 3.8994\tTriple Loss(0): 0.6410\tClassification Loss: 2.6175\r\n",
      "Train Epoch: 1 [32000/209539 (15%)]\tAll Loss: 2.7156\tTriple Loss(1): 0.1527\tClassification Loss: 2.4102\r\n",
      "Train Epoch: 1 [32640/209539 (16%)]\tAll Loss: 4.0600\tTriple Loss(0): 0.7613\tClassification Loss: 2.5375\r\n",
      "Train Epoch: 1 [33280/209539 (16%)]\tAll Loss: 3.0375\tTriple Loss(1): 0.2830\tClassification Loss: 2.4714\r\n",
      "Train Epoch: 1 [33920/209539 (16%)]\tAll Loss: 3.4021\tTriple Loss(0): 0.5922\tClassification Loss: 2.2177\r\n",
      "Train Epoch: 1 [34560/209539 (16%)]\tAll Loss: 2.5568\tTriple Loss(1): 0.2568\tClassification Loss: 2.0431\r\n",
      "Train Epoch: 1 [35200/209539 (17%)]\tAll Loss: 3.1222\tTriple Loss(1): 0.3752\tClassification Loss: 2.3719\r\n",
      "Train Epoch: 1 [35840/209539 (17%)]\tAll Loss: 3.3938\tTriple Loss(0): 0.6262\tClassification Loss: 2.1415\r\n",
      "Train Epoch: 1 [36480/209539 (17%)]\tAll Loss: 2.8964\tTriple Loss(1): 0.3086\tClassification Loss: 2.2792\r\n",
      "Train Epoch: 1 [37120/209539 (18%)]\tAll Loss: 4.0460\tTriple Loss(0): 0.8642\tClassification Loss: 2.3177\r\n",
      "Train Epoch: 1 [37760/209539 (18%)]\tAll Loss: 3.6202\tTriple Loss(0): 0.7266\tClassification Loss: 2.1670\r\n",
      "Train Epoch: 1 [38400/209539 (18%)]\tAll Loss: 2.5472\tTriple Loss(1): 0.4044\tClassification Loss: 1.7384\r\n",
      "Train Epoch: 1 [39040/209539 (19%)]\tAll Loss: 2.9538\tTriple Loss(1): 0.3759\tClassification Loss: 2.2019\r\n",
      "Train Epoch: 1 [39680/209539 (19%)]\tAll Loss: 2.8015\tTriple Loss(1): 0.3391\tClassification Loss: 2.1232\r\n",
      "Train Epoch: 1 [40320/209539 (19%)]\tAll Loss: 4.3489\tTriple Loss(0): 0.8311\tClassification Loss: 2.6867\r\n",
      "Train Epoch: 1 [40960/209539 (20%)]\tAll Loss: 3.2834\tTriple Loss(1): 0.4951\tClassification Loss: 2.2933\r\n",
      "Train Epoch: 1 [41600/209539 (20%)]\tAll Loss: 3.4330\tTriple Loss(1): 0.4388\tClassification Loss: 2.5554\r\n",
      "Train Epoch: 1 [42240/209539 (20%)]\tAll Loss: 3.0988\tTriple Loss(1): 0.3110\tClassification Loss: 2.4768\r\n",
      "Train Epoch: 1 [42880/209539 (20%)]\tAll Loss: 2.9320\tTriple Loss(1): 0.3989\tClassification Loss: 2.1343\r\n",
      "Train Epoch: 1 [43520/209539 (21%)]\tAll Loss: 4.1384\tTriple Loss(0): 0.8269\tClassification Loss: 2.4847\r\n",
      "Train Epoch: 1 [44160/209539 (21%)]\tAll Loss: 2.5165\tTriple Loss(1): 0.2654\tClassification Loss: 1.9858\r\n",
      "Train Epoch: 1 [44800/209539 (21%)]\tAll Loss: 2.8551\tTriple Loss(1): 0.3110\tClassification Loss: 2.2330\r\n",
      "Train Epoch: 1 [45440/209539 (22%)]\tAll Loss: 2.2299\tTriple Loss(1): 0.1588\tClassification Loss: 1.9122\r\n",
      "Train Epoch: 1 [46080/209539 (22%)]\tAll Loss: 3.0651\tTriple Loss(1): 0.4433\tClassification Loss: 2.1785\r\n",
      "Train Epoch: 1 [46720/209539 (22%)]\tAll Loss: 4.1449\tTriple Loss(0): 0.9094\tClassification Loss: 2.3260\r\n",
      "Train Epoch: 1 [47360/209539 (23%)]\tAll Loss: 3.1095\tTriple Loss(1): 0.4116\tClassification Loss: 2.2863\r\n",
      "Train Epoch: 1 [48000/209539 (23%)]\tAll Loss: 3.8845\tTriple Loss(0): 0.7015\tClassification Loss: 2.4816\r\n",
      "Train Epoch: 1 [48640/209539 (23%)]\tAll Loss: 2.7680\tTriple Loss(1): 0.3685\tClassification Loss: 2.0311\r\n",
      "Train Epoch: 1 [49280/209539 (24%)]\tAll Loss: 2.4470\tTriple Loss(1): 0.2145\tClassification Loss: 2.0179\r\n",
      "Train Epoch: 1 [49920/209539 (24%)]\tAll Loss: 2.8276\tTriple Loss(1): 0.2787\tClassification Loss: 2.2703\r\n",
      "Train Epoch: 1 [50560/209539 (24%)]\tAll Loss: 2.5883\tTriple Loss(1): 0.2941\tClassification Loss: 2.0000\r\n",
      "Train Epoch: 1 [51200/209539 (24%)]\tAll Loss: 2.6802\tTriple Loss(1): 0.2597\tClassification Loss: 2.1608\r\n",
      "Train Epoch: 1 [51840/209539 (25%)]\tAll Loss: 2.4502\tTriple Loss(1): 0.1539\tClassification Loss: 2.1424\r\n",
      "Train Epoch: 1 [52480/209539 (25%)]\tAll Loss: 2.8910\tTriple Loss(1): 0.3582\tClassification Loss: 2.1746\r\n",
      "Train Epoch: 1 [53120/209539 (25%)]\tAll Loss: 2.3129\tTriple Loss(1): 0.2246\tClassification Loss: 1.8636\r\n",
      "Train Epoch: 1 [53760/209539 (26%)]\tAll Loss: 3.7711\tTriple Loss(0): 0.7153\tClassification Loss: 2.3406\r\n",
      "Train Epoch: 1 [54400/209539 (26%)]\tAll Loss: 2.8462\tTriple Loss(1): 0.2682\tClassification Loss: 2.3097\r\n",
      "Train Epoch: 1 [55040/209539 (26%)]\tAll Loss: 3.2284\tTriple Loss(0): 0.6617\tClassification Loss: 1.9050\r\n",
      "Train Epoch: 1 [55680/209539 (27%)]\tAll Loss: 2.9183\tTriple Loss(1): 0.4176\tClassification Loss: 2.0831\r\n",
      "Train Epoch: 1 [56320/209539 (27%)]\tAll Loss: 2.4724\tTriple Loss(1): 0.2105\tClassification Loss: 2.0514\r\n",
      "Train Epoch: 1 [56960/209539 (27%)]\tAll Loss: 2.5922\tTriple Loss(1): 0.2516\tClassification Loss: 2.0890\r\n",
      "Train Epoch: 1 [57600/209539 (27%)]\tAll Loss: 2.5326\tTriple Loss(1): 0.2110\tClassification Loss: 2.1105\r\n",
      "Train Epoch: 1 [58240/209539 (28%)]\tAll Loss: 2.3903\tTriple Loss(1): 0.1732\tClassification Loss: 2.0439\r\n",
      "Train Epoch: 1 [58880/209539 (28%)]\tAll Loss: 2.4827\tTriple Loss(1): 0.2200\tClassification Loss: 2.0427\r\n",
      "Train Epoch: 1 [59520/209539 (28%)]\tAll Loss: 2.7943\tTriple Loss(1): 0.3099\tClassification Loss: 2.1746\r\n",
      "Train Epoch: 1 [60160/209539 (29%)]\tAll Loss: 3.2891\tTriple Loss(0): 0.6178\tClassification Loss: 2.0535\r\n",
      "Train Epoch: 1 [60800/209539 (29%)]\tAll Loss: 2.6512\tTriple Loss(1): 0.1791\tClassification Loss: 2.2930\r\n",
      "Train Epoch: 1 [61440/209539 (29%)]\tAll Loss: 2.5621\tTriple Loss(1): 0.2573\tClassification Loss: 2.0475\r\n",
      "Train Epoch: 1 [62080/209539 (30%)]\tAll Loss: 2.7549\tTriple Loss(1): 0.2399\tClassification Loss: 2.2751\r\n",
      "Train Epoch: 1 [62720/209539 (30%)]\tAll Loss: 2.5007\tTriple Loss(1): 0.1548\tClassification Loss: 2.1912\r\n",
      "Train Epoch: 1 [63360/209539 (30%)]\tAll Loss: 2.6659\tTriple Loss(1): 0.2231\tClassification Loss: 2.2197\r\n",
      "Train Epoch: 1 [64000/209539 (31%)]\tAll Loss: 2.7081\tTriple Loss(1): 0.2731\tClassification Loss: 2.1619\r\n",
      "Train Epoch: 1 [64640/209539 (31%)]\tAll Loss: 2.9771\tTriple Loss(1): 0.3180\tClassification Loss: 2.3412\r\n",
      "Train Epoch: 1 [65280/209539 (31%)]\tAll Loss: 3.5489\tTriple Loss(0): 0.6809\tClassification Loss: 2.1871\r\n",
      "Train Epoch: 1 [65920/209539 (31%)]\tAll Loss: 3.3369\tTriple Loss(0): 0.6362\tClassification Loss: 2.0646\r\n",
      "Train Epoch: 1 [66560/209539 (32%)]\tAll Loss: 2.8757\tTriple Loss(1): 0.2958\tClassification Loss: 2.2841\r\n",
      "Train Epoch: 1 [67200/209539 (32%)]\tAll Loss: 2.7572\tTriple Loss(1): 0.2395\tClassification Loss: 2.2783\r\n",
      "Train Epoch: 1 [67840/209539 (32%)]\tAll Loss: 2.6324\tTriple Loss(1): 0.2843\tClassification Loss: 2.0639\r\n",
      "Train Epoch: 1 [68480/209539 (33%)]\tAll Loss: 3.2033\tTriple Loss(0): 0.5980\tClassification Loss: 2.0074\r\n",
      "Train Epoch: 1 [69120/209539 (33%)]\tAll Loss: 2.3631\tTriple Loss(1): 0.1937\tClassification Loss: 1.9758\r\n",
      "Train Epoch: 1 [69760/209539 (33%)]\tAll Loss: 2.7293\tTriple Loss(1): 0.1924\tClassification Loss: 2.3445\r\n",
      "Train Epoch: 1 [70400/209539 (34%)]\tAll Loss: 3.2690\tTriple Loss(0): 0.6733\tClassification Loss: 1.9225\r\n",
      "Train Epoch: 1 [71040/209539 (34%)]\tAll Loss: 2.5745\tTriple Loss(1): 0.2293\tClassification Loss: 2.1159\r\n",
      "Train Epoch: 1 [71680/209539 (34%)]\tAll Loss: 2.4731\tTriple Loss(1): 0.2760\tClassification Loss: 1.9211\r\n",
      "Train Epoch: 1 [72320/209539 (35%)]\tAll Loss: 2.8069\tTriple Loss(1): 0.2571\tClassification Loss: 2.2926\r\n",
      "Train Epoch: 1 [72960/209539 (35%)]\tAll Loss: 3.3468\tTriple Loss(0): 0.6500\tClassification Loss: 2.0468\r\n",
      "Train Epoch: 1 [73600/209539 (35%)]\tAll Loss: 2.5952\tTriple Loss(1): 0.2306\tClassification Loss: 2.1339\r\n",
      "Train Epoch: 1 [74240/209539 (35%)]\tAll Loss: 2.6028\tTriple Loss(1): 0.2013\tClassification Loss: 2.2002\r\n",
      "Train Epoch: 1 [74880/209539 (36%)]\tAll Loss: 2.4134\tTriple Loss(1): 0.2607\tClassification Loss: 1.8920\r\n",
      "Train Epoch: 1 [75520/209539 (36%)]\tAll Loss: 2.7673\tTriple Loss(1): 0.3410\tClassification Loss: 2.0853\r\n",
      "Train Epoch: 1 [76160/209539 (36%)]\tAll Loss: 3.7575\tTriple Loss(0): 0.7647\tClassification Loss: 2.2281\r\n",
      "Train Epoch: 1 [76800/209539 (37%)]\tAll Loss: 2.6839\tTriple Loss(1): 0.2624\tClassification Loss: 2.1591\r\n",
      "Train Epoch: 1 [77440/209539 (37%)]\tAll Loss: 3.8478\tTriple Loss(0): 0.8548\tClassification Loss: 2.1382\r\n",
      "Train Epoch: 1 [78080/209539 (37%)]\tAll Loss: 2.6189\tTriple Loss(1): 0.2935\tClassification Loss: 2.0318\r\n",
      "Train Epoch: 1 [78720/209539 (38%)]\tAll Loss: 3.9706\tTriple Loss(0): 0.8600\tClassification Loss: 2.2506\r\n",
      "Train Epoch: 1 [79360/209539 (38%)]\tAll Loss: 4.4706\tTriple Loss(0): 0.9869\tClassification Loss: 2.4969\r\n",
      "Train Epoch: 1 [80000/209539 (38%)]\tAll Loss: 2.8170\tTriple Loss(1): 0.1896\tClassification Loss: 2.4379\r\n",
      "Train Epoch: 1 [80640/209539 (38%)]\tAll Loss: 2.1600\tTriple Loss(1): 0.1140\tClassification Loss: 1.9321\r\n",
      "Train Epoch: 1 [81280/209539 (39%)]\tAll Loss: 2.8353\tTriple Loss(1): 0.2777\tClassification Loss: 2.2799\r\n",
      "Train Epoch: 1 [81920/209539 (39%)]\tAll Loss: 2.5408\tTriple Loss(1): 0.2380\tClassification Loss: 2.0648\r\n",
      "Train Epoch: 1 [82560/209539 (39%)]\tAll Loss: 2.8026\tTriple Loss(1): 0.2747\tClassification Loss: 2.2531\r\n",
      "Train Epoch: 1 [83200/209539 (40%)]\tAll Loss: 2.6645\tTriple Loss(1): 0.3947\tClassification Loss: 1.8750\r\n",
      "Train Epoch: 1 [83840/209539 (40%)]\tAll Loss: 3.3670\tTriple Loss(0): 0.6384\tClassification Loss: 2.0901\r\n",
      "Train Epoch: 1 [84480/209539 (40%)]\tAll Loss: 2.4514\tTriple Loss(1): 0.1581\tClassification Loss: 2.1351\r\n",
      "Train Epoch: 1 [85120/209539 (41%)]\tAll Loss: 3.7555\tTriple Loss(0): 0.8673\tClassification Loss: 2.0208\r\n",
      "Train Epoch: 1 [85760/209539 (41%)]\tAll Loss: 4.0822\tTriple Loss(0): 1.0359\tClassification Loss: 2.0104\r\n",
      "Train Epoch: 1 [86400/209539 (41%)]\tAll Loss: 2.4719\tTriple Loss(1): 0.1354\tClassification Loss: 2.2012\r\n",
      "Train Epoch: 1 [87040/209539 (42%)]\tAll Loss: 2.7007\tTriple Loss(1): 0.3325\tClassification Loss: 2.0357\r\n",
      "Train Epoch: 1 [87680/209539 (42%)]\tAll Loss: 2.7828\tTriple Loss(1): 0.3051\tClassification Loss: 2.1726\r\n",
      "Train Epoch: 1 [88320/209539 (42%)]\tAll Loss: 2.6431\tTriple Loss(1): 0.2795\tClassification Loss: 2.0842\r\n",
      "Train Epoch: 1 [88960/209539 (42%)]\tAll Loss: 2.1878\tTriple Loss(1): 0.1724\tClassification Loss: 1.8431\r\n",
      "Train Epoch: 1 [89600/209539 (43%)]\tAll Loss: 2.5541\tTriple Loss(1): 0.1925\tClassification Loss: 2.1691\r\n",
      "Train Epoch: 1 [90240/209539 (43%)]\tAll Loss: 2.6535\tTriple Loss(1): 0.2823\tClassification Loss: 2.0889\r\n",
      "Train Epoch: 1 [90880/209539 (43%)]\tAll Loss: 2.6502\tTriple Loss(1): 0.3994\tClassification Loss: 1.8513\r\n",
      "Train Epoch: 1 [91520/209539 (44%)]\tAll Loss: 2.6212\tTriple Loss(1): 0.2637\tClassification Loss: 2.0938\r\n",
      "Train Epoch: 1 [92160/209539 (44%)]\tAll Loss: 2.3318\tTriple Loss(1): 0.2011\tClassification Loss: 1.9296\r\n",
      "Train Epoch: 1 [92800/209539 (44%)]\tAll Loss: 1.8304\tTriple Loss(1): 0.1678\tClassification Loss: 1.4948\r\n",
      "Train Epoch: 1 [93440/209539 (45%)]\tAll Loss: 2.9701\tTriple Loss(1): 0.3857\tClassification Loss: 2.1986\r\n",
      "Train Epoch: 1 [94080/209539 (45%)]\tAll Loss: 2.7492\tTriple Loss(1): 0.4515\tClassification Loss: 1.8462\r\n",
      "Train Epoch: 1 [94720/209539 (45%)]\tAll Loss: 2.4946\tTriple Loss(1): 0.2547\tClassification Loss: 1.9852\r\n",
      "Train Epoch: 1 [95360/209539 (46%)]\tAll Loss: 3.9944\tTriple Loss(0): 0.9576\tClassification Loss: 2.0791\r\n",
      "Train Epoch: 1 [96000/209539 (46%)]\tAll Loss: 2.8049\tTriple Loss(1): 0.2978\tClassification Loss: 2.2093\r\n",
      "Train Epoch: 1 [96640/209539 (46%)]\tAll Loss: 2.9190\tTriple Loss(1): 0.3491\tClassification Loss: 2.2208\r\n",
      "Train Epoch: 1 [97280/209539 (46%)]\tAll Loss: 2.4435\tTriple Loss(1): 0.2136\tClassification Loss: 2.0163\r\n",
      "Train Epoch: 1 [97920/209539 (47%)]\tAll Loss: 2.2449\tTriple Loss(1): 0.1959\tClassification Loss: 1.8531\r\n",
      "Train Epoch: 1 [98560/209539 (47%)]\tAll Loss: 2.3071\tTriple Loss(1): 0.2545\tClassification Loss: 1.7982\r\n",
      "Train Epoch: 1 [99200/209539 (47%)]\tAll Loss: 2.8290\tTriple Loss(1): 0.4153\tClassification Loss: 1.9983\r\n",
      "Train Epoch: 1 [99840/209539 (48%)]\tAll Loss: 2.6741\tTriple Loss(1): 0.3037\tClassification Loss: 2.0667\r\n",
      "Train Epoch: 1 [100480/209539 (48%)]\tAll Loss: 2.5053\tTriple Loss(1): 0.2180\tClassification Loss: 2.0693\r\n",
      "Train Epoch: 1 [101120/209539 (48%)]\tAll Loss: 2.6130\tTriple Loss(1): 0.2357\tClassification Loss: 2.1416\r\n",
      "Train Epoch: 1 [101760/209539 (49%)]\tAll Loss: 2.4849\tTriple Loss(1): 0.2322\tClassification Loss: 2.0206\r\n",
      "Train Epoch: 1 [102400/209539 (49%)]\tAll Loss: 2.0383\tTriple Loss(1): 0.1703\tClassification Loss: 1.6977\r\n",
      "Train Epoch: 1 [103040/209539 (49%)]\tAll Loss: 3.0643\tTriple Loss(1): 0.3065\tClassification Loss: 2.4512\r\n",
      "Train Epoch: 1 [103680/209539 (49%)]\tAll Loss: 2.9472\tTriple Loss(1): 0.2680\tClassification Loss: 2.4112\r\n",
      "Train Epoch: 1 [104320/209539 (50%)]\tAll Loss: 2.6593\tTriple Loss(1): 0.3082\tClassification Loss: 2.0428\r\n",
      "Train Epoch: 1 [104960/209539 (50%)]\tAll Loss: 3.1081\tTriple Loss(0): 0.6006\tClassification Loss: 1.9069\r\n",
      "Train Epoch: 1 [105600/209539 (50%)]\tAll Loss: 3.3656\tTriple Loss(0): 0.8067\tClassification Loss: 1.7521\r\n",
      "Train Epoch: 1 [106240/209539 (51%)]\tAll Loss: 2.3854\tTriple Loss(1): 0.2031\tClassification Loss: 1.9792\r\n",
      "Train Epoch: 1 [106880/209539 (51%)]\tAll Loss: 3.3839\tTriple Loss(0): 0.7389\tClassification Loss: 1.9062\r\n",
      "Train Epoch: 1 [107520/209539 (51%)]\tAll Loss: 2.4535\tTriple Loss(1): 0.2623\tClassification Loss: 1.9289\r\n",
      "Train Epoch: 1 [108160/209539 (52%)]\tAll Loss: 2.1136\tTriple Loss(1): 0.2304\tClassification Loss: 1.6528\r\n",
      "Train Epoch: 1 [108800/209539 (52%)]\tAll Loss: 2.6333\tTriple Loss(1): 0.3054\tClassification Loss: 2.0225\r\n",
      "Train Epoch: 1 [109440/209539 (52%)]\tAll Loss: 2.4896\tTriple Loss(1): 0.2267\tClassification Loss: 2.0362\r\n",
      "Train Epoch: 1 [110080/209539 (53%)]\tAll Loss: 3.5803\tTriple Loss(0): 0.8638\tClassification Loss: 1.8526\r\n",
      "Train Epoch: 1 [110720/209539 (53%)]\tAll Loss: 3.2466\tTriple Loss(0): 0.6245\tClassification Loss: 1.9976\r\n",
      "Train Epoch: 1 [111360/209539 (53%)]\tAll Loss: 2.1859\tTriple Loss(1): 0.2380\tClassification Loss: 1.7099\r\n",
      "Train Epoch: 1 [112000/209539 (53%)]\tAll Loss: 2.4397\tTriple Loss(1): 0.2643\tClassification Loss: 1.9111\r\n",
      "Train Epoch: 1 [112640/209539 (54%)]\tAll Loss: 2.3409\tTriple Loss(1): 0.2548\tClassification Loss: 1.8312\r\n",
      "Train Epoch: 1 [113280/209539 (54%)]\tAll Loss: 2.4599\tTriple Loss(1): 0.2734\tClassification Loss: 1.9130\r\n",
      "Train Epoch: 1 [113920/209539 (54%)]\tAll Loss: 2.6792\tTriple Loss(1): 0.3544\tClassification Loss: 1.9704\r\n",
      "Train Epoch: 1 [114560/209539 (55%)]\tAll Loss: 2.5880\tTriple Loss(1): 0.2735\tClassification Loss: 2.0410\r\n",
      "Train Epoch: 1 [115200/209539 (55%)]\tAll Loss: 3.6632\tTriple Loss(0): 0.7902\tClassification Loss: 2.0828\r\n",
      "Train Epoch: 1 [115840/209539 (55%)]\tAll Loss: 2.7157\tTriple Loss(1): 0.2826\tClassification Loss: 2.1505\r\n",
      "Train Epoch: 1 [116480/209539 (56%)]\tAll Loss: 2.3800\tTriple Loss(1): 0.2761\tClassification Loss: 1.8278\r\n",
      "Train Epoch: 1 [117120/209539 (56%)]\tAll Loss: 2.4860\tTriple Loss(1): 0.2016\tClassification Loss: 2.0829\r\n",
      "Train Epoch: 1 [117760/209539 (56%)]\tAll Loss: 2.5110\tTriple Loss(1): 0.2562\tClassification Loss: 1.9986\r\n",
      "Train Epoch: 1 [118400/209539 (57%)]\tAll Loss: 2.3863\tTriple Loss(1): 0.2993\tClassification Loss: 1.7877\r\n",
      "Train Epoch: 1 [119040/209539 (57%)]\tAll Loss: 3.7737\tTriple Loss(0): 0.7850\tClassification Loss: 2.2038\r\n",
      "Train Epoch: 1 [119680/209539 (57%)]\tAll Loss: 2.4990\tTriple Loss(1): 0.2801\tClassification Loss: 1.9387\r\n",
      "Train Epoch: 1 [120320/209539 (57%)]\tAll Loss: 3.0719\tTriple Loss(0): 0.5229\tClassification Loss: 2.0261\r\n",
      "Train Epoch: 1 [120960/209539 (58%)]\tAll Loss: 2.2124\tTriple Loss(1): 0.2845\tClassification Loss: 1.6435\r\n",
      "Train Epoch: 1 [121600/209539 (58%)]\tAll Loss: 3.6282\tTriple Loss(0): 0.8985\tClassification Loss: 1.8313\r\n",
      "Train Epoch: 1 [122240/209539 (58%)]\tAll Loss: 3.5211\tTriple Loss(0): 0.8237\tClassification Loss: 1.8738\r\n",
      "Train Epoch: 1 [122880/209539 (59%)]\tAll Loss: 2.4152\tTriple Loss(1): 0.2148\tClassification Loss: 1.9857\r\n",
      "Train Epoch: 1 [123520/209539 (59%)]\tAll Loss: 3.1072\tTriple Loss(0): 0.5609\tClassification Loss: 1.9855\r\n",
      "Train Epoch: 1 [124160/209539 (59%)]\tAll Loss: 2.6505\tTriple Loss(1): 0.3343\tClassification Loss: 1.9819\r\n",
      "Train Epoch: 1 [124800/209539 (60%)]\tAll Loss: 2.1504\tTriple Loss(1): 0.1347\tClassification Loss: 1.8809\r\n",
      "Train Epoch: 1 [125440/209539 (60%)]\tAll Loss: 2.5448\tTriple Loss(1): 0.2240\tClassification Loss: 2.0968\r\n",
      "Train Epoch: 1 [126080/209539 (60%)]\tAll Loss: 2.4034\tTriple Loss(1): 0.1928\tClassification Loss: 2.0178\r\n",
      "Train Epoch: 1 [126720/209539 (60%)]\tAll Loss: 2.2628\tTriple Loss(1): 0.2134\tClassification Loss: 1.8360\r\n",
      "Train Epoch: 1 [127360/209539 (61%)]\tAll Loss: 2.2594\tTriple Loss(1): 0.1307\tClassification Loss: 1.9980\r\n",
      "Train Epoch: 1 [128000/209539 (61%)]\tAll Loss: 2.9437\tTriple Loss(1): 0.4526\tClassification Loss: 2.0385\r\n",
      "Train Epoch: 1 [128640/209539 (61%)]\tAll Loss: 2.3253\tTriple Loss(1): 0.2476\tClassification Loss: 1.8301\r\n",
      "Train Epoch: 1 [129280/209539 (62%)]\tAll Loss: 2.4700\tTriple Loss(1): 0.3057\tClassification Loss: 1.8585\r\n",
      "Train Epoch: 1 [129920/209539 (62%)]\tAll Loss: 2.3415\tTriple Loss(1): 0.1342\tClassification Loss: 2.0732\r\n",
      "Train Epoch: 1 [130560/209539 (62%)]\tAll Loss: 2.2825\tTriple Loss(1): 0.1876\tClassification Loss: 1.9074\r\n",
      "Train Epoch: 1 [131200/209539 (63%)]\tAll Loss: 2.5128\tTriple Loss(1): 0.2339\tClassification Loss: 2.0449\r\n",
      "Train Epoch: 1 [131840/209539 (63%)]\tAll Loss: 3.1389\tTriple Loss(0): 0.6145\tClassification Loss: 1.9099\r\n",
      "Train Epoch: 1 [132480/209539 (63%)]\tAll Loss: 2.7250\tTriple Loss(1): 0.3136\tClassification Loss: 2.0979\r\n",
      "Train Epoch: 1 [133120/209539 (64%)]\tAll Loss: 2.0836\tTriple Loss(1): 0.1801\tClassification Loss: 1.7234\r\n",
      "Train Epoch: 1 [133760/209539 (64%)]\tAll Loss: 2.1721\tTriple Loss(1): 0.2252\tClassification Loss: 1.7217\r\n",
      "Train Epoch: 1 [134400/209539 (64%)]\tAll Loss: 2.2366\tTriple Loss(1): 0.1069\tClassification Loss: 2.0229\r\n",
      "Train Epoch: 1 [135040/209539 (64%)]\tAll Loss: 2.5184\tTriple Loss(1): 0.2405\tClassification Loss: 2.0375\r\n",
      "Train Epoch: 1 [135680/209539 (65%)]\tAll Loss: 4.0414\tTriple Loss(0): 0.8339\tClassification Loss: 2.3736\r\n",
      "Train Epoch: 1 [136320/209539 (65%)]\tAll Loss: 2.6041\tTriple Loss(1): 0.3624\tClassification Loss: 1.8793\r\n",
      "Train Epoch: 1 [136960/209539 (65%)]\tAll Loss: 2.5661\tTriple Loss(1): 0.2385\tClassification Loss: 2.0890\r\n",
      "Train Epoch: 1 [137600/209539 (66%)]\tAll Loss: 2.6476\tTriple Loss(1): 0.2607\tClassification Loss: 2.1262\r\n",
      "Train Epoch: 1 [138240/209539 (66%)]\tAll Loss: 2.3795\tTriple Loss(1): 0.2897\tClassification Loss: 1.8001\r\n",
      "Train Epoch: 1 [138880/209539 (66%)]\tAll Loss: 3.3019\tTriple Loss(0): 0.6470\tClassification Loss: 2.0080\r\n",
      "Train Epoch: 1 [139520/209539 (67%)]\tAll Loss: 2.3637\tTriple Loss(1): 0.1183\tClassification Loss: 2.1270\r\n",
      "Train Epoch: 1 [140160/209539 (67%)]\tAll Loss: 2.7856\tTriple Loss(1): 0.3522\tClassification Loss: 2.0811\r\n",
      "Train Epoch: 1 [140800/209539 (67%)]\tAll Loss: 2.1226\tTriple Loss(1): 0.2235\tClassification Loss: 1.6757\r\n",
      "Train Epoch: 1 [141440/209539 (68%)]\tAll Loss: 2.5323\tTriple Loss(1): 0.3051\tClassification Loss: 1.9221\r\n",
      "Train Epoch: 1 [142080/209539 (68%)]\tAll Loss: 3.6378\tTriple Loss(0): 0.8134\tClassification Loss: 2.0110\r\n",
      "Train Epoch: 1 [142720/209539 (68%)]\tAll Loss: 2.4813\tTriple Loss(1): 0.1745\tClassification Loss: 2.1323\r\n",
      "Train Epoch: 1 [143360/209539 (68%)]\tAll Loss: 3.2447\tTriple Loss(0): 0.7366\tClassification Loss: 1.7714\r\n",
      "Train Epoch: 1 [144000/209539 (69%)]\tAll Loss: 2.2763\tTriple Loss(1): 0.1763\tClassification Loss: 1.9238\r\n",
      "Train Epoch: 1 [144640/209539 (69%)]\tAll Loss: 2.4713\tTriple Loss(1): 0.1701\tClassification Loss: 2.1310\r\n",
      "Train Epoch: 1 [145280/209539 (69%)]\tAll Loss: 2.2165\tTriple Loss(1): 0.1806\tClassification Loss: 1.8553\r\n",
      "Train Epoch: 1 [145920/209539 (70%)]\tAll Loss: 2.9859\tTriple Loss(0): 0.6088\tClassification Loss: 1.7682\r\n",
      "Train Epoch: 1 [146560/209539 (70%)]\tAll Loss: 2.5657\tTriple Loss(1): 0.0987\tClassification Loss: 2.3683\r\n",
      "Train Epoch: 1 [147200/209539 (70%)]\tAll Loss: 2.6821\tTriple Loss(1): 0.2127\tClassification Loss: 2.2567\r\n",
      "Train Epoch: 1 [147840/209539 (71%)]\tAll Loss: 2.4669\tTriple Loss(1): 0.3065\tClassification Loss: 1.8538\r\n",
      "Train Epoch: 1 [148480/209539 (71%)]\tAll Loss: 2.3828\tTriple Loss(1): 0.2074\tClassification Loss: 1.9680\r\n",
      "Train Epoch: 1 [149120/209539 (71%)]\tAll Loss: 2.3919\tTriple Loss(1): 0.2159\tClassification Loss: 1.9602\r\n",
      "Train Epoch: 1 [149760/209539 (71%)]\tAll Loss: 2.7405\tTriple Loss(1): 0.2380\tClassification Loss: 2.2646\r\n",
      "Train Epoch: 1 [150400/209539 (72%)]\tAll Loss: 2.5894\tTriple Loss(1): 0.2880\tClassification Loss: 2.0134\r\n",
      "Train Epoch: 1 [151040/209539 (72%)]\tAll Loss: 2.3677\tTriple Loss(1): 0.2006\tClassification Loss: 1.9664\r\n",
      "Train Epoch: 1 [151680/209539 (72%)]\tAll Loss: 3.0471\tTriple Loss(0): 0.5572\tClassification Loss: 1.9326\r\n",
      "Train Epoch: 1 [152320/209539 (73%)]\tAll Loss: 2.3845\tTriple Loss(1): 0.1952\tClassification Loss: 1.9941\r\n",
      "Train Epoch: 1 [152960/209539 (73%)]\tAll Loss: 2.5270\tTriple Loss(1): 0.1545\tClassification Loss: 2.2181\r\n",
      "Train Epoch: 1 [153600/209539 (73%)]\tAll Loss: 2.2444\tTriple Loss(1): 0.1774\tClassification Loss: 1.8897\r\n",
      "Train Epoch: 1 [154240/209539 (74%)]\tAll Loss: 2.2102\tTriple Loss(1): 0.1354\tClassification Loss: 1.9394\r\n",
      "Train Epoch: 1 [154880/209539 (74%)]\tAll Loss: 2.4062\tTriple Loss(1): 0.3555\tClassification Loss: 1.6953\r\n",
      "Train Epoch: 1 [155520/209539 (74%)]\tAll Loss: 2.3405\tTriple Loss(1): 0.2671\tClassification Loss: 1.8063\r\n",
      "Train Epoch: 1 [156160/209539 (75%)]\tAll Loss: 3.1819\tTriple Loss(0): 0.5098\tClassification Loss: 2.1623\r\n",
      "Train Epoch: 1 [156800/209539 (75%)]\tAll Loss: 2.6047\tTriple Loss(1): 0.2717\tClassification Loss: 2.0612\r\n",
      "Train Epoch: 1 [157440/209539 (75%)]\tAll Loss: 2.4659\tTriple Loss(1): 0.1531\tClassification Loss: 2.1597\r\n",
      "Train Epoch: 1 [158080/209539 (75%)]\tAll Loss: 3.4508\tTriple Loss(0): 0.7626\tClassification Loss: 1.9256\r\n",
      "Train Epoch: 1 [158720/209539 (76%)]\tAll Loss: 2.1310\tTriple Loss(1): 0.1220\tClassification Loss: 1.8870\r\n",
      "Train Epoch: 1 [159360/209539 (76%)]\tAll Loss: 2.4338\tTriple Loss(1): 0.2532\tClassification Loss: 1.9273\r\n",
      "Train Epoch: 1 [160000/209539 (76%)]\tAll Loss: 2.5743\tTriple Loss(1): 0.2477\tClassification Loss: 2.0789\r\n",
      "Train Epoch: 1 [160640/209539 (77%)]\tAll Loss: 2.2674\tTriple Loss(1): 0.2304\tClassification Loss: 1.8065\r\n",
      "Train Epoch: 1 [161280/209539 (77%)]\tAll Loss: 2.4376\tTriple Loss(1): 0.3269\tClassification Loss: 1.7838\r\n",
      "Train Epoch: 1 [161920/209539 (77%)]\tAll Loss: 2.1464\tTriple Loss(1): 0.2103\tClassification Loss: 1.7259\r\n",
      "Train Epoch: 1 [162560/209539 (78%)]\tAll Loss: 3.0212\tTriple Loss(0): 0.6920\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 1 [163200/209539 (78%)]\tAll Loss: 2.4446\tTriple Loss(1): 0.2934\tClassification Loss: 1.8579\r\n",
      "Train Epoch: 1 [163840/209539 (78%)]\tAll Loss: 2.4579\tTriple Loss(1): 0.2104\tClassification Loss: 2.0371\r\n",
      "Train Epoch: 1 [164480/209539 (78%)]\tAll Loss: 3.4522\tTriple Loss(0): 0.8237\tClassification Loss: 1.8048\r\n",
      "Train Epoch: 1 [165120/209539 (79%)]\tAll Loss: 2.4219\tTriple Loss(1): 0.3032\tClassification Loss: 1.8156\r\n",
      "Train Epoch: 1 [165760/209539 (79%)]\tAll Loss: 2.3933\tTriple Loss(1): 0.1749\tClassification Loss: 2.0435\r\n",
      "Train Epoch: 1 [166400/209539 (79%)]\tAll Loss: 2.2557\tTriple Loss(1): 0.1970\tClassification Loss: 1.8617\r\n",
      "Train Epoch: 1 [167040/209539 (80%)]\tAll Loss: 2.2088\tTriple Loss(1): 0.2495\tClassification Loss: 1.7098\r\n",
      "Train Epoch: 1 [167680/209539 (80%)]\tAll Loss: 2.3251\tTriple Loss(1): 0.1949\tClassification Loss: 1.9352\r\n",
      "Train Epoch: 1 [168320/209539 (80%)]\tAll Loss: 2.2967\tTriple Loss(1): 0.1738\tClassification Loss: 1.9492\r\n",
      "Train Epoch: 1 [168960/209539 (81%)]\tAll Loss: 2.6461\tTriple Loss(1): 0.2733\tClassification Loss: 2.0995\r\n",
      "Train Epoch: 1 [169600/209539 (81%)]\tAll Loss: 3.2154\tTriple Loss(0): 0.5828\tClassification Loss: 2.0498\r\n",
      "Train Epoch: 1 [170240/209539 (81%)]\tAll Loss: 2.2905\tTriple Loss(1): 0.2692\tClassification Loss: 1.7522\r\n",
      "Train Epoch: 1 [170880/209539 (82%)]\tAll Loss: 2.2570\tTriple Loss(1): 0.1874\tClassification Loss: 1.8821\r\n",
      "Train Epoch: 1 [171520/209539 (82%)]\tAll Loss: 2.5076\tTriple Loss(1): 0.3058\tClassification Loss: 1.8961\r\n",
      "Train Epoch: 1 [172160/209539 (82%)]\tAll Loss: 2.7749\tTriple Loss(1): 0.3852\tClassification Loss: 2.0046\r\n",
      "Train Epoch: 1 [172800/209539 (82%)]\tAll Loss: 2.8902\tTriple Loss(1): 0.2967\tClassification Loss: 2.2968\r\n",
      "Train Epoch: 1 [173440/209539 (83%)]\tAll Loss: 2.3259\tTriple Loss(1): 0.2221\tClassification Loss: 1.8817\r\n",
      "Train Epoch: 1 [174080/209539 (83%)]\tAll Loss: 3.5778\tTriple Loss(0): 0.8746\tClassification Loss: 1.8286\r\n",
      "Train Epoch: 1 [174720/209539 (83%)]\tAll Loss: 2.1668\tTriple Loss(1): 0.1666\tClassification Loss: 1.8336\r\n",
      "Train Epoch: 1 [175360/209539 (84%)]\tAll Loss: 2.5867\tTriple Loss(1): 0.2506\tClassification Loss: 2.0855\r\n",
      "Train Epoch: 1 [176000/209539 (84%)]\tAll Loss: 2.3072\tTriple Loss(1): 0.2862\tClassification Loss: 1.7349\r\n",
      "Train Epoch: 1 [176640/209539 (84%)]\tAll Loss: 3.3718\tTriple Loss(0): 0.7178\tClassification Loss: 1.9363\r\n",
      "Train Epoch: 1 [177280/209539 (85%)]\tAll Loss: 2.5234\tTriple Loss(1): 0.2196\tClassification Loss: 2.0842\r\n",
      "Train Epoch: 1 [177920/209539 (85%)]\tAll Loss: 3.0741\tTriple Loss(0): 0.5601\tClassification Loss: 1.9538\r\n",
      "Train Epoch: 1 [178560/209539 (85%)]\tAll Loss: 1.9705\tTriple Loss(1): 0.1823\tClassification Loss: 1.6059\r\n",
      "Train Epoch: 1 [179200/209539 (86%)]\tAll Loss: 3.5948\tTriple Loss(0): 0.9072\tClassification Loss: 1.7804\r\n",
      "Train Epoch: 1 [179840/209539 (86%)]\tAll Loss: 1.9934\tTriple Loss(1): 0.1960\tClassification Loss: 1.6014\r\n",
      "Train Epoch: 1 [180480/209539 (86%)]\tAll Loss: 2.4102\tTriple Loss(1): 0.2737\tClassification Loss: 1.8628\r\n",
      "Train Epoch: 1 [181120/209539 (86%)]\tAll Loss: 2.2981\tTriple Loss(1): 0.1463\tClassification Loss: 2.0055\r\n",
      "Train Epoch: 1 [181760/209539 (87%)]\tAll Loss: 2.5676\tTriple Loss(1): 0.3390\tClassification Loss: 1.8896\r\n",
      "Train Epoch: 1 [182400/209539 (87%)]\tAll Loss: 2.6537\tTriple Loss(1): 0.3551\tClassification Loss: 1.9434\r\n",
      "Train Epoch: 1 [183040/209539 (87%)]\tAll Loss: 2.4332\tTriple Loss(1): 0.2189\tClassification Loss: 1.9954\r\n",
      "Train Epoch: 1 [183680/209539 (88%)]\tAll Loss: 2.3004\tTriple Loss(1): 0.2696\tClassification Loss: 1.7612\r\n",
      "Train Epoch: 1 [184320/209539 (88%)]\tAll Loss: 2.5301\tTriple Loss(1): 0.3735\tClassification Loss: 1.7831\r\n",
      "Train Epoch: 1 [184960/209539 (88%)]\tAll Loss: 2.2088\tTriple Loss(1): 0.1509\tClassification Loss: 1.9070\r\n",
      "Train Epoch: 1 [185600/209539 (89%)]\tAll Loss: 2.3800\tTriple Loss(1): 0.2324\tClassification Loss: 1.9152\r\n",
      "Train Epoch: 1 [186240/209539 (89%)]\tAll Loss: 2.2406\tTriple Loss(1): 0.1594\tClassification Loss: 1.9218\r\n",
      "Train Epoch: 1 [186880/209539 (89%)]\tAll Loss: 2.1751\tTriple Loss(1): 0.2298\tClassification Loss: 1.7156\r\n",
      "Train Epoch: 1 [187520/209539 (89%)]\tAll Loss: 3.4408\tTriple Loss(0): 0.6927\tClassification Loss: 2.0554\r\n",
      "Train Epoch: 1 [188160/209539 (90%)]\tAll Loss: 3.5496\tTriple Loss(0): 0.7582\tClassification Loss: 2.0333\r\n",
      "Train Epoch: 1 [188800/209539 (90%)]\tAll Loss: 2.3284\tTriple Loss(1): 0.2269\tClassification Loss: 1.8746\r\n",
      "Train Epoch: 1 [189440/209539 (90%)]\tAll Loss: 2.3514\tTriple Loss(1): 0.2426\tClassification Loss: 1.8662\r\n",
      "Train Epoch: 1 [190080/209539 (91%)]\tAll Loss: 2.0792\tTriple Loss(1): 0.2397\tClassification Loss: 1.5998\r\n",
      "Train Epoch: 1 [190720/209539 (91%)]\tAll Loss: 2.3982\tTriple Loss(1): 0.1799\tClassification Loss: 2.0385\r\n",
      "Train Epoch: 1 [191360/209539 (91%)]\tAll Loss: 2.2428\tTriple Loss(1): 0.1796\tClassification Loss: 1.8836\r\n",
      "Train Epoch: 1 [192000/209539 (92%)]\tAll Loss: 2.3251\tTriple Loss(1): 0.1313\tClassification Loss: 2.0625\r\n",
      "Train Epoch: 1 [192640/209539 (92%)]\tAll Loss: 2.2695\tTriple Loss(1): 0.1666\tClassification Loss: 1.9362\r\n",
      "Train Epoch: 1 [193280/209539 (92%)]\tAll Loss: 2.3356\tTriple Loss(1): 0.2045\tClassification Loss: 1.9267\r\n",
      "Train Epoch: 1 [193920/209539 (93%)]\tAll Loss: 2.3340\tTriple Loss(1): 0.2772\tClassification Loss: 1.7795\r\n",
      "Train Epoch: 1 [194560/209539 (93%)]\tAll Loss: 2.1559\tTriple Loss(1): 0.2009\tClassification Loss: 1.7541\r\n",
      "Train Epoch: 1 [195200/209539 (93%)]\tAll Loss: 2.0663\tTriple Loss(1): 0.1974\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 1 [195840/209539 (93%)]\tAll Loss: 1.9948\tTriple Loss(1): 0.1821\tClassification Loss: 1.6307\r\n",
      "Train Epoch: 1 [196480/209539 (94%)]\tAll Loss: 3.5152\tTriple Loss(0): 0.7669\tClassification Loss: 1.9814\r\n",
      "Train Epoch: 1 [197120/209539 (94%)]\tAll Loss: 3.0078\tTriple Loss(1): 0.4781\tClassification Loss: 2.0516\r\n",
      "Train Epoch: 1 [197760/209539 (94%)]\tAll Loss: 2.7882\tTriple Loss(0): 0.4854\tClassification Loss: 1.8175\r\n",
      "Train Epoch: 1 [198400/209539 (95%)]\tAll Loss: 2.9987\tTriple Loss(0): 0.6720\tClassification Loss: 1.6547\r\n",
      "Train Epoch: 1 [199040/209539 (95%)]\tAll Loss: 2.1887\tTriple Loss(1): 0.1683\tClassification Loss: 1.8520\r\n",
      "Train Epoch: 1 [199680/209539 (95%)]\tAll Loss: 3.3025\tTriple Loss(0): 0.6299\tClassification Loss: 2.0426\r\n",
      "Train Epoch: 1 [200320/209539 (96%)]\tAll Loss: 2.1868\tTriple Loss(1): 0.1431\tClassification Loss: 1.9006\r\n",
      "Train Epoch: 1 [200960/209539 (96%)]\tAll Loss: 2.7938\tTriple Loss(0): 0.5115\tClassification Loss: 1.7707\r\n",
      "Train Epoch: 1 [201600/209539 (96%)]\tAll Loss: 2.3861\tTriple Loss(1): 0.2133\tClassification Loss: 1.9594\r\n",
      "Train Epoch: 1 [202240/209539 (97%)]\tAll Loss: 2.4573\tTriple Loss(1): 0.2222\tClassification Loss: 2.0129\r\n",
      "Train Epoch: 1 [202880/209539 (97%)]\tAll Loss: 2.8899\tTriple Loss(0): 0.6360\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 1 [203520/209539 (97%)]\tAll Loss: 2.6819\tTriple Loss(1): 0.2923\tClassification Loss: 2.0974\r\n",
      "Train Epoch: 1 [204160/209539 (97%)]\tAll Loss: 2.4817\tTriple Loss(1): 0.2499\tClassification Loss: 1.9818\r\n",
      "Train Epoch: 1 [204800/209539 (98%)]\tAll Loss: 2.1734\tTriple Loss(1): 0.2587\tClassification Loss: 1.6560\r\n",
      "Train Epoch: 1 [205440/209539 (98%)]\tAll Loss: 2.0218\tTriple Loss(1): 0.2017\tClassification Loss: 1.6184\r\n",
      "Train Epoch: 1 [206080/209539 (98%)]\tAll Loss: 2.4105\tTriple Loss(1): 0.2340\tClassification Loss: 1.9426\r\n",
      "Train Epoch: 1 [206720/209539 (99%)]\tAll Loss: 2.5102\tTriple Loss(1): 0.2762\tClassification Loss: 1.9578\r\n",
      "Train Epoch: 1 [207360/209539 (99%)]\tAll Loss: 2.3477\tTriple Loss(1): 0.3791\tClassification Loss: 1.5894\r\n",
      "Train Epoch: 1 [208000/209539 (99%)]\tAll Loss: 3.4288\tTriple Loss(0): 0.7177\tClassification Loss: 1.9935\r\n",
      "Train Epoch: 1 [208640/209539 (100%)]\tAll Loss: 2.4745\tTriple Loss(1): 0.2340\tClassification Loss: 2.0065\r\n",
      "Train Epoch: 1 [209280/209539 (100%)]\tAll Loss: 2.0357\tTriple Loss(1): 0.1391\tClassification Loss: 1.7575\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/1_epochs\r\n",
      "Train Epoch: 2 [0/209539 (0%)]\tAll Loss: 3.0857\tTriple Loss(1): 0.3840\tClassification Loss: 2.3176\r\n",
      "\r\n",
      "Test set: Average loss: 1.7723, Accuracy: 39722/80128 (50%)\r\n",
      "\r\n",
      "Train Epoch: 2 [640/209539 (0%)]\tAll Loss: 2.4683\tTriple Loss(1): 0.2380\tClassification Loss: 1.9922\r\n",
      "Train Epoch: 2 [1280/209539 (1%)]\tAll Loss: 2.5267\tTriple Loss(1): 0.1142\tClassification Loss: 2.2983\r\n",
      "Train Epoch: 2 [1920/209539 (1%)]\tAll Loss: 3.3417\tTriple Loss(0): 0.7498\tClassification Loss: 1.8420\r\n",
      "Train Epoch: 2 [2560/209539 (1%)]\tAll Loss: 2.5504\tTriple Loss(1): 0.2789\tClassification Loss: 1.9926\r\n",
      "Train Epoch: 2 [3200/209539 (2%)]\tAll Loss: 2.6333\tTriple Loss(1): 0.3327\tClassification Loss: 1.9679\r\n",
      "Train Epoch: 2 [3840/209539 (2%)]\tAll Loss: 2.1867\tTriple Loss(1): 0.1876\tClassification Loss: 1.8114\r\n",
      "Train Epoch: 2 [4480/209539 (2%)]\tAll Loss: 2.0293\tTriple Loss(1): 0.0709\tClassification Loss: 1.8875\r\n",
      "Train Epoch: 2 [5120/209539 (2%)]\tAll Loss: 2.7349\tTriple Loss(1): 0.3444\tClassification Loss: 2.0461\r\n",
      "Train Epoch: 2 [5760/209539 (3%)]\tAll Loss: 2.1724\tTriple Loss(1): 0.2072\tClassification Loss: 1.7580\r\n",
      "Train Epoch: 2 [6400/209539 (3%)]\tAll Loss: 2.4517\tTriple Loss(1): 0.2370\tClassification Loss: 1.9777\r\n",
      "Train Epoch: 2 [7040/209539 (3%)]\tAll Loss: 2.9025\tTriple Loss(1): 0.3753\tClassification Loss: 2.1518\r\n",
      "Train Epoch: 2 [7680/209539 (4%)]\tAll Loss: 2.5836\tTriple Loss(1): 0.3673\tClassification Loss: 1.8491\r\n",
      "Train Epoch: 2 [8320/209539 (4%)]\tAll Loss: 2.1774\tTriple Loss(1): 0.1640\tClassification Loss: 1.8493\r\n",
      "Train Epoch: 2 [8960/209539 (4%)]\tAll Loss: 3.3468\tTriple Loss(0): 0.6649\tClassification Loss: 2.0170\r\n",
      "Train Epoch: 2 [9600/209539 (5%)]\tAll Loss: 2.2835\tTriple Loss(1): 0.1638\tClassification Loss: 1.9560\r\n",
      "Train Epoch: 2 [10240/209539 (5%)]\tAll Loss: 2.2660\tTriple Loss(1): 0.1885\tClassification Loss: 1.8889\r\n",
      "Train Epoch: 2 [10880/209539 (5%)]\tAll Loss: 2.6021\tTriple Loss(1): 0.2153\tClassification Loss: 2.1714\r\n",
      "Train Epoch: 2 [11520/209539 (5%)]\tAll Loss: 2.4780\tTriple Loss(1): 0.3447\tClassification Loss: 1.7886\r\n",
      "Train Epoch: 2 [12160/209539 (6%)]\tAll Loss: 3.0735\tTriple Loss(0): 0.6104\tClassification Loss: 1.8528\r\n",
      "Train Epoch: 2 [12800/209539 (6%)]\tAll Loss: 2.3313\tTriple Loss(1): 0.1710\tClassification Loss: 1.9894\r\n",
      "Train Epoch: 2 [13440/209539 (6%)]\tAll Loss: 2.1715\tTriple Loss(1): 0.1933\tClassification Loss: 1.7849\r\n",
      "Train Epoch: 2 [14080/209539 (7%)]\tAll Loss: 3.5532\tTriple Loss(0): 0.7757\tClassification Loss: 2.0018\r\n",
      "Train Epoch: 2 [14720/209539 (7%)]\tAll Loss: 2.3579\tTriple Loss(1): 0.1361\tClassification Loss: 2.0858\r\n",
      "Train Epoch: 2 [15360/209539 (7%)]\tAll Loss: 2.3979\tTriple Loss(1): 0.2790\tClassification Loss: 1.8398\r\n",
      "Train Epoch: 2 [16000/209539 (8%)]\tAll Loss: 2.2879\tTriple Loss(1): 0.1879\tClassification Loss: 1.9121\r\n",
      "Train Epoch: 2 [16640/209539 (8%)]\tAll Loss: 2.1760\tTriple Loss(1): 0.1533\tClassification Loss: 1.8693\r\n",
      "Train Epoch: 2 [17280/209539 (8%)]\tAll Loss: 3.0626\tTriple Loss(0): 0.5802\tClassification Loss: 1.9022\r\n",
      "Train Epoch: 2 [17920/209539 (9%)]\tAll Loss: 3.1536\tTriple Loss(0): 0.6844\tClassification Loss: 1.7849\r\n",
      "Train Epoch: 2 [18560/209539 (9%)]\tAll Loss: 2.5227\tTriple Loss(1): 0.2305\tClassification Loss: 2.0617\r\n",
      "Train Epoch: 2 [19200/209539 (9%)]\tAll Loss: 3.0253\tTriple Loss(0): 0.5401\tClassification Loss: 1.9450\r\n",
      "Train Epoch: 2 [19840/209539 (9%)]\tAll Loss: 2.3852\tTriple Loss(1): 0.2553\tClassification Loss: 1.8745\r\n",
      "Train Epoch: 2 [20480/209539 (10%)]\tAll Loss: 2.4121\tTriple Loss(1): 0.0980\tClassification Loss: 2.2161\r\n",
      "Train Epoch: 2 [21120/209539 (10%)]\tAll Loss: 2.2916\tTriple Loss(1): 0.2847\tClassification Loss: 1.7223\r\n",
      "Train Epoch: 2 [21760/209539 (10%)]\tAll Loss: 2.6408\tTriple Loss(1): 0.2588\tClassification Loss: 2.1232\r\n",
      "Train Epoch: 2 [22400/209539 (11%)]\tAll Loss: 2.1232\tTriple Loss(1): 0.2382\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 2 [23040/209539 (11%)]\tAll Loss: 2.5044\tTriple Loss(1): 0.3028\tClassification Loss: 1.8989\r\n",
      "Train Epoch: 2 [23680/209539 (11%)]\tAll Loss: 2.4194\tTriple Loss(1): 0.3216\tClassification Loss: 1.7761\r\n",
      "Train Epoch: 2 [24320/209539 (12%)]\tAll Loss: 2.2885\tTriple Loss(1): 0.2456\tClassification Loss: 1.7972\r\n",
      "Train Epoch: 2 [24960/209539 (12%)]\tAll Loss: 2.1420\tTriple Loss(1): 0.2037\tClassification Loss: 1.7346\r\n",
      "Train Epoch: 2 [25600/209539 (12%)]\tAll Loss: 2.6984\tTriple Loss(0): 0.5671\tClassification Loss: 1.5642\r\n",
      "Train Epoch: 2 [26240/209539 (13%)]\tAll Loss: 3.6776\tTriple Loss(0): 0.8402\tClassification Loss: 1.9972\r\n",
      "Train Epoch: 2 [26880/209539 (13%)]\tAll Loss: 2.1394\tTriple Loss(1): 0.1344\tClassification Loss: 1.8707\r\n",
      "Train Epoch: 2 [27520/209539 (13%)]\tAll Loss: 3.5021\tTriple Loss(0): 0.8435\tClassification Loss: 1.8151\r\n",
      "Train Epoch: 2 [28160/209539 (13%)]\tAll Loss: 2.3448\tTriple Loss(1): 0.2429\tClassification Loss: 1.8591\r\n",
      "Train Epoch: 2 [28800/209539 (14%)]\tAll Loss: 2.4094\tTriple Loss(1): 0.2584\tClassification Loss: 1.8926\r\n",
      "Train Epoch: 2 [29440/209539 (14%)]\tAll Loss: 3.4202\tTriple Loss(0): 0.7552\tClassification Loss: 1.9099\r\n",
      "Train Epoch: 2 [30080/209539 (14%)]\tAll Loss: 2.3713\tTriple Loss(1): 0.2506\tClassification Loss: 1.8700\r\n",
      "Train Epoch: 2 [30720/209539 (15%)]\tAll Loss: 2.2828\tTriple Loss(1): 0.1745\tClassification Loss: 1.9337\r\n",
      "Train Epoch: 2 [31360/209539 (15%)]\tAll Loss: 2.6130\tTriple Loss(1): 0.2252\tClassification Loss: 2.1626\r\n",
      "Train Epoch: 2 [32000/209539 (15%)]\tAll Loss: 2.8753\tTriple Loss(1): 0.3604\tClassification Loss: 2.1545\r\n",
      "Train Epoch: 2 [32640/209539 (16%)]\tAll Loss: 3.5212\tTriple Loss(0): 0.6516\tClassification Loss: 2.2180\r\n",
      "Train Epoch: 2 [33280/209539 (16%)]\tAll Loss: 3.0647\tTriple Loss(0): 0.4972\tClassification Loss: 2.0703\r\n",
      "Train Epoch: 2 [33920/209539 (16%)]\tAll Loss: 2.5461\tTriple Loss(1): 0.2524\tClassification Loss: 2.0412\r\n",
      "Train Epoch: 2 [34560/209539 (16%)]\tAll Loss: 3.3669\tTriple Loss(0): 0.7468\tClassification Loss: 1.8734\r\n",
      "Train Epoch: 2 [35200/209539 (17%)]\tAll Loss: 2.7483\tTriple Loss(1): 0.3568\tClassification Loss: 2.0347\r\n",
      "Train Epoch: 2 [35840/209539 (17%)]\tAll Loss: 1.9502\tTriple Loss(1): 0.0209\tClassification Loss: 1.9084\r\n",
      "Train Epoch: 2 [36480/209539 (17%)]\tAll Loss: 2.5636\tTriple Loss(1): 0.3766\tClassification Loss: 1.8105\r\n",
      "Train Epoch: 2 [37120/209539 (18%)]\tAll Loss: 2.3832\tTriple Loss(1): 0.1858\tClassification Loss: 2.0117\r\n",
      "Train Epoch: 2 [37760/209539 (18%)]\tAll Loss: 2.4026\tTriple Loss(1): 0.2817\tClassification Loss: 1.8391\r\n",
      "Train Epoch: 2 [38400/209539 (18%)]\tAll Loss: 2.5633\tTriple Loss(0): 0.5629\tClassification Loss: 1.4375\r\n",
      "Train Epoch: 2 [39040/209539 (19%)]\tAll Loss: 2.2505\tTriple Loss(1): 0.1931\tClassification Loss: 1.8643\r\n",
      "Train Epoch: 2 [39680/209539 (19%)]\tAll Loss: 2.2733\tTriple Loss(1): 0.2481\tClassification Loss: 1.7770\r\n",
      "Train Epoch: 2 [40320/209539 (19%)]\tAll Loss: 2.8337\tTriple Loss(1): 0.2735\tClassification Loss: 2.2866\r\n",
      "Train Epoch: 2 [40960/209539 (20%)]\tAll Loss: 3.2454\tTriple Loss(0): 0.6273\tClassification Loss: 1.9908\r\n",
      "Train Epoch: 2 [41600/209539 (20%)]\tAll Loss: 3.8061\tTriple Loss(0): 0.7821\tClassification Loss: 2.2420\r\n",
      "Train Epoch: 2 [42240/209539 (20%)]\tAll Loss: 2.5560\tTriple Loss(1): 0.3225\tClassification Loss: 1.9110\r\n",
      "Train Epoch: 2 [42880/209539 (20%)]\tAll Loss: 2.1574\tTriple Loss(1): 0.2061\tClassification Loss: 1.7451\r\n",
      "Train Epoch: 2 [43520/209539 (21%)]\tAll Loss: 2.7324\tTriple Loss(1): 0.2699\tClassification Loss: 2.1926\r\n",
      "Train Epoch: 2 [44160/209539 (21%)]\tAll Loss: 2.1902\tTriple Loss(1): 0.2136\tClassification Loss: 1.7630\r\n",
      "Train Epoch: 2 [44800/209539 (21%)]\tAll Loss: 2.5202\tTriple Loss(1): 0.2703\tClassification Loss: 1.9796\r\n",
      "Train Epoch: 2 [45440/209539 (22%)]\tAll Loss: 2.1781\tTriple Loss(1): 0.2569\tClassification Loss: 1.6644\r\n",
      "Train Epoch: 2 [46080/209539 (22%)]\tAll Loss: 2.4583\tTriple Loss(1): 0.2539\tClassification Loss: 1.9505\r\n",
      "Train Epoch: 2 [46720/209539 (22%)]\tAll Loss: 3.3979\tTriple Loss(0): 0.7354\tClassification Loss: 1.9270\r\n",
      "Train Epoch: 2 [47360/209539 (23%)]\tAll Loss: 2.5004\tTriple Loss(1): 0.1651\tClassification Loss: 2.1702\r\n",
      "Train Epoch: 2 [48000/209539 (23%)]\tAll Loss: 2.7219\tTriple Loss(1): 0.3083\tClassification Loss: 2.1052\r\n",
      "Train Epoch: 2 [48640/209539 (23%)]\tAll Loss: 2.0072\tTriple Loss(1): 0.1459\tClassification Loss: 1.7154\r\n",
      "Train Epoch: 2 [49280/209539 (24%)]\tAll Loss: 2.4598\tTriple Loss(1): 0.2830\tClassification Loss: 1.8938\r\n",
      "Train Epoch: 2 [49920/209539 (24%)]\tAll Loss: 2.3252\tTriple Loss(1): 0.1694\tClassification Loss: 1.9864\r\n",
      "Train Epoch: 2 [50560/209539 (24%)]\tAll Loss: 2.1382\tTriple Loss(1): 0.2120\tClassification Loss: 1.7142\r\n",
      "Train Epoch: 2 [51200/209539 (24%)]\tAll Loss: 2.4726\tTriple Loss(1): 0.2920\tClassification Loss: 1.8887\r\n",
      "Train Epoch: 2 [51840/209539 (25%)]\tAll Loss: 1.9950\tTriple Loss(1): 0.1002\tClassification Loss: 1.7946\r\n",
      "Train Epoch: 2 [52480/209539 (25%)]\tAll Loss: 2.4389\tTriple Loss(1): 0.2506\tClassification Loss: 1.9377\r\n",
      "Train Epoch: 2 [53120/209539 (25%)]\tAll Loss: 1.9797\tTriple Loss(1): 0.1727\tClassification Loss: 1.6343\r\n",
      "Train Epoch: 2 [53760/209539 (26%)]\tAll Loss: 2.2111\tTriple Loss(1): 0.0977\tClassification Loss: 2.0156\r\n",
      "Train Epoch: 2 [54400/209539 (26%)]\tAll Loss: 3.0856\tTriple Loss(0): 0.4739\tClassification Loss: 2.1377\r\n",
      "Train Epoch: 2 [55040/209539 (26%)]\tAll Loss: 3.0034\tTriple Loss(0): 0.6065\tClassification Loss: 1.7904\r\n",
      "Train Epoch: 2 [55680/209539 (27%)]\tAll Loss: 2.5912\tTriple Loss(1): 0.3478\tClassification Loss: 1.8956\r\n",
      "Train Epoch: 2 [56320/209539 (27%)]\tAll Loss: 2.1026\tTriple Loss(1): 0.1675\tClassification Loss: 1.7676\r\n",
      "Train Epoch: 2 [56960/209539 (27%)]\tAll Loss: 2.2080\tTriple Loss(1): 0.1165\tClassification Loss: 1.9750\r\n",
      "Train Epoch: 2 [57600/209539 (27%)]\tAll Loss: 2.2018\tTriple Loss(1): 0.1905\tClassification Loss: 1.8208\r\n",
      "Train Epoch: 2 [58240/209539 (28%)]\tAll Loss: 2.2758\tTriple Loss(1): 0.2693\tClassification Loss: 1.7372\r\n",
      "Train Epoch: 2 [58880/209539 (28%)]\tAll Loss: 2.0049\tTriple Loss(1): 0.0766\tClassification Loss: 1.8517\r\n",
      "Train Epoch: 2 [59520/209539 (28%)]\tAll Loss: 3.1895\tTriple Loss(0): 0.6377\tClassification Loss: 1.9141\r\n",
      "Train Epoch: 2 [60160/209539 (29%)]\tAll Loss: 2.3675\tTriple Loss(1): 0.2707\tClassification Loss: 1.8260\r\n",
      "Train Epoch: 2 [60800/209539 (29%)]\tAll Loss: 2.3902\tTriple Loss(1): 0.2272\tClassification Loss: 1.9358\r\n",
      "Train Epoch: 2 [61440/209539 (29%)]\tAll Loss: 2.2623\tTriple Loss(1): 0.2188\tClassification Loss: 1.8247\r\n",
      "Train Epoch: 2 [62080/209539 (30%)]\tAll Loss: 2.2823\tTriple Loss(1): 0.1447\tClassification Loss: 1.9929\r\n",
      "Train Epoch: 2 [62720/209539 (30%)]\tAll Loss: 3.6417\tTriple Loss(0): 0.7475\tClassification Loss: 2.1467\r\n",
      "Train Epoch: 2 [63360/209539 (30%)]\tAll Loss: 2.3318\tTriple Loss(1): 0.2240\tClassification Loss: 1.8839\r\n",
      "Train Epoch: 2 [64000/209539 (31%)]\tAll Loss: 3.2097\tTriple Loss(0): 0.6465\tClassification Loss: 1.9166\r\n",
      "Train Epoch: 2 [64640/209539 (31%)]\tAll Loss: 2.4922\tTriple Loss(1): 0.1512\tClassification Loss: 2.1899\r\n",
      "Train Epoch: 2 [65280/209539 (31%)]\tAll Loss: 2.4233\tTriple Loss(1): 0.2295\tClassification Loss: 1.9642\r\n",
      "Train Epoch: 2 [65920/209539 (31%)]\tAll Loss: 2.2199\tTriple Loss(1): 0.1823\tClassification Loss: 1.8553\r\n",
      "Train Epoch: 2 [66560/209539 (32%)]\tAll Loss: 1.9949\tTriple Loss(1): 0.1068\tClassification Loss: 1.7813\r\n",
      "Train Epoch: 2 [67200/209539 (32%)]\tAll Loss: 2.5635\tTriple Loss(1): 0.2208\tClassification Loss: 2.1219\r\n",
      "Train Epoch: 2 [67840/209539 (32%)]\tAll Loss: 3.3117\tTriple Loss(0): 0.7865\tClassification Loss: 1.7386\r\n",
      "Train Epoch: 2 [68480/209539 (33%)]\tAll Loss: 2.3547\tTriple Loss(1): 0.2568\tClassification Loss: 1.8411\r\n",
      "Train Epoch: 2 [69120/209539 (33%)]\tAll Loss: 2.0947\tTriple Loss(1): 0.2088\tClassification Loss: 1.6770\r\n",
      "Train Epoch: 2 [69760/209539 (33%)]\tAll Loss: 2.7399\tTriple Loss(1): 0.2997\tClassification Loss: 2.1405\r\n",
      "Train Epoch: 2 [70400/209539 (34%)]\tAll Loss: 2.1466\tTriple Loss(1): 0.2412\tClassification Loss: 1.6643\r\n",
      "Train Epoch: 2 [71040/209539 (34%)]\tAll Loss: 3.7723\tTriple Loss(0): 0.9640\tClassification Loss: 1.8442\r\n",
      "Train Epoch: 2 [71680/209539 (34%)]\tAll Loss: 2.3549\tTriple Loss(1): 0.2084\tClassification Loss: 1.9382\r\n",
      "Train Epoch: 2 [72320/209539 (35%)]\tAll Loss: 2.1173\tTriple Loss(1): 0.1095\tClassification Loss: 1.8983\r\n",
      "Train Epoch: 2 [72960/209539 (35%)]\tAll Loss: 2.3780\tTriple Loss(1): 0.2171\tClassification Loss: 1.9437\r\n",
      "Train Epoch: 2 [73600/209539 (35%)]\tAll Loss: 2.6566\tTriple Loss(1): 0.3715\tClassification Loss: 1.9135\r\n",
      "Train Epoch: 2 [74240/209539 (35%)]\tAll Loss: 3.6200\tTriple Loss(0): 0.8603\tClassification Loss: 1.8993\r\n",
      "Train Epoch: 2 [74880/209539 (36%)]\tAll Loss: 2.1292\tTriple Loss(1): 0.1894\tClassification Loss: 1.7505\r\n",
      "Train Epoch: 2 [75520/209539 (36%)]\tAll Loss: 2.4473\tTriple Loss(1): 0.2638\tClassification Loss: 1.9197\r\n",
      "Train Epoch: 2 [76160/209539 (36%)]\tAll Loss: 2.2120\tTriple Loss(1): 0.0971\tClassification Loss: 2.0179\r\n",
      "Train Epoch: 2 [76800/209539 (37%)]\tAll Loss: 2.7106\tTriple Loss(1): 0.3672\tClassification Loss: 1.9761\r\n",
      "Train Epoch: 2 [77440/209539 (37%)]\tAll Loss: 2.4447\tTriple Loss(1): 0.2581\tClassification Loss: 1.9286\r\n",
      "Train Epoch: 2 [78080/209539 (37%)]\tAll Loss: 3.2149\tTriple Loss(0): 0.6947\tClassification Loss: 1.8256\r\n",
      "Train Epoch: 2 [78720/209539 (38%)]\tAll Loss: 2.8330\tTriple Loss(1): 0.2923\tClassification Loss: 2.2485\r\n",
      "Train Epoch: 2 [79360/209539 (38%)]\tAll Loss: 2.5634\tTriple Loss(1): 0.1766\tClassification Loss: 2.2102\r\n",
      "Train Epoch: 2 [80000/209539 (38%)]\tAll Loss: 2.6001\tTriple Loss(1): 0.2731\tClassification Loss: 2.0539\r\n",
      "Train Epoch: 2 [80640/209539 (38%)]\tAll Loss: 2.2382\tTriple Loss(1): 0.2861\tClassification Loss: 1.6660\r\n",
      "Train Epoch: 2 [81280/209539 (39%)]\tAll Loss: 2.3760\tTriple Loss(1): 0.1856\tClassification Loss: 2.0047\r\n",
      "Train Epoch: 2 [81920/209539 (39%)]\tAll Loss: 2.4873\tTriple Loss(1): 0.2618\tClassification Loss: 1.9637\r\n",
      "Train Epoch: 2 [82560/209539 (39%)]\tAll Loss: 2.5155\tTriple Loss(1): 0.2213\tClassification Loss: 2.0729\r\n",
      "Train Epoch: 2 [83200/209539 (40%)]\tAll Loss: 2.2069\tTriple Loss(1): 0.2350\tClassification Loss: 1.7368\r\n",
      "Train Epoch: 2 [83840/209539 (40%)]\tAll Loss: 2.0586\tTriple Loss(1): 0.1142\tClassification Loss: 1.8301\r\n",
      "Train Epoch: 2 [84480/209539 (40%)]\tAll Loss: 2.2869\tTriple Loss(1): 0.1842\tClassification Loss: 1.9184\r\n",
      "Train Epoch: 2 [85120/209539 (41%)]\tAll Loss: 2.3654\tTriple Loss(1): 0.2828\tClassification Loss: 1.7998\r\n",
      "Train Epoch: 2 [85760/209539 (41%)]\tAll Loss: 3.3878\tTriple Loss(0): 0.7396\tClassification Loss: 1.9086\r\n",
      "Train Epoch: 2 [86400/209539 (41%)]\tAll Loss: 2.5545\tTriple Loss(1): 0.2954\tClassification Loss: 1.9636\r\n",
      "Train Epoch: 2 [87040/209539 (42%)]\tAll Loss: 2.4022\tTriple Loss(1): 0.2938\tClassification Loss: 1.8146\r\n",
      "Train Epoch: 2 [87680/209539 (42%)]\tAll Loss: 2.9022\tTriple Loss(0): 0.5697\tClassification Loss: 1.7628\r\n",
      "Train Epoch: 2 [88320/209539 (42%)]\tAll Loss: 2.6062\tTriple Loss(1): 0.3415\tClassification Loss: 1.9231\r\n",
      "Train Epoch: 2 [88960/209539 (42%)]\tAll Loss: 2.2116\tTriple Loss(1): 0.1686\tClassification Loss: 1.8744\r\n",
      "Train Epoch: 2 [89600/209539 (43%)]\tAll Loss: 2.4940\tTriple Loss(1): 0.1860\tClassification Loss: 2.1220\r\n",
      "Train Epoch: 2 [90240/209539 (43%)]\tAll Loss: 2.3775\tTriple Loss(1): 0.2205\tClassification Loss: 1.9365\r\n",
      "Train Epoch: 2 [90880/209539 (43%)]\tAll Loss: 3.4967\tTriple Loss(0): 0.8360\tClassification Loss: 1.8248\r\n",
      "Train Epoch: 2 [91520/209539 (44%)]\tAll Loss: 3.3608\tTriple Loss(0): 0.6518\tClassification Loss: 2.0571\r\n",
      "Train Epoch: 2 [92160/209539 (44%)]\tAll Loss: 2.3376\tTriple Loss(1): 0.2546\tClassification Loss: 1.8284\r\n",
      "Train Epoch: 2 [92800/209539 (44%)]\tAll Loss: 1.6931\tTriple Loss(1): 0.2542\tClassification Loss: 1.1848\r\n",
      "Train Epoch: 2 [93440/209539 (45%)]\tAll Loss: 3.7383\tTriple Loss(0): 0.8667\tClassification Loss: 2.0049\r\n",
      "Train Epoch: 2 [94080/209539 (45%)]\tAll Loss: 2.1778\tTriple Loss(1): 0.2806\tClassification Loss: 1.6165\r\n",
      "Train Epoch: 2 [94720/209539 (45%)]\tAll Loss: 2.7890\tTriple Loss(0): 0.4444\tClassification Loss: 1.9001\r\n",
      "Train Epoch: 2 [95360/209539 (46%)]\tAll Loss: 2.3181\tTriple Loss(1): 0.1692\tClassification Loss: 1.9798\r\n",
      "Train Epoch: 2 [96000/209539 (46%)]\tAll Loss: 3.4619\tTriple Loss(0): 0.7246\tClassification Loss: 2.0126\r\n",
      "Train Epoch: 2 [96640/209539 (46%)]\tAll Loss: 2.4268\tTriple Loss(1): 0.2083\tClassification Loss: 2.0102\r\n",
      "Train Epoch: 2 [97280/209539 (46%)]\tAll Loss: 2.3770\tTriple Loss(1): 0.2518\tClassification Loss: 1.8735\r\n",
      "Train Epoch: 2 [97920/209539 (47%)]\tAll Loss: 2.2464\tTriple Loss(1): 0.2903\tClassification Loss: 1.6658\r\n",
      "Train Epoch: 2 [98560/209539 (47%)]\tAll Loss: 2.2587\tTriple Loss(1): 0.2544\tClassification Loss: 1.7499\r\n",
      "Train Epoch: 2 [99200/209539 (47%)]\tAll Loss: 2.3681\tTriple Loss(1): 0.2013\tClassification Loss: 1.9655\r\n",
      "Train Epoch: 2 [99840/209539 (48%)]\tAll Loss: 3.5023\tTriple Loss(0): 0.7816\tClassification Loss: 1.9390\r\n",
      "Train Epoch: 2 [100480/209539 (48%)]\tAll Loss: 3.6165\tTriple Loss(0): 0.8316\tClassification Loss: 1.9534\r\n",
      "Train Epoch: 2 [101120/209539 (48%)]\tAll Loss: 2.6693\tTriple Loss(1): 0.2499\tClassification Loss: 2.1696\r\n",
      "Train Epoch: 2 [101760/209539 (49%)]\tAll Loss: 2.4174\tTriple Loss(1): 0.2290\tClassification Loss: 1.9595\r\n",
      "Train Epoch: 2 [102400/209539 (49%)]\tAll Loss: 2.1042\tTriple Loss(1): 0.2004\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 2 [103040/209539 (49%)]\tAll Loss: 2.3929\tTriple Loss(1): 0.1198\tClassification Loss: 2.1534\r\n",
      "Train Epoch: 2 [103680/209539 (49%)]\tAll Loss: 2.9156\tTriple Loss(1): 0.3689\tClassification Loss: 2.1779\r\n",
      "Train Epoch: 2 [104320/209539 (50%)]\tAll Loss: 2.2331\tTriple Loss(1): 0.1549\tClassification Loss: 1.9232\r\n",
      "Train Epoch: 2 [104960/209539 (50%)]\tAll Loss: 2.1871\tTriple Loss(1): 0.2503\tClassification Loss: 1.6865\r\n",
      "Train Epoch: 2 [105600/209539 (50%)]\tAll Loss: 1.7003\tTriple Loss(1): 0.1320\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 2 [106240/209539 (51%)]\tAll Loss: 3.6082\tTriple Loss(0): 0.8477\tClassification Loss: 1.9127\r\n",
      "Train Epoch: 2 [106880/209539 (51%)]\tAll Loss: 2.0653\tTriple Loss(1): 0.1284\tClassification Loss: 1.8085\r\n",
      "Train Epoch: 2 [107520/209539 (51%)]\tAll Loss: 2.2465\tTriple Loss(1): 0.1847\tClassification Loss: 1.8771\r\n",
      "Train Epoch: 2 [108160/209539 (52%)]\tAll Loss: 1.7333\tTriple Loss(1): 0.1439\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 2 [108800/209539 (52%)]\tAll Loss: 2.3953\tTriple Loss(1): 0.1780\tClassification Loss: 2.0393\r\n",
      "Train Epoch: 2 [109440/209539 (52%)]\tAll Loss: 3.0946\tTriple Loss(0): 0.5813\tClassification Loss: 1.9319\r\n",
      "Train Epoch: 2 [110080/209539 (53%)]\tAll Loss: 2.0033\tTriple Loss(1): 0.1587\tClassification Loss: 1.6859\r\n",
      "Train Epoch: 2 [110720/209539 (53%)]\tAll Loss: 2.4885\tTriple Loss(1): 0.2479\tClassification Loss: 1.9927\r\n",
      "Train Epoch: 2 [111360/209539 (53%)]\tAll Loss: 2.2443\tTriple Loss(1): 0.3508\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 2 [112000/209539 (53%)]\tAll Loss: 2.2916\tTriple Loss(1): 0.1803\tClassification Loss: 1.9309\r\n",
      "Train Epoch: 2 [112640/209539 (54%)]\tAll Loss: 2.0915\tTriple Loss(1): 0.2115\tClassification Loss: 1.6685\r\n",
      "Train Epoch: 2 [113280/209539 (54%)]\tAll Loss: 2.0936\tTriple Loss(1): 0.1753\tClassification Loss: 1.7430\r\n",
      "Train Epoch: 2 [113920/209539 (54%)]\tAll Loss: 2.4167\tTriple Loss(1): 0.2753\tClassification Loss: 1.8661\r\n",
      "Train Epoch: 2 [114560/209539 (55%)]\tAll Loss: 2.2742\tTriple Loss(1): 0.2148\tClassification Loss: 1.8447\r\n",
      "Train Epoch: 2 [115200/209539 (55%)]\tAll Loss: 2.3313\tTriple Loss(1): 0.2866\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 2 [115840/209539 (55%)]\tAll Loss: 2.5065\tTriple Loss(1): 0.2977\tClassification Loss: 1.9112\r\n",
      "Train Epoch: 2 [116480/209539 (56%)]\tAll Loss: 2.0301\tTriple Loss(1): 0.1615\tClassification Loss: 1.7071\r\n",
      "Train Epoch: 2 [117120/209539 (56%)]\tAll Loss: 3.9554\tTriple Loss(0): 0.9966\tClassification Loss: 1.9622\r\n",
      "Train Epoch: 2 [117760/209539 (56%)]\tAll Loss: 2.0599\tTriple Loss(1): 0.1363\tClassification Loss: 1.7873\r\n",
      "Train Epoch: 2 [118400/209539 (57%)]\tAll Loss: 1.8889\tTriple Loss(1): 0.1914\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 2 [119040/209539 (57%)]\tAll Loss: 2.2556\tTriple Loss(1): 0.1796\tClassification Loss: 1.8965\r\n",
      "Train Epoch: 2 [119680/209539 (57%)]\tAll Loss: 2.7548\tTriple Loss(0): 0.5190\tClassification Loss: 1.7168\r\n",
      "Train Epoch: 2 [120320/209539 (57%)]\tAll Loss: 2.6786\tTriple Loss(1): 0.3507\tClassification Loss: 1.9772\r\n",
      "Train Epoch: 2 [120960/209539 (58%)]\tAll Loss: 3.0459\tTriple Loss(0): 0.7149\tClassification Loss: 1.6161\r\n",
      "Train Epoch: 2 [121600/209539 (58%)]\tAll Loss: 1.8962\tTriple Loss(1): 0.1302\tClassification Loss: 1.6357\r\n",
      "Train Epoch: 2 [122240/209539 (58%)]\tAll Loss: 2.0784\tTriple Loss(1): 0.1172\tClassification Loss: 1.8441\r\n",
      "Train Epoch: 2 [122880/209539 (59%)]\tAll Loss: 3.6603\tTriple Loss(0): 0.9250\tClassification Loss: 1.8103\r\n",
      "Train Epoch: 2 [123520/209539 (59%)]\tAll Loss: 2.4334\tTriple Loss(1): 0.1906\tClassification Loss: 2.0523\r\n",
      "Train Epoch: 2 [124160/209539 (59%)]\tAll Loss: 2.2519\tTriple Loss(1): 0.2005\tClassification Loss: 1.8508\r\n",
      "Train Epoch: 2 [124800/209539 (60%)]\tAll Loss: 2.3714\tTriple Loss(1): 0.3382\tClassification Loss: 1.6949\r\n",
      "Train Epoch: 2 [125440/209539 (60%)]\tAll Loss: 2.4586\tTriple Loss(1): 0.1563\tClassification Loss: 2.1461\r\n",
      "Train Epoch: 2 [126080/209539 (60%)]\tAll Loss: 2.5340\tTriple Loss(1): 0.3491\tClassification Loss: 1.8358\r\n",
      "Train Epoch: 2 [126720/209539 (60%)]\tAll Loss: 1.9312\tTriple Loss(1): 0.1164\tClassification Loss: 1.6985\r\n",
      "Train Epoch: 2 [127360/209539 (61%)]\tAll Loss: 2.5885\tTriple Loss(1): 0.2235\tClassification Loss: 2.1415\r\n",
      "Train Epoch: 2 [128000/209539 (61%)]\tAll Loss: 2.2390\tTriple Loss(1): 0.1513\tClassification Loss: 1.9364\r\n",
      "Train Epoch: 2 [128640/209539 (61%)]\tAll Loss: 2.4784\tTriple Loss(1): 0.3529\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 2 [129280/209539 (62%)]\tAll Loss: 2.6017\tTriple Loss(1): 0.4263\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 2 [129920/209539 (62%)]\tAll Loss: 2.6182\tTriple Loss(1): 0.3287\tClassification Loss: 1.9608\r\n",
      "Train Epoch: 2 [130560/209539 (62%)]\tAll Loss: 2.0485\tTriple Loss(1): 0.1963\tClassification Loss: 1.6560\r\n",
      "Train Epoch: 2 [131200/209539 (63%)]\tAll Loss: 2.5298\tTriple Loss(1): 0.2837\tClassification Loss: 1.9623\r\n",
      "Train Epoch: 2 [131840/209539 (63%)]\tAll Loss: 2.1667\tTriple Loss(1): 0.2189\tClassification Loss: 1.7289\r\n",
      "Train Epoch: 2 [132480/209539 (63%)]\tAll Loss: 2.2536\tTriple Loss(1): 0.1343\tClassification Loss: 1.9851\r\n",
      "Train Epoch: 2 [133120/209539 (64%)]\tAll Loss: 2.1970\tTriple Loss(1): 0.2695\tClassification Loss: 1.6580\r\n",
      "Train Epoch: 2 [133760/209539 (64%)]\tAll Loss: 1.9364\tTriple Loss(1): 0.1507\tClassification Loss: 1.6350\r\n",
      "Train Epoch: 2 [134400/209539 (64%)]\tAll Loss: 2.3390\tTriple Loss(1): 0.2611\tClassification Loss: 1.8169\r\n",
      "Train Epoch: 2 [135040/209539 (64%)]\tAll Loss: 2.2962\tTriple Loss(1): 0.1956\tClassification Loss: 1.9051\r\n",
      "Train Epoch: 2 [135680/209539 (65%)]\tAll Loss: 3.8394\tTriple Loss(0): 0.8713\tClassification Loss: 2.0969\r\n",
      "Train Epoch: 2 [136320/209539 (65%)]\tAll Loss: 2.5700\tTriple Loss(1): 0.3418\tClassification Loss: 1.8864\r\n",
      "Train Epoch: 2 [136960/209539 (65%)]\tAll Loss: 2.3201\tTriple Loss(1): 0.2990\tClassification Loss: 1.7221\r\n",
      "Train Epoch: 2 [137600/209539 (66%)]\tAll Loss: 3.0226\tTriple Loss(0): 0.5623\tClassification Loss: 1.8980\r\n",
      "Train Epoch: 2 [138240/209539 (66%)]\tAll Loss: 2.4038\tTriple Loss(1): 0.3381\tClassification Loss: 1.7276\r\n",
      "Train Epoch: 2 [138880/209539 (66%)]\tAll Loss: 2.3963\tTriple Loss(1): 0.2563\tClassification Loss: 1.8836\r\n",
      "Train Epoch: 2 [139520/209539 (67%)]\tAll Loss: 2.3255\tTriple Loss(1): 0.2362\tClassification Loss: 1.8531\r\n",
      "Train Epoch: 2 [140160/209539 (67%)]\tAll Loss: 2.3167\tTriple Loss(1): 0.0984\tClassification Loss: 2.1200\r\n",
      "Train Epoch: 2 [140800/209539 (67%)]\tAll Loss: 2.0208\tTriple Loss(1): 0.1814\tClassification Loss: 1.6580\r\n",
      "Train Epoch: 2 [141440/209539 (68%)]\tAll Loss: 1.9964\tTriple Loss(1): 0.1229\tClassification Loss: 1.7505\r\n",
      "Train Epoch: 2 [142080/209539 (68%)]\tAll Loss: 2.0717\tTriple Loss(1): 0.1549\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 2 [142720/209539 (68%)]\tAll Loss: 2.4898\tTriple Loss(1): 0.2324\tClassification Loss: 2.0250\r\n",
      "Train Epoch: 2 [143360/209539 (68%)]\tAll Loss: 2.1789\tTriple Loss(1): 0.2768\tClassification Loss: 1.6254\r\n",
      "Train Epoch: 2 [144000/209539 (69%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.2624\tClassification Loss: 1.8589\r\n",
      "Train Epoch: 2 [144640/209539 (69%)]\tAll Loss: 2.6067\tTriple Loss(1): 0.3013\tClassification Loss: 2.0041\r\n",
      "Train Epoch: 2 [145280/209539 (69%)]\tAll Loss: 3.2215\tTriple Loss(0): 0.6254\tClassification Loss: 1.9708\r\n",
      "Train Epoch: 2 [145920/209539 (70%)]\tAll Loss: 2.2700\tTriple Loss(1): 0.2656\tClassification Loss: 1.7388\r\n",
      "Train Epoch: 2 [146560/209539 (70%)]\tAll Loss: 2.4673\tTriple Loss(1): 0.1246\tClassification Loss: 2.2181\r\n",
      "Train Epoch: 2 [147200/209539 (70%)]\tAll Loss: 2.4431\tTriple Loss(1): 0.1489\tClassification Loss: 2.1453\r\n",
      "Train Epoch: 2 [147840/209539 (71%)]\tAll Loss: 2.1099\tTriple Loss(1): 0.1441\tClassification Loss: 1.8218\r\n",
      "Train Epoch: 2 [148480/209539 (71%)]\tAll Loss: 2.0940\tTriple Loss(1): 0.1765\tClassification Loss: 1.7410\r\n",
      "Train Epoch: 2 [149120/209539 (71%)]\tAll Loss: 2.2838\tTriple Loss(1): 0.1953\tClassification Loss: 1.8932\r\n",
      "Train Epoch: 2 [149760/209539 (71%)]\tAll Loss: 2.7963\tTriple Loss(1): 0.2719\tClassification Loss: 2.2524\r\n",
      "Train Epoch: 2 [150400/209539 (72%)]\tAll Loss: 2.4752\tTriple Loss(1): 0.2410\tClassification Loss: 1.9931\r\n",
      "Train Epoch: 2 [151040/209539 (72%)]\tAll Loss: 3.2828\tTriple Loss(0): 0.6670\tClassification Loss: 1.9487\r\n",
      "Train Epoch: 2 [151680/209539 (72%)]\tAll Loss: 3.1831\tTriple Loss(0): 0.7370\tClassification Loss: 1.7090\r\n",
      "Train Epoch: 2 [152320/209539 (73%)]\tAll Loss: 2.1669\tTriple Loss(1): 0.1346\tClassification Loss: 1.8977\r\n",
      "Train Epoch: 2 [152960/209539 (73%)]\tAll Loss: 3.1972\tTriple Loss(0): 0.5873\tClassification Loss: 2.0227\r\n",
      "Train Epoch: 2 [153600/209539 (73%)]\tAll Loss: 2.2535\tTriple Loss(1): 0.2871\tClassification Loss: 1.6792\r\n",
      "Train Epoch: 2 [154240/209539 (74%)]\tAll Loss: 2.2284\tTriple Loss(1): 0.1627\tClassification Loss: 1.9031\r\n",
      "Train Epoch: 2 [154880/209539 (74%)]\tAll Loss: 2.3453\tTriple Loss(1): 0.2864\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 2 [155520/209539 (74%)]\tAll Loss: 2.1826\tTriple Loss(1): 0.2433\tClassification Loss: 1.6960\r\n",
      "Train Epoch: 2 [156160/209539 (75%)]\tAll Loss: 2.5795\tTriple Loss(1): 0.1561\tClassification Loss: 2.2673\r\n",
      "Train Epoch: 2 [156800/209539 (75%)]\tAll Loss: 2.3961\tTriple Loss(1): 0.2378\tClassification Loss: 1.9206\r\n",
      "Train Epoch: 2 [157440/209539 (75%)]\tAll Loss: 2.6031\tTriple Loss(1): 0.2517\tClassification Loss: 2.0997\r\n",
      "Train Epoch: 2 [158080/209539 (75%)]\tAll Loss: 1.8587\tTriple Loss(1): 0.1230\tClassification Loss: 1.6128\r\n",
      "Train Epoch: 2 [158720/209539 (76%)]\tAll Loss: 3.2337\tTriple Loss(0): 0.6812\tClassification Loss: 1.8712\r\n",
      "Train Epoch: 2 [159360/209539 (76%)]\tAll Loss: 2.2674\tTriple Loss(1): 0.1687\tClassification Loss: 1.9300\r\n",
      "Train Epoch: 2 [160000/209539 (76%)]\tAll Loss: 2.3367\tTriple Loss(1): 0.2152\tClassification Loss: 1.9064\r\n",
      "Train Epoch: 2 [160640/209539 (77%)]\tAll Loss: 2.9514\tTriple Loss(0): 0.5805\tClassification Loss: 1.7904\r\n",
      "Train Epoch: 2 [161280/209539 (77%)]\tAll Loss: 2.1361\tTriple Loss(1): 0.2158\tClassification Loss: 1.7045\r\n",
      "Train Epoch: 2 [161920/209539 (77%)]\tAll Loss: 1.8949\tTriple Loss(1): 0.1836\tClassification Loss: 1.5276\r\n",
      "Train Epoch: 2 [162560/209539 (78%)]\tAll Loss: 2.1000\tTriple Loss(1): 0.2605\tClassification Loss: 1.5789\r\n",
      "Train Epoch: 2 [163200/209539 (78%)]\tAll Loss: 3.3851\tTriple Loss(0): 0.7966\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 2 [163840/209539 (78%)]\tAll Loss: 3.1129\tTriple Loss(0): 0.5984\tClassification Loss: 1.9161\r\n",
      "Train Epoch: 2 [164480/209539 (78%)]\tAll Loss: 2.0116\tTriple Loss(1): 0.1906\tClassification Loss: 1.6304\r\n",
      "Train Epoch: 2 [165120/209539 (79%)]\tAll Loss: 2.1366\tTriple Loss(1): 0.1951\tClassification Loss: 1.7463\r\n",
      "Train Epoch: 2 [165760/209539 (79%)]\tAll Loss: 2.5989\tTriple Loss(1): 0.2588\tClassification Loss: 2.0813\r\n",
      "Train Epoch: 2 [166400/209539 (79%)]\tAll Loss: 3.5681\tTriple Loss(0): 0.8190\tClassification Loss: 1.9302\r\n",
      "Train Epoch: 2 [167040/209539 (80%)]\tAll Loss: 1.9780\tTriple Loss(1): 0.1845\tClassification Loss: 1.6090\r\n",
      "Train Epoch: 2 [167680/209539 (80%)]\tAll Loss: 2.2802\tTriple Loss(1): 0.2544\tClassification Loss: 1.7714\r\n",
      "Train Epoch: 2 [168320/209539 (80%)]\tAll Loss: 2.5033\tTriple Loss(1): 0.2421\tClassification Loss: 2.0190\r\n",
      "Train Epoch: 2 [168960/209539 (81%)]\tAll Loss: 2.2209\tTriple Loss(1): 0.1637\tClassification Loss: 1.8936\r\n",
      "Train Epoch: 2 [169600/209539 (81%)]\tAll Loss: 2.5250\tTriple Loss(1): 0.3231\tClassification Loss: 1.8788\r\n",
      "Train Epoch: 2 [170240/209539 (81%)]\tAll Loss: 2.2680\tTriple Loss(1): 0.3139\tClassification Loss: 1.6403\r\n",
      "Train Epoch: 2 [170880/209539 (82%)]\tAll Loss: 2.3568\tTriple Loss(1): 0.2727\tClassification Loss: 1.8114\r\n",
      "Train Epoch: 2 [171520/209539 (82%)]\tAll Loss: 1.9403\tTriple Loss(1): 0.0900\tClassification Loss: 1.7603\r\n",
      "Train Epoch: 2 [172160/209539 (82%)]\tAll Loss: 2.3620\tTriple Loss(1): 0.2714\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 2 [172800/209539 (82%)]\tAll Loss: 2.5463\tTriple Loss(1): 0.1943\tClassification Loss: 2.1577\r\n",
      "Train Epoch: 2 [173440/209539 (83%)]\tAll Loss: 2.9205\tTriple Loss(0): 0.5646\tClassification Loss: 1.7913\r\n",
      "Train Epoch: 2 [174080/209539 (83%)]\tAll Loss: 3.5097\tTriple Loss(0): 0.9266\tClassification Loss: 1.6565\r\n",
      "Train Epoch: 2 [174720/209539 (83%)]\tAll Loss: 2.1430\tTriple Loss(1): 0.2012\tClassification Loss: 1.7406\r\n",
      "Train Epoch: 2 [175360/209539 (84%)]\tAll Loss: 2.3739\tTriple Loss(1): 0.0777\tClassification Loss: 2.2184\r\n",
      "Train Epoch: 2 [176000/209539 (84%)]\tAll Loss: 1.9943\tTriple Loss(1): 0.1799\tClassification Loss: 1.6346\r\n",
      "Train Epoch: 2 [176640/209539 (84%)]\tAll Loss: 3.3222\tTriple Loss(0): 0.6089\tClassification Loss: 2.1043\r\n",
      "Train Epoch: 2 [177280/209539 (85%)]\tAll Loss: 3.5363\tTriple Loss(0): 0.6389\tClassification Loss: 2.2584\r\n",
      "Train Epoch: 2 [177920/209539 (85%)]\tAll Loss: 2.1567\tTriple Loss(1): 0.1877\tClassification Loss: 1.7813\r\n",
      "Train Epoch: 2 [178560/209539 (85%)]\tAll Loss: 1.8003\tTriple Loss(1): 0.1535\tClassification Loss: 1.4934\r\n",
      "Train Epoch: 2 [179200/209539 (86%)]\tAll Loss: 2.2582\tTriple Loss(1): 0.3223\tClassification Loss: 1.6136\r\n",
      "Train Epoch: 2 [179840/209539 (86%)]\tAll Loss: 2.0946\tTriple Loss(1): 0.2600\tClassification Loss: 1.5746\r\n",
      "Train Epoch: 2 [180480/209539 (86%)]\tAll Loss: 2.3312\tTriple Loss(1): 0.2903\tClassification Loss: 1.7506\r\n",
      "Train Epoch: 2 [181120/209539 (86%)]\tAll Loss: 2.5749\tTriple Loss(1): 0.2585\tClassification Loss: 2.0578\r\n",
      "Train Epoch: 2 [181760/209539 (87%)]\tAll Loss: 2.2787\tTriple Loss(1): 0.2381\tClassification Loss: 1.8025\r\n",
      "Train Epoch: 2 [182400/209539 (87%)]\tAll Loss: 2.1141\tTriple Loss(1): 0.1261\tClassification Loss: 1.8619\r\n",
      "Train Epoch: 2 [183040/209539 (87%)]\tAll Loss: 3.1940\tTriple Loss(0): 0.6579\tClassification Loss: 1.8783\r\n",
      "Train Epoch: 2 [183680/209539 (88%)]\tAll Loss: 2.0621\tTriple Loss(1): 0.1438\tClassification Loss: 1.7745\r\n",
      "Train Epoch: 2 [184320/209539 (88%)]\tAll Loss: 2.2990\tTriple Loss(1): 0.2391\tClassification Loss: 1.8208\r\n",
      "Train Epoch: 2 [184960/209539 (88%)]\tAll Loss: 2.1274\tTriple Loss(1): 0.1634\tClassification Loss: 1.8006\r\n",
      "Train Epoch: 2 [185600/209539 (89%)]\tAll Loss: 2.2022\tTriple Loss(1): 0.1813\tClassification Loss: 1.8397\r\n",
      "Train Epoch: 2 [186240/209539 (89%)]\tAll Loss: 2.0017\tTriple Loss(1): 0.1110\tClassification Loss: 1.7798\r\n",
      "Train Epoch: 2 [186880/209539 (89%)]\tAll Loss: 1.8961\tTriple Loss(1): 0.1658\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 2 [187520/209539 (89%)]\tAll Loss: 1.9732\tTriple Loss(1): 0.0707\tClassification Loss: 1.8318\r\n",
      "Train Epoch: 2 [188160/209539 (90%)]\tAll Loss: 2.5334\tTriple Loss(1): 0.3316\tClassification Loss: 1.8701\r\n",
      "Train Epoch: 2 [188800/209539 (90%)]\tAll Loss: 2.6917\tTriple Loss(1): 0.4175\tClassification Loss: 1.8567\r\n",
      "Train Epoch: 2 [189440/209539 (90%)]\tAll Loss: 2.2417\tTriple Loss(1): 0.1707\tClassification Loss: 1.9004\r\n",
      "Train Epoch: 2 [190080/209539 (91%)]\tAll Loss: 3.2869\tTriple Loss(0): 0.7964\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 2 [190720/209539 (91%)]\tAll Loss: 2.2458\tTriple Loss(1): 0.2212\tClassification Loss: 1.8035\r\n",
      "Train Epoch: 2 [191360/209539 (91%)]\tAll Loss: 1.8836\tTriple Loss(1): 0.1018\tClassification Loss: 1.6799\r\n",
      "Train Epoch: 2 [192000/209539 (92%)]\tAll Loss: 2.4488\tTriple Loss(1): 0.2470\tClassification Loss: 1.9547\r\n",
      "Train Epoch: 2 [192640/209539 (92%)]\tAll Loss: 3.3700\tTriple Loss(0): 0.6809\tClassification Loss: 2.0083\r\n",
      "Train Epoch: 2 [193280/209539 (92%)]\tAll Loss: 2.4804\tTriple Loss(1): 0.3383\tClassification Loss: 1.8037\r\n",
      "Train Epoch: 2 [193920/209539 (93%)]\tAll Loss: 2.1479\tTriple Loss(1): 0.2218\tClassification Loss: 1.7043\r\n",
      "Train Epoch: 2 [194560/209539 (93%)]\tAll Loss: 3.6601\tTriple Loss(0): 0.9333\tClassification Loss: 1.7935\r\n",
      "Train Epoch: 2 [195200/209539 (93%)]\tAll Loss: 1.9613\tTriple Loss(1): 0.1378\tClassification Loss: 1.6857\r\n",
      "Train Epoch: 2 [195840/209539 (93%)]\tAll Loss: 2.1521\tTriple Loss(1): 0.2780\tClassification Loss: 1.5961\r\n",
      "Train Epoch: 2 [196480/209539 (94%)]\tAll Loss: 1.9958\tTriple Loss(1): 0.1098\tClassification Loss: 1.7761\r\n",
      "Train Epoch: 2 [197120/209539 (94%)]\tAll Loss: 1.9200\tTriple Loss(1): 0.1011\tClassification Loss: 1.7179\r\n",
      "Train Epoch: 2 [197760/209539 (94%)]\tAll Loss: 2.0969\tTriple Loss(1): 0.2526\tClassification Loss: 1.5917\r\n",
      "Train Epoch: 2 [198400/209539 (95%)]\tAll Loss: 2.0229\tTriple Loss(1): 0.2294\tClassification Loss: 1.5641\r\n",
      "Train Epoch: 2 [199040/209539 (95%)]\tAll Loss: 2.3123\tTriple Loss(1): 0.1604\tClassification Loss: 1.9916\r\n",
      "Train Epoch: 2 [199680/209539 (95%)]\tAll Loss: 2.2476\tTriple Loss(1): 0.1295\tClassification Loss: 1.9886\r\n",
      "Train Epoch: 2 [200320/209539 (96%)]\tAll Loss: 2.4760\tTriple Loss(1): 0.3050\tClassification Loss: 1.8660\r\n",
      "Train Epoch: 2 [200960/209539 (96%)]\tAll Loss: 2.1885\tTriple Loss(1): 0.2083\tClassification Loss: 1.7719\r\n",
      "Train Epoch: 2 [201600/209539 (96%)]\tAll Loss: 2.3747\tTriple Loss(1): 0.2148\tClassification Loss: 1.9451\r\n",
      "Train Epoch: 2 [202240/209539 (97%)]\tAll Loss: 3.5916\tTriple Loss(0): 0.8565\tClassification Loss: 1.8786\r\n",
      "Train Epoch: 2 [202880/209539 (97%)]\tAll Loss: 3.1449\tTriple Loss(0): 0.8066\tClassification Loss: 1.5316\r\n",
      "Train Epoch: 2 [203520/209539 (97%)]\tAll Loss: 2.3804\tTriple Loss(1): 0.1485\tClassification Loss: 2.0834\r\n",
      "Train Epoch: 2 [204160/209539 (97%)]\tAll Loss: 2.5324\tTriple Loss(1): 0.2378\tClassification Loss: 2.0567\r\n",
      "Train Epoch: 2 [204800/209539 (98%)]\tAll Loss: 2.0282\tTriple Loss(1): 0.2298\tClassification Loss: 1.5685\r\n",
      "Train Epoch: 2 [205440/209539 (98%)]\tAll Loss: 1.9136\tTriple Loss(1): 0.1908\tClassification Loss: 1.5320\r\n",
      "Train Epoch: 2 [206080/209539 (98%)]\tAll Loss: 2.1083\tTriple Loss(1): 0.1269\tClassification Loss: 1.8545\r\n",
      "Train Epoch: 2 [206720/209539 (99%)]\tAll Loss: 2.1684\tTriple Loss(1): 0.1731\tClassification Loss: 1.8222\r\n",
      "Train Epoch: 2 [207360/209539 (99%)]\tAll Loss: 1.8688\tTriple Loss(1): 0.1190\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 2 [208000/209539 (99%)]\tAll Loss: 2.3611\tTriple Loss(1): 0.2204\tClassification Loss: 1.9204\r\n",
      "Train Epoch: 2 [208640/209539 (100%)]\tAll Loss: 3.0779\tTriple Loss(0): 0.5823\tClassification Loss: 1.9133\r\n",
      "Train Epoch: 2 [209280/209539 (100%)]\tAll Loss: 2.1059\tTriple Loss(1): 0.1873\tClassification Loss: 1.7314\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/2_epochs\r\n",
      "Train Epoch: 3 [0/209539 (0%)]\tAll Loss: 3.6699\tTriple Loss(0): 0.7717\tClassification Loss: 2.1265\r\n",
      "\r\n",
      "Test set: Average loss: 1.6799, Accuracy: 41540/80128 (52%)\r\n",
      "\r\n",
      "Train Epoch: 3 [640/209539 (0%)]\tAll Loss: 3.4170\tTriple Loss(0): 0.7362\tClassification Loss: 1.9447\r\n",
      "Train Epoch: 3 [1280/209539 (1%)]\tAll Loss: 2.6234\tTriple Loss(1): 0.2307\tClassification Loss: 2.1620\r\n",
      "Train Epoch: 3 [1920/209539 (1%)]\tAll Loss: 2.3745\tTriple Loss(1): 0.2532\tClassification Loss: 1.8682\r\n",
      "Train Epoch: 3 [2560/209539 (1%)]\tAll Loss: 3.0454\tTriple Loss(0): 0.6405\tClassification Loss: 1.7644\r\n",
      "Train Epoch: 3 [3200/209539 (2%)]\tAll Loss: 2.1897\tTriple Loss(1): 0.1328\tClassification Loss: 1.9242\r\n",
      "Train Epoch: 3 [3840/209539 (2%)]\tAll Loss: 3.4645\tTriple Loss(0): 0.7904\tClassification Loss: 1.8838\r\n",
      "Train Epoch: 3 [4480/209539 (2%)]\tAll Loss: 2.3835\tTriple Loss(1): 0.2464\tClassification Loss: 1.8907\r\n",
      "Train Epoch: 3 [5120/209539 (2%)]\tAll Loss: 2.3499\tTriple Loss(1): 0.2056\tClassification Loss: 1.9386\r\n",
      "Train Epoch: 3 [5760/209539 (3%)]\tAll Loss: 2.2906\tTriple Loss(1): 0.3588\tClassification Loss: 1.5730\r\n",
      "Train Epoch: 3 [6400/209539 (3%)]\tAll Loss: 2.2942\tTriple Loss(1): 0.2191\tClassification Loss: 1.8560\r\n",
      "Train Epoch: 3 [7040/209539 (3%)]\tAll Loss: 2.3468\tTriple Loss(1): 0.1443\tClassification Loss: 2.0583\r\n",
      "Train Epoch: 3 [7680/209539 (4%)]\tAll Loss: 2.1401\tTriple Loss(1): 0.1592\tClassification Loss: 1.8217\r\n",
      "Train Epoch: 3 [8320/209539 (4%)]\tAll Loss: 2.1981\tTriple Loss(1): 0.1880\tClassification Loss: 1.8220\r\n",
      "Train Epoch: 3 [8960/209539 (4%)]\tAll Loss: 2.4077\tTriple Loss(1): 0.2113\tClassification Loss: 1.9851\r\n",
      "Train Epoch: 3 [9600/209539 (5%)]\tAll Loss: 3.3474\tTriple Loss(0): 0.6261\tClassification Loss: 2.0953\r\n",
      "Train Epoch: 3 [10240/209539 (5%)]\tAll Loss: 3.3388\tTriple Loss(0): 0.7825\tClassification Loss: 1.7738\r\n",
      "Train Epoch: 3 [10880/209539 (5%)]\tAll Loss: 3.4373\tTriple Loss(0): 0.7022\tClassification Loss: 2.0330\r\n",
      "Train Epoch: 3 [11520/209539 (5%)]\tAll Loss: 3.4112\tTriple Loss(0): 0.7594\tClassification Loss: 1.8923\r\n",
      "Train Epoch: 3 [12160/209539 (6%)]\tAll Loss: 2.9930\tTriple Loss(0): 0.5598\tClassification Loss: 1.8734\r\n",
      "Train Epoch: 3 [12800/209539 (6%)]\tAll Loss: 2.1620\tTriple Loss(1): 0.2180\tClassification Loss: 1.7261\r\n",
      "Train Epoch: 3 [13440/209539 (6%)]\tAll Loss: 2.1655\tTriple Loss(1): 0.1698\tClassification Loss: 1.8259\r\n",
      "Train Epoch: 3 [14080/209539 (7%)]\tAll Loss: 2.5696\tTriple Loss(1): 0.2547\tClassification Loss: 2.0601\r\n",
      "Train Epoch: 3 [14720/209539 (7%)]\tAll Loss: 2.3828\tTriple Loss(1): 0.1237\tClassification Loss: 2.1353\r\n",
      "Train Epoch: 3 [15360/209539 (7%)]\tAll Loss: 2.2358\tTriple Loss(1): 0.2106\tClassification Loss: 1.8147\r\n",
      "Train Epoch: 3 [16000/209539 (8%)]\tAll Loss: 2.6318\tTriple Loss(1): 0.4056\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 3 [16640/209539 (8%)]\tAll Loss: 2.0710\tTriple Loss(1): 0.1734\tClassification Loss: 1.7242\r\n",
      "Train Epoch: 3 [17280/209539 (8%)]\tAll Loss: 3.5755\tTriple Loss(0): 0.9367\tClassification Loss: 1.7021\r\n",
      "Train Epoch: 3 [17920/209539 (9%)]\tAll Loss: 2.0219\tTriple Loss(1): 0.2019\tClassification Loss: 1.6181\r\n",
      "Train Epoch: 3 [18560/209539 (9%)]\tAll Loss: 2.0348\tTriple Loss(1): 0.0522\tClassification Loss: 1.9305\r\n",
      "Train Epoch: 3 [19200/209539 (9%)]\tAll Loss: 3.3794\tTriple Loss(0): 0.7489\tClassification Loss: 1.8815\r\n",
      "Train Epoch: 3 [19840/209539 (9%)]\tAll Loss: 2.2389\tTriple Loss(1): 0.1845\tClassification Loss: 1.8700\r\n",
      "Train Epoch: 3 [20480/209539 (10%)]\tAll Loss: 3.7033\tTriple Loss(0): 0.7659\tClassification Loss: 2.1716\r\n",
      "Train Epoch: 3 [21120/209539 (10%)]\tAll Loss: 1.9269\tTriple Loss(1): 0.1317\tClassification Loss: 1.6635\r\n",
      "Train Epoch: 3 [21760/209539 (10%)]\tAll Loss: 2.2761\tTriple Loss(1): 0.2227\tClassification Loss: 1.8307\r\n",
      "Train Epoch: 3 [22400/209539 (11%)]\tAll Loss: 2.5010\tTriple Loss(1): 0.3429\tClassification Loss: 1.8152\r\n",
      "Train Epoch: 3 [23040/209539 (11%)]\tAll Loss: 2.5487\tTriple Loss(1): 0.2713\tClassification Loss: 2.0062\r\n",
      "Train Epoch: 3 [23680/209539 (11%)]\tAll Loss: 1.8985\tTriple Loss(1): 0.1344\tClassification Loss: 1.6297\r\n",
      "Train Epoch: 3 [24320/209539 (12%)]\tAll Loss: 2.8248\tTriple Loss(0): 0.5648\tClassification Loss: 1.6951\r\n",
      "Train Epoch: 3 [24960/209539 (12%)]\tAll Loss: 2.0429\tTriple Loss(1): 0.1766\tClassification Loss: 1.6896\r\n",
      "Train Epoch: 3 [25600/209539 (12%)]\tAll Loss: 2.6042\tTriple Loss(0): 0.5610\tClassification Loss: 1.4822\r\n",
      "Train Epoch: 3 [26240/209539 (13%)]\tAll Loss: 2.4736\tTriple Loss(1): 0.2801\tClassification Loss: 1.9135\r\n",
      "Train Epoch: 3 [26880/209539 (13%)]\tAll Loss: 3.8186\tTriple Loss(0): 1.0656\tClassification Loss: 1.6873\r\n",
      "Train Epoch: 3 [27520/209539 (13%)]\tAll Loss: 2.1173\tTriple Loss(1): 0.2576\tClassification Loss: 1.6021\r\n",
      "Train Epoch: 3 [28160/209539 (13%)]\tAll Loss: 2.1290\tTriple Loss(1): 0.1462\tClassification Loss: 1.8366\r\n",
      "Train Epoch: 3 [28800/209539 (14%)]\tAll Loss: 2.1720\tTriple Loss(1): 0.1703\tClassification Loss: 1.8313\r\n",
      "Train Epoch: 3 [29440/209539 (14%)]\tAll Loss: 3.2950\tTriple Loss(0): 0.6376\tClassification Loss: 2.0198\r\n",
      "Train Epoch: 3 [30080/209539 (14%)]\tAll Loss: 2.1916\tTriple Loss(1): 0.2579\tClassification Loss: 1.6757\r\n",
      "Train Epoch: 3 [30720/209539 (15%)]\tAll Loss: 3.2821\tTriple Loss(0): 0.6936\tClassification Loss: 1.8949\r\n",
      "Train Epoch: 3 [31360/209539 (15%)]\tAll Loss: 2.6976\tTriple Loss(1): 0.2690\tClassification Loss: 2.1596\r\n",
      "Train Epoch: 3 [32000/209539 (15%)]\tAll Loss: 2.5668\tTriple Loss(1): 0.2542\tClassification Loss: 2.0584\r\n",
      "Train Epoch: 3 [32640/209539 (16%)]\tAll Loss: 2.5684\tTriple Loss(1): 0.1809\tClassification Loss: 2.2065\r\n",
      "Train Epoch: 3 [33280/209539 (16%)]\tAll Loss: 2.2438\tTriple Loss(1): 0.1735\tClassification Loss: 1.8968\r\n",
      "Train Epoch: 3 [33920/209539 (16%)]\tAll Loss: 2.5513\tTriple Loss(1): 0.2826\tClassification Loss: 1.9862\r\n",
      "Train Epoch: 3 [34560/209539 (16%)]\tAll Loss: 3.2054\tTriple Loss(0): 0.6772\tClassification Loss: 1.8510\r\n",
      "Train Epoch: 3 [35200/209539 (17%)]\tAll Loss: 2.3817\tTriple Loss(1): 0.1994\tClassification Loss: 1.9830\r\n",
      "Train Epoch: 3 [35840/209539 (17%)]\tAll Loss: 2.2635\tTriple Loss(1): 0.2049\tClassification Loss: 1.8537\r\n",
      "Train Epoch: 3 [36480/209539 (17%)]\tAll Loss: 2.1916\tTriple Loss(1): 0.2012\tClassification Loss: 1.7892\r\n",
      "Train Epoch: 3 [37120/209539 (18%)]\tAll Loss: 2.3093\tTriple Loss(1): 0.1904\tClassification Loss: 1.9285\r\n",
      "Train Epoch: 3 [37760/209539 (18%)]\tAll Loss: 2.3230\tTriple Loss(1): 0.2188\tClassification Loss: 1.8853\r\n",
      "Train Epoch: 3 [38400/209539 (18%)]\tAll Loss: 3.0093\tTriple Loss(0): 0.7353\tClassification Loss: 1.5387\r\n",
      "Train Epoch: 3 [39040/209539 (19%)]\tAll Loss: 2.3090\tTriple Loss(1): 0.1764\tClassification Loss: 1.9562\r\n",
      "Train Epoch: 3 [39680/209539 (19%)]\tAll Loss: 1.9930\tTriple Loss(1): 0.1833\tClassification Loss: 1.6263\r\n",
      "Train Epoch: 3 [40320/209539 (19%)]\tAll Loss: 2.8761\tTriple Loss(1): 0.3187\tClassification Loss: 2.2387\r\n",
      "Train Epoch: 3 [40960/209539 (20%)]\tAll Loss: 2.3549\tTriple Loss(1): 0.2478\tClassification Loss: 1.8593\r\n",
      "Train Epoch: 3 [41600/209539 (20%)]\tAll Loss: 2.4965\tTriple Loss(1): 0.1536\tClassification Loss: 2.1893\r\n",
      "Train Epoch: 3 [42240/209539 (20%)]\tAll Loss: 2.0637\tTriple Loss(1): 0.1162\tClassification Loss: 1.8314\r\n",
      "Train Epoch: 3 [42880/209539 (20%)]\tAll Loss: 3.1114\tTriple Loss(0): 0.6233\tClassification Loss: 1.8648\r\n",
      "Train Epoch: 3 [43520/209539 (21%)]\tAll Loss: 2.7720\tTriple Loss(1): 0.2990\tClassification Loss: 2.1740\r\n",
      "Train Epoch: 3 [44160/209539 (21%)]\tAll Loss: 2.2775\tTriple Loss(1): 0.2538\tClassification Loss: 1.7699\r\n",
      "Train Epoch: 3 [44800/209539 (21%)]\tAll Loss: 2.3298\tTriple Loss(1): 0.2024\tClassification Loss: 1.9251\r\n",
      "Train Epoch: 3 [45440/209539 (22%)]\tAll Loss: 1.7444\tTriple Loss(1): 0.0709\tClassification Loss: 1.6027\r\n",
      "Train Epoch: 3 [46080/209539 (22%)]\tAll Loss: 2.2821\tTriple Loss(1): 0.1547\tClassification Loss: 1.9727\r\n",
      "Train Epoch: 3 [46720/209539 (22%)]\tAll Loss: 2.1571\tTriple Loss(1): 0.1676\tClassification Loss: 1.8218\r\n",
      "Train Epoch: 3 [47360/209539 (23%)]\tAll Loss: 2.3361\tTriple Loss(1): 0.1677\tClassification Loss: 2.0007\r\n",
      "Train Epoch: 3 [48000/209539 (23%)]\tAll Loss: 2.3976\tTriple Loss(1): 0.1331\tClassification Loss: 2.1314\r\n",
      "Train Epoch: 3 [48640/209539 (23%)]\tAll Loss: 1.8980\tTriple Loss(1): 0.1504\tClassification Loss: 1.5973\r\n",
      "Train Epoch: 3 [49280/209539 (24%)]\tAll Loss: 2.2084\tTriple Loss(1): 0.1917\tClassification Loss: 1.8250\r\n",
      "Train Epoch: 3 [49920/209539 (24%)]\tAll Loss: 2.0811\tTriple Loss(1): 0.0664\tClassification Loss: 1.9483\r\n",
      "Train Epoch: 3 [50560/209539 (24%)]\tAll Loss: 2.2532\tTriple Loss(1): 0.2268\tClassification Loss: 1.7995\r\n",
      "Train Epoch: 3 [51200/209539 (24%)]\tAll Loss: 2.8839\tTriple Loss(0): 0.5196\tClassification Loss: 1.8447\r\n",
      "Train Epoch: 3 [51840/209539 (25%)]\tAll Loss: 2.0899\tTriple Loss(1): 0.1540\tClassification Loss: 1.7820\r\n",
      "Train Epoch: 3 [52480/209539 (25%)]\tAll Loss: 2.4108\tTriple Loss(1): 0.2247\tClassification Loss: 1.9615\r\n",
      "Train Epoch: 3 [53120/209539 (25%)]\tAll Loss: 2.6780\tTriple Loss(0): 0.5565\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 3 [53760/209539 (26%)]\tAll Loss: 2.3802\tTriple Loss(1): 0.2287\tClassification Loss: 1.9228\r\n",
      "Train Epoch: 3 [54400/209539 (26%)]\tAll Loss: 2.6694\tTriple Loss(1): 0.3022\tClassification Loss: 2.0651\r\n",
      "Train Epoch: 3 [55040/209539 (26%)]\tAll Loss: 2.0052\tTriple Loss(1): 0.2022\tClassification Loss: 1.6008\r\n",
      "Train Epoch: 3 [55680/209539 (27%)]\tAll Loss: 2.0744\tTriple Loss(1): 0.1415\tClassification Loss: 1.7914\r\n",
      "Train Epoch: 3 [56320/209539 (27%)]\tAll Loss: 3.1817\tTriple Loss(0): 0.7068\tClassification Loss: 1.7681\r\n",
      "Train Epoch: 3 [56960/209539 (27%)]\tAll Loss: 3.2882\tTriple Loss(0): 0.6371\tClassification Loss: 2.0139\r\n",
      "Train Epoch: 3 [57600/209539 (27%)]\tAll Loss: 2.3672\tTriple Loss(1): 0.3056\tClassification Loss: 1.7560\r\n",
      "Train Epoch: 3 [58240/209539 (28%)]\tAll Loss: 2.1692\tTriple Loss(1): 0.2707\tClassification Loss: 1.6278\r\n",
      "Train Epoch: 3 [58880/209539 (28%)]\tAll Loss: 2.3242\tTriple Loss(1): 0.2502\tClassification Loss: 1.8239\r\n",
      "Train Epoch: 3 [59520/209539 (28%)]\tAll Loss: 2.2188\tTriple Loss(1): 0.1667\tClassification Loss: 1.8854\r\n",
      "Train Epoch: 3 [60160/209539 (29%)]\tAll Loss: 2.0217\tTriple Loss(1): 0.1285\tClassification Loss: 1.7648\r\n",
      "Train Epoch: 3 [60800/209539 (29%)]\tAll Loss: 2.2585\tTriple Loss(1): 0.1814\tClassification Loss: 1.8958\r\n",
      "Train Epoch: 3 [61440/209539 (29%)]\tAll Loss: 2.4115\tTriple Loss(1): 0.3113\tClassification Loss: 1.7889\r\n",
      "Train Epoch: 3 [62080/209539 (30%)]\tAll Loss: 3.5363\tTriple Loss(0): 0.7774\tClassification Loss: 1.9815\r\n",
      "Train Epoch: 3 [62720/209539 (30%)]\tAll Loss: 2.5099\tTriple Loss(1): 0.2588\tClassification Loss: 1.9923\r\n",
      "Train Epoch: 3 [63360/209539 (30%)]\tAll Loss: 2.4988\tTriple Loss(1): 0.2522\tClassification Loss: 1.9944\r\n",
      "Train Epoch: 3 [64000/209539 (31%)]\tAll Loss: 3.2340\tTriple Loss(0): 0.6577\tClassification Loss: 1.9185\r\n",
      "Train Epoch: 3 [64640/209539 (31%)]\tAll Loss: 3.4730\tTriple Loss(0): 0.6836\tClassification Loss: 2.1059\r\n",
      "Train Epoch: 3 [65280/209539 (31%)]\tAll Loss: 2.4801\tTriple Loss(1): 0.2558\tClassification Loss: 1.9686\r\n",
      "Train Epoch: 3 [65920/209539 (31%)]\tAll Loss: 1.9921\tTriple Loss(1): 0.0860\tClassification Loss: 1.8201\r\n",
      "Train Epoch: 3 [66560/209539 (32%)]\tAll Loss: 2.1897\tTriple Loss(1): 0.1804\tClassification Loss: 1.8289\r\n",
      "Train Epoch: 3 [67200/209539 (32%)]\tAll Loss: 3.1739\tTriple Loss(0): 0.5763\tClassification Loss: 2.0213\r\n",
      "Train Epoch: 3 [67840/209539 (32%)]\tAll Loss: 1.9518\tTriple Loss(1): 0.1579\tClassification Loss: 1.6359\r\n",
      "Train Epoch: 3 [68480/209539 (33%)]\tAll Loss: 2.1569\tTriple Loss(1): 0.2248\tClassification Loss: 1.7072\r\n",
      "Train Epoch: 3 [69120/209539 (33%)]\tAll Loss: 3.3210\tTriple Loss(0): 0.8355\tClassification Loss: 1.6500\r\n",
      "Train Epoch: 3 [69760/209539 (33%)]\tAll Loss: 2.8079\tTriple Loss(1): 0.3861\tClassification Loss: 2.0356\r\n",
      "Train Epoch: 3 [70400/209539 (34%)]\tAll Loss: 2.1409\tTriple Loss(1): 0.2927\tClassification Loss: 1.5554\r\n",
      "Train Epoch: 3 [71040/209539 (34%)]\tAll Loss: 2.5512\tTriple Loss(1): 0.3551\tClassification Loss: 1.8411\r\n",
      "Train Epoch: 3 [71680/209539 (34%)]\tAll Loss: 2.2611\tTriple Loss(1): 0.2074\tClassification Loss: 1.8464\r\n",
      "Train Epoch: 3 [72320/209539 (35%)]\tAll Loss: 2.5934\tTriple Loss(1): 0.3245\tClassification Loss: 1.9443\r\n",
      "Train Epoch: 3 [72960/209539 (35%)]\tAll Loss: 3.0731\tTriple Loss(0): 0.6318\tClassification Loss: 1.8094\r\n",
      "Train Epoch: 3 [73600/209539 (35%)]\tAll Loss: 2.2098\tTriple Loss(1): 0.1363\tClassification Loss: 1.9371\r\n",
      "Train Epoch: 3 [74240/209539 (35%)]\tAll Loss: 2.3556\tTriple Loss(1): 0.2606\tClassification Loss: 1.8343\r\n",
      "Train Epoch: 3 [74880/209539 (36%)]\tAll Loss: 2.1702\tTriple Loss(1): 0.2568\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 3 [75520/209539 (36%)]\tAll Loss: 2.3573\tTriple Loss(1): 0.2253\tClassification Loss: 1.9066\r\n",
      "Train Epoch: 3 [76160/209539 (36%)]\tAll Loss: 2.4739\tTriple Loss(1): 0.3067\tClassification Loss: 1.8605\r\n",
      "Train Epoch: 3 [76800/209539 (37%)]\tAll Loss: 2.2020\tTriple Loss(1): 0.1769\tClassification Loss: 1.8481\r\n",
      "Train Epoch: 3 [77440/209539 (37%)]\tAll Loss: 3.6453\tTriple Loss(0): 0.8343\tClassification Loss: 1.9767\r\n",
      "Train Epoch: 3 [78080/209539 (37%)]\tAll Loss: 2.0494\tTriple Loss(1): 0.1050\tClassification Loss: 1.8394\r\n",
      "Train Epoch: 3 [78720/209539 (38%)]\tAll Loss: 2.3046\tTriple Loss(1): 0.0966\tClassification Loss: 2.1114\r\n",
      "Train Epoch: 3 [79360/209539 (38%)]\tAll Loss: 2.7369\tTriple Loss(1): 0.2212\tClassification Loss: 2.2945\r\n",
      "Train Epoch: 3 [80000/209539 (38%)]\tAll Loss: 2.3802\tTriple Loss(1): 0.1612\tClassification Loss: 2.0578\r\n",
      "Train Epoch: 3 [80640/209539 (38%)]\tAll Loss: 3.2292\tTriple Loss(0): 0.8011\tClassification Loss: 1.6270\r\n",
      "Train Epoch: 3 [81280/209539 (39%)]\tAll Loss: 2.0102\tTriple Loss(1): 0.0628\tClassification Loss: 1.8845\r\n",
      "Train Epoch: 3 [81920/209539 (39%)]\tAll Loss: 3.3851\tTriple Loss(0): 0.7547\tClassification Loss: 1.8757\r\n",
      "Train Epoch: 3 [82560/209539 (39%)]\tAll Loss: 2.6944\tTriple Loss(1): 0.3069\tClassification Loss: 2.0807\r\n",
      "Train Epoch: 3 [83200/209539 (40%)]\tAll Loss: 2.7547\tTriple Loss(0): 0.5227\tClassification Loss: 1.7092\r\n",
      "Train Epoch: 3 [83840/209539 (40%)]\tAll Loss: 2.3124\tTriple Loss(1): 0.2181\tClassification Loss: 1.8763\r\n",
      "Train Epoch: 3 [84480/209539 (40%)]\tAll Loss: 2.2122\tTriple Loss(1): 0.1049\tClassification Loss: 2.0025\r\n",
      "Train Epoch: 3 [85120/209539 (41%)]\tAll Loss: 3.1075\tTriple Loss(0): 0.6606\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 3 [85760/209539 (41%)]\tAll Loss: 1.8744\tTriple Loss(1): 0.0862\tClassification Loss: 1.7020\r\n",
      "Train Epoch: 3 [86400/209539 (41%)]\tAll Loss: 2.5939\tTriple Loss(1): 0.3025\tClassification Loss: 1.9889\r\n",
      "Train Epoch: 3 [87040/209539 (42%)]\tAll Loss: 2.3300\tTriple Loss(1): 0.2591\tClassification Loss: 1.8117\r\n",
      "Train Epoch: 3 [87680/209539 (42%)]\tAll Loss: 2.0255\tTriple Loss(1): 0.1437\tClassification Loss: 1.7382\r\n",
      "Train Epoch: 3 [88320/209539 (42%)]\tAll Loss: 2.0024\tTriple Loss(1): 0.0818\tClassification Loss: 1.8388\r\n",
      "Train Epoch: 3 [88960/209539 (42%)]\tAll Loss: 2.5355\tTriple Loss(1): 0.3750\tClassification Loss: 1.7855\r\n",
      "Train Epoch: 3 [89600/209539 (43%)]\tAll Loss: 2.2886\tTriple Loss(1): 0.1870\tClassification Loss: 1.9146\r\n",
      "Train Epoch: 3 [90240/209539 (43%)]\tAll Loss: 3.8499\tTriple Loss(0): 1.0399\tClassification Loss: 1.7701\r\n",
      "Train Epoch: 3 [90880/209539 (43%)]\tAll Loss: 3.6740\tTriple Loss(0): 0.9468\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 3 [91520/209539 (44%)]\tAll Loss: 2.4433\tTriple Loss(1): 0.2760\tClassification Loss: 1.8912\r\n",
      "Train Epoch: 3 [92160/209539 (44%)]\tAll Loss: 2.7683\tTriple Loss(0): 0.5344\tClassification Loss: 1.6995\r\n",
      "Train Epoch: 3 [92800/209539 (44%)]\tAll Loss: 2.5204\tTriple Loss(0): 0.6660\tClassification Loss: 1.1884\r\n",
      "Train Epoch: 3 [93440/209539 (45%)]\tAll Loss: 2.2337\tTriple Loss(1): 0.1173\tClassification Loss: 1.9990\r\n",
      "Train Epoch: 3 [94080/209539 (45%)]\tAll Loss: 2.8945\tTriple Loss(0): 0.6581\tClassification Loss: 1.5784\r\n",
      "Train Epoch: 3 [94720/209539 (45%)]\tAll Loss: 2.2439\tTriple Loss(1): 0.1320\tClassification Loss: 1.9799\r\n",
      "Train Epoch: 3 [95360/209539 (46%)]\tAll Loss: 2.6736\tTriple Loss(1): 0.3336\tClassification Loss: 2.0064\r\n",
      "Train Epoch: 3 [96000/209539 (46%)]\tAll Loss: 2.2736\tTriple Loss(1): 0.1357\tClassification Loss: 2.0021\r\n",
      "Train Epoch: 3 [96640/209539 (46%)]\tAll Loss: 2.0275\tTriple Loss(1): 0.0719\tClassification Loss: 1.8838\r\n",
      "Train Epoch: 3 [97280/209539 (46%)]\tAll Loss: 2.4551\tTriple Loss(1): 0.2613\tClassification Loss: 1.9325\r\n",
      "Train Epoch: 3 [97920/209539 (47%)]\tAll Loss: 2.3757\tTriple Loss(1): 0.3011\tClassification Loss: 1.7735\r\n",
      "Train Epoch: 3 [98560/209539 (47%)]\tAll Loss: 2.1781\tTriple Loss(1): 0.2374\tClassification Loss: 1.7033\r\n",
      "Train Epoch: 3 [99200/209539 (47%)]\tAll Loss: 2.0013\tTriple Loss(1): 0.1107\tClassification Loss: 1.7798\r\n",
      "Train Epoch: 3 [99840/209539 (48%)]\tAll Loss: 2.2162\tTriple Loss(1): 0.2080\tClassification Loss: 1.8003\r\n",
      "Train Epoch: 3 [100480/209539 (48%)]\tAll Loss: 2.4438\tTriple Loss(1): 0.2814\tClassification Loss: 1.8810\r\n",
      "Train Epoch: 3 [101120/209539 (48%)]\tAll Loss: 3.4682\tTriple Loss(0): 0.7607\tClassification Loss: 1.9468\r\n",
      "Train Epoch: 3 [101760/209539 (49%)]\tAll Loss: 2.1900\tTriple Loss(1): 0.1906\tClassification Loss: 1.8087\r\n",
      "Train Epoch: 3 [102400/209539 (49%)]\tAll Loss: 2.2278\tTriple Loss(1): 0.2390\tClassification Loss: 1.7499\r\n",
      "Train Epoch: 3 [103040/209539 (49%)]\tAll Loss: 3.4237\tTriple Loss(0): 0.6679\tClassification Loss: 2.0880\r\n",
      "Train Epoch: 3 [103680/209539 (49%)]\tAll Loss: 2.4420\tTriple Loss(1): 0.1910\tClassification Loss: 2.0600\r\n",
      "Train Epoch: 3 [104320/209539 (50%)]\tAll Loss: 2.2367\tTriple Loss(1): 0.1492\tClassification Loss: 1.9384\r\n",
      "Train Epoch: 3 [104960/209539 (50%)]\tAll Loss: 3.1511\tTriple Loss(0): 0.7588\tClassification Loss: 1.6335\r\n",
      "Train Epoch: 3 [105600/209539 (50%)]\tAll Loss: 1.8015\tTriple Loss(1): 0.1799\tClassification Loss: 1.4416\r\n",
      "Train Epoch: 3 [106240/209539 (51%)]\tAll Loss: 2.4141\tTriple Loss(1): 0.2954\tClassification Loss: 1.8232\r\n",
      "Train Epoch: 3 [106880/209539 (51%)]\tAll Loss: 2.0352\tTriple Loss(1): 0.2646\tClassification Loss: 1.5059\r\n",
      "Train Epoch: 3 [107520/209539 (51%)]\tAll Loss: 2.0664\tTriple Loss(1): 0.1542\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 3 [108160/209539 (52%)]\tAll Loss: 1.7783\tTriple Loss(1): 0.2242\tClassification Loss: 1.3298\r\n",
      "Train Epoch: 3 [108800/209539 (52%)]\tAll Loss: 2.4013\tTriple Loss(1): 0.1880\tClassification Loss: 2.0252\r\n",
      "Train Epoch: 3 [109440/209539 (52%)]\tAll Loss: 2.3310\tTriple Loss(1): 0.1985\tClassification Loss: 1.9340\r\n",
      "Train Epoch: 3 [110080/209539 (53%)]\tAll Loss: 3.1704\tTriple Loss(0): 0.8277\tClassification Loss: 1.5149\r\n",
      "Train Epoch: 3 [110720/209539 (53%)]\tAll Loss: 1.8837\tTriple Loss(1): 0.0927\tClassification Loss: 1.6983\r\n",
      "Train Epoch: 3 [111360/209539 (53%)]\tAll Loss: 2.1289\tTriple Loss(1): 0.2913\tClassification Loss: 1.5463\r\n",
      "Train Epoch: 3 [112000/209539 (53%)]\tAll Loss: 2.9289\tTriple Loss(0): 0.5832\tClassification Loss: 1.7624\r\n",
      "Train Epoch: 3 [112640/209539 (54%)]\tAll Loss: 1.8577\tTriple Loss(1): 0.1851\tClassification Loss: 1.4876\r\n",
      "Train Epoch: 3 [113280/209539 (54%)]\tAll Loss: 3.0541\tTriple Loss(0): 0.7075\tClassification Loss: 1.6391\r\n",
      "Train Epoch: 3 [113920/209539 (54%)]\tAll Loss: 2.8411\tTriple Loss(0): 0.4439\tClassification Loss: 1.9533\r\n",
      "Train Epoch: 3 [114560/209539 (55%)]\tAll Loss: 2.3695\tTriple Loss(1): 0.3256\tClassification Loss: 1.7184\r\n",
      "Train Epoch: 3 [115200/209539 (55%)]\tAll Loss: 2.3253\tTriple Loss(1): 0.1578\tClassification Loss: 2.0098\r\n",
      "Train Epoch: 3 [115840/209539 (55%)]\tAll Loss: 2.4041\tTriple Loss(1): 0.2535\tClassification Loss: 1.8972\r\n",
      "Train Epoch: 3 [116480/209539 (56%)]\tAll Loss: 2.0522\tTriple Loss(1): 0.1877\tClassification Loss: 1.6767\r\n",
      "Train Epoch: 3 [117120/209539 (56%)]\tAll Loss: 2.5429\tTriple Loss(1): 0.2621\tClassification Loss: 2.0188\r\n",
      "Train Epoch: 3 [117760/209539 (56%)]\tAll Loss: 2.1376\tTriple Loss(1): 0.2426\tClassification Loss: 1.6523\r\n",
      "Train Epoch: 3 [118400/209539 (57%)]\tAll Loss: 2.0135\tTriple Loss(1): 0.1714\tClassification Loss: 1.6707\r\n",
      "Train Epoch: 3 [119040/209539 (57%)]\tAll Loss: 2.4413\tTriple Loss(1): 0.2308\tClassification Loss: 1.9796\r\n",
      "Train Epoch: 3 [119680/209539 (57%)]\tAll Loss: 2.1511\tTriple Loss(1): 0.2147\tClassification Loss: 1.7216\r\n",
      "Train Epoch: 3 [120320/209539 (57%)]\tAll Loss: 1.9871\tTriple Loss(1): 0.1001\tClassification Loss: 1.7869\r\n",
      "Train Epoch: 3 [120960/209539 (58%)]\tAll Loss: 3.1612\tTriple Loss(0): 0.8631\tClassification Loss: 1.4351\r\n",
      "Train Epoch: 3 [121600/209539 (58%)]\tAll Loss: 3.6173\tTriple Loss(0): 0.8886\tClassification Loss: 1.8401\r\n",
      "Train Epoch: 3 [122240/209539 (58%)]\tAll Loss: 2.3186\tTriple Loss(1): 0.2036\tClassification Loss: 1.9115\r\n",
      "Train Epoch: 3 [122880/209539 (59%)]\tAll Loss: 3.6635\tTriple Loss(0): 0.9042\tClassification Loss: 1.8551\r\n",
      "Train Epoch: 3 [123520/209539 (59%)]\tAll Loss: 3.1258\tTriple Loss(0): 0.6173\tClassification Loss: 1.8912\r\n",
      "Train Epoch: 3 [124160/209539 (59%)]\tAll Loss: 2.2863\tTriple Loss(1): 0.2035\tClassification Loss: 1.8794\r\n",
      "Train Epoch: 3 [124800/209539 (60%)]\tAll Loss: 1.9827\tTriple Loss(1): 0.2168\tClassification Loss: 1.5492\r\n",
      "Train Epoch: 3 [125440/209539 (60%)]\tAll Loss: 3.8298\tTriple Loss(0): 0.9106\tClassification Loss: 2.0085\r\n",
      "Train Epoch: 3 [126080/209539 (60%)]\tAll Loss: 3.3635\tTriple Loss(0): 0.8198\tClassification Loss: 1.7240\r\n",
      "Train Epoch: 3 [126720/209539 (60%)]\tAll Loss: 2.1164\tTriple Loss(1): 0.1519\tClassification Loss: 1.8125\r\n",
      "Train Epoch: 3 [127360/209539 (61%)]\tAll Loss: 2.5640\tTriple Loss(1): 0.3096\tClassification Loss: 1.9448\r\n",
      "Train Epoch: 3 [128000/209539 (61%)]\tAll Loss: 2.1975\tTriple Loss(1): 0.2478\tClassification Loss: 1.7018\r\n",
      "Train Epoch: 3 [128640/209539 (61%)]\tAll Loss: 1.9267\tTriple Loss(1): 0.0561\tClassification Loss: 1.8145\r\n",
      "Train Epoch: 3 [129280/209539 (62%)]\tAll Loss: 2.2410\tTriple Loss(1): 0.2296\tClassification Loss: 1.7818\r\n",
      "Train Epoch: 3 [129920/209539 (62%)]\tAll Loss: 2.4418\tTriple Loss(1): 0.2515\tClassification Loss: 1.9388\r\n",
      "Train Epoch: 3 [130560/209539 (62%)]\tAll Loss: 3.2887\tTriple Loss(0): 0.7829\tClassification Loss: 1.7229\r\n",
      "Train Epoch: 3 [131200/209539 (63%)]\tAll Loss: 2.3813\tTriple Loss(1): 0.2505\tClassification Loss: 1.8804\r\n",
      "Train Epoch: 3 [131840/209539 (63%)]\tAll Loss: 2.9562\tTriple Loss(0): 0.6720\tClassification Loss: 1.6122\r\n",
      "Train Epoch: 3 [132480/209539 (63%)]\tAll Loss: 2.6578\tTriple Loss(1): 0.3615\tClassification Loss: 1.9349\r\n",
      "Train Epoch: 3 [133120/209539 (64%)]\tAll Loss: 2.0367\tTriple Loss(1): 0.2013\tClassification Loss: 1.6341\r\n",
      "Train Epoch: 3 [133760/209539 (64%)]\tAll Loss: 1.9540\tTriple Loss(1): 0.2137\tClassification Loss: 1.5266\r\n",
      "Train Epoch: 3 [134400/209539 (64%)]\tAll Loss: 2.1026\tTriple Loss(1): 0.1339\tClassification Loss: 1.8348\r\n",
      "Train Epoch: 3 [135040/209539 (64%)]\tAll Loss: 2.1096\tTriple Loss(1): 0.1634\tClassification Loss: 1.7828\r\n",
      "Train Epoch: 3 [135680/209539 (65%)]\tAll Loss: 2.5862\tTriple Loss(1): 0.2213\tClassification Loss: 2.1435\r\n",
      "Train Epoch: 3 [136320/209539 (65%)]\tAll Loss: 2.6685\tTriple Loss(1): 0.3942\tClassification Loss: 1.8800\r\n",
      "Train Epoch: 3 [136960/209539 (65%)]\tAll Loss: 2.2742\tTriple Loss(1): 0.2260\tClassification Loss: 1.8222\r\n",
      "Train Epoch: 3 [137600/209539 (66%)]\tAll Loss: 2.2388\tTriple Loss(1): 0.1904\tClassification Loss: 1.8580\r\n",
      "Train Epoch: 3 [138240/209539 (66%)]\tAll Loss: 2.0423\tTriple Loss(1): 0.2401\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 3 [138880/209539 (66%)]\tAll Loss: 2.2206\tTriple Loss(1): 0.1837\tClassification Loss: 1.8532\r\n",
      "Train Epoch: 3 [139520/209539 (67%)]\tAll Loss: 2.4943\tTriple Loss(1): 0.3144\tClassification Loss: 1.8654\r\n",
      "Train Epoch: 3 [140160/209539 (67%)]\tAll Loss: 3.3125\tTriple Loss(0): 0.6538\tClassification Loss: 2.0048\r\n",
      "Train Epoch: 3 [140800/209539 (67%)]\tAll Loss: 2.3659\tTriple Loss(1): 0.3516\tClassification Loss: 1.6626\r\n",
      "Train Epoch: 3 [141440/209539 (68%)]\tAll Loss: 2.1119\tTriple Loss(1): 0.2036\tClassification Loss: 1.7046\r\n",
      "Train Epoch: 3 [142080/209539 (68%)]\tAll Loss: 2.0108\tTriple Loss(1): 0.2318\tClassification Loss: 1.5471\r\n",
      "Train Epoch: 3 [142720/209539 (68%)]\tAll Loss: 2.2534\tTriple Loss(1): 0.1488\tClassification Loss: 1.9558\r\n",
      "Train Epoch: 3 [143360/209539 (68%)]\tAll Loss: 1.6216\tTriple Loss(1): 0.0882\tClassification Loss: 1.4452\r\n",
      "Train Epoch: 3 [144000/209539 (69%)]\tAll Loss: 2.1767\tTriple Loss(1): 0.1700\tClassification Loss: 1.8367\r\n",
      "Train Epoch: 3 [144640/209539 (69%)]\tAll Loss: 2.3278\tTriple Loss(1): 0.1950\tClassification Loss: 1.9379\r\n",
      "Train Epoch: 3 [145280/209539 (69%)]\tAll Loss: 2.3475\tTriple Loss(1): 0.1740\tClassification Loss: 1.9994\r\n",
      "Train Epoch: 3 [145920/209539 (70%)]\tAll Loss: 2.8379\tTriple Loss(0): 0.5227\tClassification Loss: 1.7925\r\n",
      "Train Epoch: 3 [146560/209539 (70%)]\tAll Loss: 2.5552\tTriple Loss(1): 0.2048\tClassification Loss: 2.1457\r\n",
      "Train Epoch: 3 [147200/209539 (70%)]\tAll Loss: 3.1049\tTriple Loss(0): 0.5655\tClassification Loss: 1.9738\r\n",
      "Train Epoch: 3 [147840/209539 (71%)]\tAll Loss: 2.4469\tTriple Loss(1): 0.3681\tClassification Loss: 1.7106\r\n",
      "Train Epoch: 3 [148480/209539 (71%)]\tAll Loss: 1.9632\tTriple Loss(1): 0.0494\tClassification Loss: 1.8643\r\n",
      "Train Epoch: 3 [149120/209539 (71%)]\tAll Loss: 2.9876\tTriple Loss(0): 0.5469\tClassification Loss: 1.8938\r\n",
      "Train Epoch: 3 [149760/209539 (71%)]\tAll Loss: 3.5070\tTriple Loss(0): 0.7066\tClassification Loss: 2.0939\r\n",
      "Train Epoch: 3 [150400/209539 (72%)]\tAll Loss: 2.5185\tTriple Loss(1): 0.2703\tClassification Loss: 1.9779\r\n",
      "Train Epoch: 3 [151040/209539 (72%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.1456\tClassification Loss: 1.8421\r\n",
      "Train Epoch: 3 [151680/209539 (72%)]\tAll Loss: 1.9863\tTriple Loss(1): 0.1311\tClassification Loss: 1.7241\r\n",
      "Train Epoch: 3 [152320/209539 (73%)]\tAll Loss: 2.5521\tTriple Loss(1): 0.2623\tClassification Loss: 2.0276\r\n",
      "Train Epoch: 3 [152960/209539 (73%)]\tAll Loss: 2.4471\tTriple Loss(1): 0.2151\tClassification Loss: 2.0168\r\n",
      "Train Epoch: 3 [153600/209539 (73%)]\tAll Loss: 2.5203\tTriple Loss(1): 0.3523\tClassification Loss: 1.8157\r\n",
      "Train Epoch: 3 [154240/209539 (74%)]\tAll Loss: 2.2858\tTriple Loss(1): 0.2503\tClassification Loss: 1.7852\r\n",
      "Train Epoch: 3 [154880/209539 (74%)]\tAll Loss: 2.0625\tTriple Loss(1): 0.2065\tClassification Loss: 1.6495\r\n",
      "Train Epoch: 3 [155520/209539 (74%)]\tAll Loss: 2.3361\tTriple Loss(1): 0.3248\tClassification Loss: 1.6865\r\n",
      "Train Epoch: 3 [156160/209539 (75%)]\tAll Loss: 2.6230\tTriple Loss(1): 0.2112\tClassification Loss: 2.2007\r\n",
      "Train Epoch: 3 [156800/209539 (75%)]\tAll Loss: 3.3544\tTriple Loss(0): 0.7362\tClassification Loss: 1.8820\r\n",
      "Train Epoch: 3 [157440/209539 (75%)]\tAll Loss: 2.1799\tTriple Loss(1): 0.1547\tClassification Loss: 1.8704\r\n",
      "Train Epoch: 3 [158080/209539 (75%)]\tAll Loss: 2.3244\tTriple Loss(1): 0.3251\tClassification Loss: 1.6743\r\n",
      "Train Epoch: 3 [158720/209539 (76%)]\tAll Loss: 1.9695\tTriple Loss(1): 0.1099\tClassification Loss: 1.7496\r\n",
      "Train Epoch: 3 [159360/209539 (76%)]\tAll Loss: 2.4389\tTriple Loss(1): 0.2746\tClassification Loss: 1.8897\r\n",
      "Train Epoch: 3 [160000/209539 (76%)]\tAll Loss: 3.4421\tTriple Loss(0): 0.7289\tClassification Loss: 1.9843\r\n",
      "Train Epoch: 3 [160640/209539 (77%)]\tAll Loss: 2.9000\tTriple Loss(0): 0.6316\tClassification Loss: 1.6369\r\n",
      "Train Epoch: 3 [161280/209539 (77%)]\tAll Loss: 2.0088\tTriple Loss(1): 0.1538\tClassification Loss: 1.7012\r\n",
      "Train Epoch: 3 [161920/209539 (77%)]\tAll Loss: 3.2774\tTriple Loss(0): 0.8630\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 3 [162560/209539 (78%)]\tAll Loss: 1.6605\tTriple Loss(1): 0.0798\tClassification Loss: 1.5009\r\n",
      "Train Epoch: 3 [163200/209539 (78%)]\tAll Loss: 3.0911\tTriple Loss(0): 0.7362\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 3 [163840/209539 (78%)]\tAll Loss: 2.2253\tTriple Loss(1): 0.2576\tClassification Loss: 1.7100\r\n",
      "Train Epoch: 3 [164480/209539 (78%)]\tAll Loss: 3.2997\tTriple Loss(0): 0.7357\tClassification Loss: 1.8283\r\n",
      "Train Epoch: 3 [165120/209539 (79%)]\tAll Loss: 2.1930\tTriple Loss(1): 0.2685\tClassification Loss: 1.6559\r\n",
      "Train Epoch: 3 [165760/209539 (79%)]\tAll Loss: 2.4940\tTriple Loss(1): 0.2296\tClassification Loss: 2.0348\r\n",
      "Train Epoch: 3 [166400/209539 (79%)]\tAll Loss: 2.2653\tTriple Loss(1): 0.2358\tClassification Loss: 1.7936\r\n",
      "Train Epoch: 3 [167040/209539 (80%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.2589\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 3 [167680/209539 (80%)]\tAll Loss: 2.0719\tTriple Loss(1): 0.1937\tClassification Loss: 1.6846\r\n",
      "Train Epoch: 3 [168320/209539 (80%)]\tAll Loss: 2.2332\tTriple Loss(1): 0.1791\tClassification Loss: 1.8750\r\n",
      "Train Epoch: 3 [168960/209539 (81%)]\tAll Loss: 2.1365\tTriple Loss(1): 0.2061\tClassification Loss: 1.7242\r\n",
      "Train Epoch: 3 [169600/209539 (81%)]\tAll Loss: 2.3803\tTriple Loss(1): 0.1506\tClassification Loss: 2.0790\r\n",
      "Train Epoch: 3 [170240/209539 (81%)]\tAll Loss: 1.7899\tTriple Loss(1): 0.0909\tClassification Loss: 1.6081\r\n",
      "Train Epoch: 3 [170880/209539 (82%)]\tAll Loss: 2.0449\tTriple Loss(1): 0.1845\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 3 [171520/209539 (82%)]\tAll Loss: 2.2972\tTriple Loss(1): 0.2899\tClassification Loss: 1.7173\r\n",
      "Train Epoch: 3 [172160/209539 (82%)]\tAll Loss: 2.0224\tTriple Loss(1): 0.1632\tClassification Loss: 1.6961\r\n",
      "Train Epoch: 3 [172800/209539 (82%)]\tAll Loss: 3.2751\tTriple Loss(0): 0.5795\tClassification Loss: 2.1160\r\n",
      "Train Epoch: 3 [173440/209539 (83%)]\tAll Loss: 3.1765\tTriple Loss(0): 0.7179\tClassification Loss: 1.7408\r\n",
      "Train Epoch: 3 [174080/209539 (83%)]\tAll Loss: 2.0502\tTriple Loss(1): 0.1522\tClassification Loss: 1.7458\r\n",
      "Train Epoch: 3 [174720/209539 (83%)]\tAll Loss: 1.9015\tTriple Loss(1): 0.1547\tClassification Loss: 1.5921\r\n",
      "Train Epoch: 3 [175360/209539 (84%)]\tAll Loss: 2.4387\tTriple Loss(1): 0.2180\tClassification Loss: 2.0028\r\n",
      "Train Epoch: 3 [176000/209539 (84%)]\tAll Loss: 1.9278\tTriple Loss(1): 0.2002\tClassification Loss: 1.5275\r\n",
      "Train Epoch: 3 [176640/209539 (84%)]\tAll Loss: 2.2381\tTriple Loss(1): 0.2112\tClassification Loss: 1.8157\r\n",
      "Train Epoch: 3 [177280/209539 (85%)]\tAll Loss: 2.3098\tTriple Loss(1): 0.1903\tClassification Loss: 1.9293\r\n",
      "Train Epoch: 3 [177920/209539 (85%)]\tAll Loss: 2.1846\tTriple Loss(1): 0.2002\tClassification Loss: 1.7842\r\n",
      "Train Epoch: 3 [178560/209539 (85%)]\tAll Loss: 1.9964\tTriple Loss(1): 0.2213\tClassification Loss: 1.5537\r\n",
      "Train Epoch: 3 [179200/209539 (86%)]\tAll Loss: 2.0702\tTriple Loss(1): 0.2822\tClassification Loss: 1.5057\r\n",
      "Train Epoch: 3 [179840/209539 (86%)]\tAll Loss: 1.9589\tTriple Loss(1): 0.2603\tClassification Loss: 1.4383\r\n",
      "Train Epoch: 3 [180480/209539 (86%)]\tAll Loss: 2.0427\tTriple Loss(1): 0.2180\tClassification Loss: 1.6067\r\n",
      "Train Epoch: 3 [181120/209539 (86%)]\tAll Loss: 3.5326\tTriple Loss(0): 0.7866\tClassification Loss: 1.9594\r\n",
      "Train Epoch: 3 [181760/209539 (87%)]\tAll Loss: 2.2434\tTriple Loss(1): 0.2241\tClassification Loss: 1.7952\r\n",
      "Train Epoch: 3 [182400/209539 (87%)]\tAll Loss: 2.1841\tTriple Loss(1): 0.1930\tClassification Loss: 1.7981\r\n",
      "Train Epoch: 3 [183040/209539 (87%)]\tAll Loss: 2.2647\tTriple Loss(1): 0.1786\tClassification Loss: 1.9074\r\n",
      "Train Epoch: 3 [183680/209539 (88%)]\tAll Loss: 2.3331\tTriple Loss(1): 0.3225\tClassification Loss: 1.6882\r\n",
      "Train Epoch: 3 [184320/209539 (88%)]\tAll Loss: 2.3923\tTriple Loss(1): 0.2931\tClassification Loss: 1.8061\r\n",
      "Train Epoch: 3 [184960/209539 (88%)]\tAll Loss: 2.0525\tTriple Loss(1): 0.1497\tClassification Loss: 1.7530\r\n",
      "Train Epoch: 3 [185600/209539 (89%)]\tAll Loss: 2.1163\tTriple Loss(1): 0.1496\tClassification Loss: 1.8171\r\n",
      "Train Epoch: 3 [186240/209539 (89%)]\tAll Loss: 2.5171\tTriple Loss(1): 0.3085\tClassification Loss: 1.9002\r\n",
      "Train Epoch: 3 [186880/209539 (89%)]\tAll Loss: 2.1554\tTriple Loss(1): 0.2350\tClassification Loss: 1.6854\r\n",
      "Train Epoch: 3 [187520/209539 (89%)]\tAll Loss: 2.2016\tTriple Loss(1): 0.2011\tClassification Loss: 1.7994\r\n",
      "Train Epoch: 3 [188160/209539 (90%)]\tAll Loss: 2.3732\tTriple Loss(1): 0.2630\tClassification Loss: 1.8472\r\n",
      "Train Epoch: 3 [188800/209539 (90%)]\tAll Loss: 2.3778\tTriple Loss(1): 0.2639\tClassification Loss: 1.8500\r\n",
      "Train Epoch: 3 [189440/209539 (90%)]\tAll Loss: 2.2091\tTriple Loss(1): 0.1417\tClassification Loss: 1.9256\r\n",
      "Train Epoch: 3 [190080/209539 (91%)]\tAll Loss: 2.0581\tTriple Loss(1): 0.2005\tClassification Loss: 1.6572\r\n",
      "Train Epoch: 3 [190720/209539 (91%)]\tAll Loss: 2.0581\tTriple Loss(1): 0.1058\tClassification Loss: 1.8464\r\n",
      "Train Epoch: 3 [191360/209539 (91%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.1727\tClassification Loss: 1.6918\r\n",
      "Train Epoch: 3 [192000/209539 (92%)]\tAll Loss: 2.1430\tTriple Loss(1): 0.0926\tClassification Loss: 1.9578\r\n",
      "Train Epoch: 3 [192640/209539 (92%)]\tAll Loss: 2.4797\tTriple Loss(1): 0.2987\tClassification Loss: 1.8823\r\n",
      "Train Epoch: 3 [193280/209539 (92%)]\tAll Loss: 2.8421\tTriple Loss(0): 0.5420\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 3 [193920/209539 (93%)]\tAll Loss: 1.8752\tTriple Loss(1): 0.1262\tClassification Loss: 1.6229\r\n",
      "Train Epoch: 3 [194560/209539 (93%)]\tAll Loss: 2.9274\tTriple Loss(0): 0.5969\tClassification Loss: 1.7336\r\n",
      "Train Epoch: 3 [195200/209539 (93%)]\tAll Loss: 1.9910\tTriple Loss(1): 0.1881\tClassification Loss: 1.6148\r\n",
      "Train Epoch: 3 [195840/209539 (93%)]\tAll Loss: 1.7770\tTriple Loss(1): 0.1426\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 3 [196480/209539 (94%)]\tAll Loss: 2.1294\tTriple Loss(1): 0.1481\tClassification Loss: 1.8332\r\n",
      "Train Epoch: 3 [197120/209539 (94%)]\tAll Loss: 2.4234\tTriple Loss(1): 0.2711\tClassification Loss: 1.8812\r\n",
      "Train Epoch: 3 [197760/209539 (94%)]\tAll Loss: 2.8148\tTriple Loss(0): 0.6005\tClassification Loss: 1.6138\r\n",
      "Train Epoch: 3 [198400/209539 (95%)]\tAll Loss: 2.0001\tTriple Loss(1): 0.2819\tClassification Loss: 1.4363\r\n",
      "Train Epoch: 3 [199040/209539 (95%)]\tAll Loss: 2.4302\tTriple Loss(1): 0.3143\tClassification Loss: 1.8017\r\n",
      "Train Epoch: 3 [199680/209539 (95%)]\tAll Loss: 2.2814\tTriple Loss(1): 0.2277\tClassification Loss: 1.8259\r\n",
      "Train Epoch: 3 [200320/209539 (96%)]\tAll Loss: 2.1804\tTriple Loss(1): 0.1516\tClassification Loss: 1.8772\r\n",
      "Train Epoch: 3 [200960/209539 (96%)]\tAll Loss: 2.2518\tTriple Loss(1): 0.2389\tClassification Loss: 1.7741\r\n",
      "Train Epoch: 3 [201600/209539 (96%)]\tAll Loss: 2.1640\tTriple Loss(1): 0.1068\tClassification Loss: 1.9504\r\n",
      "Train Epoch: 3 [202240/209539 (97%)]\tAll Loss: 2.2269\tTriple Loss(1): 0.2872\tClassification Loss: 1.6525\r\n",
      "Train Epoch: 3 [202880/209539 (97%)]\tAll Loss: 2.0255\tTriple Loss(1): 0.2698\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 3 [203520/209539 (97%)]\tAll Loss: 3.0467\tTriple Loss(0): 0.5582\tClassification Loss: 1.9303\r\n",
      "Train Epoch: 3 [204160/209539 (97%)]\tAll Loss: 2.2073\tTriple Loss(1): 0.1153\tClassification Loss: 1.9767\r\n",
      "Train Epoch: 3 [204800/209539 (98%)]\tAll Loss: 2.0218\tTriple Loss(1): 0.2522\tClassification Loss: 1.5175\r\n",
      "Train Epoch: 3 [205440/209539 (98%)]\tAll Loss: 1.8333\tTriple Loss(1): 0.1499\tClassification Loss: 1.5336\r\n",
      "Train Epoch: 3 [206080/209539 (98%)]\tAll Loss: 2.1602\tTriple Loss(1): 0.1570\tClassification Loss: 1.8462\r\n",
      "Train Epoch: 3 [206720/209539 (99%)]\tAll Loss: 2.3029\tTriple Loss(1): 0.1919\tClassification Loss: 1.9191\r\n",
      "Train Epoch: 3 [207360/209539 (99%)]\tAll Loss: 2.0188\tTriple Loss(1): 0.2423\tClassification Loss: 1.5341\r\n",
      "Train Epoch: 3 [208000/209539 (99%)]\tAll Loss: 3.4278\tTriple Loss(0): 0.7851\tClassification Loss: 1.8575\r\n",
      "Train Epoch: 3 [208640/209539 (100%)]\tAll Loss: 3.5818\tTriple Loss(0): 0.8531\tClassification Loss: 1.8756\r\n",
      "Train Epoch: 3 [209280/209539 (100%)]\tAll Loss: 2.2223\tTriple Loss(1): 0.2333\tClassification Loss: 1.7557\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/3_epochs\r\n",
      "Train Epoch: 4 [0/209539 (0%)]\tAll Loss: 2.9485\tTriple Loss(1): 0.3920\tClassification Loss: 2.1646\r\n",
      "\r\n",
      "Test set: Average loss: 1.6476, Accuracy: 42001/80128 (52%)\r\n",
      "\r\n",
      "Train Epoch: 4 [640/209539 (0%)]\tAll Loss: 3.9037\tTriple Loss(0): 0.9326\tClassification Loss: 2.0386\r\n",
      "Train Epoch: 4 [1280/209539 (1%)]\tAll Loss: 2.8068\tTriple Loss(1): 0.2526\tClassification Loss: 2.3015\r\n",
      "Train Epoch: 4 [1920/209539 (1%)]\tAll Loss: 2.2309\tTriple Loss(1): 0.2347\tClassification Loss: 1.7614\r\n",
      "Train Epoch: 4 [2560/209539 (1%)]\tAll Loss: 2.1754\tTriple Loss(1): 0.2150\tClassification Loss: 1.7455\r\n",
      "Train Epoch: 4 [3200/209539 (2%)]\tAll Loss: 2.3395\tTriple Loss(1): 0.1981\tClassification Loss: 1.9432\r\n",
      "Train Epoch: 4 [3840/209539 (2%)]\tAll Loss: 2.4824\tTriple Loss(1): 0.2971\tClassification Loss: 1.8882\r\n",
      "Train Epoch: 4 [4480/209539 (2%)]\tAll Loss: 2.5817\tTriple Loss(1): 0.3609\tClassification Loss: 1.8599\r\n",
      "Train Epoch: 4 [5120/209539 (2%)]\tAll Loss: 2.4806\tTriple Loss(1): 0.3143\tClassification Loss: 1.8520\r\n",
      "Train Epoch: 4 [5760/209539 (3%)]\tAll Loss: 1.8031\tTriple Loss(1): 0.1019\tClassification Loss: 1.5993\r\n",
      "Train Epoch: 4 [6400/209539 (3%)]\tAll Loss: 2.1932\tTriple Loss(1): 0.1497\tClassification Loss: 1.8937\r\n",
      "Train Epoch: 4 [7040/209539 (3%)]\tAll Loss: 3.4017\tTriple Loss(0): 0.7091\tClassification Loss: 1.9835\r\n",
      "Train Epoch: 4 [7680/209539 (4%)]\tAll Loss: 3.5240\tTriple Loss(0): 0.8987\tClassification Loss: 1.7266\r\n",
      "Train Epoch: 4 [8320/209539 (4%)]\tAll Loss: 2.1531\tTriple Loss(1): 0.1451\tClassification Loss: 1.8630\r\n",
      "Train Epoch: 4 [8960/209539 (4%)]\tAll Loss: 2.4463\tTriple Loss(1): 0.2433\tClassification Loss: 1.9597\r\n",
      "Train Epoch: 4 [9600/209539 (5%)]\tAll Loss: 2.0814\tTriple Loss(1): 0.1401\tClassification Loss: 1.8012\r\n",
      "Train Epoch: 4 [10240/209539 (5%)]\tAll Loss: 2.5400\tTriple Loss(1): 0.2593\tClassification Loss: 2.0214\r\n",
      "Train Epoch: 4 [10880/209539 (5%)]\tAll Loss: 2.1733\tTriple Loss(1): 0.1571\tClassification Loss: 1.8590\r\n",
      "Train Epoch: 4 [11520/209539 (5%)]\tAll Loss: 2.2327\tTriple Loss(1): 0.1672\tClassification Loss: 1.8983\r\n",
      "Train Epoch: 4 [12160/209539 (6%)]\tAll Loss: 2.1011\tTriple Loss(1): 0.0937\tClassification Loss: 1.9137\r\n",
      "Train Epoch: 4 [12800/209539 (6%)]\tAll Loss: 2.8671\tTriple Loss(0): 0.5384\tClassification Loss: 1.7903\r\n",
      "Train Epoch: 4 [13440/209539 (6%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.2315\tClassification Loss: 1.7822\r\n",
      "Train Epoch: 4 [14080/209539 (7%)]\tAll Loss: 2.3098\tTriple Loss(1): 0.1434\tClassification Loss: 2.0230\r\n",
      "Train Epoch: 4 [14720/209539 (7%)]\tAll Loss: 2.3556\tTriple Loss(1): 0.1476\tClassification Loss: 2.0604\r\n",
      "Train Epoch: 4 [15360/209539 (7%)]\tAll Loss: 2.1866\tTriple Loss(1): 0.2239\tClassification Loss: 1.7388\r\n",
      "Train Epoch: 4 [16000/209539 (8%)]\tAll Loss: 3.0981\tTriple Loss(0): 0.6870\tClassification Loss: 1.7240\r\n",
      "Train Epoch: 4 [16640/209539 (8%)]\tAll Loss: 3.0826\tTriple Loss(0): 0.6808\tClassification Loss: 1.7210\r\n",
      "Train Epoch: 4 [17280/209539 (8%)]\tAll Loss: 2.1367\tTriple Loss(1): 0.1914\tClassification Loss: 1.7539\r\n",
      "Train Epoch: 4 [17920/209539 (9%)]\tAll Loss: 2.1006\tTriple Loss(1): 0.1952\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 4 [18560/209539 (9%)]\tAll Loss: 2.3875\tTriple Loss(1): 0.1782\tClassification Loss: 2.0311\r\n",
      "Train Epoch: 4 [19200/209539 (9%)]\tAll Loss: 2.0922\tTriple Loss(1): 0.1235\tClassification Loss: 1.8452\r\n",
      "Train Epoch: 4 [19840/209539 (9%)]\tAll Loss: 2.0939\tTriple Loss(1): 0.1660\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 4 [20480/209539 (10%)]\tAll Loss: 2.4675\tTriple Loss(1): 0.1934\tClassification Loss: 2.0807\r\n",
      "Train Epoch: 4 [21120/209539 (10%)]\tAll Loss: 2.1166\tTriple Loss(1): 0.2076\tClassification Loss: 1.7015\r\n",
      "Train Epoch: 4 [21760/209539 (10%)]\tAll Loss: 3.4372\tTriple Loss(0): 0.7371\tClassification Loss: 1.9630\r\n",
      "Train Epoch: 4 [22400/209539 (11%)]\tAll Loss: 1.7980\tTriple Loss(1): 0.1350\tClassification Loss: 1.5279\r\n",
      "Train Epoch: 4 [23040/209539 (11%)]\tAll Loss: 1.9970\tTriple Loss(1): 0.0913\tClassification Loss: 1.8143\r\n",
      "Train Epoch: 4 [23680/209539 (11%)]\tAll Loss: 2.2222\tTriple Loss(1): 0.2780\tClassification Loss: 1.6662\r\n",
      "Train Epoch: 4 [24320/209539 (12%)]\tAll Loss: 1.8358\tTriple Loss(1): 0.1490\tClassification Loss: 1.5378\r\n",
      "Train Epoch: 4 [24960/209539 (12%)]\tAll Loss: 2.8063\tTriple Loss(0): 0.5400\tClassification Loss: 1.7264\r\n",
      "Train Epoch: 4 [25600/209539 (12%)]\tAll Loss: 1.9913\tTriple Loss(1): 0.2012\tClassification Loss: 1.5888\r\n",
      "Train Epoch: 4 [26240/209539 (13%)]\tAll Loss: 2.2675\tTriple Loss(1): 0.1850\tClassification Loss: 1.8975\r\n",
      "Train Epoch: 4 [26880/209539 (13%)]\tAll Loss: 2.0924\tTriple Loss(1): 0.0956\tClassification Loss: 1.9011\r\n",
      "Train Epoch: 4 [27520/209539 (13%)]\tAll Loss: 3.2667\tTriple Loss(0): 0.7832\tClassification Loss: 1.7003\r\n",
      "Train Epoch: 4 [28160/209539 (13%)]\tAll Loss: 2.0007\tTriple Loss(1): 0.1394\tClassification Loss: 1.7220\r\n",
      "Train Epoch: 4 [28800/209539 (14%)]\tAll Loss: 2.2572\tTriple Loss(1): 0.1991\tClassification Loss: 1.8590\r\n",
      "Train Epoch: 4 [29440/209539 (14%)]\tAll Loss: 2.1561\tTriple Loss(1): 0.1300\tClassification Loss: 1.8962\r\n",
      "Train Epoch: 4 [30080/209539 (14%)]\tAll Loss: 2.1263\tTriple Loss(1): 0.1994\tClassification Loss: 1.7275\r\n",
      "Train Epoch: 4 [30720/209539 (15%)]\tAll Loss: 3.6595\tTriple Loss(0): 0.8460\tClassification Loss: 1.9675\r\n",
      "Train Epoch: 4 [31360/209539 (15%)]\tAll Loss: 2.7519\tTriple Loss(1): 0.2293\tClassification Loss: 2.2934\r\n",
      "Train Epoch: 4 [32000/209539 (15%)]\tAll Loss: 2.4932\tTriple Loss(1): 0.2502\tClassification Loss: 1.9928\r\n",
      "Train Epoch: 4 [32640/209539 (16%)]\tAll Loss: 3.4377\tTriple Loss(0): 0.6059\tClassification Loss: 2.2260\r\n",
      "Train Epoch: 4 [33280/209539 (16%)]\tAll Loss: 1.9048\tTriple Loss(1): 0.0612\tClassification Loss: 1.7824\r\n",
      "Train Epoch: 4 [33920/209539 (16%)]\tAll Loss: 2.4784\tTriple Loss(1): 0.2488\tClassification Loss: 1.9808\r\n",
      "Train Epoch: 4 [34560/209539 (16%)]\tAll Loss: 2.3403\tTriple Loss(1): 0.2388\tClassification Loss: 1.8628\r\n",
      "Train Epoch: 4 [35200/209539 (17%)]\tAll Loss: 2.2816\tTriple Loss(1): 0.1251\tClassification Loss: 2.0314\r\n",
      "Train Epoch: 4 [35840/209539 (17%)]\tAll Loss: 2.2653\tTriple Loss(1): 0.1748\tClassification Loss: 1.9157\r\n",
      "Train Epoch: 4 [36480/209539 (17%)]\tAll Loss: 2.3349\tTriple Loss(1): 0.2315\tClassification Loss: 1.8720\r\n",
      "Train Epoch: 4 [37120/209539 (18%)]\tAll Loss: 2.1187\tTriple Loss(1): 0.0735\tClassification Loss: 1.9717\r\n",
      "Train Epoch: 4 [37760/209539 (18%)]\tAll Loss: 3.0293\tTriple Loss(0): 0.6230\tClassification Loss: 1.7833\r\n",
      "Train Epoch: 4 [38400/209539 (18%)]\tAll Loss: 2.0035\tTriple Loss(1): 0.2817\tClassification Loss: 1.4401\r\n",
      "Train Epoch: 4 [39040/209539 (19%)]\tAll Loss: 2.1870\tTriple Loss(1): 0.1747\tClassification Loss: 1.8376\r\n",
      "Train Epoch: 4 [39680/209539 (19%)]\tAll Loss: 2.3522\tTriple Loss(1): 0.3059\tClassification Loss: 1.7404\r\n",
      "Train Epoch: 4 [40320/209539 (19%)]\tAll Loss: 3.3073\tTriple Loss(0): 0.5267\tClassification Loss: 2.2539\r\n",
      "Train Epoch: 4 [40960/209539 (20%)]\tAll Loss: 2.0791\tTriple Loss(1): 0.1191\tClassification Loss: 1.8408\r\n",
      "Train Epoch: 4 [41600/209539 (20%)]\tAll Loss: 2.9177\tTriple Loss(1): 0.3952\tClassification Loss: 2.1273\r\n",
      "Train Epoch: 4 [42240/209539 (20%)]\tAll Loss: 2.0347\tTriple Loss(1): 0.1328\tClassification Loss: 1.7691\r\n",
      "Train Epoch: 4 [42880/209539 (20%)]\tAll Loss: 2.2578\tTriple Loss(1): 0.2726\tClassification Loss: 1.7127\r\n",
      "Train Epoch: 4 [43520/209539 (21%)]\tAll Loss: 2.6495\tTriple Loss(1): 0.2619\tClassification Loss: 2.1258\r\n",
      "Train Epoch: 4 [44160/209539 (21%)]\tAll Loss: 2.0741\tTriple Loss(1): 0.1630\tClassification Loss: 1.7481\r\n",
      "Train Epoch: 4 [44800/209539 (21%)]\tAll Loss: 2.4015\tTriple Loss(1): 0.2320\tClassification Loss: 1.9374\r\n",
      "Train Epoch: 4 [45440/209539 (22%)]\tAll Loss: 2.0775\tTriple Loss(1): 0.2586\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 4 [46080/209539 (22%)]\tAll Loss: 1.9996\tTriple Loss(1): 0.0677\tClassification Loss: 1.8641\r\n",
      "Train Epoch: 4 [46720/209539 (22%)]\tAll Loss: 2.3252\tTriple Loss(1): 0.2958\tClassification Loss: 1.7337\r\n",
      "Train Epoch: 4 [47360/209539 (23%)]\tAll Loss: 2.2240\tTriple Loss(1): 0.1374\tClassification Loss: 1.9492\r\n",
      "Train Epoch: 4 [48000/209539 (23%)]\tAll Loss: 2.5284\tTriple Loss(1): 0.1530\tClassification Loss: 2.2225\r\n",
      "Train Epoch: 4 [48640/209539 (23%)]\tAll Loss: 2.1256\tTriple Loss(1): 0.1912\tClassification Loss: 1.7433\r\n",
      "Train Epoch: 4 [49280/209539 (24%)]\tAll Loss: 1.9424\tTriple Loss(1): 0.0891\tClassification Loss: 1.7643\r\n",
      "Train Epoch: 4 [49920/209539 (24%)]\tAll Loss: 2.4018\tTriple Loss(1): 0.2536\tClassification Loss: 1.8946\r\n",
      "Train Epoch: 4 [50560/209539 (24%)]\tAll Loss: 2.0512\tTriple Loss(1): 0.2364\tClassification Loss: 1.5783\r\n",
      "Train Epoch: 4 [51200/209539 (24%)]\tAll Loss: 3.2928\tTriple Loss(0): 0.7207\tClassification Loss: 1.8514\r\n",
      "Train Epoch: 4 [51840/209539 (25%)]\tAll Loss: 2.0725\tTriple Loss(1): 0.2177\tClassification Loss: 1.6371\r\n",
      "Train Epoch: 4 [52480/209539 (25%)]\tAll Loss: 3.7553\tTriple Loss(0): 0.8985\tClassification Loss: 1.9582\r\n",
      "Train Epoch: 4 [53120/209539 (25%)]\tAll Loss: 2.0867\tTriple Loss(1): 0.2385\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 4 [53760/209539 (26%)]\tAll Loss: 2.1068\tTriple Loss(1): 0.0884\tClassification Loss: 1.9300\r\n",
      "Train Epoch: 4 [54400/209539 (26%)]\tAll Loss: 2.4882\tTriple Loss(1): 0.2500\tClassification Loss: 1.9881\r\n",
      "Train Epoch: 4 [55040/209539 (26%)]\tAll Loss: 1.9618\tTriple Loss(1): 0.2240\tClassification Loss: 1.5138\r\n",
      "Train Epoch: 4 [55680/209539 (27%)]\tAll Loss: 3.0412\tTriple Loss(0): 0.6089\tClassification Loss: 1.8235\r\n",
      "Train Epoch: 4 [56320/209539 (27%)]\tAll Loss: 2.4755\tTriple Loss(1): 0.3136\tClassification Loss: 1.8482\r\n",
      "Train Epoch: 4 [56960/209539 (27%)]\tAll Loss: 3.0751\tTriple Loss(0): 0.5436\tClassification Loss: 1.9879\r\n",
      "Train Epoch: 4 [57600/209539 (27%)]\tAll Loss: 2.5455\tTriple Loss(1): 0.3462\tClassification Loss: 1.8531\r\n",
      "Train Epoch: 4 [58240/209539 (28%)]\tAll Loss: 3.0175\tTriple Loss(0): 0.6025\tClassification Loss: 1.8125\r\n",
      "Train Epoch: 4 [58880/209539 (28%)]\tAll Loss: 2.1849\tTriple Loss(1): 0.2081\tClassification Loss: 1.7687\r\n",
      "Train Epoch: 4 [59520/209539 (28%)]\tAll Loss: 2.3982\tTriple Loss(1): 0.2802\tClassification Loss: 1.8378\r\n",
      "Train Epoch: 4 [60160/209539 (29%)]\tAll Loss: 2.5915\tTriple Loss(0): 0.4472\tClassification Loss: 1.6971\r\n",
      "Train Epoch: 4 [60800/209539 (29%)]\tAll Loss: 2.1213\tTriple Loss(1): 0.1876\tClassification Loss: 1.7461\r\n",
      "Train Epoch: 4 [61440/209539 (29%)]\tAll Loss: 2.3496\tTriple Loss(1): 0.2557\tClassification Loss: 1.8383\r\n",
      "Train Epoch: 4 [62080/209539 (30%)]\tAll Loss: 2.4026\tTriple Loss(1): 0.2445\tClassification Loss: 1.9135\r\n",
      "Train Epoch: 4 [62720/209539 (30%)]\tAll Loss: 2.5589\tTriple Loss(1): 0.2679\tClassification Loss: 2.0230\r\n",
      "Train Epoch: 4 [63360/209539 (30%)]\tAll Loss: 2.2646\tTriple Loss(1): 0.2010\tClassification Loss: 1.8626\r\n",
      "Train Epoch: 4 [64000/209539 (31%)]\tAll Loss: 2.5012\tTriple Loss(1): 0.2617\tClassification Loss: 1.9777\r\n",
      "Train Epoch: 4 [64640/209539 (31%)]\tAll Loss: 3.3115\tTriple Loss(0): 0.5839\tClassification Loss: 2.1437\r\n",
      "Train Epoch: 4 [65280/209539 (31%)]\tAll Loss: 2.4599\tTriple Loss(1): 0.2562\tClassification Loss: 1.9475\r\n",
      "Train Epoch: 4 [65920/209539 (31%)]\tAll Loss: 2.4834\tTriple Loss(1): 0.3137\tClassification Loss: 1.8560\r\n",
      "Train Epoch: 4 [66560/209539 (32%)]\tAll Loss: 2.4297\tTriple Loss(1): 0.3071\tClassification Loss: 1.8156\r\n",
      "Train Epoch: 4 [67200/209539 (32%)]\tAll Loss: 2.3388\tTriple Loss(1): 0.1248\tClassification Loss: 2.0893\r\n",
      "Train Epoch: 4 [67840/209539 (32%)]\tAll Loss: 1.8805\tTriple Loss(1): 0.1657\tClassification Loss: 1.5491\r\n",
      "Train Epoch: 4 [68480/209539 (33%)]\tAll Loss: 2.2316\tTriple Loss(1): 0.2134\tClassification Loss: 1.8048\r\n",
      "Train Epoch: 4 [69120/209539 (33%)]\tAll Loss: 2.0084\tTriple Loss(1): 0.2430\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 4 [69760/209539 (33%)]\tAll Loss: 3.3453\tTriple Loss(0): 0.6907\tClassification Loss: 1.9639\r\n",
      "Train Epoch: 4 [70400/209539 (34%)]\tAll Loss: 1.9887\tTriple Loss(1): 0.1447\tClassification Loss: 1.6994\r\n",
      "Train Epoch: 4 [71040/209539 (34%)]\tAll Loss: 2.3026\tTriple Loss(1): 0.2774\tClassification Loss: 1.7478\r\n",
      "Train Epoch: 4 [71680/209539 (34%)]\tAll Loss: 2.2579\tTriple Loss(1): 0.2166\tClassification Loss: 1.8247\r\n",
      "Train Epoch: 4 [72320/209539 (35%)]\tAll Loss: 2.2800\tTriple Loss(1): 0.1973\tClassification Loss: 1.8853\r\n",
      "Train Epoch: 4 [72960/209539 (35%)]\tAll Loss: 2.0396\tTriple Loss(1): 0.1357\tClassification Loss: 1.7682\r\n",
      "Train Epoch: 4 [73600/209539 (35%)]\tAll Loss: 2.2382\tTriple Loss(1): 0.2319\tClassification Loss: 1.7745\r\n",
      "Train Epoch: 4 [74240/209539 (35%)]\tAll Loss: 2.0536\tTriple Loss(1): 0.0990\tClassification Loss: 1.8556\r\n",
      "Train Epoch: 4 [74880/209539 (36%)]\tAll Loss: 1.8609\tTriple Loss(1): 0.0741\tClassification Loss: 1.7127\r\n",
      "Train Epoch: 4 [75520/209539 (36%)]\tAll Loss: 2.0295\tTriple Loss(1): 0.0862\tClassification Loss: 1.8571\r\n",
      "Train Epoch: 4 [76160/209539 (36%)]\tAll Loss: 3.4322\tTriple Loss(0): 0.7383\tClassification Loss: 1.9557\r\n",
      "Train Epoch: 4 [76800/209539 (37%)]\tAll Loss: 2.3274\tTriple Loss(1): 0.1741\tClassification Loss: 1.9791\r\n",
      "Train Epoch: 4 [77440/209539 (37%)]\tAll Loss: 2.1272\tTriple Loss(1): 0.1782\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 4 [78080/209539 (37%)]\tAll Loss: 3.3470\tTriple Loss(0): 0.7776\tClassification Loss: 1.7918\r\n",
      "Train Epoch: 4 [78720/209539 (38%)]\tAll Loss: 2.5889\tTriple Loss(1): 0.1303\tClassification Loss: 2.3284\r\n",
      "Train Epoch: 4 [79360/209539 (38%)]\tAll Loss: 2.7801\tTriple Loss(1): 0.2668\tClassification Loss: 2.2465\r\n",
      "Train Epoch: 4 [80000/209539 (38%)]\tAll Loss: 3.7870\tTriple Loss(0): 0.8470\tClassification Loss: 2.0930\r\n",
      "Train Epoch: 4 [80640/209539 (38%)]\tAll Loss: 2.0057\tTriple Loss(1): 0.2627\tClassification Loss: 1.4802\r\n",
      "Train Epoch: 4 [81280/209539 (39%)]\tAll Loss: 2.3010\tTriple Loss(1): 0.2411\tClassification Loss: 1.8189\r\n",
      "Train Epoch: 4 [81920/209539 (39%)]\tAll Loss: 2.2047\tTriple Loss(1): 0.1502\tClassification Loss: 1.9044\r\n",
      "Train Epoch: 4 [82560/209539 (39%)]\tAll Loss: 2.3091\tTriple Loss(1): 0.2036\tClassification Loss: 1.9018\r\n",
      "Train Epoch: 4 [83200/209539 (40%)]\tAll Loss: 2.8302\tTriple Loss(0): 0.5855\tClassification Loss: 1.6592\r\n",
      "Train Epoch: 4 [83840/209539 (40%)]\tAll Loss: 2.4043\tTriple Loss(1): 0.2411\tClassification Loss: 1.9221\r\n",
      "Train Epoch: 4 [84480/209539 (40%)]\tAll Loss: 2.3568\tTriple Loss(1): 0.2352\tClassification Loss: 1.8863\r\n",
      "Train Epoch: 4 [85120/209539 (41%)]\tAll Loss: 2.0797\tTriple Loss(1): 0.1956\tClassification Loss: 1.6885\r\n",
      "Train Epoch: 4 [85760/209539 (41%)]\tAll Loss: 2.6724\tTriple Loss(0): 0.4896\tClassification Loss: 1.6932\r\n",
      "Train Epoch: 4 [86400/209539 (41%)]\tAll Loss: 2.2564\tTriple Loss(1): 0.1468\tClassification Loss: 1.9628\r\n",
      "Train Epoch: 4 [87040/209539 (42%)]\tAll Loss: 2.0247\tTriple Loss(1): 0.1798\tClassification Loss: 1.6650\r\n",
      "Train Epoch: 4 [87680/209539 (42%)]\tAll Loss: 2.2459\tTriple Loss(1): 0.1840\tClassification Loss: 1.8779\r\n",
      "Train Epoch: 4 [88320/209539 (42%)]\tAll Loss: 2.3777\tTriple Loss(1): 0.2655\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 4 [88960/209539 (42%)]\tAll Loss: 2.2600\tTriple Loss(1): 0.2348\tClassification Loss: 1.7905\r\n",
      "Train Epoch: 4 [89600/209539 (43%)]\tAll Loss: 2.2017\tTriple Loss(1): 0.0922\tClassification Loss: 2.0174\r\n",
      "Train Epoch: 4 [90240/209539 (43%)]\tAll Loss: 2.1435\tTriple Loss(1): 0.1069\tClassification Loss: 1.9297\r\n",
      "Train Epoch: 4 [90880/209539 (43%)]\tAll Loss: 2.2107\tTriple Loss(1): 0.2204\tClassification Loss: 1.7699\r\n",
      "Train Epoch: 4 [91520/209539 (44%)]\tAll Loss: 2.3603\tTriple Loss(1): 0.2425\tClassification Loss: 1.8753\r\n",
      "Train Epoch: 4 [92160/209539 (44%)]\tAll Loss: 1.9447\tTriple Loss(1): 0.1141\tClassification Loss: 1.7165\r\n",
      "Train Epoch: 4 [92800/209539 (44%)]\tAll Loss: 2.5545\tTriple Loss(0): 0.6807\tClassification Loss: 1.1931\r\n",
      "Train Epoch: 4 [93440/209539 (45%)]\tAll Loss: 2.6186\tTriple Loss(1): 0.2449\tClassification Loss: 2.1288\r\n",
      "Train Epoch: 4 [94080/209539 (45%)]\tAll Loss: 1.9761\tTriple Loss(1): 0.2149\tClassification Loss: 1.5462\r\n",
      "Train Epoch: 4 [94720/209539 (45%)]\tAll Loss: 3.8533\tTriple Loss(0): 0.9056\tClassification Loss: 2.0421\r\n",
      "Train Epoch: 4 [95360/209539 (46%)]\tAll Loss: 2.3450\tTriple Loss(1): 0.1897\tClassification Loss: 1.9655\r\n",
      "Train Epoch: 4 [96000/209539 (46%)]\tAll Loss: 3.6334\tTriple Loss(0): 0.8374\tClassification Loss: 1.9586\r\n",
      "Train Epoch: 4 [96640/209539 (46%)]\tAll Loss: 2.2824\tTriple Loss(1): 0.1398\tClassification Loss: 2.0027\r\n",
      "Train Epoch: 4 [97280/209539 (46%)]\tAll Loss: 2.3403\tTriple Loss(1): 0.2544\tClassification Loss: 1.8316\r\n",
      "Train Epoch: 4 [97920/209539 (47%)]\tAll Loss: 2.0069\tTriple Loss(1): 0.1624\tClassification Loss: 1.6821\r\n",
      "Train Epoch: 4 [98560/209539 (47%)]\tAll Loss: 1.8106\tTriple Loss(1): 0.1456\tClassification Loss: 1.5193\r\n",
      "Train Epoch: 4 [99200/209539 (47%)]\tAll Loss: 2.1046\tTriple Loss(1): 0.1306\tClassification Loss: 1.8434\r\n",
      "Train Epoch: 4 [99840/209539 (48%)]\tAll Loss: 3.1973\tTriple Loss(0): 0.6650\tClassification Loss: 1.8672\r\n",
      "Train Epoch: 4 [100480/209539 (48%)]\tAll Loss: 2.1421\tTriple Loss(1): 0.0891\tClassification Loss: 1.9639\r\n",
      "Train Epoch: 4 [101120/209539 (48%)]\tAll Loss: 2.0878\tTriple Loss(1): 0.1392\tClassification Loss: 1.8095\r\n",
      "Train Epoch: 4 [101760/209539 (49%)]\tAll Loss: 2.2224\tTriple Loss(1): 0.2016\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 4 [102400/209539 (49%)]\tAll Loss: 2.0654\tTriple Loss(1): 0.2497\tClassification Loss: 1.5659\r\n",
      "Train Epoch: 4 [103040/209539 (49%)]\tAll Loss: 2.5752\tTriple Loss(1): 0.2028\tClassification Loss: 2.1696\r\n",
      "Train Epoch: 4 [103680/209539 (49%)]\tAll Loss: 2.5364\tTriple Loss(1): 0.2337\tClassification Loss: 2.0691\r\n",
      "Train Epoch: 4 [104320/209539 (50%)]\tAll Loss: 2.1379\tTriple Loss(1): 0.1811\tClassification Loss: 1.7757\r\n",
      "Train Epoch: 4 [104960/209539 (50%)]\tAll Loss: 1.9998\tTriple Loss(1): 0.1838\tClassification Loss: 1.6323\r\n",
      "Train Epoch: 4 [105600/209539 (50%)]\tAll Loss: 1.6203\tTriple Loss(1): 0.1416\tClassification Loss: 1.3372\r\n",
      "Train Epoch: 4 [106240/209539 (51%)]\tAll Loss: 2.1839\tTriple Loss(1): 0.1445\tClassification Loss: 1.8950\r\n",
      "Train Epoch: 4 [106880/209539 (51%)]\tAll Loss: 2.1482\tTriple Loss(1): 0.2314\tClassification Loss: 1.6853\r\n",
      "Train Epoch: 4 [107520/209539 (51%)]\tAll Loss: 2.1576\tTriple Loss(1): 0.1415\tClassification Loss: 1.8746\r\n",
      "Train Epoch: 4 [108160/209539 (52%)]\tAll Loss: 1.8227\tTriple Loss(1): 0.1836\tClassification Loss: 1.4554\r\n",
      "Train Epoch: 4 [108800/209539 (52%)]\tAll Loss: 2.5955\tTriple Loss(1): 0.2524\tClassification Loss: 2.0907\r\n",
      "Train Epoch: 4 [109440/209539 (52%)]\tAll Loss: 2.5483\tTriple Loss(1): 0.3103\tClassification Loss: 1.9278\r\n",
      "Train Epoch: 4 [110080/209539 (53%)]\tAll Loss: 2.2118\tTriple Loss(1): 0.2547\tClassification Loss: 1.7025\r\n",
      "Train Epoch: 4 [110720/209539 (53%)]\tAll Loss: 1.8328\tTriple Loss(1): 0.0593\tClassification Loss: 1.7141\r\n",
      "Train Epoch: 4 [111360/209539 (53%)]\tAll Loss: 1.7211\tTriple Loss(1): 0.0935\tClassification Loss: 1.5341\r\n",
      "Train Epoch: 4 [112000/209539 (53%)]\tAll Loss: 2.2657\tTriple Loss(1): 0.1736\tClassification Loss: 1.9185\r\n",
      "Train Epoch: 4 [112640/209539 (54%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.1163\tClassification Loss: 1.6211\r\n",
      "Train Epoch: 4 [113280/209539 (54%)]\tAll Loss: 1.9844\tTriple Loss(1): 0.1798\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 4 [113920/209539 (54%)]\tAll Loss: 2.1880\tTriple Loss(1): 0.1826\tClassification Loss: 1.8228\r\n",
      "Train Epoch: 4 [114560/209539 (55%)]\tAll Loss: 2.1462\tTriple Loss(1): 0.1438\tClassification Loss: 1.8585\r\n",
      "Train Epoch: 4 [115200/209539 (55%)]\tAll Loss: 1.9813\tTriple Loss(1): 0.1055\tClassification Loss: 1.7702\r\n",
      "Train Epoch: 4 [115840/209539 (55%)]\tAll Loss: 2.4677\tTriple Loss(1): 0.2589\tClassification Loss: 1.9499\r\n",
      "Train Epoch: 4 [116480/209539 (56%)]\tAll Loss: 1.9705\tTriple Loss(1): 0.1295\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 4 [117120/209539 (56%)]\tAll Loss: 2.2571\tTriple Loss(1): 0.1848\tClassification Loss: 1.8875\r\n",
      "Train Epoch: 4 [117760/209539 (56%)]\tAll Loss: 2.0402\tTriple Loss(1): 0.1892\tClassification Loss: 1.6617\r\n",
      "Train Epoch: 4 [118400/209539 (57%)]\tAll Loss: 1.8681\tTriple Loss(1): 0.1750\tClassification Loss: 1.5180\r\n",
      "Train Epoch: 4 [119040/209539 (57%)]\tAll Loss: 2.4656\tTriple Loss(1): 0.2691\tClassification Loss: 1.9275\r\n",
      "Train Epoch: 4 [119680/209539 (57%)]\tAll Loss: 2.0750\tTriple Loss(1): 0.2257\tClassification Loss: 1.6236\r\n",
      "Train Epoch: 4 [120320/209539 (57%)]\tAll Loss: 2.5156\tTriple Loss(1): 0.3178\tClassification Loss: 1.8801\r\n",
      "Train Epoch: 4 [120960/209539 (58%)]\tAll Loss: 2.0783\tTriple Loss(1): 0.2915\tClassification Loss: 1.4953\r\n",
      "Train Epoch: 4 [121600/209539 (58%)]\tAll Loss: 1.9935\tTriple Loss(1): 0.1452\tClassification Loss: 1.7032\r\n",
      "Train Epoch: 4 [122240/209539 (58%)]\tAll Loss: 3.2450\tTriple Loss(0): 0.6910\tClassification Loss: 1.8630\r\n",
      "Train Epoch: 4 [122880/209539 (59%)]\tAll Loss: 2.0980\tTriple Loss(1): 0.1504\tClassification Loss: 1.7972\r\n",
      "Train Epoch: 4 [123520/209539 (59%)]\tAll Loss: 2.2250\tTriple Loss(1): 0.2325\tClassification Loss: 1.7600\r\n",
      "Train Epoch: 4 [124160/209539 (59%)]\tAll Loss: 2.1363\tTriple Loss(1): 0.1519\tClassification Loss: 1.8325\r\n",
      "Train Epoch: 4 [124800/209539 (60%)]\tAll Loss: 2.0597\tTriple Loss(1): 0.2161\tClassification Loss: 1.6276\r\n",
      "Train Epoch: 4 [125440/209539 (60%)]\tAll Loss: 2.5111\tTriple Loss(1): 0.2558\tClassification Loss: 1.9995\r\n",
      "Train Epoch: 4 [126080/209539 (60%)]\tAll Loss: 1.9401\tTriple Loss(1): 0.0954\tClassification Loss: 1.7493\r\n",
      "Train Epoch: 4 [126720/209539 (60%)]\tAll Loss: 2.0565\tTriple Loss(1): 0.2313\tClassification Loss: 1.5938\r\n",
      "Train Epoch: 4 [127360/209539 (61%)]\tAll Loss: 2.2606\tTriple Loss(1): 0.1766\tClassification Loss: 1.9075\r\n",
      "Train Epoch: 4 [128000/209539 (61%)]\tAll Loss: 2.0515\tTriple Loss(1): 0.2080\tClassification Loss: 1.6355\r\n",
      "Train Epoch: 4 [128640/209539 (61%)]\tAll Loss: 3.2818\tTriple Loss(0): 0.7750\tClassification Loss: 1.7319\r\n",
      "Train Epoch: 4 [129280/209539 (62%)]\tAll Loss: 1.9811\tTriple Loss(1): 0.1366\tClassification Loss: 1.7079\r\n",
      "Train Epoch: 4 [129920/209539 (62%)]\tAll Loss: 2.2685\tTriple Loss(1): 0.1671\tClassification Loss: 1.9343\r\n",
      "Train Epoch: 4 [130560/209539 (62%)]\tAll Loss: 3.1276\tTriple Loss(0): 0.7409\tClassification Loss: 1.6458\r\n",
      "Train Epoch: 4 [131200/209539 (63%)]\tAll Loss: 2.0768\tTriple Loss(1): 0.0972\tClassification Loss: 1.8824\r\n",
      "Train Epoch: 4 [131840/209539 (63%)]\tAll Loss: 1.9112\tTriple Loss(1): 0.1504\tClassification Loss: 1.6104\r\n",
      "Train Epoch: 4 [132480/209539 (63%)]\tAll Loss: 2.1733\tTriple Loss(1): 0.1662\tClassification Loss: 1.8410\r\n",
      "Train Epoch: 4 [133120/209539 (64%)]\tAll Loss: 2.6236\tTriple Loss(0): 0.5381\tClassification Loss: 1.5473\r\n",
      "Train Epoch: 4 [133760/209539 (64%)]\tAll Loss: 2.8370\tTriple Loss(0): 0.6359\tClassification Loss: 1.5652\r\n",
      "Train Epoch: 4 [134400/209539 (64%)]\tAll Loss: 2.0751\tTriple Loss(1): 0.1362\tClassification Loss: 1.8027\r\n",
      "Train Epoch: 4 [135040/209539 (64%)]\tAll Loss: 2.3332\tTriple Loss(1): 0.2878\tClassification Loss: 1.7575\r\n",
      "Train Epoch: 4 [135680/209539 (65%)]\tAll Loss: 3.4367\tTriple Loss(0): 0.6468\tClassification Loss: 2.1431\r\n",
      "Train Epoch: 4 [136320/209539 (65%)]\tAll Loss: 2.9298\tTriple Loss(0): 0.5268\tClassification Loss: 1.8762\r\n",
      "Train Epoch: 4 [136960/209539 (65%)]\tAll Loss: 2.0194\tTriple Loss(1): 0.1135\tClassification Loss: 1.7923\r\n",
      "Train Epoch: 4 [137600/209539 (66%)]\tAll Loss: 2.0147\tTriple Loss(1): 0.1480\tClassification Loss: 1.7187\r\n",
      "Train Epoch: 4 [138240/209539 (66%)]\tAll Loss: 2.7477\tTriple Loss(0): 0.6011\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 4 [138880/209539 (66%)]\tAll Loss: 2.4504\tTriple Loss(1): 0.2651\tClassification Loss: 1.9203\r\n",
      "Train Epoch: 4 [139520/209539 (67%)]\tAll Loss: 3.0320\tTriple Loss(0): 0.5912\tClassification Loss: 1.8497\r\n",
      "Train Epoch: 4 [140160/209539 (67%)]\tAll Loss: 2.5416\tTriple Loss(1): 0.1967\tClassification Loss: 2.1482\r\n",
      "Train Epoch: 4 [140800/209539 (67%)]\tAll Loss: 2.0425\tTriple Loss(1): 0.1712\tClassification Loss: 1.7000\r\n",
      "Train Epoch: 4 [141440/209539 (68%)]\tAll Loss: 2.1161\tTriple Loss(1): 0.1501\tClassification Loss: 1.8159\r\n",
      "Train Epoch: 4 [142080/209539 (68%)]\tAll Loss: 2.1843\tTriple Loss(1): 0.2686\tClassification Loss: 1.6471\r\n",
      "Train Epoch: 4 [142720/209539 (68%)]\tAll Loss: 2.4982\tTriple Loss(1): 0.3398\tClassification Loss: 1.8186\r\n",
      "Train Epoch: 4 [143360/209539 (68%)]\tAll Loss: 2.1326\tTriple Loss(1): 0.3099\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 4 [144000/209539 (69%)]\tAll Loss: 3.2234\tTriple Loss(0): 0.6503\tClassification Loss: 1.9227\r\n",
      "Train Epoch: 4 [144640/209539 (69%)]\tAll Loss: 2.1810\tTriple Loss(1): 0.1571\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 4 [145280/209539 (69%)]\tAll Loss: 2.3446\tTriple Loss(1): 0.1575\tClassification Loss: 2.0295\r\n",
      "Train Epoch: 4 [145920/209539 (70%)]\tAll Loss: 2.1006\tTriple Loss(1): 0.1918\tClassification Loss: 1.7169\r\n",
      "Train Epoch: 4 [146560/209539 (70%)]\tAll Loss: 3.9821\tTriple Loss(0): 0.8514\tClassification Loss: 2.2793\r\n",
      "Train Epoch: 4 [147200/209539 (70%)]\tAll Loss: 2.1473\tTriple Loss(1): 0.0736\tClassification Loss: 2.0000\r\n",
      "Train Epoch: 4 [147840/209539 (71%)]\tAll Loss: 2.7327\tTriple Loss(0): 0.5520\tClassification Loss: 1.6288\r\n",
      "Train Epoch: 4 [148480/209539 (71%)]\tAll Loss: 2.0764\tTriple Loss(1): 0.1887\tClassification Loss: 1.6991\r\n",
      "Train Epoch: 4 [149120/209539 (71%)]\tAll Loss: 2.1297\tTriple Loss(1): 0.1263\tClassification Loss: 1.8771\r\n",
      "Train Epoch: 4 [149760/209539 (71%)]\tAll Loss: 3.4289\tTriple Loss(0): 0.7217\tClassification Loss: 1.9855\r\n",
      "Train Epoch: 4 [150400/209539 (72%)]\tAll Loss: 3.1499\tTriple Loss(0): 0.6410\tClassification Loss: 1.8679\r\n",
      "Train Epoch: 4 [151040/209539 (72%)]\tAll Loss: 2.2905\tTriple Loss(1): 0.2035\tClassification Loss: 1.8835\r\n",
      "Train Epoch: 4 [151680/209539 (72%)]\tAll Loss: 1.9552\tTriple Loss(1): 0.0118\tClassification Loss: 1.9316\r\n",
      "Train Epoch: 4 [152320/209539 (73%)]\tAll Loss: 2.2363\tTriple Loss(1): 0.1584\tClassification Loss: 1.9195\r\n",
      "Train Epoch: 4 [152960/209539 (73%)]\tAll Loss: 2.2949\tTriple Loss(1): 0.1373\tClassification Loss: 2.0204\r\n",
      "Train Epoch: 4 [153600/209539 (73%)]\tAll Loss: 2.2163\tTriple Loss(1): 0.2280\tClassification Loss: 1.7602\r\n",
      "Train Epoch: 4 [154240/209539 (74%)]\tAll Loss: 2.1418\tTriple Loss(1): 0.1906\tClassification Loss: 1.7606\r\n",
      "Train Epoch: 4 [154880/209539 (74%)]\tAll Loss: 1.9125\tTriple Loss(1): 0.1925\tClassification Loss: 1.5276\r\n",
      "Train Epoch: 4 [155520/209539 (74%)]\tAll Loss: 1.8673\tTriple Loss(1): 0.1507\tClassification Loss: 1.5659\r\n",
      "Train Epoch: 4 [156160/209539 (75%)]\tAll Loss: 2.4339\tTriple Loss(1): 0.1838\tClassification Loss: 2.0662\r\n",
      "Train Epoch: 4 [156800/209539 (75%)]\tAll Loss: 2.1958\tTriple Loss(1): 0.2079\tClassification Loss: 1.7800\r\n",
      "Train Epoch: 4 [157440/209539 (75%)]\tAll Loss: 2.2361\tTriple Loss(1): 0.1698\tClassification Loss: 1.8964\r\n",
      "Train Epoch: 4 [158080/209539 (75%)]\tAll Loss: 2.0135\tTriple Loss(1): 0.1396\tClassification Loss: 1.7343\r\n",
      "Train Epoch: 4 [158720/209539 (76%)]\tAll Loss: 2.0728\tTriple Loss(1): 0.2064\tClassification Loss: 1.6600\r\n",
      "Train Epoch: 4 [159360/209539 (76%)]\tAll Loss: 2.3964\tTriple Loss(1): 0.2499\tClassification Loss: 1.8965\r\n",
      "Train Epoch: 4 [160000/209539 (76%)]\tAll Loss: 2.3793\tTriple Loss(1): 0.2411\tClassification Loss: 1.8970\r\n",
      "Train Epoch: 4 [160640/209539 (77%)]\tAll Loss: 1.9906\tTriple Loss(1): 0.1707\tClassification Loss: 1.6492\r\n",
      "Train Epoch: 4 [161280/209539 (77%)]\tAll Loss: 2.0813\tTriple Loss(1): 0.2016\tClassification Loss: 1.6782\r\n",
      "Train Epoch: 4 [161920/209539 (77%)]\tAll Loss: 2.0229\tTriple Loss(1): 0.2152\tClassification Loss: 1.5926\r\n",
      "Train Epoch: 4 [162560/209539 (78%)]\tAll Loss: 1.8478\tTriple Loss(1): 0.1249\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 4 [163200/209539 (78%)]\tAll Loss: 3.3693\tTriple Loss(0): 0.8662\tClassification Loss: 1.6369\r\n",
      "Train Epoch: 4 [163840/209539 (78%)]\tAll Loss: 2.3773\tTriple Loss(1): 0.2544\tClassification Loss: 1.8686\r\n",
      "Train Epoch: 4 [164480/209539 (78%)]\tAll Loss: 1.9723\tTriple Loss(1): 0.1037\tClassification Loss: 1.7648\r\n",
      "Train Epoch: 4 [165120/209539 (79%)]\tAll Loss: 2.7366\tTriple Loss(0): 0.4765\tClassification Loss: 1.7836\r\n",
      "Train Epoch: 4 [165760/209539 (79%)]\tAll Loss: 1.9754\tTriple Loss(1): 0.0563\tClassification Loss: 1.8627\r\n",
      "Train Epoch: 4 [166400/209539 (79%)]\tAll Loss: 2.9521\tTriple Loss(0): 0.5731\tClassification Loss: 1.8058\r\n",
      "Train Epoch: 4 [167040/209539 (80%)]\tAll Loss: 1.7566\tTriple Loss(1): 0.1521\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 4 [167680/209539 (80%)]\tAll Loss: 2.2191\tTriple Loss(1): 0.2870\tClassification Loss: 1.6451\r\n",
      "Train Epoch: 4 [168320/209539 (80%)]\tAll Loss: 2.2994\tTriple Loss(1): 0.2163\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 4 [168960/209539 (81%)]\tAll Loss: 3.3788\tTriple Loss(0): 0.7574\tClassification Loss: 1.8640\r\n",
      "Train Epoch: 4 [169600/209539 (81%)]\tAll Loss: 2.4366\tTriple Loss(1): 0.2128\tClassification Loss: 2.0109\r\n",
      "Train Epoch: 4 [170240/209539 (81%)]\tAll Loss: 2.2766\tTriple Loss(1): 0.3329\tClassification Loss: 1.6107\r\n",
      "Train Epoch: 4 [170880/209539 (82%)]\tAll Loss: 2.1060\tTriple Loss(1): 0.1912\tClassification Loss: 1.7236\r\n",
      "Train Epoch: 4 [171520/209539 (82%)]\tAll Loss: 2.0775\tTriple Loss(1): 0.2154\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 4 [172160/209539 (82%)]\tAll Loss: 2.2144\tTriple Loss(1): 0.2261\tClassification Loss: 1.7621\r\n",
      "Train Epoch: 4 [172800/209539 (82%)]\tAll Loss: 2.4953\tTriple Loss(1): 0.1993\tClassification Loss: 2.0967\r\n",
      "Train Epoch: 4 [173440/209539 (83%)]\tAll Loss: 2.6778\tTriple Loss(1): 0.4901\tClassification Loss: 1.6976\r\n",
      "Train Epoch: 4 [174080/209539 (83%)]\tAll Loss: 3.2137\tTriple Loss(0): 0.7346\tClassification Loss: 1.7444\r\n",
      "Train Epoch: 4 [174720/209539 (83%)]\tAll Loss: 2.0341\tTriple Loss(1): 0.1941\tClassification Loss: 1.6459\r\n",
      "Train Epoch: 4 [175360/209539 (84%)]\tAll Loss: 2.5173\tTriple Loss(1): 0.2523\tClassification Loss: 2.0128\r\n",
      "Train Epoch: 4 [176000/209539 (84%)]\tAll Loss: 1.9189\tTriple Loss(1): 0.1471\tClassification Loss: 1.6246\r\n",
      "Train Epoch: 4 [176640/209539 (84%)]\tAll Loss: 2.1181\tTriple Loss(1): 0.0853\tClassification Loss: 1.9476\r\n",
      "Train Epoch: 4 [177280/209539 (85%)]\tAll Loss: 3.0605\tTriple Loss(0): 0.5576\tClassification Loss: 1.9453\r\n",
      "Train Epoch: 4 [177920/209539 (85%)]\tAll Loss: 2.1161\tTriple Loss(1): 0.1583\tClassification Loss: 1.7995\r\n",
      "Train Epoch: 4 [178560/209539 (85%)]\tAll Loss: 2.0725\tTriple Loss(1): 0.2333\tClassification Loss: 1.6058\r\n",
      "Train Epoch: 4 [179200/209539 (86%)]\tAll Loss: 1.8764\tTriple Loss(1): 0.1083\tClassification Loss: 1.6598\r\n",
      "Train Epoch: 4 [179840/209539 (86%)]\tAll Loss: 1.7777\tTriple Loss(1): 0.1602\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 4 [180480/209539 (86%)]\tAll Loss: 2.0011\tTriple Loss(1): 0.1644\tClassification Loss: 1.6723\r\n",
      "Train Epoch: 4 [181120/209539 (86%)]\tAll Loss: 2.2645\tTriple Loss(1): 0.2236\tClassification Loss: 1.8173\r\n",
      "Train Epoch: 4 [181760/209539 (87%)]\tAll Loss: 2.1336\tTriple Loss(1): 0.1902\tClassification Loss: 1.7532\r\n",
      "Train Epoch: 4 [182400/209539 (87%)]\tAll Loss: 1.9775\tTriple Loss(1): 0.1508\tClassification Loss: 1.6759\r\n",
      "Train Epoch: 4 [183040/209539 (87%)]\tAll Loss: 2.2192\tTriple Loss(1): 0.1384\tClassification Loss: 1.9423\r\n",
      "Train Epoch: 4 [183680/209539 (88%)]\tAll Loss: 2.0978\tTriple Loss(1): 0.2358\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 4 [184320/209539 (88%)]\tAll Loss: 2.0854\tTriple Loss(1): 0.1587\tClassification Loss: 1.7681\r\n",
      "Train Epoch: 4 [184960/209539 (88%)]\tAll Loss: 2.0733\tTriple Loss(1): 0.1524\tClassification Loss: 1.7685\r\n",
      "Train Epoch: 4 [185600/209539 (89%)]\tAll Loss: 1.9161\tTriple Loss(1): 0.1352\tClassification Loss: 1.6457\r\n",
      "Train Epoch: 4 [186240/209539 (89%)]\tAll Loss: 2.2292\tTriple Loss(1): 0.2268\tClassification Loss: 1.7757\r\n",
      "Train Epoch: 4 [186880/209539 (89%)]\tAll Loss: 1.9190\tTriple Loss(1): 0.1498\tClassification Loss: 1.6194\r\n",
      "Train Epoch: 4 [187520/209539 (89%)]\tAll Loss: 2.4397\tTriple Loss(1): 0.3029\tClassification Loss: 1.8339\r\n",
      "Train Epoch: 4 [188160/209539 (90%)]\tAll Loss: 2.1528\tTriple Loss(1): 0.1803\tClassification Loss: 1.7921\r\n",
      "Train Epoch: 4 [188800/209539 (90%)]\tAll Loss: 2.0921\tTriple Loss(1): 0.1978\tClassification Loss: 1.6965\r\n",
      "Train Epoch: 4 [189440/209539 (90%)]\tAll Loss: 3.1469\tTriple Loss(0): 0.6806\tClassification Loss: 1.7858\r\n",
      "Train Epoch: 4 [190080/209539 (91%)]\tAll Loss: 3.0504\tTriple Loss(0): 0.7314\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 4 [190720/209539 (91%)]\tAll Loss: 3.2882\tTriple Loss(0): 0.7454\tClassification Loss: 1.7975\r\n",
      "Train Epoch: 4 [191360/209539 (91%)]\tAll Loss: 2.1287\tTriple Loss(1): 0.2227\tClassification Loss: 1.6833\r\n",
      "Train Epoch: 4 [192000/209539 (92%)]\tAll Loss: 3.2784\tTriple Loss(0): 0.6306\tClassification Loss: 2.0172\r\n",
      "Train Epoch: 4 [192640/209539 (92%)]\tAll Loss: 2.0776\tTriple Loss(1): 0.0960\tClassification Loss: 1.8856\r\n",
      "Train Epoch: 4 [193280/209539 (92%)]\tAll Loss: 2.0607\tTriple Loss(1): 0.1607\tClassification Loss: 1.7392\r\n",
      "Train Epoch: 4 [193920/209539 (93%)]\tAll Loss: 1.8436\tTriple Loss(1): 0.1227\tClassification Loss: 1.5981\r\n",
      "Train Epoch: 4 [194560/209539 (93%)]\tAll Loss: 2.1463\tTriple Loss(1): 0.1623\tClassification Loss: 1.8218\r\n",
      "Train Epoch: 4 [195200/209539 (93%)]\tAll Loss: 1.9004\tTriple Loss(1): 0.1080\tClassification Loss: 1.6844\r\n",
      "Train Epoch: 4 [195840/209539 (93%)]\tAll Loss: 1.7892\tTriple Loss(1): 0.1051\tClassification Loss: 1.5789\r\n",
      "Train Epoch: 4 [196480/209539 (94%)]\tAll Loss: 2.2886\tTriple Loss(1): 0.2127\tClassification Loss: 1.8633\r\n",
      "Train Epoch: 4 [197120/209539 (94%)]\tAll Loss: 2.6036\tTriple Loss(1): 0.3270\tClassification Loss: 1.9496\r\n",
      "Train Epoch: 4 [197760/209539 (94%)]\tAll Loss: 2.0508\tTriple Loss(1): 0.1592\tClassification Loss: 1.7323\r\n",
      "Train Epoch: 4 [198400/209539 (95%)]\tAll Loss: 2.8198\tTriple Loss(0): 0.6805\tClassification Loss: 1.4588\r\n",
      "Train Epoch: 4 [199040/209539 (95%)]\tAll Loss: 1.9730\tTriple Loss(1): 0.1222\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 4 [199680/209539 (95%)]\tAll Loss: 2.5393\tTriple Loss(1): 0.3427\tClassification Loss: 1.8539\r\n",
      "Train Epoch: 4 [200320/209539 (96%)]\tAll Loss: 2.0780\tTriple Loss(1): 0.1039\tClassification Loss: 1.8702\r\n",
      "Train Epoch: 4 [200960/209539 (96%)]\tAll Loss: 3.1539\tTriple Loss(0): 0.7459\tClassification Loss: 1.6621\r\n",
      "Train Epoch: 4 [201600/209539 (96%)]\tAll Loss: 2.2354\tTriple Loss(1): 0.1569\tClassification Loss: 1.9216\r\n",
      "Train Epoch: 4 [202240/209539 (97%)]\tAll Loss: 2.9780\tTriple Loss(0): 0.6333\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 4 [202880/209539 (97%)]\tAll Loss: 2.8684\tTriple Loss(0): 0.6744\tClassification Loss: 1.5196\r\n",
      "Train Epoch: 4 [203520/209539 (97%)]\tAll Loss: 3.3307\tTriple Loss(0): 0.7105\tClassification Loss: 1.9097\r\n",
      "Train Epoch: 4 [204160/209539 (97%)]\tAll Loss: 2.5094\tTriple Loss(1): 0.2396\tClassification Loss: 2.0303\r\n",
      "Train Epoch: 4 [204800/209539 (98%)]\tAll Loss: 2.0329\tTriple Loss(1): 0.1880\tClassification Loss: 1.6569\r\n",
      "Train Epoch: 4 [205440/209539 (98%)]\tAll Loss: 2.1463\tTriple Loss(1): 0.2684\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 4 [206080/209539 (98%)]\tAll Loss: 2.1384\tTriple Loss(1): 0.1897\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 4 [206720/209539 (99%)]\tAll Loss: 2.2384\tTriple Loss(1): 0.2027\tClassification Loss: 1.8331\r\n",
      "Train Epoch: 4 [207360/209539 (99%)]\tAll Loss: 1.8855\tTriple Loss(1): 0.1851\tClassification Loss: 1.5153\r\n",
      "Train Epoch: 4 [208000/209539 (99%)]\tAll Loss: 1.9184\tTriple Loss(1): 0.0747\tClassification Loss: 1.7691\r\n",
      "Train Epoch: 4 [208640/209539 (100%)]\tAll Loss: 2.8063\tTriple Loss(1): 0.4519\tClassification Loss: 1.9026\r\n",
      "Train Epoch: 4 [209280/209539 (100%)]\tAll Loss: 1.9537\tTriple Loss(1): 0.1292\tClassification Loss: 1.6952\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/4_epochs\r\n",
      "Train Epoch: 5 [0/209539 (0%)]\tAll Loss: 2.9862\tTriple Loss(1): 0.4053\tClassification Loss: 2.1756\r\n",
      "\r\n",
      "Test set: Average loss: 1.6353, Accuracy: 42224/80128 (53%)\r\n",
      "\r\n",
      "Train Epoch: 5 [640/209539 (0%)]\tAll Loss: 2.3254\tTriple Loss(1): 0.1423\tClassification Loss: 2.0408\r\n",
      "Train Epoch: 5 [1280/209539 (1%)]\tAll Loss: 2.4763\tTriple Loss(1): 0.1009\tClassification Loss: 2.2744\r\n",
      "Train Epoch: 5 [1920/209539 (1%)]\tAll Loss: 2.0580\tTriple Loss(1): 0.1772\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 5 [2560/209539 (1%)]\tAll Loss: 2.3144\tTriple Loss(1): 0.2369\tClassification Loss: 1.8407\r\n",
      "Train Epoch: 5 [3200/209539 (2%)]\tAll Loss: 3.2208\tTriple Loss(0): 0.7042\tClassification Loss: 1.8124\r\n",
      "Train Epoch: 5 [3840/209539 (2%)]\tAll Loss: 3.9794\tTriple Loss(0): 1.0166\tClassification Loss: 1.9462\r\n",
      "Train Epoch: 5 [4480/209539 (2%)]\tAll Loss: 3.0640\tTriple Loss(0): 0.5998\tClassification Loss: 1.8644\r\n",
      "Train Epoch: 5 [5120/209539 (2%)]\tAll Loss: 3.0282\tTriple Loss(0): 0.5342\tClassification Loss: 1.9599\r\n",
      "Train Epoch: 5 [5760/209539 (3%)]\tAll Loss: 2.1022\tTriple Loss(1): 0.2863\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 5 [6400/209539 (3%)]\tAll Loss: 2.1999\tTriple Loss(1): 0.1326\tClassification Loss: 1.9347\r\n",
      "Train Epoch: 5 [7040/209539 (3%)]\tAll Loss: 2.4024\tTriple Loss(1): 0.1784\tClassification Loss: 2.0456\r\n",
      "Train Epoch: 5 [7680/209539 (4%)]\tAll Loss: 1.9407\tTriple Loss(1): 0.1249\tClassification Loss: 1.6908\r\n",
      "Train Epoch: 5 [8320/209539 (4%)]\tAll Loss: 2.4668\tTriple Loss(1): 0.2685\tClassification Loss: 1.9298\r\n",
      "Train Epoch: 5 [8960/209539 (4%)]\tAll Loss: 1.9747\tTriple Loss(1): 0.0750\tClassification Loss: 1.8248\r\n",
      "Train Epoch: 5 [9600/209539 (5%)]\tAll Loss: 2.2416\tTriple Loss(1): 0.1312\tClassification Loss: 1.9791\r\n",
      "Train Epoch: 5 [10240/209539 (5%)]\tAll Loss: 2.3952\tTriple Loss(1): 0.2506\tClassification Loss: 1.8939\r\n",
      "Train Epoch: 5 [10880/209539 (5%)]\tAll Loss: 2.2577\tTriple Loss(1): 0.1651\tClassification Loss: 1.9274\r\n",
      "Train Epoch: 5 [11520/209539 (5%)]\tAll Loss: 2.1816\tTriple Loss(1): 0.1943\tClassification Loss: 1.7931\r\n",
      "Train Epoch: 5 [12160/209539 (6%)]\tAll Loss: 2.3846\tTriple Loss(1): 0.2315\tClassification Loss: 1.9216\r\n",
      "Train Epoch: 5 [12800/209539 (6%)]\tAll Loss: 2.2824\tTriple Loss(1): 0.1600\tClassification Loss: 1.9624\r\n",
      "Train Epoch: 5 [13440/209539 (6%)]\tAll Loss: 3.4615\tTriple Loss(0): 0.8038\tClassification Loss: 1.8538\r\n",
      "Train Epoch: 5 [14080/209539 (7%)]\tAll Loss: 2.1755\tTriple Loss(1): 0.1550\tClassification Loss: 1.8654\r\n",
      "Train Epoch: 5 [14720/209539 (7%)]\tAll Loss: 2.2426\tTriple Loss(1): 0.1524\tClassification Loss: 1.9377\r\n",
      "Train Epoch: 5 [15360/209539 (7%)]\tAll Loss: 2.2314\tTriple Loss(1): 0.1941\tClassification Loss: 1.8432\r\n",
      "Train Epoch: 5 [16000/209539 (8%)]\tAll Loss: 2.1259\tTriple Loss(1): 0.1835\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 5 [16640/209539 (8%)]\tAll Loss: 3.1283\tTriple Loss(0): 0.7014\tClassification Loss: 1.7256\r\n",
      "Train Epoch: 5 [17280/209539 (8%)]\tAll Loss: 2.1589\tTriple Loss(1): 0.1637\tClassification Loss: 1.8316\r\n",
      "Train Epoch: 5 [17920/209539 (9%)]\tAll Loss: 1.8403\tTriple Loss(1): 0.0919\tClassification Loss: 1.6566\r\n",
      "Train Epoch: 5 [18560/209539 (9%)]\tAll Loss: 2.4492\tTriple Loss(1): 0.1913\tClassification Loss: 2.0665\r\n",
      "Train Epoch: 5 [19200/209539 (9%)]\tAll Loss: 2.2522\tTriple Loss(1): 0.2612\tClassification Loss: 1.7298\r\n",
      "Train Epoch: 5 [19840/209539 (9%)]\tAll Loss: 2.0420\tTriple Loss(1): 0.1377\tClassification Loss: 1.7667\r\n",
      "Train Epoch: 5 [20480/209539 (10%)]\tAll Loss: 2.4611\tTriple Loss(1): 0.2101\tClassification Loss: 2.0408\r\n",
      "Train Epoch: 5 [21120/209539 (10%)]\tAll Loss: 2.2919\tTriple Loss(1): 0.3247\tClassification Loss: 1.6426\r\n",
      "Train Epoch: 5 [21760/209539 (10%)]\tAll Loss: 2.1441\tTriple Loss(1): 0.1132\tClassification Loss: 1.9176\r\n",
      "Train Epoch: 5 [22400/209539 (11%)]\tAll Loss: 2.3330\tTriple Loss(1): 0.3220\tClassification Loss: 1.6891\r\n",
      "Train Epoch: 5 [23040/209539 (11%)]\tAll Loss: 2.1376\tTriple Loss(1): 0.1172\tClassification Loss: 1.9031\r\n",
      "Train Epoch: 5 [23680/209539 (11%)]\tAll Loss: 2.2292\tTriple Loss(1): 0.3025\tClassification Loss: 1.6242\r\n",
      "Train Epoch: 5 [24320/209539 (12%)]\tAll Loss: 2.3652\tTriple Loss(1): 0.3123\tClassification Loss: 1.7407\r\n",
      "Train Epoch: 5 [24960/209539 (12%)]\tAll Loss: 1.8331\tTriple Loss(1): 0.1262\tClassification Loss: 1.5808\r\n",
      "Train Epoch: 5 [25600/209539 (12%)]\tAll Loss: 2.1538\tTriple Loss(1): 0.3020\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 5 [26240/209539 (13%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.1927\tClassification Loss: 1.8632\r\n",
      "Train Epoch: 5 [26880/209539 (13%)]\tAll Loss: 1.9838\tTriple Loss(1): 0.0840\tClassification Loss: 1.8158\r\n",
      "Train Epoch: 5 [27520/209539 (13%)]\tAll Loss: 1.9426\tTriple Loss(1): 0.1916\tClassification Loss: 1.5594\r\n",
      "Train Epoch: 5 [28160/209539 (13%)]\tAll Loss: 2.0216\tTriple Loss(1): 0.1889\tClassification Loss: 1.6438\r\n",
      "Train Epoch: 5 [28800/209539 (14%)]\tAll Loss: 2.3921\tTriple Loss(1): 0.2157\tClassification Loss: 1.9608\r\n",
      "Train Epoch: 5 [29440/209539 (14%)]\tAll Loss: 2.2322\tTriple Loss(1): 0.0947\tClassification Loss: 2.0429\r\n",
      "Train Epoch: 5 [30080/209539 (14%)]\tAll Loss: 2.1473\tTriple Loss(1): 0.1994\tClassification Loss: 1.7485\r\n",
      "Train Epoch: 5 [30720/209539 (15%)]\tAll Loss: 2.1630\tTriple Loss(1): 0.1829\tClassification Loss: 1.7971\r\n",
      "Train Epoch: 5 [31360/209539 (15%)]\tAll Loss: 2.3255\tTriple Loss(1): 0.1187\tClassification Loss: 2.0882\r\n",
      "Train Epoch: 5 [32000/209539 (15%)]\tAll Loss: 3.3101\tTriple Loss(0): 0.6731\tClassification Loss: 1.9639\r\n",
      "Train Epoch: 5 [32640/209539 (16%)]\tAll Loss: 2.4822\tTriple Loss(1): 0.1504\tClassification Loss: 2.1815\r\n",
      "Train Epoch: 5 [33280/209539 (16%)]\tAll Loss: 2.1175\tTriple Loss(1): 0.1401\tClassification Loss: 1.8373\r\n",
      "Train Epoch: 5 [33920/209539 (16%)]\tAll Loss: 3.2961\tTriple Loss(0): 0.6670\tClassification Loss: 1.9622\r\n",
      "Train Epoch: 5 [34560/209539 (16%)]\tAll Loss: 2.0806\tTriple Loss(1): 0.1502\tClassification Loss: 1.7803\r\n",
      "Train Epoch: 5 [35200/209539 (17%)]\tAll Loss: 2.2534\tTriple Loss(1): 0.0449\tClassification Loss: 2.1637\r\n",
      "Train Epoch: 5 [35840/209539 (17%)]\tAll Loss: 2.2563\tTriple Loss(1): 0.2020\tClassification Loss: 1.8524\r\n",
      "Train Epoch: 5 [36480/209539 (17%)]\tAll Loss: 2.2147\tTriple Loss(1): 0.1720\tClassification Loss: 1.8706\r\n",
      "Train Epoch: 5 [37120/209539 (18%)]\tAll Loss: 2.3650\tTriple Loss(1): 0.1453\tClassification Loss: 2.0744\r\n",
      "Train Epoch: 5 [37760/209539 (18%)]\tAll Loss: 2.1314\tTriple Loss(1): 0.1464\tClassification Loss: 1.8385\r\n",
      "Train Epoch: 5 [38400/209539 (18%)]\tAll Loss: 3.1150\tTriple Loss(0): 0.8973\tClassification Loss: 1.3203\r\n",
      "Train Epoch: 5 [39040/209539 (19%)]\tAll Loss: 2.3112\tTriple Loss(1): 0.1139\tClassification Loss: 2.0834\r\n",
      "Train Epoch: 5 [39680/209539 (19%)]\tAll Loss: 2.1504\tTriple Loss(1): 0.2697\tClassification Loss: 1.6110\r\n",
      "Train Epoch: 5 [40320/209539 (19%)]\tAll Loss: 2.6400\tTriple Loss(1): 0.1765\tClassification Loss: 2.2871\r\n",
      "Train Epoch: 5 [40960/209539 (20%)]\tAll Loss: 2.1842\tTriple Loss(1): 0.2340\tClassification Loss: 1.7163\r\n",
      "Train Epoch: 5 [41600/209539 (20%)]\tAll Loss: 2.4320\tTriple Loss(1): 0.1774\tClassification Loss: 2.0771\r\n",
      "Train Epoch: 5 [42240/209539 (20%)]\tAll Loss: 1.9794\tTriple Loss(1): 0.1174\tClassification Loss: 1.7446\r\n",
      "Train Epoch: 5 [42880/209539 (20%)]\tAll Loss: 2.3117\tTriple Loss(1): 0.2961\tClassification Loss: 1.7194\r\n",
      "Train Epoch: 5 [43520/209539 (21%)]\tAll Loss: 2.5380\tTriple Loss(1): 0.2265\tClassification Loss: 2.0850\r\n",
      "Train Epoch: 5 [44160/209539 (21%)]\tAll Loss: 2.0874\tTriple Loss(1): 0.1880\tClassification Loss: 1.7113\r\n",
      "Train Epoch: 5 [44800/209539 (21%)]\tAll Loss: 2.4250\tTriple Loss(1): 0.3055\tClassification Loss: 1.8140\r\n",
      "Train Epoch: 5 [45440/209539 (22%)]\tAll Loss: 2.0822\tTriple Loss(1): 0.2510\tClassification Loss: 1.5802\r\n",
      "Train Epoch: 5 [46080/209539 (22%)]\tAll Loss: 2.5290\tTriple Loss(1): 0.3092\tClassification Loss: 1.9107\r\n",
      "Train Epoch: 5 [46720/209539 (22%)]\tAll Loss: 2.1011\tTriple Loss(1): 0.2281\tClassification Loss: 1.6450\r\n",
      "Train Epoch: 5 [47360/209539 (23%)]\tAll Loss: 2.4607\tTriple Loss(1): 0.1869\tClassification Loss: 2.0868\r\n",
      "Train Epoch: 5 [48000/209539 (23%)]\tAll Loss: 2.4362\tTriple Loss(1): 0.2467\tClassification Loss: 1.9429\r\n",
      "Train Epoch: 5 [48640/209539 (23%)]\tAll Loss: 1.8121\tTriple Loss(1): 0.1224\tClassification Loss: 1.5674\r\n",
      "Train Epoch: 5 [49280/209539 (24%)]\tAll Loss: 2.4043\tTriple Loss(1): 0.3369\tClassification Loss: 1.7306\r\n",
      "Train Epoch: 5 [49920/209539 (24%)]\tAll Loss: 3.5435\tTriple Loss(0): 0.8293\tClassification Loss: 1.8849\r\n",
      "Train Epoch: 5 [50560/209539 (24%)]\tAll Loss: 2.1416\tTriple Loss(1): 0.1919\tClassification Loss: 1.7579\r\n",
      "Train Epoch: 5 [51200/209539 (24%)]\tAll Loss: 2.0925\tTriple Loss(1): 0.1948\tClassification Loss: 1.7029\r\n",
      "Train Epoch: 5 [51840/209539 (25%)]\tAll Loss: 2.0940\tTriple Loss(1): 0.1624\tClassification Loss: 1.7693\r\n",
      "Train Epoch: 5 [52480/209539 (25%)]\tAll Loss: 2.4224\tTriple Loss(1): 0.2915\tClassification Loss: 1.8395\r\n",
      "Train Epoch: 5 [53120/209539 (25%)]\tAll Loss: 2.9592\tTriple Loss(0): 0.6854\tClassification Loss: 1.5885\r\n",
      "Train Epoch: 5 [53760/209539 (26%)]\tAll Loss: 2.3771\tTriple Loss(1): 0.1982\tClassification Loss: 1.9807\r\n",
      "Train Epoch: 5 [54400/209539 (26%)]\tAll Loss: 2.3545\tTriple Loss(1): 0.1240\tClassification Loss: 2.1065\r\n",
      "Train Epoch: 5 [55040/209539 (26%)]\tAll Loss: 1.8146\tTriple Loss(1): 0.1584\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 5 [55680/209539 (27%)]\tAll Loss: 2.1037\tTriple Loss(1): 0.1939\tClassification Loss: 1.7159\r\n",
      "Train Epoch: 5 [56320/209539 (27%)]\tAll Loss: 1.8221\tTriple Loss(1): 0.1063\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 5 [56960/209539 (27%)]\tAll Loss: 2.3471\tTriple Loss(1): 0.1912\tClassification Loss: 1.9646\r\n",
      "Train Epoch: 5 [57600/209539 (27%)]\tAll Loss: 2.3651\tTriple Loss(1): 0.3138\tClassification Loss: 1.7375\r\n",
      "Train Epoch: 5 [58240/209539 (28%)]\tAll Loss: 2.0937\tTriple Loss(1): 0.2258\tClassification Loss: 1.6420\r\n",
      "Train Epoch: 5 [58880/209539 (28%)]\tAll Loss: 2.0164\tTriple Loss(1): 0.1574\tClassification Loss: 1.7016\r\n",
      "Train Epoch: 5 [59520/209539 (28%)]\tAll Loss: 2.0270\tTriple Loss(1): 0.1230\tClassification Loss: 1.7811\r\n",
      "Train Epoch: 5 [60160/209539 (29%)]\tAll Loss: 2.1249\tTriple Loss(1): 0.2113\tClassification Loss: 1.7023\r\n",
      "Train Epoch: 5 [60800/209539 (29%)]\tAll Loss: 2.9021\tTriple Loss(0): 0.5606\tClassification Loss: 1.7809\r\n",
      "Train Epoch: 5 [61440/209539 (29%)]\tAll Loss: 2.1535\tTriple Loss(1): 0.2194\tClassification Loss: 1.7147\r\n",
      "Train Epoch: 5 [62080/209539 (30%)]\tAll Loss: 2.1732\tTriple Loss(1): 0.1688\tClassification Loss: 1.8355\r\n",
      "Train Epoch: 5 [62720/209539 (30%)]\tAll Loss: 2.5791\tTriple Loss(1): 0.3129\tClassification Loss: 1.9532\r\n",
      "Train Epoch: 5 [63360/209539 (30%)]\tAll Loss: 2.3646\tTriple Loss(1): 0.2242\tClassification Loss: 1.9163\r\n",
      "Train Epoch: 5 [64000/209539 (31%)]\tAll Loss: 2.0246\tTriple Loss(1): 0.0452\tClassification Loss: 1.9343\r\n",
      "Train Epoch: 5 [64640/209539 (31%)]\tAll Loss: 2.3601\tTriple Loss(1): 0.2033\tClassification Loss: 1.9534\r\n",
      "Train Epoch: 5 [65280/209539 (31%)]\tAll Loss: 3.0365\tTriple Loss(0): 0.5361\tClassification Loss: 1.9643\r\n",
      "Train Epoch: 5 [65920/209539 (31%)]\tAll Loss: 2.3186\tTriple Loss(1): 0.2401\tClassification Loss: 1.8384\r\n",
      "Train Epoch: 5 [66560/209539 (32%)]\tAll Loss: 2.2327\tTriple Loss(1): 0.2652\tClassification Loss: 1.7023\r\n",
      "Train Epoch: 5 [67200/209539 (32%)]\tAll Loss: 2.2427\tTriple Loss(1): 0.1594\tClassification Loss: 1.9239\r\n",
      "Train Epoch: 5 [67840/209539 (32%)]\tAll Loss: 1.8593\tTriple Loss(1): 0.1148\tClassification Loss: 1.6297\r\n",
      "Train Epoch: 5 [68480/209539 (33%)]\tAll Loss: 1.9355\tTriple Loss(1): 0.1198\tClassification Loss: 1.6958\r\n",
      "Train Epoch: 5 [69120/209539 (33%)]\tAll Loss: 1.8863\tTriple Loss(1): 0.1316\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 5 [69760/209539 (33%)]\tAll Loss: 2.4929\tTriple Loss(1): 0.2788\tClassification Loss: 1.9353\r\n",
      "Train Epoch: 5 [70400/209539 (34%)]\tAll Loss: 2.8305\tTriple Loss(0): 0.5800\tClassification Loss: 1.6706\r\n",
      "Train Epoch: 5 [71040/209539 (34%)]\tAll Loss: 3.1952\tTriple Loss(0): 0.7478\tClassification Loss: 1.6995\r\n",
      "Train Epoch: 5 [71680/209539 (34%)]\tAll Loss: 2.1820\tTriple Loss(1): 0.1837\tClassification Loss: 1.8146\r\n",
      "Train Epoch: 5 [72320/209539 (35%)]\tAll Loss: 2.1223\tTriple Loss(1): 0.0939\tClassification Loss: 1.9344\r\n",
      "Train Epoch: 5 [72960/209539 (35%)]\tAll Loss: 2.1039\tTriple Loss(1): 0.1540\tClassification Loss: 1.7960\r\n",
      "Train Epoch: 5 [73600/209539 (35%)]\tAll Loss: 2.1405\tTriple Loss(1): 0.2378\tClassification Loss: 1.6650\r\n",
      "Train Epoch: 5 [74240/209539 (35%)]\tAll Loss: 2.2851\tTriple Loss(1): 0.2246\tClassification Loss: 1.8358\r\n",
      "Train Epoch: 5 [74880/209539 (36%)]\tAll Loss: 1.7254\tTriple Loss(1): 0.0578\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 5 [75520/209539 (36%)]\tAll Loss: 2.1590\tTriple Loss(1): 0.1873\tClassification Loss: 1.7844\r\n",
      "Train Epoch: 5 [76160/209539 (36%)]\tAll Loss: 2.1736\tTriple Loss(1): 0.1482\tClassification Loss: 1.8773\r\n",
      "Train Epoch: 5 [76800/209539 (37%)]\tAll Loss: 2.3650\tTriple Loss(1): 0.2448\tClassification Loss: 1.8754\r\n",
      "Train Epoch: 5 [77440/209539 (37%)]\tAll Loss: 2.3847\tTriple Loss(1): 0.2685\tClassification Loss: 1.8476\r\n",
      "Train Epoch: 5 [78080/209539 (37%)]\tAll Loss: 2.1301\tTriple Loss(1): 0.1839\tClassification Loss: 1.7623\r\n",
      "Train Epoch: 5 [78720/209539 (38%)]\tAll Loss: 2.5664\tTriple Loss(1): 0.1691\tClassification Loss: 2.2283\r\n",
      "Train Epoch: 5 [79360/209539 (38%)]\tAll Loss: 2.4206\tTriple Loss(1): 0.1234\tClassification Loss: 2.1739\r\n",
      "Train Epoch: 5 [80000/209539 (38%)]\tAll Loss: 3.2101\tTriple Loss(0): 0.6183\tClassification Loss: 1.9734\r\n",
      "Train Epoch: 5 [80640/209539 (38%)]\tAll Loss: 2.0330\tTriple Loss(1): 0.2689\tClassification Loss: 1.4953\r\n",
      "Train Epoch: 5 [81280/209539 (39%)]\tAll Loss: 3.3893\tTriple Loss(0): 0.6140\tClassification Loss: 2.1614\r\n",
      "Train Epoch: 5 [81920/209539 (39%)]\tAll Loss: 3.0425\tTriple Loss(0): 0.6205\tClassification Loss: 1.8015\r\n",
      "Train Epoch: 5 [82560/209539 (39%)]\tAll Loss: 2.4844\tTriple Loss(1): 0.3165\tClassification Loss: 1.8515\r\n",
      "Train Epoch: 5 [83200/209539 (40%)]\tAll Loss: 2.0509\tTriple Loss(1): 0.2066\tClassification Loss: 1.6378\r\n",
      "Train Epoch: 5 [83840/209539 (40%)]\tAll Loss: 3.4347\tTriple Loss(0): 0.7662\tClassification Loss: 1.9024\r\n",
      "Train Epoch: 5 [84480/209539 (40%)]\tAll Loss: 2.4513\tTriple Loss(1): 0.2789\tClassification Loss: 1.8934\r\n",
      "Train Epoch: 5 [85120/209539 (41%)]\tAll Loss: 2.2195\tTriple Loss(1): 0.1907\tClassification Loss: 1.8381\r\n",
      "Train Epoch: 5 [85760/209539 (41%)]\tAll Loss: 3.1233\tTriple Loss(0): 0.7357\tClassification Loss: 1.6520\r\n",
      "Train Epoch: 5 [86400/209539 (41%)]\tAll Loss: 2.3607\tTriple Loss(1): 0.2024\tClassification Loss: 1.9558\r\n",
      "Train Epoch: 5 [87040/209539 (42%)]\tAll Loss: 2.2899\tTriple Loss(1): 0.3197\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 5 [87680/209539 (42%)]\tAll Loss: 2.2380\tTriple Loss(1): 0.1972\tClassification Loss: 1.8437\r\n",
      "Train Epoch: 5 [88320/209539 (42%)]\tAll Loss: 2.4388\tTriple Loss(1): 0.2101\tClassification Loss: 2.0185\r\n",
      "Train Epoch: 5 [88960/209539 (42%)]\tAll Loss: 2.4545\tTriple Loss(1): 0.3141\tClassification Loss: 1.8263\r\n",
      "Train Epoch: 5 [89600/209539 (43%)]\tAll Loss: 2.2235\tTriple Loss(1): 0.1973\tClassification Loss: 1.8288\r\n",
      "Train Epoch: 5 [90240/209539 (43%)]\tAll Loss: 2.1290\tTriple Loss(1): 0.1101\tClassification Loss: 1.9087\r\n",
      "Train Epoch: 5 [90880/209539 (43%)]\tAll Loss: 2.0839\tTriple Loss(1): 0.1502\tClassification Loss: 1.7835\r\n",
      "Train Epoch: 5 [91520/209539 (44%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.1326\tClassification Loss: 1.9832\r\n",
      "Train Epoch: 5 [92160/209539 (44%)]\tAll Loss: 2.1698\tTriple Loss(1): 0.1953\tClassification Loss: 1.7792\r\n",
      "Train Epoch: 5 [92800/209539 (44%)]\tAll Loss: 2.8019\tTriple Loss(0): 0.8573\tClassification Loss: 1.0873\r\n",
      "Train Epoch: 5 [93440/209539 (45%)]\tAll Loss: 2.5062\tTriple Loss(1): 0.2800\tClassification Loss: 1.9462\r\n",
      "Train Epoch: 5 [94080/209539 (45%)]\tAll Loss: 1.8531\tTriple Loss(1): 0.2209\tClassification Loss: 1.4114\r\n",
      "Train Epoch: 5 [94720/209539 (45%)]\tAll Loss: 2.2165\tTriple Loss(1): 0.0806\tClassification Loss: 2.0553\r\n",
      "Train Epoch: 5 [95360/209539 (46%)]\tAll Loss: 2.1702\tTriple Loss(1): 0.1546\tClassification Loss: 1.8609\r\n",
      "Train Epoch: 5 [96000/209539 (46%)]\tAll Loss: 3.5271\tTriple Loss(0): 0.8141\tClassification Loss: 1.8988\r\n",
      "Train Epoch: 5 [96640/209539 (46%)]\tAll Loss: 2.4292\tTriple Loss(1): 0.2236\tClassification Loss: 1.9820\r\n",
      "Train Epoch: 5 [97280/209539 (46%)]\tAll Loss: 2.0456\tTriple Loss(1): 0.1330\tClassification Loss: 1.7796\r\n",
      "Train Epoch: 5 [97920/209539 (47%)]\tAll Loss: 1.9694\tTriple Loss(1): 0.1088\tClassification Loss: 1.7519\r\n",
      "Train Epoch: 5 [98560/209539 (47%)]\tAll Loss: 1.9203\tTriple Loss(1): 0.1938\tClassification Loss: 1.5327\r\n",
      "Train Epoch: 5 [99200/209539 (47%)]\tAll Loss: 2.1930\tTriple Loss(1): 0.1473\tClassification Loss: 1.8983\r\n",
      "Train Epoch: 5 [99840/209539 (48%)]\tAll Loss: 2.3751\tTriple Loss(1): 0.2278\tClassification Loss: 1.9196\r\n",
      "Train Epoch: 5 [100480/209539 (48%)]\tAll Loss: 2.3408\tTriple Loss(1): 0.2252\tClassification Loss: 1.8904\r\n",
      "Train Epoch: 5 [101120/209539 (48%)]\tAll Loss: 2.2125\tTriple Loss(1): 0.1475\tClassification Loss: 1.9175\r\n",
      "Train Epoch: 5 [101760/209539 (49%)]\tAll Loss: 2.2918\tTriple Loss(1): 0.2637\tClassification Loss: 1.7645\r\n",
      "Train Epoch: 5 [102400/209539 (49%)]\tAll Loss: 2.1092\tTriple Loss(1): 0.2218\tClassification Loss: 1.6656\r\n",
      "Train Epoch: 5 [103040/209539 (49%)]\tAll Loss: 2.7722\tTriple Loss(1): 0.3212\tClassification Loss: 2.1297\r\n",
      "Train Epoch: 5 [103680/209539 (49%)]\tAll Loss: 3.6932\tTriple Loss(0): 0.8196\tClassification Loss: 2.0540\r\n",
      "Train Epoch: 5 [104320/209539 (50%)]\tAll Loss: 2.1049\tTriple Loss(1): 0.1478\tClassification Loss: 1.8092\r\n",
      "Train Epoch: 5 [104960/209539 (50%)]\tAll Loss: 2.0755\tTriple Loss(1): 0.2488\tClassification Loss: 1.5779\r\n",
      "Train Epoch: 5 [105600/209539 (50%)]\tAll Loss: 1.9921\tTriple Loss(1): 0.2248\tClassification Loss: 1.5425\r\n",
      "Train Epoch: 5 [106240/209539 (51%)]\tAll Loss: 2.2079\tTriple Loss(1): 0.1648\tClassification Loss: 1.8783\r\n",
      "Train Epoch: 5 [106880/209539 (51%)]\tAll Loss: 3.3676\tTriple Loss(0): 0.8395\tClassification Loss: 1.6886\r\n",
      "Train Epoch: 5 [107520/209539 (51%)]\tAll Loss: 2.1722\tTriple Loss(1): 0.1713\tClassification Loss: 1.8296\r\n",
      "Train Epoch: 5 [108160/209539 (52%)]\tAll Loss: 1.8823\tTriple Loss(1): 0.1644\tClassification Loss: 1.5536\r\n",
      "Train Epoch: 5 [108800/209539 (52%)]\tAll Loss: 2.3075\tTriple Loss(1): 0.2087\tClassification Loss: 1.8900\r\n",
      "Train Epoch: 5 [109440/209539 (52%)]\tAll Loss: 3.6324\tTriple Loss(0): 0.8453\tClassification Loss: 1.9417\r\n",
      "Train Epoch: 5 [110080/209539 (53%)]\tAll Loss: 1.9179\tTriple Loss(1): 0.1992\tClassification Loss: 1.5195\r\n",
      "Train Epoch: 5 [110720/209539 (53%)]\tAll Loss: 2.1424\tTriple Loss(1): 0.1069\tClassification Loss: 1.9286\r\n",
      "Train Epoch: 5 [111360/209539 (53%)]\tAll Loss: 2.8600\tTriple Loss(0): 0.7051\tClassification Loss: 1.4499\r\n",
      "Train Epoch: 5 [112000/209539 (53%)]\tAll Loss: 2.1217\tTriple Loss(1): 0.1346\tClassification Loss: 1.8525\r\n",
      "Train Epoch: 5 [112640/209539 (54%)]\tAll Loss: 3.1529\tTriple Loss(0): 0.7291\tClassification Loss: 1.6946\r\n",
      "Train Epoch: 5 [113280/209539 (54%)]\tAll Loss: 2.1502\tTriple Loss(1): 0.2631\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 5 [113920/209539 (54%)]\tAll Loss: 2.0725\tTriple Loss(1): 0.0690\tClassification Loss: 1.9344\r\n",
      "Train Epoch: 5 [114560/209539 (55%)]\tAll Loss: 2.4842\tTriple Loss(1): 0.3178\tClassification Loss: 1.8486\r\n",
      "Train Epoch: 5 [115200/209539 (55%)]\tAll Loss: 2.3161\tTriple Loss(1): 0.2930\tClassification Loss: 1.7301\r\n",
      "Train Epoch: 5 [115840/209539 (55%)]\tAll Loss: 3.2076\tTriple Loss(0): 0.6972\tClassification Loss: 1.8132\r\n",
      "Train Epoch: 5 [116480/209539 (56%)]\tAll Loss: 1.8890\tTriple Loss(1): 0.1397\tClassification Loss: 1.6095\r\n",
      "Train Epoch: 5 [117120/209539 (56%)]\tAll Loss: 2.2819\tTriple Loss(1): 0.1687\tClassification Loss: 1.9445\r\n",
      "Train Epoch: 5 [117760/209539 (56%)]\tAll Loss: 2.0920\tTriple Loss(1): 0.2005\tClassification Loss: 1.6910\r\n",
      "Train Epoch: 5 [118400/209539 (57%)]\tAll Loss: 1.8669\tTriple Loss(1): 0.1816\tClassification Loss: 1.5038\r\n",
      "Train Epoch: 5 [119040/209539 (57%)]\tAll Loss: 2.7635\tTriple Loss(1): 0.3164\tClassification Loss: 2.1307\r\n",
      "Train Epoch: 5 [119680/209539 (57%)]\tAll Loss: 1.9214\tTriple Loss(1): 0.1690\tClassification Loss: 1.5833\r\n",
      "Train Epoch: 5 [120320/209539 (57%)]\tAll Loss: 2.8802\tTriple Loss(0): 0.5284\tClassification Loss: 1.8234\r\n",
      "Train Epoch: 5 [120960/209539 (58%)]\tAll Loss: 1.8548\tTriple Loss(1): 0.2274\tClassification Loss: 1.4000\r\n",
      "Train Epoch: 5 [121600/209539 (58%)]\tAll Loss: 2.8020\tTriple Loss(0): 0.5733\tClassification Loss: 1.6554\r\n",
      "Train Epoch: 5 [122240/209539 (58%)]\tAll Loss: 2.0462\tTriple Loss(1): 0.1610\tClassification Loss: 1.7242\r\n",
      "Train Epoch: 5 [122880/209539 (59%)]\tAll Loss: 2.3686\tTriple Loss(1): 0.1966\tClassification Loss: 1.9753\r\n",
      "Train Epoch: 5 [123520/209539 (59%)]\tAll Loss: 2.3267\tTriple Loss(1): 0.2174\tClassification Loss: 1.8919\r\n",
      "Train Epoch: 5 [124160/209539 (59%)]\tAll Loss: 3.1493\tTriple Loss(0): 0.6536\tClassification Loss: 1.8422\r\n",
      "Train Epoch: 5 [124800/209539 (60%)]\tAll Loss: 2.0315\tTriple Loss(1): 0.1932\tClassification Loss: 1.6451\r\n",
      "Train Epoch: 5 [125440/209539 (60%)]\tAll Loss: 2.3264\tTriple Loss(1): 0.2933\tClassification Loss: 1.7398\r\n",
      "Train Epoch: 5 [126080/209539 (60%)]\tAll Loss: 2.9452\tTriple Loss(0): 0.6130\tClassification Loss: 1.7191\r\n",
      "Train Epoch: 5 [126720/209539 (60%)]\tAll Loss: 2.1031\tTriple Loss(1): 0.2265\tClassification Loss: 1.6500\r\n",
      "Train Epoch: 5 [127360/209539 (61%)]\tAll Loss: 2.1128\tTriple Loss(1): 0.1344\tClassification Loss: 1.8441\r\n",
      "Train Epoch: 5 [128000/209539 (61%)]\tAll Loss: 1.9789\tTriple Loss(1): 0.1501\tClassification Loss: 1.6786\r\n",
      "Train Epoch: 5 [128640/209539 (61%)]\tAll Loss: 2.2516\tTriple Loss(1): 0.3135\tClassification Loss: 1.6246\r\n",
      "Train Epoch: 5 [129280/209539 (62%)]\tAll Loss: 2.2636\tTriple Loss(1): 0.3396\tClassification Loss: 1.5844\r\n",
      "Train Epoch: 5 [129920/209539 (62%)]\tAll Loss: 3.6672\tTriple Loss(0): 0.8960\tClassification Loss: 1.8752\r\n",
      "Train Epoch: 5 [130560/209539 (62%)]\tAll Loss: 2.2665\tTriple Loss(1): 0.2973\tClassification Loss: 1.6720\r\n",
      "Train Epoch: 5 [131200/209539 (63%)]\tAll Loss: 2.3120\tTriple Loss(1): 0.1709\tClassification Loss: 1.9702\r\n",
      "Train Epoch: 5 [131840/209539 (63%)]\tAll Loss: 1.8975\tTriple Loss(1): 0.2053\tClassification Loss: 1.4870\r\n",
      "Train Epoch: 5 [132480/209539 (63%)]\tAll Loss: 2.3472\tTriple Loss(1): 0.2456\tClassification Loss: 1.8560\r\n",
      "Train Epoch: 5 [133120/209539 (64%)]\tAll Loss: 3.0352\tTriple Loss(0): 0.7121\tClassification Loss: 1.6110\r\n",
      "Train Epoch: 5 [133760/209539 (64%)]\tAll Loss: 1.6583\tTriple Loss(1): 0.1296\tClassification Loss: 1.3991\r\n",
      "Train Epoch: 5 [134400/209539 (64%)]\tAll Loss: 2.8118\tTriple Loss(0): 0.5347\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 5 [135040/209539 (64%)]\tAll Loss: 2.4492\tTriple Loss(1): 0.2870\tClassification Loss: 1.8753\r\n",
      "Train Epoch: 5 [135680/209539 (65%)]\tAll Loss: 2.3802\tTriple Loss(1): 0.0926\tClassification Loss: 2.1951\r\n",
      "Train Epoch: 5 [136320/209539 (65%)]\tAll Loss: 2.0889\tTriple Loss(1): 0.2024\tClassification Loss: 1.6840\r\n",
      "Train Epoch: 5 [136960/209539 (65%)]\tAll Loss: 2.2674\tTriple Loss(1): 0.1951\tClassification Loss: 1.8773\r\n",
      "Train Epoch: 5 [137600/209539 (66%)]\tAll Loss: 2.0523\tTriple Loss(1): 0.1328\tClassification Loss: 1.7867\r\n",
      "Train Epoch: 5 [138240/209539 (66%)]\tAll Loss: 1.8932\tTriple Loss(1): 0.1586\tClassification Loss: 1.5759\r\n",
      "Train Epoch: 5 [138880/209539 (66%)]\tAll Loss: 3.2894\tTriple Loss(0): 0.7103\tClassification Loss: 1.8688\r\n",
      "Train Epoch: 5 [139520/209539 (67%)]\tAll Loss: 3.2935\tTriple Loss(0): 0.6913\tClassification Loss: 1.9108\r\n",
      "Train Epoch: 5 [140160/209539 (67%)]\tAll Loss: 2.2315\tTriple Loss(1): 0.1514\tClassification Loss: 1.9287\r\n",
      "Train Epoch: 5 [140800/209539 (67%)]\tAll Loss: 2.0504\tTriple Loss(1): 0.1763\tClassification Loss: 1.6978\r\n",
      "Train Epoch: 5 [141440/209539 (68%)]\tAll Loss: 1.6761\tTriple Loss(1): 0.1268\tClassification Loss: 1.4225\r\n",
      "Train Epoch: 5 [142080/209539 (68%)]\tAll Loss: 1.9664\tTriple Loss(1): 0.1787\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 5 [142720/209539 (68%)]\tAll Loss: 2.3657\tTriple Loss(1): 0.2159\tClassification Loss: 1.9340\r\n",
      "Train Epoch: 5 [143360/209539 (68%)]\tAll Loss: 1.8400\tTriple Loss(1): 0.2118\tClassification Loss: 1.4163\r\n",
      "Train Epoch: 5 [144000/209539 (69%)]\tAll Loss: 2.3047\tTriple Loss(1): 0.2236\tClassification Loss: 1.8575\r\n",
      "Train Epoch: 5 [144640/209539 (69%)]\tAll Loss: 2.9843\tTriple Loss(0): 0.6038\tClassification Loss: 1.7767\r\n",
      "Train Epoch: 5 [145280/209539 (69%)]\tAll Loss: 2.5299\tTriple Loss(1): 0.2481\tClassification Loss: 2.0337\r\n",
      "Train Epoch: 5 [145920/209539 (70%)]\tAll Loss: 3.1337\tTriple Loss(0): 0.6971\tClassification Loss: 1.7394\r\n",
      "Train Epoch: 5 [146560/209539 (70%)]\tAll Loss: 2.5183\tTriple Loss(1): 0.1892\tClassification Loss: 2.1398\r\n",
      "Train Epoch: 5 [147200/209539 (70%)]\tAll Loss: 2.2096\tTriple Loss(1): 0.1728\tClassification Loss: 1.8641\r\n",
      "Train Epoch: 5 [147840/209539 (71%)]\tAll Loss: 1.9731\tTriple Loss(1): 0.1477\tClassification Loss: 1.6777\r\n",
      "Train Epoch: 5 [148480/209539 (71%)]\tAll Loss: 2.2293\tTriple Loss(1): 0.2176\tClassification Loss: 1.7940\r\n",
      "Train Epoch: 5 [149120/209539 (71%)]\tAll Loss: 3.2306\tTriple Loss(0): 0.7761\tClassification Loss: 1.6783\r\n",
      "Train Epoch: 5 [149760/209539 (71%)]\tAll Loss: 2.5603\tTriple Loss(1): 0.3187\tClassification Loss: 1.9230\r\n",
      "Train Epoch: 5 [150400/209539 (72%)]\tAll Loss: 2.1626\tTriple Loss(1): 0.2363\tClassification Loss: 1.6901\r\n",
      "Train Epoch: 5 [151040/209539 (72%)]\tAll Loss: 2.2344\tTriple Loss(1): 0.1359\tClassification Loss: 1.9626\r\n",
      "Train Epoch: 5 [151680/209539 (72%)]\tAll Loss: 2.2280\tTriple Loss(1): 0.2584\tClassification Loss: 1.7111\r\n",
      "Train Epoch: 5 [152320/209539 (73%)]\tAll Loss: 2.1470\tTriple Loss(1): 0.0731\tClassification Loss: 2.0008\r\n",
      "Train Epoch: 5 [152960/209539 (73%)]\tAll Loss: 2.2743\tTriple Loss(1): 0.1452\tClassification Loss: 1.9839\r\n",
      "Train Epoch: 5 [153600/209539 (73%)]\tAll Loss: 2.4032\tTriple Loss(1): 0.3128\tClassification Loss: 1.7776\r\n",
      "Train Epoch: 5 [154240/209539 (74%)]\tAll Loss: 2.1607\tTriple Loss(1): 0.1830\tClassification Loss: 1.7948\r\n",
      "Train Epoch: 5 [154880/209539 (74%)]\tAll Loss: 1.8226\tTriple Loss(1): 0.1047\tClassification Loss: 1.6132\r\n",
      "Train Epoch: 5 [155520/209539 (74%)]\tAll Loss: 1.9619\tTriple Loss(1): 0.1710\tClassification Loss: 1.6198\r\n",
      "Train Epoch: 5 [156160/209539 (75%)]\tAll Loss: 2.4451\tTriple Loss(1): 0.1636\tClassification Loss: 2.1179\r\n",
      "Train Epoch: 5 [156800/209539 (75%)]\tAll Loss: 2.4425\tTriple Loss(1): 0.2178\tClassification Loss: 2.0069\r\n",
      "Train Epoch: 5 [157440/209539 (75%)]\tAll Loss: 2.3206\tTriple Loss(1): 0.1962\tClassification Loss: 1.9282\r\n",
      "Train Epoch: 5 [158080/209539 (75%)]\tAll Loss: 1.9829\tTriple Loss(1): 0.1587\tClassification Loss: 1.6655\r\n",
      "Train Epoch: 5 [158720/209539 (76%)]\tAll Loss: 2.0856\tTriple Loss(1): 0.1536\tClassification Loss: 1.7783\r\n",
      "Train Epoch: 5 [159360/209539 (76%)]\tAll Loss: 2.9518\tTriple Loss(0): 0.6310\tClassification Loss: 1.6898\r\n",
      "Train Epoch: 5 [160000/209539 (76%)]\tAll Loss: 2.3614\tTriple Loss(1): 0.2165\tClassification Loss: 1.9284\r\n",
      "Train Epoch: 5 [160640/209539 (77%)]\tAll Loss: 2.0294\tTriple Loss(1): 0.1742\tClassification Loss: 1.6809\r\n",
      "Train Epoch: 5 [161280/209539 (77%)]\tAll Loss: 2.1537\tTriple Loss(1): 0.2578\tClassification Loss: 1.6381\r\n",
      "Train Epoch: 5 [161920/209539 (77%)]\tAll Loss: 1.8939\tTriple Loss(1): 0.1999\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 5 [162560/209539 (78%)]\tAll Loss: 2.5423\tTriple Loss(0): 0.5510\tClassification Loss: 1.4404\r\n",
      "Train Epoch: 5 [163200/209539 (78%)]\tAll Loss: 1.9967\tTriple Loss(1): 0.1506\tClassification Loss: 1.6955\r\n",
      "Train Epoch: 5 [163840/209539 (78%)]\tAll Loss: 2.1980\tTriple Loss(1): 0.1702\tClassification Loss: 1.8576\r\n",
      "Train Epoch: 5 [164480/209539 (78%)]\tAll Loss: 2.0373\tTriple Loss(1): 0.2079\tClassification Loss: 1.6214\r\n",
      "Train Epoch: 5 [165120/209539 (79%)]\tAll Loss: 2.8411\tTriple Loss(0): 0.5983\tClassification Loss: 1.6446\r\n",
      "Train Epoch: 5 [165760/209539 (79%)]\tAll Loss: 2.7263\tTriple Loss(1): 0.4001\tClassification Loss: 1.9261\r\n",
      "Train Epoch: 5 [166400/209539 (79%)]\tAll Loss: 2.2482\tTriple Loss(1): 0.2641\tClassification Loss: 1.7201\r\n",
      "Train Epoch: 5 [167040/209539 (80%)]\tAll Loss: 2.1433\tTriple Loss(1): 0.2409\tClassification Loss: 1.6616\r\n",
      "Train Epoch: 5 [167680/209539 (80%)]\tAll Loss: 1.8593\tTriple Loss(1): 0.0943\tClassification Loss: 1.6706\r\n",
      "Train Epoch: 5 [168320/209539 (80%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.1868\tClassification Loss: 1.8034\r\n",
      "Train Epoch: 5 [168960/209539 (81%)]\tAll Loss: 2.1553\tTriple Loss(1): 0.1622\tClassification Loss: 1.8308\r\n",
      "Train Epoch: 5 [169600/209539 (81%)]\tAll Loss: 2.3921\tTriple Loss(1): 0.1873\tClassification Loss: 2.0176\r\n",
      "Train Epoch: 5 [170240/209539 (81%)]\tAll Loss: 2.1106\tTriple Loss(1): 0.2248\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 5 [170880/209539 (82%)]\tAll Loss: 1.9155\tTriple Loss(1): 0.1056\tClassification Loss: 1.7044\r\n",
      "Train Epoch: 5 [171520/209539 (82%)]\tAll Loss: 2.0999\tTriple Loss(1): 0.2029\tClassification Loss: 1.6941\r\n",
      "Train Epoch: 5 [172160/209539 (82%)]\tAll Loss: 2.0862\tTriple Loss(1): 0.1631\tClassification Loss: 1.7599\r\n",
      "Train Epoch: 5 [172800/209539 (82%)]\tAll Loss: 2.5123\tTriple Loss(1): 0.2761\tClassification Loss: 1.9601\r\n",
      "Train Epoch: 5 [173440/209539 (83%)]\tAll Loss: 1.9859\tTriple Loss(1): 0.1436\tClassification Loss: 1.6987\r\n",
      "Train Epoch: 5 [174080/209539 (83%)]\tAll Loss: 3.0447\tTriple Loss(0): 0.5853\tClassification Loss: 1.8741\r\n",
      "Train Epoch: 5 [174720/209539 (83%)]\tAll Loss: 2.1163\tTriple Loss(1): 0.1701\tClassification Loss: 1.7761\r\n",
      "Train Epoch: 5 [175360/209539 (84%)]\tAll Loss: 2.4060\tTriple Loss(1): 0.1792\tClassification Loss: 2.0475\r\n",
      "Train Epoch: 5 [176000/209539 (84%)]\tAll Loss: 3.7044\tTriple Loss(0): 1.0308\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 5 [176640/209539 (84%)]\tAll Loss: 2.4221\tTriple Loss(1): 0.2240\tClassification Loss: 1.9741\r\n",
      "Train Epoch: 5 [177280/209539 (85%)]\tAll Loss: 2.2320\tTriple Loss(1): 0.0993\tClassification Loss: 2.0333\r\n",
      "Train Epoch: 5 [177920/209539 (85%)]\tAll Loss: 2.2307\tTriple Loss(1): 0.2754\tClassification Loss: 1.6800\r\n",
      "Train Epoch: 5 [178560/209539 (85%)]\tAll Loss: 1.6809\tTriple Loss(1): 0.0996\tClassification Loss: 1.4817\r\n",
      "Train Epoch: 5 [179200/209539 (86%)]\tAll Loss: 1.8891\tTriple Loss(1): 0.1582\tClassification Loss: 1.5726\r\n",
      "Train Epoch: 5 [179840/209539 (86%)]\tAll Loss: 1.7507\tTriple Loss(1): 0.1237\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 5 [180480/209539 (86%)]\tAll Loss: 1.9301\tTriple Loss(1): 0.1201\tClassification Loss: 1.6900\r\n",
      "Train Epoch: 5 [181120/209539 (86%)]\tAll Loss: 2.5432\tTriple Loss(1): 0.3011\tClassification Loss: 1.9410\r\n",
      "Train Epoch: 5 [181760/209539 (87%)]\tAll Loss: 2.3624\tTriple Loss(1): 0.2754\tClassification Loss: 1.8115\r\n",
      "Train Epoch: 5 [182400/209539 (87%)]\tAll Loss: 1.9256\tTriple Loss(1): 0.1029\tClassification Loss: 1.7198\r\n",
      "Train Epoch: 5 [183040/209539 (87%)]\tAll Loss: 1.9092\tTriple Loss(1): 0.0651\tClassification Loss: 1.7790\r\n",
      "Train Epoch: 5 [183680/209539 (88%)]\tAll Loss: 2.2418\tTriple Loss(1): 0.2733\tClassification Loss: 1.6952\r\n",
      "Train Epoch: 5 [184320/209539 (88%)]\tAll Loss: 2.3486\tTriple Loss(1): 0.2868\tClassification Loss: 1.7750\r\n",
      "Train Epoch: 5 [184960/209539 (88%)]\tAll Loss: 1.9630\tTriple Loss(1): 0.1404\tClassification Loss: 1.6823\r\n",
      "Train Epoch: 5 [185600/209539 (89%)]\tAll Loss: 2.1554\tTriple Loss(1): 0.1765\tClassification Loss: 1.8024\r\n",
      "Train Epoch: 5 [186240/209539 (89%)]\tAll Loss: 2.0169\tTriple Loss(1): 0.0939\tClassification Loss: 1.8291\r\n",
      "Train Epoch: 5 [186880/209539 (89%)]\tAll Loss: 2.2420\tTriple Loss(1): 0.3529\tClassification Loss: 1.5362\r\n",
      "Train Epoch: 5 [187520/209539 (89%)]\tAll Loss: 1.8741\tTriple Loss(1): 0.1191\tClassification Loss: 1.6359\r\n",
      "Train Epoch: 5 [188160/209539 (90%)]\tAll Loss: 2.2821\tTriple Loss(1): 0.2546\tClassification Loss: 1.7728\r\n",
      "Train Epoch: 5 [188800/209539 (90%)]\tAll Loss: 2.0342\tTriple Loss(1): 0.1137\tClassification Loss: 1.8068\r\n",
      "Train Epoch: 5 [189440/209539 (90%)]\tAll Loss: 2.1535\tTriple Loss(1): 0.2008\tClassification Loss: 1.7518\r\n",
      "Train Epoch: 5 [190080/209539 (91%)]\tAll Loss: 1.7344\tTriple Loss(1): 0.0848\tClassification Loss: 1.5649\r\n",
      "Train Epoch: 5 [190720/209539 (91%)]\tAll Loss: 2.0025\tTriple Loss(1): 0.0926\tClassification Loss: 1.8172\r\n",
      "Train Epoch: 5 [191360/209539 (91%)]\tAll Loss: 3.3906\tTriple Loss(0): 0.7760\tClassification Loss: 1.8386\r\n",
      "Train Epoch: 5 [192000/209539 (92%)]\tAll Loss: 2.6204\tTriple Loss(1): 0.3469\tClassification Loss: 1.9266\r\n",
      "Train Epoch: 5 [192640/209539 (92%)]\tAll Loss: 2.5839\tTriple Loss(1): 0.2609\tClassification Loss: 2.0622\r\n",
      "Train Epoch: 5 [193280/209539 (92%)]\tAll Loss: 2.0871\tTriple Loss(1): 0.1489\tClassification Loss: 1.7893\r\n",
      "Train Epoch: 5 [193920/209539 (93%)]\tAll Loss: 1.9731\tTriple Loss(1): 0.1979\tClassification Loss: 1.5773\r\n",
      "Train Epoch: 5 [194560/209539 (93%)]\tAll Loss: 2.0238\tTriple Loss(1): 0.1528\tClassification Loss: 1.7183\r\n",
      "Train Epoch: 5 [195200/209539 (93%)]\tAll Loss: 1.9145\tTriple Loss(1): 0.0940\tClassification Loss: 1.7266\r\n",
      "Train Epoch: 5 [195840/209539 (93%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.2000\tClassification Loss: 1.7426\r\n",
      "Train Epoch: 5 [196480/209539 (94%)]\tAll Loss: 2.2475\tTriple Loss(1): 0.1749\tClassification Loss: 1.8977\r\n",
      "Train Epoch: 5 [197120/209539 (94%)]\tAll Loss: 2.1409\tTriple Loss(1): 0.1716\tClassification Loss: 1.7977\r\n",
      "Train Epoch: 5 [197760/209539 (94%)]\tAll Loss: 1.9346\tTriple Loss(1): 0.0875\tClassification Loss: 1.7595\r\n",
      "Train Epoch: 5 [198400/209539 (95%)]\tAll Loss: 2.0647\tTriple Loss(1): 0.2717\tClassification Loss: 1.5213\r\n",
      "Train Epoch: 5 [199040/209539 (95%)]\tAll Loss: 1.9336\tTriple Loss(1): 0.0799\tClassification Loss: 1.7738\r\n",
      "Train Epoch: 5 [199680/209539 (95%)]\tAll Loss: 2.5305\tTriple Loss(1): 0.2848\tClassification Loss: 1.9608\r\n",
      "Train Epoch: 5 [200320/209539 (96%)]\tAll Loss: 2.1859\tTriple Loss(1): 0.2123\tClassification Loss: 1.7613\r\n",
      "Train Epoch: 5 [200960/209539 (96%)]\tAll Loss: 2.4289\tTriple Loss(1): 0.3273\tClassification Loss: 1.7743\r\n",
      "Train Epoch: 5 [201600/209539 (96%)]\tAll Loss: 2.3056\tTriple Loss(1): 0.2266\tClassification Loss: 1.8524\r\n",
      "Train Epoch: 5 [202240/209539 (97%)]\tAll Loss: 1.7500\tTriple Loss(1): 0.0642\tClassification Loss: 1.6215\r\n",
      "Train Epoch: 5 [202880/209539 (97%)]\tAll Loss: 1.9543\tTriple Loss(1): 0.1281\tClassification Loss: 1.6981\r\n",
      "Train Epoch: 5 [203520/209539 (97%)]\tAll Loss: 2.3106\tTriple Loss(1): 0.1866\tClassification Loss: 1.9374\r\n",
      "Train Epoch: 5 [204160/209539 (97%)]\tAll Loss: 3.3852\tTriple Loss(0): 0.6848\tClassification Loss: 2.0155\r\n",
      "Train Epoch: 5 [204800/209539 (98%)]\tAll Loss: 2.9108\tTriple Loss(0): 0.6566\tClassification Loss: 1.5976\r\n",
      "Train Epoch: 5 [205440/209539 (98%)]\tAll Loss: 1.7288\tTriple Loss(1): 0.0896\tClassification Loss: 1.5496\r\n",
      "Train Epoch: 5 [206080/209539 (98%)]\tAll Loss: 3.3873\tTriple Loss(0): 0.7638\tClassification Loss: 1.8596\r\n",
      "Train Epoch: 5 [206720/209539 (99%)]\tAll Loss: 3.3057\tTriple Loss(0): 0.7594\tClassification Loss: 1.7869\r\n",
      "Train Epoch: 5 [207360/209539 (99%)]\tAll Loss: 3.5877\tTriple Loss(0): 0.9557\tClassification Loss: 1.6764\r\n",
      "Train Epoch: 5 [208000/209539 (99%)]\tAll Loss: 2.0098\tTriple Loss(1): 0.1585\tClassification Loss: 1.6928\r\n",
      "Train Epoch: 5 [208640/209539 (100%)]\tAll Loss: 2.2645\tTriple Loss(1): 0.2021\tClassification Loss: 1.8602\r\n",
      "Train Epoch: 5 [209280/209539 (100%)]\tAll Loss: 2.3080\tTriple Loss(1): 0.1843\tClassification Loss: 1.9394\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/5_epochs\r\n",
      "Train Epoch: 6 [0/209539 (0%)]\tAll Loss: 2.6026\tTriple Loss(1): 0.2723\tClassification Loss: 2.0581\r\n",
      "\r\n",
      "Test set: Average loss: 1.6265, Accuracy: 42343/80128 (53%)\r\n",
      "\r\n",
      "Train Epoch: 6 [640/209539 (0%)]\tAll Loss: 2.2225\tTriple Loss(1): 0.1792\tClassification Loss: 1.8641\r\n",
      "Train Epoch: 6 [1280/209539 (1%)]\tAll Loss: 2.6748\tTriple Loss(1): 0.2670\tClassification Loss: 2.1408\r\n",
      "Train Epoch: 6 [1920/209539 (1%)]\tAll Loss: 2.0434\tTriple Loss(1): 0.1903\tClassification Loss: 1.6628\r\n",
      "Train Epoch: 6 [2560/209539 (1%)]\tAll Loss: 3.3231\tTriple Loss(0): 0.8097\tClassification Loss: 1.7038\r\n",
      "Train Epoch: 6 [3200/209539 (2%)]\tAll Loss: 2.0882\tTriple Loss(1): 0.1266\tClassification Loss: 1.8349\r\n",
      "Train Epoch: 6 [3840/209539 (2%)]\tAll Loss: 2.3270\tTriple Loss(1): 0.1500\tClassification Loss: 2.0269\r\n",
      "Train Epoch: 6 [4480/209539 (2%)]\tAll Loss: 2.3894\tTriple Loss(1): 0.2380\tClassification Loss: 1.9135\r\n",
      "Train Epoch: 6 [5120/209539 (2%)]\tAll Loss: 2.3574\tTriple Loss(1): 0.2416\tClassification Loss: 1.8743\r\n",
      "Train Epoch: 6 [5760/209539 (3%)]\tAll Loss: 1.9728\tTriple Loss(1): 0.1544\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 6 [6400/209539 (3%)]\tAll Loss: 2.2409\tTriple Loss(1): 0.1728\tClassification Loss: 1.8954\r\n",
      "Train Epoch: 6 [7040/209539 (3%)]\tAll Loss: 3.2769\tTriple Loss(0): 0.6114\tClassification Loss: 2.0541\r\n",
      "Train Epoch: 6 [7680/209539 (4%)]\tAll Loss: 2.0143\tTriple Loss(1): 0.1224\tClassification Loss: 1.7695\r\n",
      "Train Epoch: 6 [8320/209539 (4%)]\tAll Loss: 2.7886\tTriple Loss(0): 0.4821\tClassification Loss: 1.8245\r\n",
      "Train Epoch: 6 [8960/209539 (4%)]\tAll Loss: 2.2330\tTriple Loss(1): 0.2094\tClassification Loss: 1.8142\r\n",
      "Train Epoch: 6 [9600/209539 (5%)]\tAll Loss: 2.5576\tTriple Loss(1): 0.2970\tClassification Loss: 1.9637\r\n",
      "Train Epoch: 6 [10240/209539 (5%)]\tAll Loss: 3.5187\tTriple Loss(0): 0.8786\tClassification Loss: 1.7614\r\n",
      "Train Epoch: 6 [10880/209539 (5%)]\tAll Loss: 2.5286\tTriple Loss(1): 0.2158\tClassification Loss: 2.0969\r\n",
      "Train Epoch: 6 [11520/209539 (5%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.2335\tClassification Loss: 1.8625\r\n",
      "Train Epoch: 6 [12160/209539 (6%)]\tAll Loss: 2.3118\tTriple Loss(1): 0.2441\tClassification Loss: 1.8236\r\n",
      "Train Epoch: 6 [12800/209539 (6%)]\tAll Loss: 3.3105\tTriple Loss(0): 0.7689\tClassification Loss: 1.7728\r\n",
      "Train Epoch: 6 [13440/209539 (6%)]\tAll Loss: 2.4367\tTriple Loss(1): 0.3221\tClassification Loss: 1.7925\r\n",
      "Train Epoch: 6 [14080/209539 (7%)]\tAll Loss: 2.4251\tTriple Loss(1): 0.2065\tClassification Loss: 2.0121\r\n",
      "Train Epoch: 6 [14720/209539 (7%)]\tAll Loss: 2.3568\tTriple Loss(1): 0.1675\tClassification Loss: 2.0219\r\n",
      "Train Epoch: 6 [15360/209539 (7%)]\tAll Loss: 2.0424\tTriple Loss(1): 0.1304\tClassification Loss: 1.7816\r\n",
      "Train Epoch: 6 [16000/209539 (8%)]\tAll Loss: 2.1838\tTriple Loss(1): 0.1402\tClassification Loss: 1.9034\r\n",
      "Train Epoch: 6 [16640/209539 (8%)]\tAll Loss: 3.6934\tTriple Loss(0): 0.9639\tClassification Loss: 1.7657\r\n",
      "Train Epoch: 6 [17280/209539 (8%)]\tAll Loss: 2.0670\tTriple Loss(1): 0.1401\tClassification Loss: 1.7868\r\n",
      "Train Epoch: 6 [17920/209539 (9%)]\tAll Loss: 1.8081\tTriple Loss(1): 0.1800\tClassification Loss: 1.4482\r\n",
      "Train Epoch: 6 [18560/209539 (9%)]\tAll Loss: 2.2979\tTriple Loss(1): 0.1397\tClassification Loss: 2.0184\r\n",
      "Train Epoch: 6 [19200/209539 (9%)]\tAll Loss: 3.0865\tTriple Loss(0): 0.6321\tClassification Loss: 1.8223\r\n",
      "Train Epoch: 6 [19840/209539 (9%)]\tAll Loss: 2.4136\tTriple Loss(1): 0.2665\tClassification Loss: 1.8806\r\n",
      "Train Epoch: 6 [20480/209539 (10%)]\tAll Loss: 2.3132\tTriple Loss(1): 0.1286\tClassification Loss: 2.0560\r\n",
      "Train Epoch: 6 [21120/209539 (10%)]\tAll Loss: 1.9581\tTriple Loss(1): 0.1303\tClassification Loss: 1.6974\r\n",
      "Train Epoch: 6 [21760/209539 (10%)]\tAll Loss: 2.3230\tTriple Loss(1): 0.2332\tClassification Loss: 1.8566\r\n",
      "Train Epoch: 6 [22400/209539 (11%)]\tAll Loss: 2.2337\tTriple Loss(1): 0.2172\tClassification Loss: 1.7994\r\n",
      "Train Epoch: 6 [23040/209539 (11%)]\tAll Loss: 2.1310\tTriple Loss(1): 0.1452\tClassification Loss: 1.8405\r\n",
      "Train Epoch: 6 [23680/209539 (11%)]\tAll Loss: 2.1152\tTriple Loss(1): 0.2272\tClassification Loss: 1.6608\r\n",
      "Train Epoch: 6 [24320/209539 (12%)]\tAll Loss: 1.9028\tTriple Loss(1): 0.2111\tClassification Loss: 1.4806\r\n",
      "Train Epoch: 6 [24960/209539 (12%)]\tAll Loss: 1.8605\tTriple Loss(1): 0.1358\tClassification Loss: 1.5889\r\n",
      "Train Epoch: 6 [25600/209539 (12%)]\tAll Loss: 1.7465\tTriple Loss(1): 0.0690\tClassification Loss: 1.6086\r\n",
      "Train Epoch: 6 [26240/209539 (13%)]\tAll Loss: 2.7731\tTriple Loss(1): 0.4466\tClassification Loss: 1.8799\r\n",
      "Train Epoch: 6 [26880/209539 (13%)]\tAll Loss: 2.4547\tTriple Loss(1): 0.3030\tClassification Loss: 1.8486\r\n",
      "Train Epoch: 6 [27520/209539 (13%)]\tAll Loss: 2.3946\tTriple Loss(1): 0.3323\tClassification Loss: 1.7300\r\n",
      "Train Epoch: 6 [28160/209539 (13%)]\tAll Loss: 3.3767\tTriple Loss(0): 0.8607\tClassification Loss: 1.6554\r\n",
      "Train Epoch: 6 [28800/209539 (14%)]\tAll Loss: 2.2179\tTriple Loss(1): 0.1966\tClassification Loss: 1.8246\r\n",
      "Train Epoch: 6 [29440/209539 (14%)]\tAll Loss: 2.4323\tTriple Loss(1): 0.2393\tClassification Loss: 1.9537\r\n",
      "Train Epoch: 6 [30080/209539 (14%)]\tAll Loss: 2.1618\tTriple Loss(1): 0.2477\tClassification Loss: 1.6664\r\n",
      "Train Epoch: 6 [30720/209539 (15%)]\tAll Loss: 2.2171\tTriple Loss(1): 0.1316\tClassification Loss: 1.9538\r\n",
      "Train Epoch: 6 [31360/209539 (15%)]\tAll Loss: 2.2760\tTriple Loss(1): 0.1663\tClassification Loss: 1.9434\r\n",
      "Train Epoch: 6 [32000/209539 (15%)]\tAll Loss: 2.2464\tTriple Loss(1): 0.1411\tClassification Loss: 1.9642\r\n",
      "Train Epoch: 6 [32640/209539 (16%)]\tAll Loss: 2.3433\tTriple Loss(1): 0.1409\tClassification Loss: 2.0615\r\n",
      "Train Epoch: 6 [33280/209539 (16%)]\tAll Loss: 1.9792\tTriple Loss(1): 0.1101\tClassification Loss: 1.7589\r\n",
      "Train Epoch: 6 [33920/209539 (16%)]\tAll Loss: 2.1760\tTriple Loss(1): 0.1558\tClassification Loss: 1.8645\r\n",
      "Train Epoch: 6 [34560/209539 (16%)]\tAll Loss: 2.0410\tTriple Loss(1): 0.1229\tClassification Loss: 1.7952\r\n",
      "Train Epoch: 6 [35200/209539 (17%)]\tAll Loss: 2.3975\tTriple Loss(1): 0.1716\tClassification Loss: 2.0544\r\n",
      "Train Epoch: 6 [35840/209539 (17%)]\tAll Loss: 2.1724\tTriple Loss(1): 0.1856\tClassification Loss: 1.8011\r\n",
      "Train Epoch: 6 [36480/209539 (17%)]\tAll Loss: 1.9226\tTriple Loss(1): 0.1287\tClassification Loss: 1.6652\r\n",
      "Train Epoch: 6 [37120/209539 (18%)]\tAll Loss: 2.1309\tTriple Loss(1): 0.1676\tClassification Loss: 1.7958\r\n",
      "Train Epoch: 6 [37760/209539 (18%)]\tAll Loss: 2.5327\tTriple Loss(1): 0.3728\tClassification Loss: 1.7871\r\n",
      "Train Epoch: 6 [38400/209539 (18%)]\tAll Loss: 1.6624\tTriple Loss(1): 0.1615\tClassification Loss: 1.3394\r\n",
      "Train Epoch: 6 [39040/209539 (19%)]\tAll Loss: 2.0345\tTriple Loss(1): 0.1598\tClassification Loss: 1.7148\r\n",
      "Train Epoch: 6 [39680/209539 (19%)]\tAll Loss: 1.8435\tTriple Loss(1): 0.1224\tClassification Loss: 1.5987\r\n",
      "Train Epoch: 6 [40320/209539 (19%)]\tAll Loss: 2.3824\tTriple Loss(1): 0.1425\tClassification Loss: 2.0974\r\n",
      "Train Epoch: 6 [40960/209539 (20%)]\tAll Loss: 2.1189\tTriple Loss(1): 0.1222\tClassification Loss: 1.8744\r\n",
      "Train Epoch: 6 [41600/209539 (20%)]\tAll Loss: 2.4365\tTriple Loss(1): 0.1566\tClassification Loss: 2.1234\r\n",
      "Train Epoch: 6 [42240/209539 (20%)]\tAll Loss: 2.1120\tTriple Loss(1): 0.1722\tClassification Loss: 1.7676\r\n",
      "Train Epoch: 6 [42880/209539 (20%)]\tAll Loss: 3.0191\tTriple Loss(0): 0.6457\tClassification Loss: 1.7276\r\n",
      "Train Epoch: 6 [43520/209539 (21%)]\tAll Loss: 2.4370\tTriple Loss(1): 0.2212\tClassification Loss: 1.9945\r\n",
      "Train Epoch: 6 [44160/209539 (21%)]\tAll Loss: 2.1044\tTriple Loss(1): 0.1741\tClassification Loss: 1.7562\r\n",
      "Train Epoch: 6 [44800/209539 (21%)]\tAll Loss: 2.1951\tTriple Loss(1): 0.1739\tClassification Loss: 1.8474\r\n",
      "Train Epoch: 6 [45440/209539 (22%)]\tAll Loss: 1.8036\tTriple Loss(1): 0.1528\tClassification Loss: 1.4980\r\n",
      "Train Epoch: 6 [46080/209539 (22%)]\tAll Loss: 2.4223\tTriple Loss(1): 0.2243\tClassification Loss: 1.9738\r\n",
      "Train Epoch: 6 [46720/209539 (22%)]\tAll Loss: 3.3063\tTriple Loss(0): 0.8084\tClassification Loss: 1.6894\r\n",
      "Train Epoch: 6 [47360/209539 (23%)]\tAll Loss: 2.3884\tTriple Loss(1): 0.1869\tClassification Loss: 2.0147\r\n",
      "Train Epoch: 6 [48000/209539 (23%)]\tAll Loss: 2.4050\tTriple Loss(1): 0.2173\tClassification Loss: 1.9704\r\n",
      "Train Epoch: 6 [48640/209539 (23%)]\tAll Loss: 2.2785\tTriple Loss(1): 0.2639\tClassification Loss: 1.7507\r\n",
      "Train Epoch: 6 [49280/209539 (24%)]\tAll Loss: 2.0648\tTriple Loss(1): 0.1536\tClassification Loss: 1.7575\r\n",
      "Train Epoch: 6 [49920/209539 (24%)]\tAll Loss: 2.4250\tTriple Loss(1): 0.2072\tClassification Loss: 2.0105\r\n",
      "Train Epoch: 6 [50560/209539 (24%)]\tAll Loss: 3.2622\tTriple Loss(0): 0.7908\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 6 [51200/209539 (24%)]\tAll Loss: 2.2536\tTriple Loss(1): 0.1566\tClassification Loss: 1.9404\r\n",
      "Train Epoch: 6 [51840/209539 (25%)]\tAll Loss: 2.1548\tTriple Loss(1): 0.2532\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 6 [52480/209539 (25%)]\tAll Loss: 2.5813\tTriple Loss(1): 0.2297\tClassification Loss: 2.1219\r\n",
      "Train Epoch: 6 [53120/209539 (25%)]\tAll Loss: 1.9266\tTriple Loss(1): 0.1553\tClassification Loss: 1.6161\r\n",
      "Train Epoch: 6 [53760/209539 (26%)]\tAll Loss: 2.2340\tTriple Loss(1): 0.1716\tClassification Loss: 1.8909\r\n",
      "Train Epoch: 6 [54400/209539 (26%)]\tAll Loss: 3.3715\tTriple Loss(0): 0.6771\tClassification Loss: 2.0173\r\n",
      "Train Epoch: 6 [55040/209539 (26%)]\tAll Loss: 3.2484\tTriple Loss(0): 0.9116\tClassification Loss: 1.4252\r\n",
      "Train Epoch: 6 [55680/209539 (27%)]\tAll Loss: 2.3897\tTriple Loss(1): 0.2773\tClassification Loss: 1.8350\r\n",
      "Train Epoch: 6 [56320/209539 (27%)]\tAll Loss: 1.9743\tTriple Loss(1): 0.1084\tClassification Loss: 1.7575\r\n",
      "Train Epoch: 6 [56960/209539 (27%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.1521\tClassification Loss: 1.8729\r\n",
      "Train Epoch: 6 [57600/209539 (27%)]\tAll Loss: 1.9028\tTriple Loss(1): 0.1264\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 6 [58240/209539 (28%)]\tAll Loss: 1.9756\tTriple Loss(1): 0.1531\tClassification Loss: 1.6695\r\n",
      "Train Epoch: 6 [58880/209539 (28%)]\tAll Loss: 2.7704\tTriple Loss(0): 0.5334\tClassification Loss: 1.7036\r\n",
      "Train Epoch: 6 [59520/209539 (28%)]\tAll Loss: 2.5127\tTriple Loss(1): 0.2734\tClassification Loss: 1.9659\r\n",
      "Train Epoch: 6 [60160/209539 (29%)]\tAll Loss: 2.0450\tTriple Loss(1): 0.1692\tClassification Loss: 1.7066\r\n",
      "Train Epoch: 6 [60800/209539 (29%)]\tAll Loss: 2.0493\tTriple Loss(1): 0.1680\tClassification Loss: 1.7133\r\n",
      "Train Epoch: 6 [61440/209539 (29%)]\tAll Loss: 3.4230\tTriple Loss(0): 0.8950\tClassification Loss: 1.6330\r\n",
      "Train Epoch: 6 [62080/209539 (30%)]\tAll Loss: 2.1779\tTriple Loss(1): 0.1643\tClassification Loss: 1.8493\r\n",
      "Train Epoch: 6 [62720/209539 (30%)]\tAll Loss: 2.7627\tTriple Loss(1): 0.3153\tClassification Loss: 2.1320\r\n",
      "Train Epoch: 6 [63360/209539 (30%)]\tAll Loss: 2.1694\tTriple Loss(1): 0.1393\tClassification Loss: 1.8908\r\n",
      "Train Epoch: 6 [64000/209539 (31%)]\tAll Loss: 2.2459\tTriple Loss(1): 0.2014\tClassification Loss: 1.8431\r\n",
      "Train Epoch: 6 [64640/209539 (31%)]\tAll Loss: 2.3713\tTriple Loss(1): 0.1557\tClassification Loss: 2.0599\r\n",
      "Train Epoch: 6 [65280/209539 (31%)]\tAll Loss: 2.0158\tTriple Loss(1): 0.1222\tClassification Loss: 1.7714\r\n",
      "Train Epoch: 6 [65920/209539 (31%)]\tAll Loss: 3.8410\tTriple Loss(0): 0.9113\tClassification Loss: 2.0184\r\n",
      "Train Epoch: 6 [66560/209539 (32%)]\tAll Loss: 1.8897\tTriple Loss(1): 0.1445\tClassification Loss: 1.6008\r\n",
      "Train Epoch: 6 [67200/209539 (32%)]\tAll Loss: 3.4123\tTriple Loss(0): 0.7590\tClassification Loss: 1.8943\r\n",
      "Train Epoch: 6 [67840/209539 (32%)]\tAll Loss: 3.0522\tTriple Loss(0): 0.6730\tClassification Loss: 1.7062\r\n",
      "Train Epoch: 6 [68480/209539 (33%)]\tAll Loss: 2.0524\tTriple Loss(1): 0.1715\tClassification Loss: 1.7094\r\n",
      "Train Epoch: 6 [69120/209539 (33%)]\tAll Loss: 1.6768\tTriple Loss(1): 0.0856\tClassification Loss: 1.5056\r\n",
      "Train Epoch: 6 [69760/209539 (33%)]\tAll Loss: 2.3771\tTriple Loss(1): 0.1700\tClassification Loss: 2.0372\r\n",
      "Train Epoch: 6 [70400/209539 (34%)]\tAll Loss: 2.9312\tTriple Loss(0): 0.5054\tClassification Loss: 1.9205\r\n",
      "Train Epoch: 6 [71040/209539 (34%)]\tAll Loss: 2.2172\tTriple Loss(1): 0.1724\tClassification Loss: 1.8723\r\n",
      "Train Epoch: 6 [71680/209539 (34%)]\tAll Loss: 2.2666\tTriple Loss(1): 0.2400\tClassification Loss: 1.7866\r\n",
      "Train Epoch: 6 [72320/209539 (35%)]\tAll Loss: 2.2386\tTriple Loss(1): 0.1959\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 6 [72960/209539 (35%)]\tAll Loss: 2.0270\tTriple Loss(1): 0.1102\tClassification Loss: 1.8065\r\n",
      "Train Epoch: 6 [73600/209539 (35%)]\tAll Loss: 2.2289\tTriple Loss(1): 0.2240\tClassification Loss: 1.7810\r\n",
      "Train Epoch: 6 [74240/209539 (35%)]\tAll Loss: 2.1303\tTriple Loss(1): 0.1374\tClassification Loss: 1.8554\r\n",
      "Train Epoch: 6 [74880/209539 (36%)]\tAll Loss: 2.0313\tTriple Loss(1): 0.2203\tClassification Loss: 1.5907\r\n",
      "Train Epoch: 6 [75520/209539 (36%)]\tAll Loss: 2.3098\tTriple Loss(1): 0.1879\tClassification Loss: 1.9341\r\n",
      "Train Epoch: 6 [76160/209539 (36%)]\tAll Loss: 2.1747\tTriple Loss(1): 0.1373\tClassification Loss: 1.9000\r\n",
      "Train Epoch: 6 [76800/209539 (37%)]\tAll Loss: 2.2217\tTriple Loss(1): 0.1463\tClassification Loss: 1.9291\r\n",
      "Train Epoch: 6 [77440/209539 (37%)]\tAll Loss: 2.3656\tTriple Loss(1): 0.2077\tClassification Loss: 1.9502\r\n",
      "Train Epoch: 6 [78080/209539 (37%)]\tAll Loss: 2.0630\tTriple Loss(1): 0.1656\tClassification Loss: 1.7319\r\n",
      "Train Epoch: 6 [78720/209539 (38%)]\tAll Loss: 2.4304\tTriple Loss(1): 0.1637\tClassification Loss: 2.1031\r\n",
      "Train Epoch: 6 [79360/209539 (38%)]\tAll Loss: 2.2999\tTriple Loss(1): 0.1176\tClassification Loss: 2.0647\r\n",
      "Train Epoch: 6 [80000/209539 (38%)]\tAll Loss: 3.3775\tTriple Loss(0): 0.7496\tClassification Loss: 1.8784\r\n",
      "Train Epoch: 6 [80640/209539 (38%)]\tAll Loss: 2.2347\tTriple Loss(1): 0.3102\tClassification Loss: 1.6142\r\n",
      "Train Epoch: 6 [81280/209539 (39%)]\tAll Loss: 3.1096\tTriple Loss(0): 0.5665\tClassification Loss: 1.9766\r\n",
      "Train Epoch: 6 [81920/209539 (39%)]\tAll Loss: 3.1451\tTriple Loss(0): 0.6656\tClassification Loss: 1.8140\r\n",
      "Train Epoch: 6 [82560/209539 (39%)]\tAll Loss: 3.5466\tTriple Loss(0): 0.7720\tClassification Loss: 2.0027\r\n",
      "Train Epoch: 6 [83200/209539 (40%)]\tAll Loss: 1.6863\tTriple Loss(1): 0.0922\tClassification Loss: 1.5018\r\n",
      "Train Epoch: 6 [83840/209539 (40%)]\tAll Loss: 2.0324\tTriple Loss(1): 0.0856\tClassification Loss: 1.8612\r\n",
      "Train Epoch: 6 [84480/209539 (40%)]\tAll Loss: 2.3397\tTriple Loss(1): 0.2769\tClassification Loss: 1.7858\r\n",
      "Train Epoch: 6 [85120/209539 (41%)]\tAll Loss: 1.9230\tTriple Loss(1): 0.1014\tClassification Loss: 1.7201\r\n",
      "Train Epoch: 6 [85760/209539 (41%)]\tAll Loss: 1.8828\tTriple Loss(1): 0.1157\tClassification Loss: 1.6514\r\n",
      "Train Epoch: 6 [86400/209539 (41%)]\tAll Loss: 2.2588\tTriple Loss(1): 0.0976\tClassification Loss: 2.0636\r\n",
      "Train Epoch: 6 [87040/209539 (42%)]\tAll Loss: 2.0708\tTriple Loss(1): 0.1891\tClassification Loss: 1.6925\r\n",
      "Train Epoch: 6 [87680/209539 (42%)]\tAll Loss: 3.3655\tTriple Loss(0): 0.7971\tClassification Loss: 1.7714\r\n",
      "Train Epoch: 6 [88320/209539 (42%)]\tAll Loss: 3.1524\tTriple Loss(0): 0.5766\tClassification Loss: 1.9991\r\n",
      "Train Epoch: 6 [88960/209539 (42%)]\tAll Loss: 3.0250\tTriple Loss(0): 0.6432\tClassification Loss: 1.7386\r\n",
      "Train Epoch: 6 [89600/209539 (43%)]\tAll Loss: 2.5342\tTriple Loss(1): 0.2512\tClassification Loss: 2.0318\r\n",
      "Train Epoch: 6 [90240/209539 (43%)]\tAll Loss: 2.3012\tTriple Loss(1): 0.2226\tClassification Loss: 1.8560\r\n",
      "Train Epoch: 6 [90880/209539 (43%)]\tAll Loss: 2.0257\tTriple Loss(1): 0.1345\tClassification Loss: 1.7567\r\n",
      "Train Epoch: 6 [91520/209539 (44%)]\tAll Loss: 2.1281\tTriple Loss(1): 0.1438\tClassification Loss: 1.8405\r\n",
      "Train Epoch: 6 [92160/209539 (44%)]\tAll Loss: 3.2162\tTriple Loss(0): 0.7807\tClassification Loss: 1.6548\r\n",
      "Train Epoch: 6 [92800/209539 (44%)]\tAll Loss: 1.5036\tTriple Loss(1): 0.1448\tClassification Loss: 1.2140\r\n",
      "Train Epoch: 6 [93440/209539 (45%)]\tAll Loss: 3.5874\tTriple Loss(0): 0.8305\tClassification Loss: 1.9263\r\n",
      "Train Epoch: 6 [94080/209539 (45%)]\tAll Loss: 1.9225\tTriple Loss(1): 0.1841\tClassification Loss: 1.5543\r\n",
      "Train Epoch: 6 [94720/209539 (45%)]\tAll Loss: 2.2267\tTriple Loss(1): 0.2085\tClassification Loss: 1.8097\r\n",
      "Train Epoch: 6 [95360/209539 (46%)]\tAll Loss: 3.0754\tTriple Loss(0): 0.6026\tClassification Loss: 1.8701\r\n",
      "Train Epoch: 6 [96000/209539 (46%)]\tAll Loss: 2.2964\tTriple Loss(1): 0.1854\tClassification Loss: 1.9257\r\n",
      "Train Epoch: 6 [96640/209539 (46%)]\tAll Loss: 2.0739\tTriple Loss(1): 0.1256\tClassification Loss: 1.8226\r\n",
      "Train Epoch: 6 [97280/209539 (46%)]\tAll Loss: 2.2021\tTriple Loss(1): 0.1832\tClassification Loss: 1.8357\r\n",
      "Train Epoch: 6 [97920/209539 (47%)]\tAll Loss: 2.1255\tTriple Loss(1): 0.2474\tClassification Loss: 1.6308\r\n",
      "Train Epoch: 6 [98560/209539 (47%)]\tAll Loss: 1.7789\tTriple Loss(1): 0.1466\tClassification Loss: 1.4856\r\n",
      "Train Epoch: 6 [99200/209539 (47%)]\tAll Loss: 2.2179\tTriple Loss(1): 0.1886\tClassification Loss: 1.8406\r\n",
      "Train Epoch: 6 [99840/209539 (48%)]\tAll Loss: 2.2391\tTriple Loss(1): 0.1843\tClassification Loss: 1.8706\r\n",
      "Train Epoch: 6 [100480/209539 (48%)]\tAll Loss: 3.3164\tTriple Loss(0): 0.6927\tClassification Loss: 1.9310\r\n",
      "Train Epoch: 6 [101120/209539 (48%)]\tAll Loss: 2.9807\tTriple Loss(0): 0.5491\tClassification Loss: 1.8825\r\n",
      "Train Epoch: 6 [101760/209539 (49%)]\tAll Loss: 2.0147\tTriple Loss(1): 0.0994\tClassification Loss: 1.8159\r\n",
      "Train Epoch: 6 [102400/209539 (49%)]\tAll Loss: 1.9493\tTriple Loss(1): 0.1792\tClassification Loss: 1.5910\r\n",
      "Train Epoch: 6 [103040/209539 (49%)]\tAll Loss: 3.3675\tTriple Loss(0): 0.6372\tClassification Loss: 2.0930\r\n",
      "Train Epoch: 6 [103680/209539 (49%)]\tAll Loss: 2.4309\tTriple Loss(1): 0.2497\tClassification Loss: 1.9315\r\n",
      "Train Epoch: 6 [104320/209539 (50%)]\tAll Loss: 1.7948\tTriple Loss(1): 0.0776\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 6 [104960/209539 (50%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.1189\tClassification Loss: 1.6755\r\n",
      "Train Epoch: 6 [105600/209539 (50%)]\tAll Loss: 1.8177\tTriple Loss(1): 0.1989\tClassification Loss: 1.4198\r\n",
      "Train Epoch: 6 [106240/209539 (51%)]\tAll Loss: 3.1150\tTriple Loss(0): 0.6236\tClassification Loss: 1.8679\r\n",
      "Train Epoch: 6 [106880/209539 (51%)]\tAll Loss: 1.6671\tTriple Loss(1): 0.0168\tClassification Loss: 1.6336\r\n",
      "Train Epoch: 6 [107520/209539 (51%)]\tAll Loss: 2.0346\tTriple Loss(1): 0.1111\tClassification Loss: 1.8125\r\n",
      "Train Epoch: 6 [108160/209539 (52%)]\tAll Loss: 3.2427\tTriple Loss(0): 0.8333\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 6 [108800/209539 (52%)]\tAll Loss: 2.3606\tTriple Loss(1): 0.1657\tClassification Loss: 2.0292\r\n",
      "Train Epoch: 6 [109440/209539 (52%)]\tAll Loss: 1.9556\tTriple Loss(1): 0.0380\tClassification Loss: 1.8795\r\n",
      "Train Epoch: 6 [110080/209539 (53%)]\tAll Loss: 1.9879\tTriple Loss(1): 0.1656\tClassification Loss: 1.6567\r\n",
      "Train Epoch: 6 [110720/209539 (53%)]\tAll Loss: 2.1140\tTriple Loss(1): 0.1142\tClassification Loss: 1.8857\r\n",
      "Train Epoch: 6 [111360/209539 (53%)]\tAll Loss: 2.6601\tTriple Loss(0): 0.5754\tClassification Loss: 1.5094\r\n",
      "Train Epoch: 6 [112000/209539 (53%)]\tAll Loss: 2.0937\tTriple Loss(1): 0.1321\tClassification Loss: 1.8295\r\n",
      "Train Epoch: 6 [112640/209539 (54%)]\tAll Loss: 1.9873\tTriple Loss(1): 0.2111\tClassification Loss: 1.5652\r\n",
      "Train Epoch: 6 [113280/209539 (54%)]\tAll Loss: 3.1674\tTriple Loss(0): 0.7995\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 6 [113920/209539 (54%)]\tAll Loss: 2.3221\tTriple Loss(1): 0.1838\tClassification Loss: 1.9545\r\n",
      "Train Epoch: 6 [114560/209539 (55%)]\tAll Loss: 3.4049\tTriple Loss(0): 0.7887\tClassification Loss: 1.8275\r\n",
      "Train Epoch: 6 [115200/209539 (55%)]\tAll Loss: 2.2965\tTriple Loss(1): 0.2227\tClassification Loss: 1.8512\r\n",
      "Train Epoch: 6 [115840/209539 (55%)]\tAll Loss: 2.2358\tTriple Loss(1): 0.1489\tClassification Loss: 1.9379\r\n",
      "Train Epoch: 6 [116480/209539 (56%)]\tAll Loss: 2.9886\tTriple Loss(0): 0.6088\tClassification Loss: 1.7710\r\n",
      "Train Epoch: 6 [117120/209539 (56%)]\tAll Loss: 3.1483\tTriple Loss(0): 0.6392\tClassification Loss: 1.8699\r\n",
      "Train Epoch: 6 [117760/209539 (56%)]\tAll Loss: 3.0317\tTriple Loss(0): 0.6310\tClassification Loss: 1.7698\r\n",
      "Train Epoch: 6 [118400/209539 (57%)]\tAll Loss: 2.1411\tTriple Loss(1): 0.2816\tClassification Loss: 1.5778\r\n",
      "Train Epoch: 6 [119040/209539 (57%)]\tAll Loss: 2.2823\tTriple Loss(1): 0.1589\tClassification Loss: 1.9646\r\n",
      "Train Epoch: 6 [119680/209539 (57%)]\tAll Loss: 1.9919\tTriple Loss(1): 0.1310\tClassification Loss: 1.7299\r\n",
      "Train Epoch: 6 [120320/209539 (57%)]\tAll Loss: 2.8877\tTriple Loss(0): 0.4953\tClassification Loss: 1.8971\r\n",
      "Train Epoch: 6 [120960/209539 (58%)]\tAll Loss: 1.7838\tTriple Loss(1): 0.1448\tClassification Loss: 1.4942\r\n",
      "Train Epoch: 6 [121600/209539 (58%)]\tAll Loss: 2.0603\tTriple Loss(1): 0.2059\tClassification Loss: 1.6485\r\n",
      "Train Epoch: 6 [122240/209539 (58%)]\tAll Loss: 2.6923\tTriple Loss(0): 0.4382\tClassification Loss: 1.8160\r\n",
      "Train Epoch: 6 [122880/209539 (59%)]\tAll Loss: 2.0548\tTriple Loss(1): 0.1416\tClassification Loss: 1.7716\r\n",
      "Train Epoch: 6 [123520/209539 (59%)]\tAll Loss: 2.1287\tTriple Loss(1): 0.1699\tClassification Loss: 1.7889\r\n",
      "Train Epoch: 6 [124160/209539 (59%)]\tAll Loss: 3.4285\tTriple Loss(0): 0.7572\tClassification Loss: 1.9141\r\n",
      "Train Epoch: 6 [124800/209539 (60%)]\tAll Loss: 2.2497\tTriple Loss(1): 0.2706\tClassification Loss: 1.7086\r\n",
      "Train Epoch: 6 [125440/209539 (60%)]\tAll Loss: 2.1021\tTriple Loss(1): 0.0770\tClassification Loss: 1.9482\r\n",
      "Train Epoch: 6 [126080/209539 (60%)]\tAll Loss: 2.2446\tTriple Loss(1): 0.2609\tClassification Loss: 1.7227\r\n",
      "Train Epoch: 6 [126720/209539 (60%)]\tAll Loss: 2.0638\tTriple Loss(1): 0.1835\tClassification Loss: 1.6967\r\n",
      "Train Epoch: 6 [127360/209539 (61%)]\tAll Loss: 3.5723\tTriple Loss(0): 0.8737\tClassification Loss: 1.8249\r\n",
      "Train Epoch: 6 [128000/209539 (61%)]\tAll Loss: 2.0311\tTriple Loss(1): 0.1706\tClassification Loss: 1.6899\r\n",
      "Train Epoch: 6 [128640/209539 (61%)]\tAll Loss: 3.4117\tTriple Loss(0): 0.8474\tClassification Loss: 1.7169\r\n",
      "Train Epoch: 6 [129280/209539 (62%)]\tAll Loss: 2.0835\tTriple Loss(1): 0.2184\tClassification Loss: 1.6466\r\n",
      "Train Epoch: 6 [129920/209539 (62%)]\tAll Loss: 2.2224\tTriple Loss(1): 0.1344\tClassification Loss: 1.9536\r\n",
      "Train Epoch: 6 [130560/209539 (62%)]\tAll Loss: 2.0551\tTriple Loss(1): 0.1493\tClassification Loss: 1.7565\r\n",
      "Train Epoch: 6 [131200/209539 (63%)]\tAll Loss: 2.3945\tTriple Loss(1): 0.2552\tClassification Loss: 1.8841\r\n",
      "Train Epoch: 6 [131840/209539 (63%)]\tAll Loss: 2.0970\tTriple Loss(1): 0.1997\tClassification Loss: 1.6975\r\n",
      "Train Epoch: 6 [132480/209539 (63%)]\tAll Loss: 2.5474\tTriple Loss(1): 0.2893\tClassification Loss: 1.9688\r\n",
      "Train Epoch: 6 [133120/209539 (64%)]\tAll Loss: 1.8760\tTriple Loss(1): 0.1658\tClassification Loss: 1.5443\r\n",
      "Train Epoch: 6 [133760/209539 (64%)]\tAll Loss: 2.9155\tTriple Loss(0): 0.6694\tClassification Loss: 1.5766\r\n",
      "Train Epoch: 6 [134400/209539 (64%)]\tAll Loss: 3.3837\tTriple Loss(0): 0.7145\tClassification Loss: 1.9546\r\n",
      "Train Epoch: 6 [135040/209539 (64%)]\tAll Loss: 2.2982\tTriple Loss(1): 0.1756\tClassification Loss: 1.9470\r\n",
      "Train Epoch: 6 [135680/209539 (65%)]\tAll Loss: 3.5115\tTriple Loss(0): 0.7107\tClassification Loss: 2.0901\r\n",
      "Train Epoch: 6 [136320/209539 (65%)]\tAll Loss: 2.0248\tTriple Loss(1): 0.1032\tClassification Loss: 1.8184\r\n",
      "Train Epoch: 6 [136960/209539 (65%)]\tAll Loss: 2.1917\tTriple Loss(1): 0.1872\tClassification Loss: 1.8173\r\n",
      "Train Epoch: 6 [137600/209539 (66%)]\tAll Loss: 2.6694\tTriple Loss(0): 0.3522\tClassification Loss: 1.9650\r\n",
      "Train Epoch: 6 [138240/209539 (66%)]\tAll Loss: 3.4503\tTriple Loss(0): 0.8171\tClassification Loss: 1.8162\r\n",
      "Train Epoch: 6 [138880/209539 (66%)]\tAll Loss: 2.2044\tTriple Loss(1): 0.1483\tClassification Loss: 1.9079\r\n",
      "Train Epoch: 6 [139520/209539 (67%)]\tAll Loss: 2.4156\tTriple Loss(1): 0.2854\tClassification Loss: 1.8448\r\n",
      "Train Epoch: 6 [140160/209539 (67%)]\tAll Loss: 2.1711\tTriple Loss(1): 0.1056\tClassification Loss: 1.9600\r\n",
      "Train Epoch: 6 [140800/209539 (67%)]\tAll Loss: 2.9146\tTriple Loss(0): 0.5643\tClassification Loss: 1.7859\r\n",
      "Train Epoch: 6 [141440/209539 (68%)]\tAll Loss: 2.1124\tTriple Loss(1): 0.1828\tClassification Loss: 1.7468\r\n",
      "Train Epoch: 6 [142080/209539 (68%)]\tAll Loss: 2.0766\tTriple Loss(1): 0.1900\tClassification Loss: 1.6965\r\n",
      "Train Epoch: 6 [142720/209539 (68%)]\tAll Loss: 2.2434\tTriple Loss(1): 0.1364\tClassification Loss: 1.9705\r\n",
      "Train Epoch: 6 [143360/209539 (68%)]\tAll Loss: 2.6877\tTriple Loss(0): 0.6631\tClassification Loss: 1.3614\r\n",
      "Train Epoch: 6 [144000/209539 (69%)]\tAll Loss: 2.2111\tTriple Loss(1): 0.1106\tClassification Loss: 1.9899\r\n",
      "Train Epoch: 6 [144640/209539 (69%)]\tAll Loss: 2.0672\tTriple Loss(1): 0.1777\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 6 [145280/209539 (69%)]\tAll Loss: 2.3446\tTriple Loss(1): 0.1791\tClassification Loss: 1.9865\r\n",
      "Train Epoch: 6 [145920/209539 (70%)]\tAll Loss: 3.0531\tTriple Loss(0): 0.6451\tClassification Loss: 1.7628\r\n",
      "Train Epoch: 6 [146560/209539 (70%)]\tAll Loss: 3.7760\tTriple Loss(0): 0.7425\tClassification Loss: 2.2910\r\n",
      "Train Epoch: 6 [147200/209539 (70%)]\tAll Loss: 2.3467\tTriple Loss(1): 0.1521\tClassification Loss: 2.0426\r\n",
      "Train Epoch: 6 [147840/209539 (71%)]\tAll Loss: 1.9743\tTriple Loss(1): 0.0989\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 6 [148480/209539 (71%)]\tAll Loss: 1.8474\tTriple Loss(1): 0.1441\tClassification Loss: 1.5593\r\n",
      "Train Epoch: 6 [149120/209539 (71%)]\tAll Loss: 2.1363\tTriple Loss(1): 0.2035\tClassification Loss: 1.7293\r\n",
      "Train Epoch: 6 [149760/209539 (71%)]\tAll Loss: 2.3059\tTriple Loss(1): 0.0970\tClassification Loss: 2.1120\r\n",
      "Train Epoch: 6 [150400/209539 (72%)]\tAll Loss: 2.1556\tTriple Loss(1): 0.1163\tClassification Loss: 1.9231\r\n",
      "Train Epoch: 6 [151040/209539 (72%)]\tAll Loss: 2.4535\tTriple Loss(1): 0.2668\tClassification Loss: 1.9199\r\n",
      "Train Epoch: 6 [151680/209539 (72%)]\tAll Loss: 2.1405\tTriple Loss(1): 0.1418\tClassification Loss: 1.8568\r\n",
      "Train Epoch: 6 [152320/209539 (73%)]\tAll Loss: 2.7837\tTriple Loss(0): 0.4801\tClassification Loss: 1.8235\r\n",
      "Train Epoch: 6 [152960/209539 (73%)]\tAll Loss: 2.1210\tTriple Loss(1): 0.1592\tClassification Loss: 1.8026\r\n",
      "Train Epoch: 6 [153600/209539 (73%)]\tAll Loss: 2.1046\tTriple Loss(1): 0.2112\tClassification Loss: 1.6822\r\n",
      "Train Epoch: 6 [154240/209539 (74%)]\tAll Loss: 2.0712\tTriple Loss(1): 0.1158\tClassification Loss: 1.8396\r\n",
      "Train Epoch: 6 [154880/209539 (74%)]\tAll Loss: 2.1744\tTriple Loss(1): 0.2579\tClassification Loss: 1.6587\r\n",
      "Train Epoch: 6 [155520/209539 (74%)]\tAll Loss: 3.0194\tTriple Loss(0): 0.7101\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 6 [156160/209539 (75%)]\tAll Loss: 2.4629\tTriple Loss(1): 0.1380\tClassification Loss: 2.1869\r\n",
      "Train Epoch: 6 [156800/209539 (75%)]\tAll Loss: 2.4159\tTriple Loss(1): 0.2465\tClassification Loss: 1.9229\r\n",
      "Train Epoch: 6 [157440/209539 (75%)]\tAll Loss: 2.5627\tTriple Loss(1): 0.2611\tClassification Loss: 2.0406\r\n",
      "Train Epoch: 6 [158080/209539 (75%)]\tAll Loss: 1.9390\tTriple Loss(1): 0.1550\tClassification Loss: 1.6291\r\n",
      "Train Epoch: 6 [158720/209539 (76%)]\tAll Loss: 2.0766\tTriple Loss(1): 0.1853\tClassification Loss: 1.7059\r\n",
      "Train Epoch: 6 [159360/209539 (76%)]\tAll Loss: 2.1449\tTriple Loss(1): 0.1550\tClassification Loss: 1.8349\r\n",
      "Train Epoch: 6 [160000/209539 (76%)]\tAll Loss: 2.4105\tTriple Loss(1): 0.1861\tClassification Loss: 2.0383\r\n",
      "Train Epoch: 6 [160640/209539 (77%)]\tAll Loss: 2.2047\tTriple Loss(1): 0.2440\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 6 [161280/209539 (77%)]\tAll Loss: 1.7903\tTriple Loss(1): 0.1198\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 6 [161920/209539 (77%)]\tAll Loss: 3.1065\tTriple Loss(0): 0.7770\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 6 [162560/209539 (78%)]\tAll Loss: 1.9079\tTriple Loss(1): 0.1863\tClassification Loss: 1.5353\r\n",
      "Train Epoch: 6 [163200/209539 (78%)]\tAll Loss: 2.0935\tTriple Loss(1): 0.1510\tClassification Loss: 1.7916\r\n",
      "Train Epoch: 6 [163840/209539 (78%)]\tAll Loss: 2.1019\tTriple Loss(1): 0.0933\tClassification Loss: 1.9153\r\n",
      "Train Epoch: 6 [164480/209539 (78%)]\tAll Loss: 3.0741\tTriple Loss(0): 0.6516\tClassification Loss: 1.7708\r\n",
      "Train Epoch: 6 [165120/209539 (79%)]\tAll Loss: 1.8978\tTriple Loss(1): 0.1411\tClassification Loss: 1.6156\r\n",
      "Train Epoch: 6 [165760/209539 (79%)]\tAll Loss: 3.1673\tTriple Loss(0): 0.5913\tClassification Loss: 1.9846\r\n",
      "Train Epoch: 6 [166400/209539 (79%)]\tAll Loss: 2.1708\tTriple Loss(1): 0.1639\tClassification Loss: 1.8430\r\n",
      "Train Epoch: 6 [167040/209539 (80%)]\tAll Loss: 1.9253\tTriple Loss(1): 0.1732\tClassification Loss: 1.5789\r\n",
      "Train Epoch: 6 [167680/209539 (80%)]\tAll Loss: 1.7854\tTriple Loss(1): 0.0661\tClassification Loss: 1.6532\r\n",
      "Train Epoch: 6 [168320/209539 (80%)]\tAll Loss: 3.6915\tTriple Loss(0): 0.8814\tClassification Loss: 1.9288\r\n",
      "Train Epoch: 6 [168960/209539 (81%)]\tAll Loss: 2.2616\tTriple Loss(1): 0.2718\tClassification Loss: 1.7180\r\n",
      "Train Epoch: 6 [169600/209539 (81%)]\tAll Loss: 2.1685\tTriple Loss(1): 0.1690\tClassification Loss: 1.8306\r\n",
      "Train Epoch: 6 [170240/209539 (81%)]\tAll Loss: 2.0419\tTriple Loss(1): 0.2204\tClassification Loss: 1.6011\r\n",
      "Train Epoch: 6 [170880/209539 (82%)]\tAll Loss: 2.1162\tTriple Loss(1): 0.1993\tClassification Loss: 1.7176\r\n",
      "Train Epoch: 6 [171520/209539 (82%)]\tAll Loss: 3.1570\tTriple Loss(0): 0.7291\tClassification Loss: 1.6989\r\n",
      "Train Epoch: 6 [172160/209539 (82%)]\tAll Loss: 1.9994\tTriple Loss(1): 0.0942\tClassification Loss: 1.8110\r\n",
      "Train Epoch: 6 [172800/209539 (82%)]\tAll Loss: 2.2279\tTriple Loss(1): 0.0928\tClassification Loss: 2.0423\r\n",
      "Train Epoch: 6 [173440/209539 (83%)]\tAll Loss: 3.3267\tTriple Loss(0): 0.7270\tClassification Loss: 1.8728\r\n",
      "Train Epoch: 6 [174080/209539 (83%)]\tAll Loss: 3.8415\tTriple Loss(0): 0.9716\tClassification Loss: 1.8984\r\n",
      "Train Epoch: 6 [174720/209539 (83%)]\tAll Loss: 2.1749\tTriple Loss(1): 0.2952\tClassification Loss: 1.5845\r\n",
      "Train Epoch: 6 [175360/209539 (84%)]\tAll Loss: 2.3757\tTriple Loss(1): 0.1849\tClassification Loss: 2.0060\r\n",
      "Train Epoch: 6 [176000/209539 (84%)]\tAll Loss: 3.4153\tTriple Loss(0): 0.8998\tClassification Loss: 1.6157\r\n",
      "Train Epoch: 6 [176640/209539 (84%)]\tAll Loss: 2.1207\tTriple Loss(1): 0.1171\tClassification Loss: 1.8865\r\n",
      "Train Epoch: 6 [177280/209539 (85%)]\tAll Loss: 2.2050\tTriple Loss(1): 0.1593\tClassification Loss: 1.8864\r\n",
      "Train Epoch: 6 [177920/209539 (85%)]\tAll Loss: 1.9987\tTriple Loss(1): 0.1839\tClassification Loss: 1.6309\r\n",
      "Train Epoch: 6 [178560/209539 (85%)]\tAll Loss: 2.1327\tTriple Loss(1): 0.2883\tClassification Loss: 1.5560\r\n",
      "Train Epoch: 6 [179200/209539 (86%)]\tAll Loss: 1.9421\tTriple Loss(1): 0.2354\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 6 [179840/209539 (86%)]\tAll Loss: 1.7554\tTriple Loss(1): 0.0899\tClassification Loss: 1.5756\r\n",
      "Train Epoch: 6 [180480/209539 (86%)]\tAll Loss: 2.1961\tTriple Loss(1): 0.1789\tClassification Loss: 1.8382\r\n",
      "Train Epoch: 6 [181120/209539 (86%)]\tAll Loss: 3.0963\tTriple Loss(0): 0.5189\tClassification Loss: 2.0585\r\n",
      "Train Epoch: 6 [181760/209539 (87%)]\tAll Loss: 2.2023\tTriple Loss(1): 0.2483\tClassification Loss: 1.7057\r\n",
      "Train Epoch: 6 [182400/209539 (87%)]\tAll Loss: 2.1627\tTriple Loss(1): 0.1874\tClassification Loss: 1.7879\r\n",
      "Train Epoch: 6 [183040/209539 (87%)]\tAll Loss: 2.3752\tTriple Loss(1): 0.2823\tClassification Loss: 1.8105\r\n",
      "Train Epoch: 6 [183680/209539 (88%)]\tAll Loss: 2.1896\tTriple Loss(1): 0.1488\tClassification Loss: 1.8920\r\n",
      "Train Epoch: 6 [184320/209539 (88%)]\tAll Loss: 1.9234\tTriple Loss(1): 0.1120\tClassification Loss: 1.6993\r\n",
      "Train Epoch: 6 [184960/209539 (88%)]\tAll Loss: 2.2804\tTriple Loss(1): 0.2433\tClassification Loss: 1.7938\r\n",
      "Train Epoch: 6 [185600/209539 (89%)]\tAll Loss: 3.4141\tTriple Loss(0): 0.8274\tClassification Loss: 1.7594\r\n",
      "Train Epoch: 6 [186240/209539 (89%)]\tAll Loss: 2.1766\tTriple Loss(1): 0.2021\tClassification Loss: 1.7724\r\n",
      "Train Epoch: 6 [186880/209539 (89%)]\tAll Loss: 1.9636\tTriple Loss(1): 0.1674\tClassification Loss: 1.6289\r\n",
      "Train Epoch: 6 [187520/209539 (89%)]\tAll Loss: 2.1421\tTriple Loss(1): 0.1435\tClassification Loss: 1.8552\r\n",
      "Train Epoch: 6 [188160/209539 (90%)]\tAll Loss: 1.9607\tTriple Loss(1): 0.1188\tClassification Loss: 1.7232\r\n",
      "Train Epoch: 6 [188800/209539 (90%)]\tAll Loss: 2.2463\tTriple Loss(1): 0.1701\tClassification Loss: 1.9062\r\n",
      "Train Epoch: 6 [189440/209539 (90%)]\tAll Loss: 2.2454\tTriple Loss(1): 0.2090\tClassification Loss: 1.8273\r\n",
      "Train Epoch: 6 [190080/209539 (91%)]\tAll Loss: 1.9831\tTriple Loss(1): 0.2405\tClassification Loss: 1.5022\r\n",
      "Train Epoch: 6 [190720/209539 (91%)]\tAll Loss: 2.0902\tTriple Loss(1): 0.1559\tClassification Loss: 1.7784\r\n",
      "Train Epoch: 6 [191360/209539 (91%)]\tAll Loss: 2.2151\tTriple Loss(1): 0.2420\tClassification Loss: 1.7311\r\n",
      "Train Epoch: 6 [192000/209539 (92%)]\tAll Loss: 2.1924\tTriple Loss(1): 0.1746\tClassification Loss: 1.8432\r\n",
      "Train Epoch: 6 [192640/209539 (92%)]\tAll Loss: 3.2285\tTriple Loss(0): 0.6871\tClassification Loss: 1.8543\r\n",
      "Train Epoch: 6 [193280/209539 (92%)]\tAll Loss: 2.1092\tTriple Loss(1): 0.1415\tClassification Loss: 1.8262\r\n",
      "Train Epoch: 6 [193920/209539 (93%)]\tAll Loss: 2.4808\tTriple Loss(1): 0.3015\tClassification Loss: 1.8778\r\n",
      "Train Epoch: 6 [194560/209539 (93%)]\tAll Loss: 1.9338\tTriple Loss(1): 0.1238\tClassification Loss: 1.6863\r\n",
      "Train Epoch: 6 [195200/209539 (93%)]\tAll Loss: 2.7905\tTriple Loss(0): 0.5509\tClassification Loss: 1.6887\r\n",
      "Train Epoch: 6 [195840/209539 (93%)]\tAll Loss: 2.0914\tTriple Loss(1): 0.2278\tClassification Loss: 1.6357\r\n",
      "Train Epoch: 6 [196480/209539 (94%)]\tAll Loss: 2.0991\tTriple Loss(1): 0.0912\tClassification Loss: 1.9168\r\n",
      "Train Epoch: 6 [197120/209539 (94%)]\tAll Loss: 2.1504\tTriple Loss(1): 0.1309\tClassification Loss: 1.8885\r\n",
      "Train Epoch: 6 [197760/209539 (94%)]\tAll Loss: 2.2445\tTriple Loss(1): 0.2028\tClassification Loss: 1.8389\r\n",
      "Train Epoch: 6 [198400/209539 (95%)]\tAll Loss: 1.8545\tTriple Loss(1): 0.1453\tClassification Loss: 1.5639\r\n",
      "Train Epoch: 6 [199040/209539 (95%)]\tAll Loss: 2.3076\tTriple Loss(1): 0.2114\tClassification Loss: 1.8848\r\n",
      "Train Epoch: 6 [199680/209539 (95%)]\tAll Loss: 3.2861\tTriple Loss(0): 0.6105\tClassification Loss: 2.0650\r\n",
      "Train Epoch: 6 [200320/209539 (96%)]\tAll Loss: 2.4239\tTriple Loss(1): 0.2135\tClassification Loss: 1.9969\r\n",
      "Train Epoch: 6 [200960/209539 (96%)]\tAll Loss: 1.9821\tTriple Loss(1): 0.1983\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 6 [201600/209539 (96%)]\tAll Loss: 2.2253\tTriple Loss(1): 0.1784\tClassification Loss: 1.8685\r\n",
      "Train Epoch: 6 [202240/209539 (97%)]\tAll Loss: 2.0924\tTriple Loss(1): 0.1744\tClassification Loss: 1.7437\r\n",
      "Train Epoch: 6 [202880/209539 (97%)]\tAll Loss: 2.0302\tTriple Loss(1): 0.2669\tClassification Loss: 1.4964\r\n",
      "Train Epoch: 6 [203520/209539 (97%)]\tAll Loss: 3.1158\tTriple Loss(0): 0.6648\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 6 [204160/209539 (97%)]\tAll Loss: 3.1739\tTriple Loss(0): 0.6539\tClassification Loss: 1.8661\r\n",
      "Train Epoch: 6 [204800/209539 (98%)]\tAll Loss: 1.8707\tTriple Loss(1): 0.1294\tClassification Loss: 1.6118\r\n",
      "Train Epoch: 6 [205440/209539 (98%)]\tAll Loss: 1.7805\tTriple Loss(1): 0.1316\tClassification Loss: 1.5173\r\n",
      "Train Epoch: 6 [206080/209539 (98%)]\tAll Loss: 2.0559\tTriple Loss(1): 0.1317\tClassification Loss: 1.7924\r\n",
      "Train Epoch: 6 [206720/209539 (99%)]\tAll Loss: 2.9925\tTriple Loss(0): 0.6357\tClassification Loss: 1.7211\r\n",
      "Train Epoch: 6 [207360/209539 (99%)]\tAll Loss: 2.2241\tTriple Loss(1): 0.3343\tClassification Loss: 1.5555\r\n",
      "Train Epoch: 6 [208000/209539 (99%)]\tAll Loss: 2.1386\tTriple Loss(1): 0.2119\tClassification Loss: 1.7149\r\n",
      "Train Epoch: 6 [208640/209539 (100%)]\tAll Loss: 3.0321\tTriple Loss(0): 0.6185\tClassification Loss: 1.7950\r\n",
      "Train Epoch: 6 [209280/209539 (100%)]\tAll Loss: 1.9915\tTriple Loss(1): 0.2041\tClassification Loss: 1.5834\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/6_epochs\r\n",
      "Train Epoch: 7 [0/209539 (0%)]\tAll Loss: 2.6293\tTriple Loss(1): 0.3116\tClassification Loss: 2.0061\r\n",
      "\r\n",
      "Test set: Average loss: 1.6220, Accuracy: 42284/80128 (53%)\r\n",
      "\r\n",
      "Train Epoch: 7 [640/209539 (0%)]\tAll Loss: 2.4279\tTriple Loss(1): 0.1830\tClassification Loss: 2.0619\r\n",
      "Train Epoch: 7 [1280/209539 (1%)]\tAll Loss: 2.5313\tTriple Loss(1): 0.1534\tClassification Loss: 2.2245\r\n",
      "Train Epoch: 7 [1920/209539 (1%)]\tAll Loss: 2.7579\tTriple Loss(0): 0.6404\tClassification Loss: 1.4772\r\n",
      "Train Epoch: 7 [2560/209539 (1%)]\tAll Loss: 2.3670\tTriple Loss(1): 0.2589\tClassification Loss: 1.8492\r\n",
      "Train Epoch: 7 [3200/209539 (2%)]\tAll Loss: 2.0795\tTriple Loss(1): 0.1480\tClassification Loss: 1.7836\r\n",
      "Train Epoch: 7 [3840/209539 (2%)]\tAll Loss: 3.1910\tTriple Loss(0): 0.6425\tClassification Loss: 1.9060\r\n",
      "Train Epoch: 7 [4480/209539 (2%)]\tAll Loss: 2.3213\tTriple Loss(1): 0.2496\tClassification Loss: 1.8221\r\n",
      "Train Epoch: 7 [5120/209539 (2%)]\tAll Loss: 3.0626\tTriple Loss(0): 0.5993\tClassification Loss: 1.8639\r\n",
      "Train Epoch: 7 [5760/209539 (3%)]\tAll Loss: 1.8692\tTriple Loss(1): 0.1230\tClassification Loss: 1.6233\r\n",
      "Train Epoch: 7 [6400/209539 (3%)]\tAll Loss: 1.9329\tTriple Loss(1): 0.1529\tClassification Loss: 1.6270\r\n",
      "Train Epoch: 7 [7040/209539 (3%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.1036\tClassification Loss: 2.0381\r\n",
      "Train Epoch: 7 [7680/209539 (4%)]\tAll Loss: 2.2250\tTriple Loss(1): 0.2379\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 7 [8320/209539 (4%)]\tAll Loss: 3.3479\tTriple Loss(0): 0.7426\tClassification Loss: 1.8627\r\n",
      "Train Epoch: 7 [8960/209539 (4%)]\tAll Loss: 2.1579\tTriple Loss(1): 0.1415\tClassification Loss: 1.8749\r\n",
      "Train Epoch: 7 [9600/209539 (5%)]\tAll Loss: 2.0230\tTriple Loss(1): 0.0901\tClassification Loss: 1.8428\r\n",
      "Train Epoch: 7 [10240/209539 (5%)]\tAll Loss: 2.2868\tTriple Loss(1): 0.1761\tClassification Loss: 1.9346\r\n",
      "Train Epoch: 7 [10880/209539 (5%)]\tAll Loss: 2.3481\tTriple Loss(1): 0.1909\tClassification Loss: 1.9663\r\n",
      "Train Epoch: 7 [11520/209539 (5%)]\tAll Loss: 1.9548\tTriple Loss(1): 0.1139\tClassification Loss: 1.7270\r\n",
      "Train Epoch: 7 [12160/209539 (6%)]\tAll Loss: 3.1839\tTriple Loss(0): 0.7334\tClassification Loss: 1.7171\r\n",
      "Train Epoch: 7 [12800/209539 (6%)]\tAll Loss: 2.1117\tTriple Loss(1): 0.1718\tClassification Loss: 1.7681\r\n",
      "Train Epoch: 7 [13440/209539 (6%)]\tAll Loss: 2.9843\tTriple Loss(0): 0.5575\tClassification Loss: 1.8694\r\n",
      "Train Epoch: 7 [14080/209539 (7%)]\tAll Loss: 2.1682\tTriple Loss(1): 0.1379\tClassification Loss: 1.8924\r\n",
      "Train Epoch: 7 [14720/209539 (7%)]\tAll Loss: 3.5618\tTriple Loss(0): 0.8189\tClassification Loss: 1.9239\r\n",
      "Train Epoch: 7 [15360/209539 (7%)]\tAll Loss: 2.1536\tTriple Loss(1): 0.1933\tClassification Loss: 1.7671\r\n",
      "Train Epoch: 7 [16000/209539 (8%)]\tAll Loss: 2.1977\tTriple Loss(1): 0.1791\tClassification Loss: 1.8395\r\n",
      "Train Epoch: 7 [16640/209539 (8%)]\tAll Loss: 2.1285\tTriple Loss(1): 0.2435\tClassification Loss: 1.6415\r\n",
      "Train Epoch: 7 [17280/209539 (8%)]\tAll Loss: 2.1370\tTriple Loss(1): 0.1999\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 7 [17920/209539 (9%)]\tAll Loss: 1.9651\tTriple Loss(1): 0.1701\tClassification Loss: 1.6249\r\n",
      "Train Epoch: 7 [18560/209539 (9%)]\tAll Loss: 3.2488\tTriple Loss(0): 0.6837\tClassification Loss: 1.8813\r\n",
      "Train Epoch: 7 [19200/209539 (9%)]\tAll Loss: 1.7976\tTriple Loss(1): 0.0614\tClassification Loss: 1.6749\r\n",
      "Train Epoch: 7 [19840/209539 (9%)]\tAll Loss: 2.1690\tTriple Loss(1): 0.1433\tClassification Loss: 1.8824\r\n",
      "Train Epoch: 7 [20480/209539 (10%)]\tAll Loss: 2.5202\tTriple Loss(1): 0.2146\tClassification Loss: 2.0909\r\n",
      "Train Epoch: 7 [21120/209539 (10%)]\tAll Loss: 3.1124\tTriple Loss(0): 0.7241\tClassification Loss: 1.6642\r\n",
      "Train Epoch: 7 [21760/209539 (10%)]\tAll Loss: 2.3366\tTriple Loss(1): 0.2301\tClassification Loss: 1.8764\r\n",
      "Train Epoch: 7 [22400/209539 (11%)]\tAll Loss: 2.1215\tTriple Loss(1): 0.2009\tClassification Loss: 1.7197\r\n",
      "Train Epoch: 7 [23040/209539 (11%)]\tAll Loss: 2.0924\tTriple Loss(1): 0.1352\tClassification Loss: 1.8220\r\n",
      "Train Epoch: 7 [23680/209539 (11%)]\tAll Loss: 1.9491\tTriple Loss(1): 0.1628\tClassification Loss: 1.6235\r\n",
      "Train Epoch: 7 [24320/209539 (12%)]\tAll Loss: 1.9625\tTriple Loss(1): 0.1133\tClassification Loss: 1.7358\r\n",
      "Train Epoch: 7 [24960/209539 (12%)]\tAll Loss: 1.9385\tTriple Loss(1): 0.0947\tClassification Loss: 1.7490\r\n",
      "Train Epoch: 7 [25600/209539 (12%)]\tAll Loss: 1.9933\tTriple Loss(1): 0.2699\tClassification Loss: 1.4535\r\n",
      "Train Epoch: 7 [26240/209539 (13%)]\tAll Loss: 3.4708\tTriple Loss(0): 0.7299\tClassification Loss: 2.0110\r\n",
      "Train Epoch: 7 [26880/209539 (13%)]\tAll Loss: 2.2934\tTriple Loss(1): 0.2156\tClassification Loss: 1.8621\r\n",
      "Train Epoch: 7 [27520/209539 (13%)]\tAll Loss: 2.3252\tTriple Loss(1): 0.3084\tClassification Loss: 1.7085\r\n",
      "Train Epoch: 7 [28160/209539 (13%)]\tAll Loss: 1.9915\tTriple Loss(1): 0.1514\tClassification Loss: 1.6888\r\n",
      "Train Epoch: 7 [28800/209539 (14%)]\tAll Loss: 2.0983\tTriple Loss(1): 0.1422\tClassification Loss: 1.8139\r\n",
      "Train Epoch: 7 [29440/209539 (14%)]\tAll Loss: 2.2173\tTriple Loss(1): 0.1684\tClassification Loss: 1.8805\r\n",
      "Train Epoch: 7 [30080/209539 (14%)]\tAll Loss: 1.9630\tTriple Loss(1): 0.1468\tClassification Loss: 1.6693\r\n",
      "Train Epoch: 7 [30720/209539 (15%)]\tAll Loss: 2.1615\tTriple Loss(1): 0.1425\tClassification Loss: 1.8766\r\n",
      "Train Epoch: 7 [31360/209539 (15%)]\tAll Loss: 2.4400\tTriple Loss(1): 0.1621\tClassification Loss: 2.1158\r\n",
      "Train Epoch: 7 [32000/209539 (15%)]\tAll Loss: 2.3606\tTriple Loss(1): 0.2415\tClassification Loss: 1.8776\r\n",
      "Train Epoch: 7 [32640/209539 (16%)]\tAll Loss: 2.3794\tTriple Loss(1): 0.1382\tClassification Loss: 2.1029\r\n",
      "Train Epoch: 7 [33280/209539 (16%)]\tAll Loss: 2.3939\tTriple Loss(1): 0.2663\tClassification Loss: 1.8614\r\n",
      "Train Epoch: 7 [33920/209539 (16%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.1832\tClassification Loss: 1.8821\r\n",
      "Train Epoch: 7 [34560/209539 (16%)]\tAll Loss: 2.3826\tTriple Loss(1): 0.2223\tClassification Loss: 1.9380\r\n",
      "Train Epoch: 7 [35200/209539 (17%)]\tAll Loss: 2.2070\tTriple Loss(1): 0.1013\tClassification Loss: 2.0044\r\n",
      "Train Epoch: 7 [35840/209539 (17%)]\tAll Loss: 2.2560\tTriple Loss(1): 0.2480\tClassification Loss: 1.7600\r\n",
      "Train Epoch: 7 [36480/209539 (17%)]\tAll Loss: 2.1099\tTriple Loss(1): 0.1464\tClassification Loss: 1.8170\r\n",
      "Train Epoch: 7 [37120/209539 (18%)]\tAll Loss: 3.4000\tTriple Loss(0): 0.7509\tClassification Loss: 1.8981\r\n",
      "Train Epoch: 7 [37760/209539 (18%)]\tAll Loss: 2.1572\tTriple Loss(1): 0.1705\tClassification Loss: 1.8161\r\n",
      "Train Epoch: 7 [38400/209539 (18%)]\tAll Loss: 1.7751\tTriple Loss(1): 0.1757\tClassification Loss: 1.4237\r\n",
      "Train Epoch: 7 [39040/209539 (19%)]\tAll Loss: 2.2620\tTriple Loss(1): 0.2170\tClassification Loss: 1.8279\r\n",
      "Train Epoch: 7 [39680/209539 (19%)]\tAll Loss: 2.1288\tTriple Loss(1): 0.2373\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 7 [40320/209539 (19%)]\tAll Loss: 3.4945\tTriple Loss(0): 0.6573\tClassification Loss: 2.1799\r\n",
      "Train Epoch: 7 [40960/209539 (20%)]\tAll Loss: 2.3446\tTriple Loss(1): 0.2343\tClassification Loss: 1.8761\r\n",
      "Train Epoch: 7 [41600/209539 (20%)]\tAll Loss: 2.6216\tTriple Loss(1): 0.2334\tClassification Loss: 2.1547\r\n",
      "Train Epoch: 7 [42240/209539 (20%)]\tAll Loss: 3.3098\tTriple Loss(0): 0.7296\tClassification Loss: 1.8505\r\n",
      "Train Epoch: 7 [42880/209539 (20%)]\tAll Loss: 2.0158\tTriple Loss(1): 0.1802\tClassification Loss: 1.6553\r\n",
      "Train Epoch: 7 [43520/209539 (21%)]\tAll Loss: 2.3438\tTriple Loss(1): 0.1572\tClassification Loss: 2.0294\r\n",
      "Train Epoch: 7 [44160/209539 (21%)]\tAll Loss: 3.0563\tTriple Loss(0): 0.6559\tClassification Loss: 1.7446\r\n",
      "Train Epoch: 7 [44800/209539 (21%)]\tAll Loss: 2.3385\tTriple Loss(1): 0.2965\tClassification Loss: 1.7455\r\n",
      "Train Epoch: 7 [45440/209539 (22%)]\tAll Loss: 1.9883\tTriple Loss(1): 0.2560\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 7 [46080/209539 (22%)]\tAll Loss: 2.5431\tTriple Loss(1): 0.2877\tClassification Loss: 1.9677\r\n",
      "Train Epoch: 7 [46720/209539 (22%)]\tAll Loss: 1.6766\tTriple Loss(1): 0.0290\tClassification Loss: 1.6185\r\n",
      "Train Epoch: 7 [47360/209539 (23%)]\tAll Loss: 2.4538\tTriple Loss(1): 0.2292\tClassification Loss: 1.9954\r\n",
      "Train Epoch: 7 [48000/209539 (23%)]\tAll Loss: 3.2481\tTriple Loss(0): 0.6677\tClassification Loss: 1.9126\r\n",
      "Train Epoch: 7 [48640/209539 (23%)]\tAll Loss: 2.3313\tTriple Loss(1): 0.2963\tClassification Loss: 1.7387\r\n",
      "Train Epoch: 7 [49280/209539 (24%)]\tAll Loss: 2.1463\tTriple Loss(1): 0.2407\tClassification Loss: 1.6650\r\n",
      "Train Epoch: 7 [49920/209539 (24%)]\tAll Loss: 2.2527\tTriple Loss(1): 0.2115\tClassification Loss: 1.8298\r\n",
      "Train Epoch: 7 [50560/209539 (24%)]\tAll Loss: 2.0576\tTriple Loss(1): 0.1387\tClassification Loss: 1.7802\r\n",
      "Train Epoch: 7 [51200/209539 (24%)]\tAll Loss: 2.5695\tTriple Loss(1): 0.3087\tClassification Loss: 1.9522\r\n",
      "Train Epoch: 7 [51840/209539 (25%)]\tAll Loss: 2.3019\tTriple Loss(1): 0.2659\tClassification Loss: 1.7702\r\n",
      "Train Epoch: 7 [52480/209539 (25%)]\tAll Loss: 2.4841\tTriple Loss(1): 0.2607\tClassification Loss: 1.9626\r\n",
      "Train Epoch: 7 [53120/209539 (25%)]\tAll Loss: 2.0359\tTriple Loss(1): 0.2169\tClassification Loss: 1.6022\r\n",
      "Train Epoch: 7 [53760/209539 (26%)]\tAll Loss: 2.6284\tTriple Loss(1): 0.3503\tClassification Loss: 1.9278\r\n",
      "Train Epoch: 7 [54400/209539 (26%)]\tAll Loss: 2.4943\tTriple Loss(1): 0.2205\tClassification Loss: 2.0534\r\n",
      "Train Epoch: 7 [55040/209539 (26%)]\tAll Loss: 1.7281\tTriple Loss(1): 0.0955\tClassification Loss: 1.5370\r\n",
      "Train Epoch: 7 [55680/209539 (27%)]\tAll Loss: 2.0658\tTriple Loss(1): 0.1348\tClassification Loss: 1.7963\r\n",
      "Train Epoch: 7 [56320/209539 (27%)]\tAll Loss: 2.0831\tTriple Loss(1): 0.2085\tClassification Loss: 1.6661\r\n",
      "Train Epoch: 7 [56960/209539 (27%)]\tAll Loss: 2.4550\tTriple Loss(1): 0.2142\tClassification Loss: 2.0265\r\n",
      "Train Epoch: 7 [57600/209539 (27%)]\tAll Loss: 2.1414\tTriple Loss(1): 0.1214\tClassification Loss: 1.8985\r\n",
      "Train Epoch: 7 [58240/209539 (28%)]\tAll Loss: 1.8534\tTriple Loss(1): 0.0839\tClassification Loss: 1.6856\r\n",
      "Train Epoch: 7 [58880/209539 (28%)]\tAll Loss: 2.2390\tTriple Loss(1): 0.2312\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 7 [59520/209539 (28%)]\tAll Loss: 2.4658\tTriple Loss(1): 0.2730\tClassification Loss: 1.9198\r\n",
      "Train Epoch: 7 [60160/209539 (29%)]\tAll Loss: 1.8889\tTriple Loss(1): 0.1610\tClassification Loss: 1.5669\r\n",
      "Train Epoch: 7 [60800/209539 (29%)]\tAll Loss: 3.3460\tTriple Loss(0): 0.8169\tClassification Loss: 1.7122\r\n",
      "Train Epoch: 7 [61440/209539 (29%)]\tAll Loss: 2.1845\tTriple Loss(1): 0.2178\tClassification Loss: 1.7489\r\n",
      "Train Epoch: 7 [62080/209539 (30%)]\tAll Loss: 2.3513\tTriple Loss(1): 0.1519\tClassification Loss: 2.0476\r\n",
      "Train Epoch: 7 [62720/209539 (30%)]\tAll Loss: 2.1991\tTriple Loss(1): 0.0887\tClassification Loss: 2.0217\r\n",
      "Train Epoch: 7 [63360/209539 (30%)]\tAll Loss: 2.1478\tTriple Loss(1): 0.1678\tClassification Loss: 1.8121\r\n",
      "Train Epoch: 7 [64000/209539 (31%)]\tAll Loss: 2.0084\tTriple Loss(1): 0.1108\tClassification Loss: 1.7867\r\n",
      "Train Epoch: 7 [64640/209539 (31%)]\tAll Loss: 2.4887\tTriple Loss(1): 0.1840\tClassification Loss: 2.1206\r\n",
      "Train Epoch: 7 [65280/209539 (31%)]\tAll Loss: 2.1512\tTriple Loss(1): 0.1897\tClassification Loss: 1.7718\r\n",
      "Train Epoch: 7 [65920/209539 (31%)]\tAll Loss: 2.1392\tTriple Loss(1): 0.1457\tClassification Loss: 1.8478\r\n",
      "Train Epoch: 7 [66560/209539 (32%)]\tAll Loss: 2.1280\tTriple Loss(1): 0.1802\tClassification Loss: 1.7676\r\n",
      "Train Epoch: 7 [67200/209539 (32%)]\tAll Loss: 2.4712\tTriple Loss(1): 0.1890\tClassification Loss: 2.0931\r\n",
      "Train Epoch: 7 [67840/209539 (32%)]\tAll Loss: 1.7806\tTriple Loss(1): 0.1257\tClassification Loss: 1.5292\r\n",
      "Train Epoch: 7 [68480/209539 (33%)]\tAll Loss: 1.9288\tTriple Loss(1): 0.1165\tClassification Loss: 1.6959\r\n",
      "Train Epoch: 7 [69120/209539 (33%)]\tAll Loss: 3.3615\tTriple Loss(0): 0.9558\tClassification Loss: 1.4499\r\n",
      "Train Epoch: 7 [69760/209539 (33%)]\tAll Loss: 2.4340\tTriple Loss(1): 0.1893\tClassification Loss: 2.0555\r\n",
      "Train Epoch: 7 [70400/209539 (34%)]\tAll Loss: 2.0983\tTriple Loss(1): 0.2590\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 7 [71040/209539 (34%)]\tAll Loss: 3.0538\tTriple Loss(0): 0.5965\tClassification Loss: 1.8607\r\n",
      "Train Epoch: 7 [71680/209539 (34%)]\tAll Loss: 2.1597\tTriple Loss(1): 0.1704\tClassification Loss: 1.8188\r\n",
      "Train Epoch: 7 [72320/209539 (35%)]\tAll Loss: 2.1953\tTriple Loss(1): 0.1563\tClassification Loss: 1.8827\r\n",
      "Train Epoch: 7 [72960/209539 (35%)]\tAll Loss: 1.9400\tTriple Loss(1): 0.1081\tClassification Loss: 1.7238\r\n",
      "Train Epoch: 7 [73600/209539 (35%)]\tAll Loss: 2.0413\tTriple Loss(1): 0.1886\tClassification Loss: 1.6640\r\n",
      "Train Epoch: 7 [74240/209539 (35%)]\tAll Loss: 2.5205\tTriple Loss(1): 0.2806\tClassification Loss: 1.9593\r\n",
      "Train Epoch: 7 [74880/209539 (36%)]\tAll Loss: 2.9571\tTriple Loss(0): 0.6823\tClassification Loss: 1.5924\r\n",
      "Train Epoch: 7 [75520/209539 (36%)]\tAll Loss: 2.5681\tTriple Loss(0): 0.3428\tClassification Loss: 1.8825\r\n",
      "Train Epoch: 7 [76160/209539 (36%)]\tAll Loss: 2.1922\tTriple Loss(1): 0.1538\tClassification Loss: 1.8845\r\n",
      "Train Epoch: 7 [76800/209539 (37%)]\tAll Loss: 2.2391\tTriple Loss(1): 0.2240\tClassification Loss: 1.7912\r\n",
      "Train Epoch: 7 [77440/209539 (37%)]\tAll Loss: 2.2167\tTriple Loss(1): 0.1919\tClassification Loss: 1.8329\r\n",
      "Train Epoch: 7 [78080/209539 (37%)]\tAll Loss: 2.2479\tTriple Loss(1): 0.2063\tClassification Loss: 1.8353\r\n",
      "Train Epoch: 7 [78720/209539 (38%)]\tAll Loss: 2.3459\tTriple Loss(1): 0.1324\tClassification Loss: 2.0811\r\n",
      "Train Epoch: 7 [79360/209539 (38%)]\tAll Loss: 2.5207\tTriple Loss(1): 0.1248\tClassification Loss: 2.2710\r\n",
      "Train Epoch: 7 [80000/209539 (38%)]\tAll Loss: 2.3553\tTriple Loss(1): 0.2443\tClassification Loss: 1.8668\r\n",
      "Train Epoch: 7 [80640/209539 (38%)]\tAll Loss: 3.1609\tTriple Loss(0): 0.8075\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 7 [81280/209539 (39%)]\tAll Loss: 2.6366\tTriple Loss(1): 0.3096\tClassification Loss: 2.0174\r\n",
      "Train Epoch: 7 [81920/209539 (39%)]\tAll Loss: 1.7665\tTriple Loss(1): 0.0222\tClassification Loss: 1.7220\r\n",
      "Train Epoch: 7 [82560/209539 (39%)]\tAll Loss: 2.1891\tTriple Loss(1): 0.1995\tClassification Loss: 1.7900\r\n",
      "Train Epoch: 7 [83200/209539 (40%)]\tAll Loss: 1.8820\tTriple Loss(1): 0.1302\tClassification Loss: 1.6215\r\n",
      "Train Epoch: 7 [83840/209539 (40%)]\tAll Loss: 2.1395\tTriple Loss(1): 0.1046\tClassification Loss: 1.9304\r\n",
      "Train Epoch: 7 [84480/209539 (40%)]\tAll Loss: 2.5381\tTriple Loss(1): 0.3300\tClassification Loss: 1.8782\r\n",
      "Train Epoch: 7 [85120/209539 (41%)]\tAll Loss: 2.0910\tTriple Loss(1): 0.1777\tClassification Loss: 1.7355\r\n",
      "Train Epoch: 7 [85760/209539 (41%)]\tAll Loss: 2.0324\tTriple Loss(1): 0.1742\tClassification Loss: 1.6839\r\n",
      "Train Epoch: 7 [86400/209539 (41%)]\tAll Loss: 2.0798\tTriple Loss(1): 0.1127\tClassification Loss: 1.8543\r\n",
      "Train Epoch: 7 [87040/209539 (42%)]\tAll Loss: 2.2018\tTriple Loss(1): 0.2837\tClassification Loss: 1.6343\r\n",
      "Train Epoch: 7 [87680/209539 (42%)]\tAll Loss: 2.0508\tTriple Loss(1): 0.0988\tClassification Loss: 1.8532\r\n",
      "Train Epoch: 7 [88320/209539 (42%)]\tAll Loss: 2.2399\tTriple Loss(1): 0.2149\tClassification Loss: 1.8101\r\n",
      "Train Epoch: 7 [88960/209539 (42%)]\tAll Loss: 2.3774\tTriple Loss(1): 0.3080\tClassification Loss: 1.7614\r\n",
      "Train Epoch: 7 [89600/209539 (43%)]\tAll Loss: 2.2074\tTriple Loss(1): 0.1511\tClassification Loss: 1.9053\r\n",
      "Train Epoch: 7 [90240/209539 (43%)]\tAll Loss: 2.1286\tTriple Loss(1): 0.1200\tClassification Loss: 1.8886\r\n",
      "Train Epoch: 7 [90880/209539 (43%)]\tAll Loss: 2.0400\tTriple Loss(1): 0.1038\tClassification Loss: 1.8324\r\n",
      "Train Epoch: 7 [91520/209539 (44%)]\tAll Loss: 2.4703\tTriple Loss(1): 0.2694\tClassification Loss: 1.9315\r\n",
      "Train Epoch: 7 [92160/209539 (44%)]\tAll Loss: 3.1349\tTriple Loss(0): 0.7514\tClassification Loss: 1.6321\r\n",
      "Train Epoch: 7 [92800/209539 (44%)]\tAll Loss: 1.4516\tTriple Loss(1): 0.1368\tClassification Loss: 1.1780\r\n",
      "Train Epoch: 7 [93440/209539 (45%)]\tAll Loss: 2.3510\tTriple Loss(1): 0.2640\tClassification Loss: 1.8230\r\n",
      "Train Epoch: 7 [94080/209539 (45%)]\tAll Loss: 1.7107\tTriple Loss(1): 0.1461\tClassification Loss: 1.4186\r\n",
      "Train Epoch: 7 [94720/209539 (45%)]\tAll Loss: 2.4251\tTriple Loss(1): 0.2035\tClassification Loss: 2.0181\r\n",
      "Train Epoch: 7 [95360/209539 (46%)]\tAll Loss: 3.6591\tTriple Loss(0): 0.7929\tClassification Loss: 2.0733\r\n",
      "Train Epoch: 7 [96000/209539 (46%)]\tAll Loss: 3.7210\tTriple Loss(0): 0.9341\tClassification Loss: 1.8529\r\n",
      "Train Epoch: 7 [96640/209539 (46%)]\tAll Loss: 2.4773\tTriple Loss(1): 0.2940\tClassification Loss: 1.8893\r\n",
      "Train Epoch: 7 [97280/209539 (46%)]\tAll Loss: 2.2353\tTriple Loss(1): 0.2197\tClassification Loss: 1.7960\r\n",
      "Train Epoch: 7 [97920/209539 (47%)]\tAll Loss: 2.4165\tTriple Loss(1): 0.3460\tClassification Loss: 1.7245\r\n",
      "Train Epoch: 7 [98560/209539 (47%)]\tAll Loss: 1.9965\tTriple Loss(1): 0.1854\tClassification Loss: 1.6256\r\n",
      "Train Epoch: 7 [99200/209539 (47%)]\tAll Loss: 3.4077\tTriple Loss(0): 0.7553\tClassification Loss: 1.8972\r\n",
      "Train Epoch: 7 [99840/209539 (48%)]\tAll Loss: 1.9739\tTriple Loss(1): 0.0671\tClassification Loss: 1.8397\r\n",
      "Train Epoch: 7 [100480/209539 (48%)]\tAll Loss: 2.1241\tTriple Loss(1): 0.1045\tClassification Loss: 1.9151\r\n",
      "Train Epoch: 7 [101120/209539 (48%)]\tAll Loss: 2.0457\tTriple Loss(1): 0.1765\tClassification Loss: 1.6928\r\n",
      "Train Epoch: 7 [101760/209539 (49%)]\tAll Loss: 1.9520\tTriple Loss(1): 0.0923\tClassification Loss: 1.7675\r\n",
      "Train Epoch: 7 [102400/209539 (49%)]\tAll Loss: 1.9180\tTriple Loss(1): 0.1959\tClassification Loss: 1.5261\r\n",
      "Train Epoch: 7 [103040/209539 (49%)]\tAll Loss: 2.3750\tTriple Loss(1): 0.0880\tClassification Loss: 2.1991\r\n",
      "Train Epoch: 7 [103680/209539 (49%)]\tAll Loss: 2.3428\tTriple Loss(1): 0.1312\tClassification Loss: 2.0804\r\n",
      "Train Epoch: 7 [104320/209539 (50%)]\tAll Loss: 2.9455\tTriple Loss(0): 0.5845\tClassification Loss: 1.7766\r\n",
      "Train Epoch: 7 [104960/209539 (50%)]\tAll Loss: 3.1307\tTriple Loss(0): 0.7129\tClassification Loss: 1.7050\r\n",
      "Train Epoch: 7 [105600/209539 (50%)]\tAll Loss: 1.7198\tTriple Loss(1): 0.1324\tClassification Loss: 1.4550\r\n",
      "Train Epoch: 7 [106240/209539 (51%)]\tAll Loss: 2.4110\tTriple Loss(1): 0.1789\tClassification Loss: 2.0533\r\n",
      "Train Epoch: 7 [106880/209539 (51%)]\tAll Loss: 1.9338\tTriple Loss(1): 0.1768\tClassification Loss: 1.5802\r\n",
      "Train Epoch: 7 [107520/209539 (51%)]\tAll Loss: 2.1437\tTriple Loss(1): 0.1888\tClassification Loss: 1.7660\r\n",
      "Train Epoch: 7 [108160/209539 (52%)]\tAll Loss: 1.8169\tTriple Loss(1): 0.1727\tClassification Loss: 1.4714\r\n",
      "Train Epoch: 7 [108800/209539 (52%)]\tAll Loss: 2.3142\tTriple Loss(1): 0.1457\tClassification Loss: 2.0229\r\n",
      "Train Epoch: 7 [109440/209539 (52%)]\tAll Loss: 2.1942\tTriple Loss(1): 0.1774\tClassification Loss: 1.8394\r\n",
      "Train Epoch: 7 [110080/209539 (53%)]\tAll Loss: 1.9359\tTriple Loss(1): 0.1240\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 7 [110720/209539 (53%)]\tAll Loss: 2.4312\tTriple Loss(1): 0.2312\tClassification Loss: 1.9687\r\n",
      "Train Epoch: 7 [111360/209539 (53%)]\tAll Loss: 1.7171\tTriple Loss(1): 0.1553\tClassification Loss: 1.4064\r\n",
      "Train Epoch: 7 [112000/209539 (53%)]\tAll Loss: 3.3186\tTriple Loss(0): 0.7175\tClassification Loss: 1.8835\r\n",
      "Train Epoch: 7 [112640/209539 (54%)]\tAll Loss: 1.9531\tTriple Loss(1): 0.1707\tClassification Loss: 1.6117\r\n",
      "Train Epoch: 7 [113280/209539 (54%)]\tAll Loss: 1.9972\tTriple Loss(1): 0.1867\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 7 [113920/209539 (54%)]\tAll Loss: 2.4113\tTriple Loss(1): 0.2571\tClassification Loss: 1.8971\r\n",
      "Train Epoch: 7 [114560/209539 (55%)]\tAll Loss: 2.1610\tTriple Loss(1): 0.2297\tClassification Loss: 1.7015\r\n",
      "Train Epoch: 7 [115200/209539 (55%)]\tAll Loss: 2.3570\tTriple Loss(1): 0.1947\tClassification Loss: 1.9676\r\n",
      "Train Epoch: 7 [115840/209539 (55%)]\tAll Loss: 3.1019\tTriple Loss(0): 0.7104\tClassification Loss: 1.6811\r\n",
      "Train Epoch: 7 [116480/209539 (56%)]\tAll Loss: 1.9926\tTriple Loss(1): 0.1870\tClassification Loss: 1.6187\r\n",
      "Train Epoch: 7 [117120/209539 (56%)]\tAll Loss: 2.3398\tTriple Loss(1): 0.1708\tClassification Loss: 1.9982\r\n",
      "Train Epoch: 7 [117760/209539 (56%)]\tAll Loss: 2.1527\tTriple Loss(1): 0.2047\tClassification Loss: 1.7434\r\n",
      "Train Epoch: 7 [118400/209539 (57%)]\tAll Loss: 2.0415\tTriple Loss(1): 0.2229\tClassification Loss: 1.5958\r\n",
      "Train Epoch: 7 [119040/209539 (57%)]\tAll Loss: 2.3027\tTriple Loss(1): 0.1733\tClassification Loss: 1.9561\r\n",
      "Train Epoch: 7 [119680/209539 (57%)]\tAll Loss: 2.0746\tTriple Loss(1): 0.1827\tClassification Loss: 1.7092\r\n",
      "Train Epoch: 7 [120320/209539 (57%)]\tAll Loss: 2.2292\tTriple Loss(1): 0.1900\tClassification Loss: 1.8491\r\n",
      "Train Epoch: 7 [120960/209539 (58%)]\tAll Loss: 2.6382\tTriple Loss(0): 0.6065\tClassification Loss: 1.4252\r\n",
      "Train Epoch: 7 [121600/209539 (58%)]\tAll Loss: 3.1531\tTriple Loss(0): 0.8040\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 7 [122240/209539 (58%)]\tAll Loss: 2.3173\tTriple Loss(1): 0.2396\tClassification Loss: 1.8381\r\n",
      "Train Epoch: 7 [122880/209539 (59%)]\tAll Loss: 2.2788\tTriple Loss(1): 0.2369\tClassification Loss: 1.8050\r\n",
      "Train Epoch: 7 [123520/209539 (59%)]\tAll Loss: 3.7466\tTriple Loss(0): 0.9576\tClassification Loss: 1.8313\r\n",
      "Train Epoch: 7 [124160/209539 (59%)]\tAll Loss: 1.9311\tTriple Loss(1): 0.0833\tClassification Loss: 1.7645\r\n",
      "Train Epoch: 7 [124800/209539 (60%)]\tAll Loss: 1.7443\tTriple Loss(1): 0.0840\tClassification Loss: 1.5762\r\n",
      "Train Epoch: 7 [125440/209539 (60%)]\tAll Loss: 2.1668\tTriple Loss(1): 0.1574\tClassification Loss: 1.8520\r\n",
      "Train Epoch: 7 [126080/209539 (60%)]\tAll Loss: 2.4210\tTriple Loss(1): 0.2539\tClassification Loss: 1.9131\r\n",
      "Train Epoch: 7 [126720/209539 (60%)]\tAll Loss: 2.1883\tTriple Loss(1): 0.2117\tClassification Loss: 1.7649\r\n",
      "Train Epoch: 7 [127360/209539 (61%)]\tAll Loss: 1.9079\tTriple Loss(1): 0.0867\tClassification Loss: 1.7345\r\n",
      "Train Epoch: 7 [128000/209539 (61%)]\tAll Loss: 1.9926\tTriple Loss(1): 0.1704\tClassification Loss: 1.6517\r\n",
      "Train Epoch: 7 [128640/209539 (61%)]\tAll Loss: 2.0967\tTriple Loss(1): 0.1764\tClassification Loss: 1.7439\r\n",
      "Train Epoch: 7 [129280/209539 (62%)]\tAll Loss: 1.8787\tTriple Loss(1): 0.1209\tClassification Loss: 1.6369\r\n",
      "Train Epoch: 7 [129920/209539 (62%)]\tAll Loss: 2.4693\tTriple Loss(1): 0.2818\tClassification Loss: 1.9057\r\n",
      "Train Epoch: 7 [130560/209539 (62%)]\tAll Loss: 2.1810\tTriple Loss(1): 0.2805\tClassification Loss: 1.6200\r\n",
      "Train Epoch: 7 [131200/209539 (63%)]\tAll Loss: 3.6093\tTriple Loss(0): 0.9031\tClassification Loss: 1.8032\r\n",
      "Train Epoch: 7 [131840/209539 (63%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.2004\tClassification Loss: 1.4529\r\n",
      "Train Epoch: 7 [132480/209539 (63%)]\tAll Loss: 2.3475\tTriple Loss(1): 0.2405\tClassification Loss: 1.8665\r\n",
      "Train Epoch: 7 [133120/209539 (64%)]\tAll Loss: 3.2230\tTriple Loss(0): 0.8987\tClassification Loss: 1.4256\r\n",
      "Train Epoch: 7 [133760/209539 (64%)]\tAll Loss: 2.1513\tTriple Loss(1): 0.2996\tClassification Loss: 1.5521\r\n",
      "Train Epoch: 7 [134400/209539 (64%)]\tAll Loss: 3.2457\tTriple Loss(0): 0.7196\tClassification Loss: 1.8065\r\n",
      "Train Epoch: 7 [135040/209539 (64%)]\tAll Loss: 2.1671\tTriple Loss(1): 0.1430\tClassification Loss: 1.8811\r\n",
      "Train Epoch: 7 [135680/209539 (65%)]\tAll Loss: 2.3735\tTriple Loss(1): 0.1656\tClassification Loss: 2.0424\r\n",
      "Train Epoch: 7 [136320/209539 (65%)]\tAll Loss: 2.0692\tTriple Loss(1): 0.1814\tClassification Loss: 1.7064\r\n",
      "Train Epoch: 7 [136960/209539 (65%)]\tAll Loss: 1.9468\tTriple Loss(1): 0.1017\tClassification Loss: 1.7435\r\n",
      "Train Epoch: 7 [137600/209539 (66%)]\tAll Loss: 2.0296\tTriple Loss(1): 0.1338\tClassification Loss: 1.7620\r\n",
      "Train Epoch: 7 [138240/209539 (66%)]\tAll Loss: 2.1657\tTriple Loss(1): 0.2188\tClassification Loss: 1.7282\r\n",
      "Train Epoch: 7 [138880/209539 (66%)]\tAll Loss: 3.1132\tTriple Loss(0): 0.6133\tClassification Loss: 1.8865\r\n",
      "Train Epoch: 7 [139520/209539 (67%)]\tAll Loss: 2.4183\tTriple Loss(1): 0.2446\tClassification Loss: 1.9291\r\n",
      "Train Epoch: 7 [140160/209539 (67%)]\tAll Loss: 3.6661\tTriple Loss(0): 0.8249\tClassification Loss: 2.0163\r\n",
      "Train Epoch: 7 [140800/209539 (67%)]\tAll Loss: 1.9111\tTriple Loss(1): 0.1554\tClassification Loss: 1.6003\r\n",
      "Train Epoch: 7 [141440/209539 (68%)]\tAll Loss: 2.1147\tTriple Loss(1): 0.1448\tClassification Loss: 1.8250\r\n",
      "Train Epoch: 7 [142080/209539 (68%)]\tAll Loss: 2.1230\tTriple Loss(1): 0.1829\tClassification Loss: 1.7572\r\n",
      "Train Epoch: 7 [142720/209539 (68%)]\tAll Loss: 2.2933\tTriple Loss(1): 0.1155\tClassification Loss: 2.0623\r\n",
      "Train Epoch: 7 [143360/209539 (68%)]\tAll Loss: 1.7290\tTriple Loss(1): 0.1155\tClassification Loss: 1.4980\r\n",
      "Train Epoch: 7 [144000/209539 (69%)]\tAll Loss: 2.1230\tTriple Loss(1): 0.1368\tClassification Loss: 1.8493\r\n",
      "Train Epoch: 7 [144640/209539 (69%)]\tAll Loss: 2.8068\tTriple Loss(0): 0.4877\tClassification Loss: 1.8315\r\n",
      "Train Epoch: 7 [145280/209539 (69%)]\tAll Loss: 2.1679\tTriple Loss(1): 0.1505\tClassification Loss: 1.8669\r\n",
      "Train Epoch: 7 [145920/209539 (70%)]\tAll Loss: 3.2942\tTriple Loss(0): 0.7534\tClassification Loss: 1.7874\r\n",
      "Train Epoch: 7 [146560/209539 (70%)]\tAll Loss: 2.5601\tTriple Loss(1): 0.1435\tClassification Loss: 2.2731\r\n",
      "Train Epoch: 7 [147200/209539 (70%)]\tAll Loss: 2.3261\tTriple Loss(1): 0.1318\tClassification Loss: 2.0625\r\n",
      "Train Epoch: 7 [147840/209539 (71%)]\tAll Loss: 2.0306\tTriple Loss(1): 0.2205\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 7 [148480/209539 (71%)]\tAll Loss: 3.3901\tTriple Loss(0): 0.8114\tClassification Loss: 1.7673\r\n",
      "Train Epoch: 7 [149120/209539 (71%)]\tAll Loss: 1.8718\tTriple Loss(1): 0.0230\tClassification Loss: 1.8259\r\n",
      "Train Epoch: 7 [149760/209539 (71%)]\tAll Loss: 2.5399\tTriple Loss(1): 0.2064\tClassification Loss: 2.1271\r\n",
      "Train Epoch: 7 [150400/209539 (72%)]\tAll Loss: 2.4104\tTriple Loss(1): 0.2797\tClassification Loss: 1.8510\r\n",
      "Train Epoch: 7 [151040/209539 (72%)]\tAll Loss: 2.4769\tTriple Loss(1): 0.3173\tClassification Loss: 1.8424\r\n",
      "Train Epoch: 7 [151680/209539 (72%)]\tAll Loss: 2.1224\tTriple Loss(1): 0.2081\tClassification Loss: 1.7061\r\n",
      "Train Epoch: 7 [152320/209539 (73%)]\tAll Loss: 2.1878\tTriple Loss(1): 0.1439\tClassification Loss: 1.9001\r\n",
      "Train Epoch: 7 [152960/209539 (73%)]\tAll Loss: 2.3852\tTriple Loss(1): 0.1966\tClassification Loss: 1.9921\r\n",
      "Train Epoch: 7 [153600/209539 (73%)]\tAll Loss: 3.0785\tTriple Loss(0): 0.6907\tClassification Loss: 1.6972\r\n",
      "Train Epoch: 7 [154240/209539 (74%)]\tAll Loss: 2.0198\tTriple Loss(1): 0.1296\tClassification Loss: 1.7605\r\n",
      "Train Epoch: 7 [154880/209539 (74%)]\tAll Loss: 1.8315\tTriple Loss(1): 0.1124\tClassification Loss: 1.6066\r\n",
      "Train Epoch: 7 [155520/209539 (74%)]\tAll Loss: 2.1127\tTriple Loss(1): 0.2245\tClassification Loss: 1.6637\r\n",
      "Train Epoch: 7 [156160/209539 (75%)]\tAll Loss: 2.6495\tTriple Loss(1): 0.2146\tClassification Loss: 2.2204\r\n",
      "Train Epoch: 7 [156800/209539 (75%)]\tAll Loss: 2.3655\tTriple Loss(1): 0.2379\tClassification Loss: 1.8897\r\n",
      "Train Epoch: 7 [157440/209539 (75%)]\tAll Loss: 3.3118\tTriple Loss(0): 0.6632\tClassification Loss: 1.9854\r\n",
      "Train Epoch: 7 [158080/209539 (75%)]\tAll Loss: 2.1165\tTriple Loss(1): 0.2598\tClassification Loss: 1.5969\r\n",
      "Train Epoch: 7 [158720/209539 (76%)]\tAll Loss: 1.8484\tTriple Loss(1): 0.1094\tClassification Loss: 1.6297\r\n",
      "Train Epoch: 7 [159360/209539 (76%)]\tAll Loss: 2.1109\tTriple Loss(1): 0.1414\tClassification Loss: 1.8280\r\n",
      "Train Epoch: 7 [160000/209539 (76%)]\tAll Loss: 2.4093\tTriple Loss(1): 0.1613\tClassification Loss: 2.0867\r\n",
      "Train Epoch: 7 [160640/209539 (77%)]\tAll Loss: 2.0181\tTriple Loss(1): 0.1178\tClassification Loss: 1.7826\r\n",
      "Train Epoch: 7 [161280/209539 (77%)]\tAll Loss: 2.0919\tTriple Loss(1): 0.1596\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 7 [161920/209539 (77%)]\tAll Loss: 2.0006\tTriple Loss(1): 0.1946\tClassification Loss: 1.6114\r\n",
      "Train Epoch: 7 [162560/209539 (78%)]\tAll Loss: 1.9605\tTriple Loss(1): 0.2361\tClassification Loss: 1.4884\r\n",
      "Train Epoch: 7 [163200/209539 (78%)]\tAll Loss: 2.3328\tTriple Loss(1): 0.2823\tClassification Loss: 1.7683\r\n",
      "Train Epoch: 7 [163840/209539 (78%)]\tAll Loss: 2.3746\tTriple Loss(1): 0.1511\tClassification Loss: 2.0724\r\n",
      "Train Epoch: 7 [164480/209539 (78%)]\tAll Loss: 2.1831\tTriple Loss(1): 0.0993\tClassification Loss: 1.9845\r\n",
      "Train Epoch: 7 [165120/209539 (79%)]\tAll Loss: 1.9492\tTriple Loss(1): 0.1063\tClassification Loss: 1.7366\r\n",
      "Train Epoch: 7 [165760/209539 (79%)]\tAll Loss: 2.3534\tTriple Loss(1): 0.1919\tClassification Loss: 1.9697\r\n",
      "Train Epoch: 7 [166400/209539 (79%)]\tAll Loss: 2.4028\tTriple Loss(1): 0.2616\tClassification Loss: 1.8796\r\n",
      "Train Epoch: 7 [167040/209539 (80%)]\tAll Loss: 1.7644\tTriple Loss(1): 0.0250\tClassification Loss: 1.7144\r\n",
      "Train Epoch: 7 [167680/209539 (80%)]\tAll Loss: 1.9069\tTriple Loss(1): 0.1602\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 7 [168320/209539 (80%)]\tAll Loss: 3.3327\tTriple Loss(0): 0.7343\tClassification Loss: 1.8641\r\n",
      "Train Epoch: 7 [168960/209539 (81%)]\tAll Loss: 3.3308\tTriple Loss(0): 0.7303\tClassification Loss: 1.8703\r\n",
      "Train Epoch: 7 [169600/209539 (81%)]\tAll Loss: 2.1827\tTriple Loss(1): 0.1464\tClassification Loss: 1.8900\r\n",
      "Train Epoch: 7 [170240/209539 (81%)]\tAll Loss: 2.0589\tTriple Loss(1): 0.1891\tClassification Loss: 1.6807\r\n",
      "Train Epoch: 7 [170880/209539 (82%)]\tAll Loss: 2.0831\tTriple Loss(1): 0.2045\tClassification Loss: 1.6740\r\n",
      "Train Epoch: 7 [171520/209539 (82%)]\tAll Loss: 2.0863\tTriple Loss(1): 0.1852\tClassification Loss: 1.7159\r\n",
      "Train Epoch: 7 [172160/209539 (82%)]\tAll Loss: 1.8561\tTriple Loss(1): 0.1428\tClassification Loss: 1.5704\r\n",
      "Train Epoch: 7 [172800/209539 (82%)]\tAll Loss: 2.6209\tTriple Loss(1): 0.2622\tClassification Loss: 2.0965\r\n",
      "Train Epoch: 7 [173440/209539 (83%)]\tAll Loss: 2.0755\tTriple Loss(1): 0.1586\tClassification Loss: 1.7582\r\n",
      "Train Epoch: 7 [174080/209539 (83%)]\tAll Loss: 2.3149\tTriple Loss(1): 0.2447\tClassification Loss: 1.8255\r\n",
      "Train Epoch: 7 [174720/209539 (83%)]\tAll Loss: 2.0559\tTriple Loss(1): 0.2412\tClassification Loss: 1.5736\r\n",
      "Train Epoch: 7 [175360/209539 (84%)]\tAll Loss: 2.1917\tTriple Loss(1): 0.1309\tClassification Loss: 1.9299\r\n",
      "Train Epoch: 7 [176000/209539 (84%)]\tAll Loss: 1.7693\tTriple Loss(1): 0.1091\tClassification Loss: 1.5511\r\n",
      "Train Epoch: 7 [176640/209539 (84%)]\tAll Loss: 2.2456\tTriple Loss(1): 0.1915\tClassification Loss: 1.8627\r\n",
      "Train Epoch: 7 [177280/209539 (85%)]\tAll Loss: 3.1508\tTriple Loss(0): 0.6665\tClassification Loss: 1.8178\r\n",
      "Train Epoch: 7 [177920/209539 (85%)]\tAll Loss: 2.2173\tTriple Loss(1): 0.2297\tClassification Loss: 1.7580\r\n",
      "Train Epoch: 7 [178560/209539 (85%)]\tAll Loss: 1.7593\tTriple Loss(1): 0.0887\tClassification Loss: 1.5820\r\n",
      "Train Epoch: 7 [179200/209539 (86%)]\tAll Loss: 2.8885\tTriple Loss(0): 0.6634\tClassification Loss: 1.5617\r\n",
      "Train Epoch: 7 [179840/209539 (86%)]\tAll Loss: 1.9072\tTriple Loss(1): 0.2422\tClassification Loss: 1.4228\r\n",
      "Train Epoch: 7 [180480/209539 (86%)]\tAll Loss: 1.8785\tTriple Loss(1): 0.0936\tClassification Loss: 1.6913\r\n",
      "Train Epoch: 7 [181120/209539 (86%)]\tAll Loss: 2.3266\tTriple Loss(1): 0.1537\tClassification Loss: 2.0193\r\n",
      "Train Epoch: 7 [181760/209539 (87%)]\tAll Loss: 2.2502\tTriple Loss(1): 0.2461\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 7 [182400/209539 (87%)]\tAll Loss: 2.1470\tTriple Loss(1): 0.1914\tClassification Loss: 1.7641\r\n",
      "Train Epoch: 7 [183040/209539 (87%)]\tAll Loss: 2.1744\tTriple Loss(1): 0.1801\tClassification Loss: 1.8143\r\n",
      "Train Epoch: 7 [183680/209539 (88%)]\tAll Loss: 2.0837\tTriple Loss(1): 0.1714\tClassification Loss: 1.7409\r\n",
      "Train Epoch: 7 [184320/209539 (88%)]\tAll Loss: 1.9101\tTriple Loss(1): 0.0796\tClassification Loss: 1.7509\r\n",
      "Train Epoch: 7 [184960/209539 (88%)]\tAll Loss: 2.0359\tTriple Loss(1): 0.1500\tClassification Loss: 1.7359\r\n",
      "Train Epoch: 7 [185600/209539 (89%)]\tAll Loss: 2.1518\tTriple Loss(1): 0.1216\tClassification Loss: 1.9086\r\n",
      "Train Epoch: 7 [186240/209539 (89%)]\tAll Loss: 3.1402\tTriple Loss(0): 0.6424\tClassification Loss: 1.8554\r\n",
      "Train Epoch: 7 [186880/209539 (89%)]\tAll Loss: 1.8897\tTriple Loss(1): 0.1439\tClassification Loss: 1.6020\r\n",
      "Train Epoch: 7 [187520/209539 (89%)]\tAll Loss: 2.1983\tTriple Loss(1): 0.1496\tClassification Loss: 1.8992\r\n",
      "Train Epoch: 7 [188160/209539 (90%)]\tAll Loss: 1.9914\tTriple Loss(1): 0.1299\tClassification Loss: 1.7316\r\n",
      "Train Epoch: 7 [188800/209539 (90%)]\tAll Loss: 2.3567\tTriple Loss(1): 0.2529\tClassification Loss: 1.8510\r\n",
      "Train Epoch: 7 [189440/209539 (90%)]\tAll Loss: 2.4353\tTriple Loss(1): 0.2555\tClassification Loss: 1.9243\r\n",
      "Train Epoch: 7 [190080/209539 (91%)]\tAll Loss: 1.8521\tTriple Loss(1): 0.1199\tClassification Loss: 1.6123\r\n",
      "Train Epoch: 7 [190720/209539 (91%)]\tAll Loss: 2.1303\tTriple Loss(1): 0.1290\tClassification Loss: 1.8724\r\n",
      "Train Epoch: 7 [191360/209539 (91%)]\tAll Loss: 2.1356\tTriple Loss(1): 0.1632\tClassification Loss: 1.8092\r\n",
      "Train Epoch: 7 [192000/209539 (92%)]\tAll Loss: 2.3201\tTriple Loss(1): 0.2121\tClassification Loss: 1.8959\r\n",
      "Train Epoch: 7 [192640/209539 (92%)]\tAll Loss: 2.3046\tTriple Loss(1): 0.2076\tClassification Loss: 1.8893\r\n",
      "Train Epoch: 7 [193280/209539 (92%)]\tAll Loss: 2.1855\tTriple Loss(1): 0.1533\tClassification Loss: 1.8789\r\n",
      "Train Epoch: 7 [193920/209539 (93%)]\tAll Loss: 2.1145\tTriple Loss(1): 0.1947\tClassification Loss: 1.7251\r\n",
      "Train Epoch: 7 [194560/209539 (93%)]\tAll Loss: 2.0957\tTriple Loss(1): 0.1841\tClassification Loss: 1.7275\r\n",
      "Train Epoch: 7 [195200/209539 (93%)]\tAll Loss: 2.3198\tTriple Loss(1): 0.2800\tClassification Loss: 1.7598\r\n",
      "Train Epoch: 7 [195840/209539 (93%)]\tAll Loss: 2.0360\tTriple Loss(1): 0.1645\tClassification Loss: 1.7070\r\n",
      "Train Epoch: 7 [196480/209539 (94%)]\tAll Loss: 1.9658\tTriple Loss(1): 0.1741\tClassification Loss: 1.6176\r\n",
      "Train Epoch: 7 [197120/209539 (94%)]\tAll Loss: 2.2519\tTriple Loss(1): 0.1437\tClassification Loss: 1.9646\r\n",
      "Train Epoch: 7 [197760/209539 (94%)]\tAll Loss: 1.8298\tTriple Loss(1): 0.0695\tClassification Loss: 1.6908\r\n",
      "Train Epoch: 7 [198400/209539 (95%)]\tAll Loss: 2.0274\tTriple Loss(1): 0.1966\tClassification Loss: 1.6342\r\n",
      "Train Epoch: 7 [199040/209539 (95%)]\tAll Loss: 2.1989\tTriple Loss(1): 0.1560\tClassification Loss: 1.8869\r\n",
      "Train Epoch: 7 [199680/209539 (95%)]\tAll Loss: 2.4759\tTriple Loss(1): 0.2490\tClassification Loss: 1.9779\r\n",
      "Train Epoch: 7 [200320/209539 (96%)]\tAll Loss: 2.1808\tTriple Loss(1): 0.2051\tClassification Loss: 1.7705\r\n",
      "Train Epoch: 7 [200960/209539 (96%)]\tAll Loss: 1.9730\tTriple Loss(1): 0.1635\tClassification Loss: 1.6460\r\n",
      "Train Epoch: 7 [201600/209539 (96%)]\tAll Loss: 2.0829\tTriple Loss(1): 0.1263\tClassification Loss: 1.8302\r\n",
      "Train Epoch: 7 [202240/209539 (97%)]\tAll Loss: 1.9583\tTriple Loss(1): 0.1325\tClassification Loss: 1.6932\r\n",
      "Train Epoch: 7 [202880/209539 (97%)]\tAll Loss: 1.8731\tTriple Loss(1): 0.1356\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 7 [203520/209539 (97%)]\tAll Loss: 2.5892\tTriple Loss(1): 0.3033\tClassification Loss: 1.9826\r\n",
      "Train Epoch: 7 [204160/209539 (97%)]\tAll Loss: 2.6910\tTriple Loss(1): 0.3177\tClassification Loss: 2.0555\r\n",
      "Train Epoch: 7 [204800/209539 (98%)]\tAll Loss: 1.8542\tTriple Loss(1): 0.1650\tClassification Loss: 1.5241\r\n",
      "Train Epoch: 7 [205440/209539 (98%)]\tAll Loss: 1.9795\tTriple Loss(1): 0.1745\tClassification Loss: 1.6304\r\n",
      "Train Epoch: 7 [206080/209539 (98%)]\tAll Loss: 2.0491\tTriple Loss(1): 0.1476\tClassification Loss: 1.7538\r\n",
      "Train Epoch: 7 [206720/209539 (99%)]\tAll Loss: 2.1979\tTriple Loss(1): 0.1728\tClassification Loss: 1.8524\r\n",
      "Train Epoch: 7 [207360/209539 (99%)]\tAll Loss: 3.2320\tTriple Loss(0): 0.8411\tClassification Loss: 1.5498\r\n",
      "Train Epoch: 7 [208000/209539 (99%)]\tAll Loss: 2.1032\tTriple Loss(1): 0.1193\tClassification Loss: 1.8646\r\n",
      "Train Epoch: 7 [208640/209539 (100%)]\tAll Loss: 2.2421\tTriple Loss(1): 0.1322\tClassification Loss: 1.9777\r\n",
      "Train Epoch: 7 [209280/209539 (100%)]\tAll Loss: 2.3884\tTriple Loss(1): 0.3375\tClassification Loss: 1.7135\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/7_epochs\r\n",
      "Train Epoch: 8 [0/209539 (0%)]\tAll Loss: 2.6103\tTriple Loss(1): 0.2664\tClassification Loss: 2.0775\r\n",
      "\r\n",
      "Test set: Average loss: 1.6269, Accuracy: 41980/80128 (52%)\r\n",
      "\r\n",
      "Train Epoch: 8 [640/209539 (0%)]\tAll Loss: 2.4985\tTriple Loss(1): 0.2220\tClassification Loss: 2.0544\r\n",
      "Train Epoch: 8 [1280/209539 (1%)]\tAll Loss: 2.5921\tTriple Loss(1): 0.1975\tClassification Loss: 2.1970\r\n",
      "Train Epoch: 8 [1920/209539 (1%)]\tAll Loss: 2.1079\tTriple Loss(1): 0.2010\tClassification Loss: 1.7059\r\n",
      "Train Epoch: 8 [2560/209539 (1%)]\tAll Loss: 2.0748\tTriple Loss(1): 0.1700\tClassification Loss: 1.7348\r\n",
      "Train Epoch: 8 [3200/209539 (2%)]\tAll Loss: 3.1305\tTriple Loss(0): 0.6611\tClassification Loss: 1.8084\r\n",
      "Train Epoch: 8 [3840/209539 (2%)]\tAll Loss: 3.0602\tTriple Loss(0): 0.6025\tClassification Loss: 1.8552\r\n",
      "Train Epoch: 8 [4480/209539 (2%)]\tAll Loss: 2.2229\tTriple Loss(1): 0.1853\tClassification Loss: 1.8523\r\n",
      "Train Epoch: 8 [5120/209539 (2%)]\tAll Loss: 2.2515\tTriple Loss(1): 0.1806\tClassification Loss: 1.8902\r\n",
      "Train Epoch: 8 [5760/209539 (3%)]\tAll Loss: 2.1217\tTriple Loss(1): 0.1724\tClassification Loss: 1.7769\r\n",
      "Train Epoch: 8 [6400/209539 (3%)]\tAll Loss: 2.3261\tTriple Loss(1): 0.2355\tClassification Loss: 1.8551\r\n",
      "Train Epoch: 8 [7040/209539 (3%)]\tAll Loss: 2.2843\tTriple Loss(1): 0.1624\tClassification Loss: 1.9596\r\n",
      "Train Epoch: 8 [7680/209539 (4%)]\tAll Loss: 1.8653\tTriple Loss(1): 0.0961\tClassification Loss: 1.6731\r\n",
      "Train Epoch: 8 [8320/209539 (4%)]\tAll Loss: 2.1367\tTriple Loss(1): 0.1506\tClassification Loss: 1.8355\r\n",
      "Train Epoch: 8 [8960/209539 (4%)]\tAll Loss: 1.9861\tTriple Loss(1): 0.1000\tClassification Loss: 1.7862\r\n",
      "Train Epoch: 8 [9600/209539 (5%)]\tAll Loss: 2.6756\tTriple Loss(1): 0.3242\tClassification Loss: 2.0272\r\n",
      "Train Epoch: 8 [10240/209539 (5%)]\tAll Loss: 2.1448\tTriple Loss(1): 0.1695\tClassification Loss: 1.8059\r\n",
      "Train Epoch: 8 [10880/209539 (5%)]\tAll Loss: 2.2521\tTriple Loss(1): 0.1509\tClassification Loss: 1.9503\r\n",
      "Train Epoch: 8 [11520/209539 (5%)]\tAll Loss: 2.1589\tTriple Loss(1): 0.2144\tClassification Loss: 1.7302\r\n",
      "Train Epoch: 8 [12160/209539 (6%)]\tAll Loss: 2.2156\tTriple Loss(1): 0.2019\tClassification Loss: 1.8117\r\n",
      "Train Epoch: 8 [12800/209539 (6%)]\tAll Loss: 3.1725\tTriple Loss(0): 0.7637\tClassification Loss: 1.6452\r\n",
      "Train Epoch: 8 [13440/209539 (6%)]\tAll Loss: 2.3131\tTriple Loss(1): 0.2315\tClassification Loss: 1.8501\r\n",
      "Train Epoch: 8 [14080/209539 (7%)]\tAll Loss: 2.3348\tTriple Loss(1): 0.1386\tClassification Loss: 2.0576\r\n",
      "Train Epoch: 8 [14720/209539 (7%)]\tAll Loss: 2.1875\tTriple Loss(1): 0.1230\tClassification Loss: 1.9415\r\n",
      "Train Epoch: 8 [15360/209539 (7%)]\tAll Loss: 3.3533\tTriple Loss(0): 0.8320\tClassification Loss: 1.6893\r\n",
      "Train Epoch: 8 [16000/209539 (8%)]\tAll Loss: 1.8629\tTriple Loss(1): 0.0793\tClassification Loss: 1.7044\r\n",
      "Train Epoch: 8 [16640/209539 (8%)]\tAll Loss: 2.1088\tTriple Loss(1): 0.1787\tClassification Loss: 1.7515\r\n",
      "Train Epoch: 8 [17280/209539 (8%)]\tAll Loss: 2.1319\tTriple Loss(1): 0.1545\tClassification Loss: 1.8228\r\n",
      "Train Epoch: 8 [17920/209539 (9%)]\tAll Loss: 1.7569\tTriple Loss(1): 0.1309\tClassification Loss: 1.4951\r\n",
      "Train Epoch: 8 [18560/209539 (9%)]\tAll Loss: 3.3201\tTriple Loss(0): 0.6876\tClassification Loss: 1.9449\r\n",
      "Train Epoch: 8 [19200/209539 (9%)]\tAll Loss: 3.3981\tTriple Loss(0): 0.8291\tClassification Loss: 1.7400\r\n",
      "Train Epoch: 8 [19840/209539 (9%)]\tAll Loss: 3.0029\tTriple Loss(0): 0.6351\tClassification Loss: 1.7328\r\n",
      "Train Epoch: 8 [20480/209539 (10%)]\tAll Loss: 2.3045\tTriple Loss(1): 0.1481\tClassification Loss: 2.0084\r\n",
      "Train Epoch: 8 [21120/209539 (10%)]\tAll Loss: 2.4258\tTriple Loss(1): 0.3032\tClassification Loss: 1.8193\r\n",
      "Train Epoch: 8 [21760/209539 (10%)]\tAll Loss: 2.6016\tTriple Loss(1): 0.3067\tClassification Loss: 1.9881\r\n",
      "Train Epoch: 8 [22400/209539 (11%)]\tAll Loss: 2.1903\tTriple Loss(1): 0.2592\tClassification Loss: 1.6720\r\n",
      "Train Epoch: 8 [23040/209539 (11%)]\tAll Loss: 2.4640\tTriple Loss(1): 0.2498\tClassification Loss: 1.9644\r\n",
      "Train Epoch: 8 [23680/209539 (11%)]\tAll Loss: 1.9524\tTriple Loss(1): 0.1611\tClassification Loss: 1.6302\r\n",
      "Train Epoch: 8 [24320/209539 (12%)]\tAll Loss: 2.1276\tTriple Loss(1): 0.2280\tClassification Loss: 1.6716\r\n",
      "Train Epoch: 8 [24960/209539 (12%)]\tAll Loss: 1.8290\tTriple Loss(1): 0.1469\tClassification Loss: 1.5351\r\n",
      "Train Epoch: 8 [25600/209539 (12%)]\tAll Loss: 3.6272\tTriple Loss(0): 1.0491\tClassification Loss: 1.5290\r\n",
      "Train Epoch: 8 [26240/209539 (13%)]\tAll Loss: 3.2282\tTriple Loss(0): 0.6587\tClassification Loss: 1.9108\r\n",
      "Train Epoch: 8 [26880/209539 (13%)]\tAll Loss: 2.2937\tTriple Loss(1): 0.2114\tClassification Loss: 1.8709\r\n",
      "Train Epoch: 8 [27520/209539 (13%)]\tAll Loss: 1.9900\tTriple Loss(1): 0.2111\tClassification Loss: 1.5679\r\n",
      "Train Epoch: 8 [28160/209539 (13%)]\tAll Loss: 2.0655\tTriple Loss(1): 0.1804\tClassification Loss: 1.7047\r\n",
      "Train Epoch: 8 [28800/209539 (14%)]\tAll Loss: 1.9271\tTriple Loss(1): 0.0747\tClassification Loss: 1.7777\r\n",
      "Train Epoch: 8 [29440/209539 (14%)]\tAll Loss: 2.3748\tTriple Loss(1): 0.2152\tClassification Loss: 1.9444\r\n",
      "Train Epoch: 8 [30080/209539 (14%)]\tAll Loss: 1.8061\tTriple Loss(1): 0.0554\tClassification Loss: 1.6952\r\n",
      "Train Epoch: 8 [30720/209539 (15%)]\tAll Loss: 2.1301\tTriple Loss(1): 0.1546\tClassification Loss: 1.8208\r\n",
      "Train Epoch: 8 [31360/209539 (15%)]\tAll Loss: 3.4031\tTriple Loss(0): 0.6654\tClassification Loss: 2.0722\r\n",
      "Train Epoch: 8 [32000/209539 (15%)]\tAll Loss: 2.3490\tTriple Loss(1): 0.1771\tClassification Loss: 1.9948\r\n",
      "Train Epoch: 8 [32640/209539 (16%)]\tAll Loss: 2.6582\tTriple Loss(1): 0.2037\tClassification Loss: 2.2509\r\n",
      "Train Epoch: 8 [33280/209539 (16%)]\tAll Loss: 3.3126\tTriple Loss(0): 0.7184\tClassification Loss: 1.8759\r\n",
      "Train Epoch: 8 [33920/209539 (16%)]\tAll Loss: 2.4498\tTriple Loss(1): 0.2053\tClassification Loss: 2.0392\r\n",
      "Train Epoch: 8 [34560/209539 (16%)]\tAll Loss: 2.2463\tTriple Loss(1): 0.2214\tClassification Loss: 1.8034\r\n",
      "Train Epoch: 8 [35200/209539 (17%)]\tAll Loss: 2.2624\tTriple Loss(1): 0.1152\tClassification Loss: 2.0320\r\n",
      "Train Epoch: 8 [35840/209539 (17%)]\tAll Loss: 2.3210\tTriple Loss(1): 0.2416\tClassification Loss: 1.8377\r\n",
      "Train Epoch: 8 [36480/209539 (17%)]\tAll Loss: 3.5078\tTriple Loss(0): 0.8670\tClassification Loss: 1.7738\r\n",
      "Train Epoch: 8 [37120/209539 (18%)]\tAll Loss: 3.2181\tTriple Loss(0): 0.6670\tClassification Loss: 1.8841\r\n",
      "Train Epoch: 8 [37760/209539 (18%)]\tAll Loss: 1.7620\tTriple Loss(1): 0.0548\tClassification Loss: 1.6523\r\n",
      "Train Epoch: 8 [38400/209539 (18%)]\tAll Loss: 3.1504\tTriple Loss(0): 0.8842\tClassification Loss: 1.3820\r\n",
      "Train Epoch: 8 [39040/209539 (19%)]\tAll Loss: 3.1772\tTriple Loss(0): 0.6431\tClassification Loss: 1.8911\r\n",
      "Train Epoch: 8 [39680/209539 (19%)]\tAll Loss: 2.0446\tTriple Loss(1): 0.2731\tClassification Loss: 1.4985\r\n",
      "Train Epoch: 8 [40320/209539 (19%)]\tAll Loss: 2.5495\tTriple Loss(1): 0.1758\tClassification Loss: 2.1980\r\n",
      "Train Epoch: 8 [40960/209539 (20%)]\tAll Loss: 2.1300\tTriple Loss(1): 0.1736\tClassification Loss: 1.7827\r\n",
      "Train Epoch: 8 [41600/209539 (20%)]\tAll Loss: 2.6304\tTriple Loss(1): 0.1592\tClassification Loss: 2.3119\r\n",
      "Train Epoch: 8 [42240/209539 (20%)]\tAll Loss: 2.1740\tTriple Loss(1): 0.2618\tClassification Loss: 1.6503\r\n",
      "Train Epoch: 8 [42880/209539 (20%)]\tAll Loss: 3.2635\tTriple Loss(0): 0.7302\tClassification Loss: 1.8031\r\n",
      "Train Epoch: 8 [43520/209539 (21%)]\tAll Loss: 2.5047\tTriple Loss(1): 0.1994\tClassification Loss: 2.1058\r\n",
      "Train Epoch: 8 [44160/209539 (21%)]\tAll Loss: 2.2265\tTriple Loss(1): 0.2428\tClassification Loss: 1.7410\r\n",
      "Train Epoch: 8 [44800/209539 (21%)]\tAll Loss: 2.7213\tTriple Loss(1): 0.4490\tClassification Loss: 1.8233\r\n",
      "Train Epoch: 8 [45440/209539 (22%)]\tAll Loss: 1.8270\tTriple Loss(1): 0.1690\tClassification Loss: 1.4890\r\n",
      "Train Epoch: 8 [46080/209539 (22%)]\tAll Loss: 2.1563\tTriple Loss(1): 0.1689\tClassification Loss: 1.8185\r\n",
      "Train Epoch: 8 [46720/209539 (22%)]\tAll Loss: 1.8169\tTriple Loss(1): 0.0752\tClassification Loss: 1.6664\r\n",
      "Train Epoch: 8 [47360/209539 (23%)]\tAll Loss: 3.2070\tTriple Loss(0): 0.6161\tClassification Loss: 1.9747\r\n",
      "Train Epoch: 8 [48000/209539 (23%)]\tAll Loss: 2.6338\tTriple Loss(1): 0.2932\tClassification Loss: 2.0473\r\n",
      "Train Epoch: 8 [48640/209539 (23%)]\tAll Loss: 2.0065\tTriple Loss(1): 0.1486\tClassification Loss: 1.7093\r\n",
      "Train Epoch: 8 [49280/209539 (24%)]\tAll Loss: 2.4377\tTriple Loss(1): 0.3238\tClassification Loss: 1.7901\r\n",
      "Train Epoch: 8 [49920/209539 (24%)]\tAll Loss: 2.3703\tTriple Loss(1): 0.2437\tClassification Loss: 1.8828\r\n",
      "Train Epoch: 8 [50560/209539 (24%)]\tAll Loss: 2.0134\tTriple Loss(1): 0.1844\tClassification Loss: 1.6446\r\n",
      "Train Epoch: 8 [51200/209539 (24%)]\tAll Loss: 2.2336\tTriple Loss(1): 0.1800\tClassification Loss: 1.8737\r\n",
      "Train Epoch: 8 [51840/209539 (25%)]\tAll Loss: 3.3112\tTriple Loss(0): 0.8537\tClassification Loss: 1.6038\r\n",
      "Train Epoch: 8 [52480/209539 (25%)]\tAll Loss: 1.9559\tTriple Loss(1): 0.0435\tClassification Loss: 1.8689\r\n",
      "Train Epoch: 8 [53120/209539 (25%)]\tAll Loss: 2.0189\tTriple Loss(1): 0.1897\tClassification Loss: 1.6394\r\n",
      "Train Epoch: 8 [53760/209539 (26%)]\tAll Loss: 2.4395\tTriple Loss(1): 0.2266\tClassification Loss: 1.9862\r\n",
      "Train Epoch: 8 [54400/209539 (26%)]\tAll Loss: 2.4263\tTriple Loss(1): 0.2560\tClassification Loss: 1.9142\r\n",
      "Train Epoch: 8 [55040/209539 (26%)]\tAll Loss: 1.9097\tTriple Loss(1): 0.1601\tClassification Loss: 1.5895\r\n",
      "Train Epoch: 8 [55680/209539 (27%)]\tAll Loss: 2.2916\tTriple Loss(1): 0.2098\tClassification Loss: 1.8720\r\n",
      "Train Epoch: 8 [56320/209539 (27%)]\tAll Loss: 2.8583\tTriple Loss(0): 0.5465\tClassification Loss: 1.7652\r\n",
      "Train Epoch: 8 [56960/209539 (27%)]\tAll Loss: 2.0933\tTriple Loss(1): 0.1236\tClassification Loss: 1.8461\r\n",
      "Train Epoch: 8 [57600/209539 (27%)]\tAll Loss: 2.0517\tTriple Loss(1): 0.1045\tClassification Loss: 1.8428\r\n",
      "Train Epoch: 8 [58240/209539 (28%)]\tAll Loss: 3.4888\tTriple Loss(0): 0.8661\tClassification Loss: 1.7566\r\n",
      "Train Epoch: 8 [58880/209539 (28%)]\tAll Loss: 2.0386\tTriple Loss(1): 0.1897\tClassification Loss: 1.6591\r\n",
      "Train Epoch: 8 [59520/209539 (28%)]\tAll Loss: 2.0931\tTriple Loss(1): 0.1942\tClassification Loss: 1.7046\r\n",
      "Train Epoch: 8 [60160/209539 (29%)]\tAll Loss: 1.8060\tTriple Loss(1): 0.0564\tClassification Loss: 1.6931\r\n",
      "Train Epoch: 8 [60800/209539 (29%)]\tAll Loss: 2.1014\tTriple Loss(1): 0.1269\tClassification Loss: 1.8475\r\n",
      "Train Epoch: 8 [61440/209539 (29%)]\tAll Loss: 2.1227\tTriple Loss(1): 0.2245\tClassification Loss: 1.6737\r\n",
      "Train Epoch: 8 [62080/209539 (30%)]\tAll Loss: 2.0685\tTriple Loss(1): 0.1488\tClassification Loss: 1.7709\r\n",
      "Train Epoch: 8 [62720/209539 (30%)]\tAll Loss: 2.2630\tTriple Loss(1): 0.1620\tClassification Loss: 1.9389\r\n",
      "Train Epoch: 8 [63360/209539 (30%)]\tAll Loss: 2.4249\tTriple Loss(1): 0.2215\tClassification Loss: 1.9818\r\n",
      "Train Epoch: 8 [64000/209539 (31%)]\tAll Loss: 2.2825\tTriple Loss(1): 0.1726\tClassification Loss: 1.9373\r\n",
      "Train Epoch: 8 [64640/209539 (31%)]\tAll Loss: 2.1795\tTriple Loss(1): 0.1637\tClassification Loss: 1.8521\r\n",
      "Train Epoch: 8 [65280/209539 (31%)]\tAll Loss: 2.2140\tTriple Loss(1): 0.1853\tClassification Loss: 1.8434\r\n",
      "Train Epoch: 8 [65920/209539 (31%)]\tAll Loss: 2.1447\tTriple Loss(1): 0.0997\tClassification Loss: 1.9453\r\n",
      "Train Epoch: 8 [66560/209539 (32%)]\tAll Loss: 2.0649\tTriple Loss(1): 0.1758\tClassification Loss: 1.7133\r\n",
      "Train Epoch: 8 [67200/209539 (32%)]\tAll Loss: 3.6989\tTriple Loss(0): 0.7593\tClassification Loss: 2.1802\r\n",
      "Train Epoch: 8 [67840/209539 (32%)]\tAll Loss: 2.2008\tTriple Loss(1): 0.2496\tClassification Loss: 1.7016\r\n",
      "Train Epoch: 8 [68480/209539 (33%)]\tAll Loss: 2.0341\tTriple Loss(1): 0.1881\tClassification Loss: 1.6579\r\n",
      "Train Epoch: 8 [69120/209539 (33%)]\tAll Loss: 2.1617\tTriple Loss(1): 0.3031\tClassification Loss: 1.5555\r\n",
      "Train Epoch: 8 [69760/209539 (33%)]\tAll Loss: 2.2312\tTriple Loss(1): 0.1226\tClassification Loss: 1.9861\r\n",
      "Train Epoch: 8 [70400/209539 (34%)]\tAll Loss: 2.1480\tTriple Loss(1): 0.1614\tClassification Loss: 1.8253\r\n",
      "Train Epoch: 8 [71040/209539 (34%)]\tAll Loss: 2.3042\tTriple Loss(1): 0.2374\tClassification Loss: 1.8295\r\n",
      "Train Epoch: 8 [71680/209539 (34%)]\tAll Loss: 3.4164\tTriple Loss(0): 0.7592\tClassification Loss: 1.8980\r\n",
      "Train Epoch: 8 [72320/209539 (35%)]\tAll Loss: 3.0562\tTriple Loss(0): 0.5893\tClassification Loss: 1.8777\r\n",
      "Train Epoch: 8 [72960/209539 (35%)]\tAll Loss: 2.1522\tTriple Loss(1): 0.1978\tClassification Loss: 1.7567\r\n",
      "Train Epoch: 8 [73600/209539 (35%)]\tAll Loss: 1.8697\tTriple Loss(1): 0.0668\tClassification Loss: 1.7360\r\n",
      "Train Epoch: 8 [74240/209539 (35%)]\tAll Loss: 2.3568\tTriple Loss(1): 0.2371\tClassification Loss: 1.8827\r\n",
      "Train Epoch: 8 [74880/209539 (36%)]\tAll Loss: 1.9281\tTriple Loss(1): 0.0933\tClassification Loss: 1.7414\r\n",
      "Train Epoch: 8 [75520/209539 (36%)]\tAll Loss: 3.2096\tTriple Loss(0): 0.7098\tClassification Loss: 1.7900\r\n",
      "Train Epoch: 8 [76160/209539 (36%)]\tAll Loss: 2.2783\tTriple Loss(1): 0.1895\tClassification Loss: 1.8993\r\n",
      "Train Epoch: 8 [76800/209539 (37%)]\tAll Loss: 3.1276\tTriple Loss(0): 0.6682\tClassification Loss: 1.7912\r\n",
      "Train Epoch: 8 [77440/209539 (37%)]\tAll Loss: 2.0611\tTriple Loss(1): 0.1255\tClassification Loss: 1.8101\r\n",
      "Train Epoch: 8 [78080/209539 (37%)]\tAll Loss: 2.1876\tTriple Loss(1): 0.1805\tClassification Loss: 1.8265\r\n",
      "Train Epoch: 8 [78720/209539 (38%)]\tAll Loss: 2.4096\tTriple Loss(1): 0.1444\tClassification Loss: 2.1209\r\n",
      "Train Epoch: 8 [79360/209539 (38%)]\tAll Loss: 3.7834\tTriple Loss(0): 0.8510\tClassification Loss: 2.0814\r\n",
      "Train Epoch: 8 [80000/209539 (38%)]\tAll Loss: 2.1955\tTriple Loss(1): 0.1828\tClassification Loss: 1.8299\r\n",
      "Train Epoch: 8 [80640/209539 (38%)]\tAll Loss: 1.8951\tTriple Loss(1): 0.1554\tClassification Loss: 1.5843\r\n",
      "Train Epoch: 8 [81280/209539 (39%)]\tAll Loss: 2.2698\tTriple Loss(1): 0.2554\tClassification Loss: 1.7590\r\n",
      "Train Epoch: 8 [81920/209539 (39%)]\tAll Loss: 2.0594\tTriple Loss(1): 0.1922\tClassification Loss: 1.6750\r\n",
      "Train Epoch: 8 [82560/209539 (39%)]\tAll Loss: 2.9749\tTriple Loss(0): 0.6076\tClassification Loss: 1.7596\r\n",
      "Train Epoch: 8 [83200/209539 (40%)]\tAll Loss: 2.6365\tTriple Loss(0): 0.5290\tClassification Loss: 1.5784\r\n",
      "Train Epoch: 8 [83840/209539 (40%)]\tAll Loss: 2.2353\tTriple Loss(1): 0.1442\tClassification Loss: 1.9468\r\n",
      "Train Epoch: 8 [84480/209539 (40%)]\tAll Loss: 2.9949\tTriple Loss(0): 0.5312\tClassification Loss: 1.9325\r\n",
      "Train Epoch: 8 [85120/209539 (41%)]\tAll Loss: 2.1778\tTriple Loss(1): 0.1720\tClassification Loss: 1.8339\r\n",
      "Train Epoch: 8 [85760/209539 (41%)]\tAll Loss: 2.1038\tTriple Loss(1): 0.2018\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 8 [86400/209539 (41%)]\tAll Loss: 2.3580\tTriple Loss(1): 0.1491\tClassification Loss: 2.0599\r\n",
      "Train Epoch: 8 [87040/209539 (42%)]\tAll Loss: 3.7399\tTriple Loss(0): 0.9355\tClassification Loss: 1.8689\r\n",
      "Train Epoch: 8 [87680/209539 (42%)]\tAll Loss: 1.9762\tTriple Loss(1): 0.1212\tClassification Loss: 1.7338\r\n",
      "Train Epoch: 8 [88320/209539 (42%)]\tAll Loss: 2.9408\tTriple Loss(0): 0.5393\tClassification Loss: 1.8622\r\n",
      "Train Epoch: 8 [88960/209539 (42%)]\tAll Loss: 2.4037\tTriple Loss(1): 0.3220\tClassification Loss: 1.7598\r\n",
      "Train Epoch: 8 [89600/209539 (43%)]\tAll Loss: 2.2673\tTriple Loss(1): 0.1622\tClassification Loss: 1.9428\r\n",
      "Train Epoch: 8 [90240/209539 (43%)]\tAll Loss: 3.4619\tTriple Loss(0): 0.7768\tClassification Loss: 1.9084\r\n",
      "Train Epoch: 8 [90880/209539 (43%)]\tAll Loss: 3.2961\tTriple Loss(0): 0.7434\tClassification Loss: 1.8092\r\n",
      "Train Epoch: 8 [91520/209539 (44%)]\tAll Loss: 2.4624\tTriple Loss(1): 0.1858\tClassification Loss: 2.0908\r\n",
      "Train Epoch: 8 [92160/209539 (44%)]\tAll Loss: 2.7230\tTriple Loss(0): 0.5177\tClassification Loss: 1.6877\r\n",
      "Train Epoch: 8 [92800/209539 (44%)]\tAll Loss: 1.5444\tTriple Loss(1): 0.1342\tClassification Loss: 1.2761\r\n",
      "Train Epoch: 8 [93440/209539 (45%)]\tAll Loss: 2.2537\tTriple Loss(1): 0.1461\tClassification Loss: 1.9616\r\n",
      "Train Epoch: 8 [94080/209539 (45%)]\tAll Loss: 2.9670\tTriple Loss(0): 0.7133\tClassification Loss: 1.5404\r\n",
      "Train Epoch: 8 [94720/209539 (45%)]\tAll Loss: 2.2608\tTriple Loss(1): 0.1610\tClassification Loss: 1.9389\r\n",
      "Train Epoch: 8 [95360/209539 (46%)]\tAll Loss: 2.2073\tTriple Loss(1): 0.1668\tClassification Loss: 1.8737\r\n",
      "Train Epoch: 8 [96000/209539 (46%)]\tAll Loss: 2.1472\tTriple Loss(1): 0.1242\tClassification Loss: 1.8989\r\n",
      "Train Epoch: 8 [96640/209539 (46%)]\tAll Loss: 2.2551\tTriple Loss(1): 0.2117\tClassification Loss: 1.8318\r\n",
      "Train Epoch: 8 [97280/209539 (46%)]\tAll Loss: 3.4695\tTriple Loss(0): 0.8129\tClassification Loss: 1.8437\r\n",
      "Train Epoch: 8 [97920/209539 (47%)]\tAll Loss: 2.1835\tTriple Loss(1): 0.2561\tClassification Loss: 1.6713\r\n",
      "Train Epoch: 8 [98560/209539 (47%)]\tAll Loss: 1.9154\tTriple Loss(1): 0.1696\tClassification Loss: 1.5761\r\n",
      "Train Epoch: 8 [99200/209539 (47%)]\tAll Loss: 2.3465\tTriple Loss(1): 0.1937\tClassification Loss: 1.9591\r\n",
      "Train Epoch: 8 [99840/209539 (48%)]\tAll Loss: 2.2295\tTriple Loss(1): 0.2045\tClassification Loss: 1.8205\r\n",
      "Train Epoch: 8 [100480/209539 (48%)]\tAll Loss: 2.4407\tTriple Loss(1): 0.2576\tClassification Loss: 1.9255\r\n",
      "Train Epoch: 8 [101120/209539 (48%)]\tAll Loss: 3.0052\tTriple Loss(0): 0.6259\tClassification Loss: 1.7534\r\n",
      "Train Epoch: 8 [101760/209539 (49%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.1461\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 8 [102400/209539 (49%)]\tAll Loss: 1.8116\tTriple Loss(1): 0.1190\tClassification Loss: 1.5737\r\n",
      "Train Epoch: 8 [103040/209539 (49%)]\tAll Loss: 2.7120\tTriple Loss(1): 0.2988\tClassification Loss: 2.1145\r\n",
      "Train Epoch: 8 [103680/209539 (49%)]\tAll Loss: 2.5002\tTriple Loss(1): 0.2315\tClassification Loss: 2.0372\r\n",
      "Train Epoch: 8 [104320/209539 (50%)]\tAll Loss: 1.9177\tTriple Loss(1): 0.0536\tClassification Loss: 1.8105\r\n",
      "Train Epoch: 8 [104960/209539 (50%)]\tAll Loss: 1.9555\tTriple Loss(1): 0.2277\tClassification Loss: 1.5001\r\n",
      "Train Epoch: 8 [105600/209539 (50%)]\tAll Loss: 1.7411\tTriple Loss(1): 0.1894\tClassification Loss: 1.3622\r\n",
      "Train Epoch: 8 [106240/209539 (51%)]\tAll Loss: 2.2637\tTriple Loss(1): 0.2221\tClassification Loss: 1.8195\r\n",
      "Train Epoch: 8 [106880/209539 (51%)]\tAll Loss: 1.8051\tTriple Loss(1): 0.1009\tClassification Loss: 1.6032\r\n",
      "Train Epoch: 8 [107520/209539 (51%)]\tAll Loss: 2.0469\tTriple Loss(1): 0.1582\tClassification Loss: 1.7306\r\n",
      "Train Epoch: 8 [108160/209539 (52%)]\tAll Loss: 1.9256\tTriple Loss(1): 0.1957\tClassification Loss: 1.5343\r\n",
      "Train Epoch: 8 [108800/209539 (52%)]\tAll Loss: 3.6998\tTriple Loss(0): 0.8351\tClassification Loss: 2.0295\r\n",
      "Train Epoch: 8 [109440/209539 (52%)]\tAll Loss: 2.3672\tTriple Loss(1): 0.1707\tClassification Loss: 2.0258\r\n",
      "Train Epoch: 8 [110080/209539 (53%)]\tAll Loss: 2.1377\tTriple Loss(1): 0.2429\tClassification Loss: 1.6518\r\n",
      "Train Epoch: 8 [110720/209539 (53%)]\tAll Loss: 2.3430\tTriple Loss(1): 0.2297\tClassification Loss: 1.8836\r\n",
      "Train Epoch: 8 [111360/209539 (53%)]\tAll Loss: 3.1343\tTriple Loss(0): 0.7875\tClassification Loss: 1.5592\r\n",
      "Train Epoch: 8 [112000/209539 (53%)]\tAll Loss: 2.2244\tTriple Loss(1): 0.2007\tClassification Loss: 1.8231\r\n",
      "Train Epoch: 8 [112640/209539 (54%)]\tAll Loss: 2.0062\tTriple Loss(1): 0.1780\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 8 [113280/209539 (54%)]\tAll Loss: 1.7870\tTriple Loss(1): 0.1527\tClassification Loss: 1.4817\r\n",
      "Train Epoch: 8 [113920/209539 (54%)]\tAll Loss: 2.2178\tTriple Loss(1): 0.1844\tClassification Loss: 1.8490\r\n",
      "Train Epoch: 8 [114560/209539 (55%)]\tAll Loss: 1.9748\tTriple Loss(1): 0.1038\tClassification Loss: 1.7672\r\n",
      "Train Epoch: 8 [115200/209539 (55%)]\tAll Loss: 2.0456\tTriple Loss(1): 0.1654\tClassification Loss: 1.7149\r\n",
      "Train Epoch: 8 [115840/209539 (55%)]\tAll Loss: 2.0229\tTriple Loss(1): 0.1018\tClassification Loss: 1.8192\r\n",
      "Train Epoch: 8 [116480/209539 (56%)]\tAll Loss: 2.3297\tTriple Loss(1): 0.3444\tClassification Loss: 1.6409\r\n",
      "Train Epoch: 8 [117120/209539 (56%)]\tAll Loss: 2.2156\tTriple Loss(1): 0.1649\tClassification Loss: 1.8857\r\n",
      "Train Epoch: 8 [117760/209539 (56%)]\tAll Loss: 1.9848\tTriple Loss(1): 0.1619\tClassification Loss: 1.6611\r\n",
      "Train Epoch: 8 [118400/209539 (57%)]\tAll Loss: 2.7600\tTriple Loss(0): 0.6446\tClassification Loss: 1.4707\r\n",
      "Train Epoch: 8 [119040/209539 (57%)]\tAll Loss: 2.7738\tTriple Loss(1): 0.3684\tClassification Loss: 2.0371\r\n",
      "Train Epoch: 8 [119680/209539 (57%)]\tAll Loss: 1.9449\tTriple Loss(1): 0.1373\tClassification Loss: 1.6703\r\n",
      "Train Epoch: 8 [120320/209539 (57%)]\tAll Loss: 2.0294\tTriple Loss(1): 0.1195\tClassification Loss: 1.7904\r\n",
      "Train Epoch: 8 [120960/209539 (58%)]\tAll Loss: 1.7672\tTriple Loss(1): 0.1424\tClassification Loss: 1.4824\r\n",
      "Train Epoch: 8 [121600/209539 (58%)]\tAll Loss: 2.0159\tTriple Loss(1): 0.1796\tClassification Loss: 1.6567\r\n",
      "Train Epoch: 8 [122240/209539 (58%)]\tAll Loss: 1.7604\tTriple Loss(1): 0.0258\tClassification Loss: 1.7087\r\n",
      "Train Epoch: 8 [122880/209539 (59%)]\tAll Loss: 2.1055\tTriple Loss(1): 0.2041\tClassification Loss: 1.6973\r\n",
      "Train Epoch: 8 [123520/209539 (59%)]\tAll Loss: 2.3099\tTriple Loss(1): 0.1777\tClassification Loss: 1.9546\r\n",
      "Train Epoch: 8 [124160/209539 (59%)]\tAll Loss: 2.2450\tTriple Loss(1): 0.2285\tClassification Loss: 1.7880\r\n",
      "Train Epoch: 8 [124800/209539 (60%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.2841\tClassification Loss: 1.6088\r\n",
      "Train Epoch: 8 [125440/209539 (60%)]\tAll Loss: 2.2432\tTriple Loss(1): 0.1964\tClassification Loss: 1.8503\r\n",
      "Train Epoch: 8 [126080/209539 (60%)]\tAll Loss: 2.0361\tTriple Loss(1): 0.0000\tClassification Loss: 2.0361\r\n",
      "Train Epoch: 8 [126720/209539 (60%)]\tAll Loss: 2.1555\tTriple Loss(1): 0.1903\tClassification Loss: 1.7750\r\n",
      "Train Epoch: 8 [127360/209539 (61%)]\tAll Loss: 2.1154\tTriple Loss(1): 0.0626\tClassification Loss: 1.9902\r\n",
      "Train Epoch: 8 [128000/209539 (61%)]\tAll Loss: 2.1040\tTriple Loss(1): 0.2117\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 8 [128640/209539 (61%)]\tAll Loss: 2.0177\tTriple Loss(1): 0.1987\tClassification Loss: 1.6203\r\n",
      "Train Epoch: 8 [129280/209539 (62%)]\tAll Loss: 3.2390\tTriple Loss(0): 0.7389\tClassification Loss: 1.7613\r\n",
      "Train Epoch: 8 [129920/209539 (62%)]\tAll Loss: 2.1884\tTriple Loss(1): 0.1369\tClassification Loss: 1.9145\r\n",
      "Train Epoch: 8 [130560/209539 (62%)]\tAll Loss: 1.9957\tTriple Loss(1): 0.1765\tClassification Loss: 1.6426\r\n",
      "Train Epoch: 8 [131200/209539 (63%)]\tAll Loss: 2.4024\tTriple Loss(1): 0.2360\tClassification Loss: 1.9304\r\n",
      "Train Epoch: 8 [131840/209539 (63%)]\tAll Loss: 1.9520\tTriple Loss(1): 0.1683\tClassification Loss: 1.6153\r\n",
      "Train Epoch: 8 [132480/209539 (63%)]\tAll Loss: 2.0801\tTriple Loss(1): 0.1502\tClassification Loss: 1.7797\r\n",
      "Train Epoch: 8 [133120/209539 (64%)]\tAll Loss: 1.8791\tTriple Loss(1): 0.1357\tClassification Loss: 1.6076\r\n",
      "Train Epoch: 8 [133760/209539 (64%)]\tAll Loss: 1.8086\tTriple Loss(1): 0.1867\tClassification Loss: 1.4351\r\n",
      "Train Epoch: 8 [134400/209539 (64%)]\tAll Loss: 3.4315\tTriple Loss(0): 0.8274\tClassification Loss: 1.7767\r\n",
      "Train Epoch: 8 [135040/209539 (64%)]\tAll Loss: 2.9539\tTriple Loss(0): 0.6690\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 8 [135680/209539 (65%)]\tAll Loss: 3.2362\tTriple Loss(0): 0.5573\tClassification Loss: 2.1217\r\n",
      "Train Epoch: 8 [136320/209539 (65%)]\tAll Loss: 2.0681\tTriple Loss(1): 0.1750\tClassification Loss: 1.7181\r\n",
      "Train Epoch: 8 [136960/209539 (65%)]\tAll Loss: 1.8267\tTriple Loss(1): 0.0932\tClassification Loss: 1.6403\r\n",
      "Train Epoch: 8 [137600/209539 (66%)]\tAll Loss: 2.0387\tTriple Loss(1): 0.1391\tClassification Loss: 1.7606\r\n",
      "Train Epoch: 8 [138240/209539 (66%)]\tAll Loss: 1.9704\tTriple Loss(1): 0.1335\tClassification Loss: 1.7034\r\n",
      "Train Epoch: 8 [138880/209539 (66%)]\tAll Loss: 1.9903\tTriple Loss(1): 0.0763\tClassification Loss: 1.8378\r\n",
      "Train Epoch: 8 [139520/209539 (67%)]\tAll Loss: 2.1150\tTriple Loss(1): 0.0601\tClassification Loss: 1.9947\r\n",
      "Train Epoch: 8 [140160/209539 (67%)]\tAll Loss: 2.5270\tTriple Loss(1): 0.2630\tClassification Loss: 2.0010\r\n",
      "Train Epoch: 8 [140800/209539 (67%)]\tAll Loss: 3.7178\tTriple Loss(0): 1.0057\tClassification Loss: 1.7065\r\n",
      "Train Epoch: 8 [141440/209539 (68%)]\tAll Loss: 2.1867\tTriple Loss(1): 0.2295\tClassification Loss: 1.7277\r\n",
      "Train Epoch: 8 [142080/209539 (68%)]\tAll Loss: 1.9123\tTriple Loss(1): 0.1496\tClassification Loss: 1.6131\r\n",
      "Train Epoch: 8 [142720/209539 (68%)]\tAll Loss: 2.1881\tTriple Loss(1): 0.1548\tClassification Loss: 1.8784\r\n",
      "Train Epoch: 8 [143360/209539 (68%)]\tAll Loss: 1.7178\tTriple Loss(1): 0.0973\tClassification Loss: 1.5232\r\n",
      "Train Epoch: 8 [144000/209539 (69%)]\tAll Loss: 1.9873\tTriple Loss(1): 0.1199\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 8 [144640/209539 (69%)]\tAll Loss: 2.2112\tTriple Loss(1): 0.1790\tClassification Loss: 1.8532\r\n",
      "Train Epoch: 8 [145280/209539 (69%)]\tAll Loss: 2.2695\tTriple Loss(1): 0.1563\tClassification Loss: 1.9569\r\n",
      "Train Epoch: 8 [145920/209539 (70%)]\tAll Loss: 1.8884\tTriple Loss(1): 0.1536\tClassification Loss: 1.5813\r\n",
      "Train Epoch: 8 [146560/209539 (70%)]\tAll Loss: 2.6679\tTriple Loss(1): 0.1725\tClassification Loss: 2.3230\r\n",
      "Train Epoch: 8 [147200/209539 (70%)]\tAll Loss: 2.1730\tTriple Loss(1): 0.1551\tClassification Loss: 1.8628\r\n",
      "Train Epoch: 8 [147840/209539 (71%)]\tAll Loss: 2.0376\tTriple Loss(1): 0.1215\tClassification Loss: 1.7946\r\n",
      "Train Epoch: 8 [148480/209539 (71%)]\tAll Loss: 1.9739\tTriple Loss(1): 0.0806\tClassification Loss: 1.8128\r\n",
      "Train Epoch: 8 [149120/209539 (71%)]\tAll Loss: 2.0709\tTriple Loss(1): 0.1419\tClassification Loss: 1.7870\r\n",
      "Train Epoch: 8 [149760/209539 (71%)]\tAll Loss: 2.5207\tTriple Loss(1): 0.1746\tClassification Loss: 2.1715\r\n",
      "Train Epoch: 8 [150400/209539 (72%)]\tAll Loss: 2.3082\tTriple Loss(1): 0.1370\tClassification Loss: 2.0343\r\n",
      "Train Epoch: 8 [151040/209539 (72%)]\tAll Loss: 2.2131\tTriple Loss(1): 0.1546\tClassification Loss: 1.9040\r\n",
      "Train Epoch: 8 [151680/209539 (72%)]\tAll Loss: 2.9321\tTriple Loss(0): 0.5385\tClassification Loss: 1.8551\r\n",
      "Train Epoch: 8 [152320/209539 (73%)]\tAll Loss: 2.4080\tTriple Loss(1): 0.1925\tClassification Loss: 2.0230\r\n",
      "Train Epoch: 8 [152960/209539 (73%)]\tAll Loss: 3.5591\tTriple Loss(0): 0.8588\tClassification Loss: 1.8415\r\n",
      "Train Epoch: 8 [153600/209539 (73%)]\tAll Loss: 2.2974\tTriple Loss(1): 0.2676\tClassification Loss: 1.7621\r\n",
      "Train Epoch: 8 [154240/209539 (74%)]\tAll Loss: 2.0614\tTriple Loss(1): 0.0976\tClassification Loss: 1.8662\r\n",
      "Train Epoch: 8 [154880/209539 (74%)]\tAll Loss: 1.9399\tTriple Loss(1): 0.1995\tClassification Loss: 1.5410\r\n",
      "Train Epoch: 8 [155520/209539 (74%)]\tAll Loss: 1.6978\tTriple Loss(1): 0.0579\tClassification Loss: 1.5819\r\n",
      "Train Epoch: 8 [156160/209539 (75%)]\tAll Loss: 2.4329\tTriple Loss(1): 0.1824\tClassification Loss: 2.0680\r\n",
      "Train Epoch: 8 [156800/209539 (75%)]\tAll Loss: 3.2381\tTriple Loss(0): 0.7470\tClassification Loss: 1.7442\r\n",
      "Train Epoch: 8 [157440/209539 (75%)]\tAll Loss: 2.3224\tTriple Loss(1): 0.1272\tClassification Loss: 2.0679\r\n",
      "Train Epoch: 8 [158080/209539 (75%)]\tAll Loss: 1.9409\tTriple Loss(1): 0.0813\tClassification Loss: 1.7782\r\n",
      "Train Epoch: 8 [158720/209539 (76%)]\tAll Loss: 1.9524\tTriple Loss(1): 0.1585\tClassification Loss: 1.6354\r\n",
      "Train Epoch: 8 [159360/209539 (76%)]\tAll Loss: 2.1357\tTriple Loss(1): 0.1609\tClassification Loss: 1.8139\r\n",
      "Train Epoch: 8 [160000/209539 (76%)]\tAll Loss: 2.4476\tTriple Loss(1): 0.1672\tClassification Loss: 2.1132\r\n",
      "Train Epoch: 8 [160640/209539 (77%)]\tAll Loss: 1.8902\tTriple Loss(1): 0.1048\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 8 [161280/209539 (77%)]\tAll Loss: 2.2556\tTriple Loss(1): 0.2276\tClassification Loss: 1.8005\r\n",
      "Train Epoch: 8 [161920/209539 (77%)]\tAll Loss: 1.9481\tTriple Loss(1): 0.1855\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 8 [162560/209539 (78%)]\tAll Loss: 1.8153\tTriple Loss(1): 0.1681\tClassification Loss: 1.4790\r\n",
      "Train Epoch: 8 [163200/209539 (78%)]\tAll Loss: 1.9754\tTriple Loss(1): 0.1374\tClassification Loss: 1.7006\r\n",
      "Train Epoch: 8 [163840/209539 (78%)]\tAll Loss: 2.2148\tTriple Loss(1): 0.1456\tClassification Loss: 1.9237\r\n",
      "Train Epoch: 8 [164480/209539 (78%)]\tAll Loss: 2.2189\tTriple Loss(1): 0.1804\tClassification Loss: 1.8582\r\n",
      "Train Epoch: 8 [165120/209539 (79%)]\tAll Loss: 2.8451\tTriple Loss(0): 0.6621\tClassification Loss: 1.5209\r\n",
      "Train Epoch: 8 [165760/209539 (79%)]\tAll Loss: 3.8240\tTriple Loss(0): 0.8201\tClassification Loss: 2.1837\r\n",
      "Train Epoch: 8 [166400/209539 (79%)]\tAll Loss: 2.2765\tTriple Loss(1): 0.2064\tClassification Loss: 1.8637\r\n",
      "Train Epoch: 8 [167040/209539 (80%)]\tAll Loss: 2.2280\tTriple Loss(1): 0.3224\tClassification Loss: 1.5832\r\n",
      "Train Epoch: 8 [167680/209539 (80%)]\tAll Loss: 1.9908\tTriple Loss(1): 0.1083\tClassification Loss: 1.7741\r\n",
      "Train Epoch: 8 [168320/209539 (80%)]\tAll Loss: 3.2749\tTriple Loss(0): 0.6979\tClassification Loss: 1.8790\r\n",
      "Train Epoch: 8 [168960/209539 (81%)]\tAll Loss: 2.2404\tTriple Loss(1): 0.1913\tClassification Loss: 1.8577\r\n",
      "Train Epoch: 8 [169600/209539 (81%)]\tAll Loss: 2.1927\tTriple Loss(1): 0.1140\tClassification Loss: 1.9647\r\n",
      "Train Epoch: 8 [170240/209539 (81%)]\tAll Loss: 2.0432\tTriple Loss(1): 0.2071\tClassification Loss: 1.6290\r\n",
      "Train Epoch: 8 [170880/209539 (82%)]\tAll Loss: 1.7880\tTriple Loss(1): 0.1278\tClassification Loss: 1.5324\r\n",
      "Train Epoch: 8 [171520/209539 (82%)]\tAll Loss: 1.7921\tTriple Loss(1): 0.0961\tClassification Loss: 1.5999\r\n",
      "Train Epoch: 8 [172160/209539 (82%)]\tAll Loss: 2.2529\tTriple Loss(1): 0.2053\tClassification Loss: 1.8424\r\n",
      "Train Epoch: 8 [172800/209539 (82%)]\tAll Loss: 2.5446\tTriple Loss(1): 0.2288\tClassification Loss: 2.0869\r\n",
      "Train Epoch: 8 [173440/209539 (83%)]\tAll Loss: 3.1071\tTriple Loss(0): 0.6704\tClassification Loss: 1.7663\r\n",
      "Train Epoch: 8 [174080/209539 (83%)]\tAll Loss: 3.2684\tTriple Loss(0): 0.7759\tClassification Loss: 1.7166\r\n",
      "Train Epoch: 8 [174720/209539 (83%)]\tAll Loss: 2.7683\tTriple Loss(0): 0.4978\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 8 [175360/209539 (84%)]\tAll Loss: 3.7555\tTriple Loss(0): 0.8102\tClassification Loss: 2.1352\r\n",
      "Train Epoch: 8 [176000/209539 (84%)]\tAll Loss: 1.8184\tTriple Loss(1): 0.1424\tClassification Loss: 1.5337\r\n",
      "Train Epoch: 8 [176640/209539 (84%)]\tAll Loss: 2.3284\tTriple Loss(1): 0.1801\tClassification Loss: 1.9681\r\n",
      "Train Epoch: 8 [177280/209539 (85%)]\tAll Loss: 2.1507\tTriple Loss(1): 0.1031\tClassification Loss: 1.9444\r\n",
      "Train Epoch: 8 [177920/209539 (85%)]\tAll Loss: 2.0964\tTriple Loss(1): 0.1587\tClassification Loss: 1.7790\r\n",
      "Train Epoch: 8 [178560/209539 (85%)]\tAll Loss: 2.1711\tTriple Loss(1): 0.2417\tClassification Loss: 1.6876\r\n",
      "Train Epoch: 8 [179200/209539 (86%)]\tAll Loss: 2.1004\tTriple Loss(1): 0.2304\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 8 [179840/209539 (86%)]\tAll Loss: 1.9365\tTriple Loss(1): 0.2501\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 8 [180480/209539 (86%)]\tAll Loss: 1.8401\tTriple Loss(1): 0.1371\tClassification Loss: 1.5658\r\n",
      "Train Epoch: 8 [181120/209539 (86%)]\tAll Loss: 3.4964\tTriple Loss(0): 0.7402\tClassification Loss: 2.0161\r\n",
      "Train Epoch: 8 [181760/209539 (87%)]\tAll Loss: 2.1997\tTriple Loss(1): 0.2091\tClassification Loss: 1.7815\r\n",
      "Train Epoch: 8 [182400/209539 (87%)]\tAll Loss: 3.1450\tTriple Loss(0): 0.6372\tClassification Loss: 1.8707\r\n",
      "Train Epoch: 8 [183040/209539 (87%)]\tAll Loss: 3.0398\tTriple Loss(0): 0.5483\tClassification Loss: 1.9432\r\n",
      "Train Epoch: 8 [183680/209539 (88%)]\tAll Loss: 2.1799\tTriple Loss(1): 0.2557\tClassification Loss: 1.6685\r\n",
      "Train Epoch: 8 [184320/209539 (88%)]\tAll Loss: 2.0069\tTriple Loss(1): 0.1472\tClassification Loss: 1.7125\r\n",
      "Train Epoch: 8 [184960/209539 (88%)]\tAll Loss: 2.9865\tTriple Loss(0): 0.5797\tClassification Loss: 1.8271\r\n",
      "Train Epoch: 8 [185600/209539 (89%)]\tAll Loss: 2.1708\tTriple Loss(1): 0.1779\tClassification Loss: 1.8151\r\n",
      "Train Epoch: 8 [186240/209539 (89%)]\tAll Loss: 2.2563\tTriple Loss(1): 0.1366\tClassification Loss: 1.9831\r\n",
      "Train Epoch: 8 [186880/209539 (89%)]\tAll Loss: 3.5533\tTriple Loss(0): 0.9039\tClassification Loss: 1.7454\r\n",
      "Train Epoch: 8 [187520/209539 (89%)]\tAll Loss: 3.1410\tTriple Loss(0): 0.6017\tClassification Loss: 1.9376\r\n",
      "Train Epoch: 8 [188160/209539 (90%)]\tAll Loss: 3.4791\tTriple Loss(0): 0.7712\tClassification Loss: 1.9367\r\n",
      "Train Epoch: 8 [188800/209539 (90%)]\tAll Loss: 3.2099\tTriple Loss(0): 0.6233\tClassification Loss: 1.9633\r\n",
      "Train Epoch: 8 [189440/209539 (90%)]\tAll Loss: 3.4114\tTriple Loss(0): 0.6954\tClassification Loss: 2.0206\r\n",
      "Train Epoch: 8 [190080/209539 (91%)]\tAll Loss: 1.9576\tTriple Loss(1): 0.1695\tClassification Loss: 1.6187\r\n",
      "Train Epoch: 8 [190720/209539 (91%)]\tAll Loss: 3.4612\tTriple Loss(0): 0.8129\tClassification Loss: 1.8355\r\n",
      "Train Epoch: 8 [191360/209539 (91%)]\tAll Loss: 1.9235\tTriple Loss(1): 0.1065\tClassification Loss: 1.7106\r\n",
      "Train Epoch: 8 [192000/209539 (92%)]\tAll Loss: 2.0810\tTriple Loss(1): 0.0414\tClassification Loss: 1.9982\r\n",
      "Train Epoch: 8 [192640/209539 (92%)]\tAll Loss: 2.9763\tTriple Loss(0): 0.6023\tClassification Loss: 1.7718\r\n",
      "Train Epoch: 8 [193280/209539 (92%)]\tAll Loss: 2.1031\tTriple Loss(1): 0.1241\tClassification Loss: 1.8550\r\n",
      "Train Epoch: 8 [193920/209539 (93%)]\tAll Loss: 1.9913\tTriple Loss(1): 0.1862\tClassification Loss: 1.6189\r\n",
      "Train Epoch: 8 [194560/209539 (93%)]\tAll Loss: 1.9322\tTriple Loss(1): 0.1414\tClassification Loss: 1.6495\r\n",
      "Train Epoch: 8 [195200/209539 (93%)]\tAll Loss: 2.1031\tTriple Loss(1): 0.1679\tClassification Loss: 1.7673\r\n",
      "Train Epoch: 8 [195840/209539 (93%)]\tAll Loss: 3.0503\tTriple Loss(0): 0.6987\tClassification Loss: 1.6528\r\n",
      "Train Epoch: 8 [196480/209539 (94%)]\tAll Loss: 2.1961\tTriple Loss(1): 0.1581\tClassification Loss: 1.8799\r\n",
      "Train Epoch: 8 [197120/209539 (94%)]\tAll Loss: 3.2414\tTriple Loss(0): 0.7361\tClassification Loss: 1.7693\r\n",
      "Train Epoch: 8 [197760/209539 (94%)]\tAll Loss: 2.2155\tTriple Loss(1): 0.2720\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 8 [198400/209539 (95%)]\tAll Loss: 1.8878\tTriple Loss(1): 0.1978\tClassification Loss: 1.4921\r\n",
      "Train Epoch: 8 [199040/209539 (95%)]\tAll Loss: 2.2975\tTriple Loss(1): 0.2151\tClassification Loss: 1.8672\r\n",
      "Train Epoch: 8 [199680/209539 (95%)]\tAll Loss: 2.0990\tTriple Loss(1): 0.1053\tClassification Loss: 1.8883\r\n",
      "Train Epoch: 8 [200320/209539 (96%)]\tAll Loss: 2.1924\tTriple Loss(1): 0.1961\tClassification Loss: 1.8003\r\n",
      "Train Epoch: 8 [200960/209539 (96%)]\tAll Loss: 1.9953\tTriple Loss(1): 0.1690\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 8 [201600/209539 (96%)]\tAll Loss: 2.0436\tTriple Loss(1): 0.1212\tClassification Loss: 1.8012\r\n",
      "Train Epoch: 8 [202240/209539 (97%)]\tAll Loss: 2.0873\tTriple Loss(1): 0.1467\tClassification Loss: 1.7939\r\n",
      "Train Epoch: 8 [202880/209539 (97%)]\tAll Loss: 1.8393\tTriple Loss(1): 0.1771\tClassification Loss: 1.4851\r\n",
      "Train Epoch: 8 [203520/209539 (97%)]\tAll Loss: 1.9813\tTriple Loss(1): 0.0832\tClassification Loss: 1.8149\r\n",
      "Train Epoch: 8 [204160/209539 (97%)]\tAll Loss: 2.4090\tTriple Loss(1): 0.1833\tClassification Loss: 2.0424\r\n",
      "Train Epoch: 8 [204800/209539 (98%)]\tAll Loss: 1.6626\tTriple Loss(1): 0.1124\tClassification Loss: 1.4377\r\n",
      "Train Epoch: 8 [205440/209539 (98%)]\tAll Loss: 1.8365\tTriple Loss(1): 0.1525\tClassification Loss: 1.5314\r\n",
      "Train Epoch: 8 [206080/209539 (98%)]\tAll Loss: 2.6797\tTriple Loss(0): 0.5474\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 8 [206720/209539 (99%)]\tAll Loss: 2.0535\tTriple Loss(1): 0.0846\tClassification Loss: 1.8842\r\n",
      "Train Epoch: 8 [207360/209539 (99%)]\tAll Loss: 2.0083\tTriple Loss(1): 0.2250\tClassification Loss: 1.5582\r\n",
      "Train Epoch: 8 [208000/209539 (99%)]\tAll Loss: 2.3760\tTriple Loss(1): 0.2770\tClassification Loss: 1.8220\r\n",
      "Train Epoch: 8 [208640/209539 (100%)]\tAll Loss: 1.9275\tTriple Loss(1): 0.0652\tClassification Loss: 1.7971\r\n",
      "Train Epoch: 8 [209280/209539 (100%)]\tAll Loss: 2.0713\tTriple Loss(1): 0.1412\tClassification Loss: 1.7889\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/8_epochs\r\n",
      "Train Epoch: 9 [0/209539 (0%)]\tAll Loss: 2.5876\tTriple Loss(1): 0.2463\tClassification Loss: 2.0951\r\n",
      "\r\n",
      "Test set: Average loss: 1.6227, Accuracy: 42158/80128 (53%)\r\n",
      "\r\n",
      "Train Epoch: 9 [640/209539 (0%)]\tAll Loss: 2.2871\tTriple Loss(1): 0.1452\tClassification Loss: 1.9967\r\n",
      "Train Epoch: 9 [1280/209539 (1%)]\tAll Loss: 2.6567\tTriple Loss(1): 0.2331\tClassification Loss: 2.1906\r\n",
      "Train Epoch: 9 [1920/209539 (1%)]\tAll Loss: 3.5332\tTriple Loss(0): 0.9840\tClassification Loss: 1.5651\r\n",
      "Train Epoch: 9 [2560/209539 (1%)]\tAll Loss: 1.9550\tTriple Loss(1): 0.1080\tClassification Loss: 1.7389\r\n",
      "Train Epoch: 9 [3200/209539 (2%)]\tAll Loss: 2.4179\tTriple Loss(1): 0.2274\tClassification Loss: 1.9631\r\n",
      "Train Epoch: 9 [3840/209539 (2%)]\tAll Loss: 2.1601\tTriple Loss(1): 0.1007\tClassification Loss: 1.9586\r\n",
      "Train Epoch: 9 [4480/209539 (2%)]\tAll Loss: 2.1256\tTriple Loss(1): 0.1558\tClassification Loss: 1.8140\r\n",
      "Train Epoch: 9 [5120/209539 (2%)]\tAll Loss: 2.0030\tTriple Loss(1): 0.1283\tClassification Loss: 1.7464\r\n",
      "Train Epoch: 9 [5760/209539 (3%)]\tAll Loss: 1.9053\tTriple Loss(1): 0.1402\tClassification Loss: 1.6250\r\n",
      "Train Epoch: 9 [6400/209539 (3%)]\tAll Loss: 3.6828\tTriple Loss(0): 0.9387\tClassification Loss: 1.8054\r\n",
      "Train Epoch: 9 [7040/209539 (3%)]\tAll Loss: 2.2968\tTriple Loss(1): 0.1795\tClassification Loss: 1.9378\r\n",
      "Train Epoch: 9 [7680/209539 (4%)]\tAll Loss: 1.9299\tTriple Loss(1): 0.1570\tClassification Loss: 1.6159\r\n",
      "Train Epoch: 9 [8320/209539 (4%)]\tAll Loss: 2.2888\tTriple Loss(1): 0.1977\tClassification Loss: 1.8935\r\n",
      "Train Epoch: 9 [8960/209539 (4%)]\tAll Loss: 3.1993\tTriple Loss(0): 0.6782\tClassification Loss: 1.8428\r\n",
      "Train Epoch: 9 [9600/209539 (5%)]\tAll Loss: 2.3762\tTriple Loss(1): 0.1229\tClassification Loss: 2.1304\r\n",
      "Train Epoch: 9 [10240/209539 (5%)]\tAll Loss: 2.4378\tTriple Loss(1): 0.3431\tClassification Loss: 1.7516\r\n",
      "Train Epoch: 9 [10880/209539 (5%)]\tAll Loss: 2.1705\tTriple Loss(1): 0.1229\tClassification Loss: 1.9248\r\n",
      "Train Epoch: 9 [11520/209539 (5%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.1292\tClassification Loss: 1.7621\r\n",
      "Train Epoch: 9 [12160/209539 (6%)]\tAll Loss: 3.6295\tTriple Loss(0): 0.8813\tClassification Loss: 1.8669\r\n",
      "Train Epoch: 9 [12800/209539 (6%)]\tAll Loss: 2.2766\tTriple Loss(1): 0.1671\tClassification Loss: 1.9424\r\n",
      "Train Epoch: 9 [13440/209539 (6%)]\tAll Loss: 3.3186\tTriple Loss(0): 0.7615\tClassification Loss: 1.7956\r\n",
      "Train Epoch: 9 [14080/209539 (7%)]\tAll Loss: 3.2577\tTriple Loss(0): 0.5941\tClassification Loss: 2.0695\r\n",
      "Train Epoch: 9 [14720/209539 (7%)]\tAll Loss: 2.2666\tTriple Loss(1): 0.1426\tClassification Loss: 1.9813\r\n",
      "Train Epoch: 9 [15360/209539 (7%)]\tAll Loss: 3.0129\tTriple Loss(0): 0.6014\tClassification Loss: 1.8101\r\n",
      "Train Epoch: 9 [16000/209539 (8%)]\tAll Loss: 3.1359\tTriple Loss(0): 0.6616\tClassification Loss: 1.8127\r\n",
      "Train Epoch: 9 [16640/209539 (8%)]\tAll Loss: 2.2218\tTriple Loss(1): 0.2548\tClassification Loss: 1.7122\r\n",
      "Train Epoch: 9 [17280/209539 (8%)]\tAll Loss: 2.1785\tTriple Loss(1): 0.1773\tClassification Loss: 1.8240\r\n",
      "Train Epoch: 9 [17920/209539 (9%)]\tAll Loss: 1.9386\tTriple Loss(1): 0.1973\tClassification Loss: 1.5440\r\n",
      "Train Epoch: 9 [18560/209539 (9%)]\tAll Loss: 3.4284\tTriple Loss(0): 0.7605\tClassification Loss: 1.9074\r\n",
      "Train Epoch: 9 [19200/209539 (9%)]\tAll Loss: 2.3311\tTriple Loss(1): 0.2644\tClassification Loss: 1.8022\r\n",
      "Train Epoch: 9 [19840/209539 (9%)]\tAll Loss: 2.1344\tTriple Loss(1): 0.1997\tClassification Loss: 1.7350\r\n",
      "Train Epoch: 9 [20480/209539 (10%)]\tAll Loss: 3.6646\tTriple Loss(0): 0.7783\tClassification Loss: 2.1080\r\n",
      "Train Epoch: 9 [21120/209539 (10%)]\tAll Loss: 3.0332\tTriple Loss(0): 0.6562\tClassification Loss: 1.7208\r\n",
      "Train Epoch: 9 [21760/209539 (10%)]\tAll Loss: 2.1575\tTriple Loss(1): 0.1240\tClassification Loss: 1.9095\r\n",
      "Train Epoch: 9 [22400/209539 (11%)]\tAll Loss: 2.7994\tTriple Loss(0): 0.5798\tClassification Loss: 1.6399\r\n",
      "Train Epoch: 9 [23040/209539 (11%)]\tAll Loss: 2.4285\tTriple Loss(1): 0.2237\tClassification Loss: 1.9811\r\n",
      "Train Epoch: 9 [23680/209539 (11%)]\tAll Loss: 1.8994\tTriple Loss(1): 0.1503\tClassification Loss: 1.5989\r\n",
      "Train Epoch: 9 [24320/209539 (12%)]\tAll Loss: 1.8168\tTriple Loss(1): 0.0644\tClassification Loss: 1.6880\r\n",
      "Train Epoch: 9 [24960/209539 (12%)]\tAll Loss: 2.0347\tTriple Loss(1): 0.1580\tClassification Loss: 1.7188\r\n",
      "Train Epoch: 9 [25600/209539 (12%)]\tAll Loss: 3.0360\tTriple Loss(0): 0.6912\tClassification Loss: 1.6536\r\n",
      "Train Epoch: 9 [26240/209539 (13%)]\tAll Loss: 3.3809\tTriple Loss(0): 0.7386\tClassification Loss: 1.9036\r\n",
      "Train Epoch: 9 [26880/209539 (13%)]\tAll Loss: 3.5662\tTriple Loss(0): 0.8169\tClassification Loss: 1.9324\r\n",
      "Train Epoch: 9 [27520/209539 (13%)]\tAll Loss: 1.8404\tTriple Loss(1): 0.1008\tClassification Loss: 1.6387\r\n",
      "Train Epoch: 9 [28160/209539 (13%)]\tAll Loss: 1.7962\tTriple Loss(1): 0.0867\tClassification Loss: 1.6227\r\n",
      "Train Epoch: 9 [28800/209539 (14%)]\tAll Loss: 3.6640\tTriple Loss(0): 0.9321\tClassification Loss: 1.7997\r\n",
      "Train Epoch: 9 [29440/209539 (14%)]\tAll Loss: 3.6353\tTriple Loss(0): 0.8050\tClassification Loss: 2.0253\r\n",
      "Train Epoch: 9 [30080/209539 (14%)]\tAll Loss: 2.3652\tTriple Loss(1): 0.3483\tClassification Loss: 1.6686\r\n",
      "Train Epoch: 9 [30720/209539 (15%)]\tAll Loss: 2.3447\tTriple Loss(1): 0.2046\tClassification Loss: 1.9354\r\n",
      "Train Epoch: 9 [31360/209539 (15%)]\tAll Loss: 2.4277\tTriple Loss(1): 0.2107\tClassification Loss: 2.0063\r\n",
      "Train Epoch: 9 [32000/209539 (15%)]\tAll Loss: 2.3921\tTriple Loss(1): 0.1522\tClassification Loss: 2.0877\r\n",
      "Train Epoch: 9 [32640/209539 (16%)]\tAll Loss: 2.5864\tTriple Loss(1): 0.2542\tClassification Loss: 2.0780\r\n",
      "Train Epoch: 9 [33280/209539 (16%)]\tAll Loss: 2.3932\tTriple Loss(1): 0.2716\tClassification Loss: 1.8499\r\n",
      "Train Epoch: 9 [33920/209539 (16%)]\tAll Loss: 2.0790\tTriple Loss(1): 0.1496\tClassification Loss: 1.7798\r\n",
      "Train Epoch: 9 [34560/209539 (16%)]\tAll Loss: 2.2130\tTriple Loss(1): 0.1936\tClassification Loss: 1.8258\r\n",
      "Train Epoch: 9 [35200/209539 (17%)]\tAll Loss: 2.2990\tTriple Loss(1): 0.0889\tClassification Loss: 2.1211\r\n",
      "Train Epoch: 9 [35840/209539 (17%)]\tAll Loss: 3.1509\tTriple Loss(0): 0.6798\tClassification Loss: 1.7912\r\n",
      "Train Epoch: 9 [36480/209539 (17%)]\tAll Loss: 2.0681\tTriple Loss(1): 0.1365\tClassification Loss: 1.7952\r\n",
      "Train Epoch: 9 [37120/209539 (18%)]\tAll Loss: 2.5536\tTriple Loss(1): 0.2177\tClassification Loss: 2.1182\r\n",
      "Train Epoch: 9 [37760/209539 (18%)]\tAll Loss: 2.1588\tTriple Loss(1): 0.1937\tClassification Loss: 1.7714\r\n",
      "Train Epoch: 9 [38400/209539 (18%)]\tAll Loss: 2.6807\tTriple Loss(0): 0.6174\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 9 [39040/209539 (19%)]\tAll Loss: 2.4761\tTriple Loss(1): 0.1539\tClassification Loss: 2.1684\r\n",
      "Train Epoch: 9 [39680/209539 (19%)]\tAll Loss: 2.2007\tTriple Loss(1): 0.2193\tClassification Loss: 1.7621\r\n",
      "Train Epoch: 9 [40320/209539 (19%)]\tAll Loss: 3.5532\tTriple Loss(0): 0.7042\tClassification Loss: 2.1449\r\n",
      "Train Epoch: 9 [40960/209539 (20%)]\tAll Loss: 2.1743\tTriple Loss(1): 0.1664\tClassification Loss: 1.8415\r\n",
      "Train Epoch: 9 [41600/209539 (20%)]\tAll Loss: 2.4738\tTriple Loss(1): 0.1520\tClassification Loss: 2.1697\r\n",
      "Train Epoch: 9 [42240/209539 (20%)]\tAll Loss: 1.8860\tTriple Loss(1): 0.0898\tClassification Loss: 1.7065\r\n",
      "Train Epoch: 9 [42880/209539 (20%)]\tAll Loss: 2.9623\tTriple Loss(0): 0.6034\tClassification Loss: 1.7555\r\n",
      "Train Epoch: 9 [43520/209539 (21%)]\tAll Loss: 2.2782\tTriple Loss(1): 0.1676\tClassification Loss: 1.9431\r\n",
      "Train Epoch: 9 [44160/209539 (21%)]\tAll Loss: 1.7649\tTriple Loss(1): 0.1049\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 9 [44800/209539 (21%)]\tAll Loss: 2.1331\tTriple Loss(1): 0.1705\tClassification Loss: 1.7921\r\n",
      "Train Epoch: 9 [45440/209539 (22%)]\tAll Loss: 3.0824\tTriple Loss(0): 0.7988\tClassification Loss: 1.4848\r\n",
      "Train Epoch: 9 [46080/209539 (22%)]\tAll Loss: 2.2759\tTriple Loss(1): 0.2031\tClassification Loss: 1.8697\r\n",
      "Train Epoch: 9 [46720/209539 (22%)]\tAll Loss: 2.0311\tTriple Loss(1): 0.1907\tClassification Loss: 1.6498\r\n",
      "Train Epoch: 9 [47360/209539 (23%)]\tAll Loss: 2.4445\tTriple Loss(1): 0.1801\tClassification Loss: 2.0843\r\n",
      "Train Epoch: 9 [48000/209539 (23%)]\tAll Loss: 3.6076\tTriple Loss(0): 0.7151\tClassification Loss: 2.1774\r\n",
      "Train Epoch: 9 [48640/209539 (23%)]\tAll Loss: 2.1681\tTriple Loss(1): 0.1866\tClassification Loss: 1.7950\r\n",
      "Train Epoch: 9 [49280/209539 (24%)]\tAll Loss: 2.1664\tTriple Loss(1): 0.1645\tClassification Loss: 1.8374\r\n",
      "Train Epoch: 9 [49920/209539 (24%)]\tAll Loss: 2.5141\tTriple Loss(1): 0.2864\tClassification Loss: 1.9412\r\n",
      "Train Epoch: 9 [50560/209539 (24%)]\tAll Loss: 2.1865\tTriple Loss(1): 0.2457\tClassification Loss: 1.6951\r\n",
      "Train Epoch: 9 [51200/209539 (24%)]\tAll Loss: 2.0503\tTriple Loss(1): 0.1249\tClassification Loss: 1.8005\r\n",
      "Train Epoch: 9 [51840/209539 (25%)]\tAll Loss: 2.1262\tTriple Loss(1): 0.1726\tClassification Loss: 1.7811\r\n",
      "Train Epoch: 9 [52480/209539 (25%)]\tAll Loss: 2.6927\tTriple Loss(1): 0.3016\tClassification Loss: 2.0896\r\n",
      "Train Epoch: 9 [53120/209539 (25%)]\tAll Loss: 2.0603\tTriple Loss(1): 0.1953\tClassification Loss: 1.6696\r\n",
      "Train Epoch: 9 [53760/209539 (26%)]\tAll Loss: 2.4437\tTriple Loss(1): 0.2124\tClassification Loss: 2.0188\r\n",
      "Train Epoch: 9 [54400/209539 (26%)]\tAll Loss: 3.4091\tTriple Loss(0): 0.6789\tClassification Loss: 2.0513\r\n",
      "Train Epoch: 9 [55040/209539 (26%)]\tAll Loss: 1.9117\tTriple Loss(1): 0.1804\tClassification Loss: 1.5509\r\n",
      "Train Epoch: 9 [55680/209539 (27%)]\tAll Loss: 1.9822\tTriple Loss(1): 0.1139\tClassification Loss: 1.7543\r\n",
      "Train Epoch: 9 [56320/209539 (27%)]\tAll Loss: 2.2050\tTriple Loss(1): 0.1702\tClassification Loss: 1.8646\r\n",
      "Train Epoch: 9 [56960/209539 (27%)]\tAll Loss: 2.1681\tTriple Loss(1): 0.1438\tClassification Loss: 1.8805\r\n",
      "Train Epoch: 9 [57600/209539 (27%)]\tAll Loss: 1.9862\tTriple Loss(1): 0.0778\tClassification Loss: 1.8306\r\n",
      "Train Epoch: 9 [58240/209539 (28%)]\tAll Loss: 1.9665\tTriple Loss(1): 0.1075\tClassification Loss: 1.7515\r\n",
      "Train Epoch: 9 [58880/209539 (28%)]\tAll Loss: 2.2809\tTriple Loss(1): 0.2360\tClassification Loss: 1.8089\r\n",
      "Train Epoch: 9 [59520/209539 (28%)]\tAll Loss: 1.8774\tTriple Loss(1): 0.0157\tClassification Loss: 1.8460\r\n",
      "Train Epoch: 9 [60160/209539 (29%)]\tAll Loss: 3.3588\tTriple Loss(0): 0.8552\tClassification Loss: 1.6484\r\n",
      "Train Epoch: 9 [60800/209539 (29%)]\tAll Loss: 2.2021\tTriple Loss(1): 0.2023\tClassification Loss: 1.7975\r\n",
      "Train Epoch: 9 [61440/209539 (29%)]\tAll Loss: 2.0683\tTriple Loss(1): 0.1500\tClassification Loss: 1.7684\r\n",
      "Train Epoch: 9 [62080/209539 (30%)]\tAll Loss: 2.4219\tTriple Loss(1): 0.2768\tClassification Loss: 1.8683\r\n",
      "Train Epoch: 9 [62720/209539 (30%)]\tAll Loss: 2.3627\tTriple Loss(1): 0.2173\tClassification Loss: 1.9282\r\n",
      "Train Epoch: 9 [63360/209539 (30%)]\tAll Loss: 2.3259\tTriple Loss(1): 0.1309\tClassification Loss: 2.0642\r\n",
      "Train Epoch: 9 [64000/209539 (31%)]\tAll Loss: 2.2053\tTriple Loss(1): 0.2252\tClassification Loss: 1.7549\r\n",
      "Train Epoch: 9 [64640/209539 (31%)]\tAll Loss: 2.5366\tTriple Loss(1): 0.2531\tClassification Loss: 2.0303\r\n",
      "Train Epoch: 9 [65280/209539 (31%)]\tAll Loss: 2.3398\tTriple Loss(1): 0.2317\tClassification Loss: 1.8765\r\n",
      "Train Epoch: 9 [65920/209539 (31%)]\tAll Loss: 2.2532\tTriple Loss(1): 0.2029\tClassification Loss: 1.8475\r\n",
      "Train Epoch: 9 [66560/209539 (32%)]\tAll Loss: 2.0740\tTriple Loss(1): 0.2143\tClassification Loss: 1.6455\r\n",
      "Train Epoch: 9 [67200/209539 (32%)]\tAll Loss: 2.6438\tTriple Loss(1): 0.2319\tClassification Loss: 2.1801\r\n",
      "Train Epoch: 9 [67840/209539 (32%)]\tAll Loss: 2.4503\tTriple Loss(1): 0.3478\tClassification Loss: 1.7547\r\n",
      "Train Epoch: 9 [68480/209539 (33%)]\tAll Loss: 2.9197\tTriple Loss(0): 0.6609\tClassification Loss: 1.5979\r\n",
      "Train Epoch: 9 [69120/209539 (33%)]\tAll Loss: 1.7379\tTriple Loss(1): 0.1190\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 9 [69760/209539 (33%)]\tAll Loss: 2.4134\tTriple Loss(1): 0.2148\tClassification Loss: 1.9837\r\n",
      "Train Epoch: 9 [70400/209539 (34%)]\tAll Loss: 1.8002\tTriple Loss(1): 0.0792\tClassification Loss: 1.6418\r\n",
      "Train Epoch: 9 [71040/209539 (34%)]\tAll Loss: 2.0161\tTriple Loss(1): 0.1219\tClassification Loss: 1.7723\r\n",
      "Train Epoch: 9 [71680/209539 (34%)]\tAll Loss: 2.2512\tTriple Loss(1): 0.2917\tClassification Loss: 1.6678\r\n",
      "Train Epoch: 9 [72320/209539 (35%)]\tAll Loss: 3.1420\tTriple Loss(0): 0.6705\tClassification Loss: 1.8010\r\n",
      "Train Epoch: 9 [72960/209539 (35%)]\tAll Loss: 2.1703\tTriple Loss(1): 0.2337\tClassification Loss: 1.7029\r\n",
      "Train Epoch: 9 [73600/209539 (35%)]\tAll Loss: 1.9609\tTriple Loss(1): 0.1252\tClassification Loss: 1.7106\r\n",
      "Train Epoch: 9 [74240/209539 (35%)]\tAll Loss: 2.4653\tTriple Loss(1): 0.3010\tClassification Loss: 1.8633\r\n",
      "Train Epoch: 9 [74880/209539 (36%)]\tAll Loss: 3.5200\tTriple Loss(0): 0.9195\tClassification Loss: 1.6810\r\n",
      "Train Epoch: 9 [75520/209539 (36%)]\tAll Loss: 2.1437\tTriple Loss(1): 0.1964\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 9 [76160/209539 (36%)]\tAll Loss: 2.1912\tTriple Loss(1): 0.1777\tClassification Loss: 1.8357\r\n",
      "Train Epoch: 9 [76800/209539 (37%)]\tAll Loss: 2.2828\tTriple Loss(1): 0.2348\tClassification Loss: 1.8131\r\n",
      "Train Epoch: 9 [77440/209539 (37%)]\tAll Loss: 2.6079\tTriple Loss(1): 0.3197\tClassification Loss: 1.9685\r\n",
      "Train Epoch: 9 [78080/209539 (37%)]\tAll Loss: 2.1609\tTriple Loss(1): 0.1629\tClassification Loss: 1.8352\r\n",
      "Train Epoch: 9 [78720/209539 (38%)]\tAll Loss: 2.7894\tTriple Loss(1): 0.2471\tClassification Loss: 2.2953\r\n",
      "Train Epoch: 9 [79360/209539 (38%)]\tAll Loss: 2.4137\tTriple Loss(1): 0.1749\tClassification Loss: 2.0639\r\n",
      "Train Epoch: 9 [80000/209539 (38%)]\tAll Loss: 3.9973\tTriple Loss(0): 1.0305\tClassification Loss: 1.9363\r\n",
      "Train Epoch: 9 [80640/209539 (38%)]\tAll Loss: 2.1637\tTriple Loss(1): 0.2961\tClassification Loss: 1.5715\r\n",
      "Train Epoch: 9 [81280/209539 (39%)]\tAll Loss: 3.0576\tTriple Loss(0): 0.5884\tClassification Loss: 1.8808\r\n",
      "Train Epoch: 9 [81920/209539 (39%)]\tAll Loss: 2.2736\tTriple Loss(1): 0.2605\tClassification Loss: 1.7527\r\n",
      "Train Epoch: 9 [82560/209539 (39%)]\tAll Loss: 2.0666\tTriple Loss(1): 0.1042\tClassification Loss: 1.8582\r\n",
      "Train Epoch: 9 [83200/209539 (40%)]\tAll Loss: 1.8076\tTriple Loss(1): 0.1055\tClassification Loss: 1.5966\r\n",
      "Train Epoch: 9 [83840/209539 (40%)]\tAll Loss: 2.5634\tTriple Loss(1): 0.2633\tClassification Loss: 2.0368\r\n",
      "Train Epoch: 9 [84480/209539 (40%)]\tAll Loss: 3.5841\tTriple Loss(0): 0.9125\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 9 [85120/209539 (41%)]\tAll Loss: 3.1691\tTriple Loss(0): 0.7440\tClassification Loss: 1.6810\r\n",
      "Train Epoch: 9 [85760/209539 (41%)]\tAll Loss: 1.8228\tTriple Loss(1): 0.0653\tClassification Loss: 1.6921\r\n",
      "Train Epoch: 9 [86400/209539 (41%)]\tAll Loss: 2.1388\tTriple Loss(1): 0.1505\tClassification Loss: 1.8379\r\n",
      "Train Epoch: 9 [87040/209539 (42%)]\tAll Loss: 2.3865\tTriple Loss(1): 0.2445\tClassification Loss: 1.8975\r\n",
      "Train Epoch: 9 [87680/209539 (42%)]\tAll Loss: 2.2458\tTriple Loss(1): 0.2379\tClassification Loss: 1.7701\r\n",
      "Train Epoch: 9 [88320/209539 (42%)]\tAll Loss: 3.3575\tTriple Loss(0): 0.7013\tClassification Loss: 1.9550\r\n",
      "Train Epoch: 9 [88960/209539 (42%)]\tAll Loss: 2.5681\tTriple Loss(1): 0.3702\tClassification Loss: 1.8276\r\n",
      "Train Epoch: 9 [89600/209539 (43%)]\tAll Loss: 2.3088\tTriple Loss(1): 0.2204\tClassification Loss: 1.8680\r\n",
      "Train Epoch: 9 [90240/209539 (43%)]\tAll Loss: 2.2127\tTriple Loss(1): 0.2087\tClassification Loss: 1.7952\r\n",
      "Train Epoch: 9 [90880/209539 (43%)]\tAll Loss: 2.0484\tTriple Loss(1): 0.1604\tClassification Loss: 1.7277\r\n",
      "Train Epoch: 9 [91520/209539 (44%)]\tAll Loss: 2.3460\tTriple Loss(1): 0.2335\tClassification Loss: 1.8791\r\n",
      "Train Epoch: 9 [92160/209539 (44%)]\tAll Loss: 3.1879\tTriple Loss(0): 0.7753\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 9 [92800/209539 (44%)]\tAll Loss: 1.6529\tTriple Loss(1): 0.2394\tClassification Loss: 1.1742\r\n",
      "Train Epoch: 9 [93440/209539 (45%)]\tAll Loss: 2.4652\tTriple Loss(1): 0.2333\tClassification Loss: 1.9986\r\n",
      "Train Epoch: 9 [94080/209539 (45%)]\tAll Loss: 1.9117\tTriple Loss(1): 0.1428\tClassification Loss: 1.6261\r\n",
      "Train Epoch: 9 [94720/209539 (45%)]\tAll Loss: 3.2353\tTriple Loss(0): 0.6502\tClassification Loss: 1.9349\r\n",
      "Train Epoch: 9 [95360/209539 (46%)]\tAll Loss: 2.2069\tTriple Loss(1): 0.1109\tClassification Loss: 1.9852\r\n",
      "Train Epoch: 9 [96000/209539 (46%)]\tAll Loss: 2.4663\tTriple Loss(1): 0.2283\tClassification Loss: 2.0097\r\n",
      "Train Epoch: 9 [96640/209539 (46%)]\tAll Loss: 2.4489\tTriple Loss(1): 0.2153\tClassification Loss: 2.0182\r\n",
      "Train Epoch: 9 [97280/209539 (46%)]\tAll Loss: 2.0929\tTriple Loss(1): 0.1269\tClassification Loss: 1.8391\r\n",
      "Train Epoch: 9 [97920/209539 (47%)]\tAll Loss: 3.2257\tTriple Loss(0): 0.7227\tClassification Loss: 1.7804\r\n",
      "Train Epoch: 9 [98560/209539 (47%)]\tAll Loss: 1.9470\tTriple Loss(1): 0.1858\tClassification Loss: 1.5754\r\n",
      "Train Epoch: 9 [99200/209539 (47%)]\tAll Loss: 3.3295\tTriple Loss(0): 0.7497\tClassification Loss: 1.8301\r\n",
      "Train Epoch: 9 [99840/209539 (48%)]\tAll Loss: 2.4436\tTriple Loss(1): 0.3143\tClassification Loss: 1.8150\r\n",
      "Train Epoch: 9 [100480/209539 (48%)]\tAll Loss: 2.2870\tTriple Loss(1): 0.1504\tClassification Loss: 1.9862\r\n",
      "Train Epoch: 9 [101120/209539 (48%)]\tAll Loss: 1.8942\tTriple Loss(1): 0.0737\tClassification Loss: 1.7468\r\n",
      "Train Epoch: 9 [101760/209539 (49%)]\tAll Loss: 3.2739\tTriple Loss(0): 0.7480\tClassification Loss: 1.7780\r\n",
      "Train Epoch: 9 [102400/209539 (49%)]\tAll Loss: 2.1906\tTriple Loss(1): 0.2838\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 9 [103040/209539 (49%)]\tAll Loss: 2.7883\tTriple Loss(1): 0.2641\tClassification Loss: 2.2601\r\n",
      "Train Epoch: 9 [103680/209539 (49%)]\tAll Loss: 2.2315\tTriple Loss(1): 0.1508\tClassification Loss: 1.9298\r\n",
      "Train Epoch: 9 [104320/209539 (50%)]\tAll Loss: 2.2057\tTriple Loss(1): 0.1823\tClassification Loss: 1.8412\r\n",
      "Train Epoch: 9 [104960/209539 (50%)]\tAll Loss: 2.0422\tTriple Loss(1): 0.2421\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 9 [105600/209539 (50%)]\tAll Loss: 2.5528\tTriple Loss(0): 0.6140\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 9 [106240/209539 (51%)]\tAll Loss: 2.3620\tTriple Loss(1): 0.1905\tClassification Loss: 1.9809\r\n",
      "Train Epoch: 9 [106880/209539 (51%)]\tAll Loss: 1.9156\tTriple Loss(1): 0.1304\tClassification Loss: 1.6549\r\n",
      "Train Epoch: 9 [107520/209539 (51%)]\tAll Loss: 2.1682\tTriple Loss(1): 0.1653\tClassification Loss: 1.8376\r\n",
      "Train Epoch: 9 [108160/209539 (52%)]\tAll Loss: 1.9439\tTriple Loss(1): 0.1675\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 9 [108800/209539 (52%)]\tAll Loss: 2.5068\tTriple Loss(1): 0.2507\tClassification Loss: 2.0054\r\n",
      "Train Epoch: 9 [109440/209539 (52%)]\tAll Loss: 2.1020\tTriple Loss(1): 0.1580\tClassification Loss: 1.7860\r\n",
      "Train Epoch: 9 [110080/209539 (53%)]\tAll Loss: 1.9039\tTriple Loss(1): 0.1472\tClassification Loss: 1.6094\r\n",
      "Train Epoch: 9 [110720/209539 (53%)]\tAll Loss: 2.2633\tTriple Loss(1): 0.2244\tClassification Loss: 1.8144\r\n",
      "Train Epoch: 9 [111360/209539 (53%)]\tAll Loss: 1.6695\tTriple Loss(1): 0.1152\tClassification Loss: 1.4391\r\n",
      "Train Epoch: 9 [112000/209539 (53%)]\tAll Loss: 3.3358\tTriple Loss(0): 0.7814\tClassification Loss: 1.7730\r\n",
      "Train Epoch: 9 [112640/209539 (54%)]\tAll Loss: 1.7206\tTriple Loss(1): 0.1082\tClassification Loss: 1.5043\r\n",
      "Train Epoch: 9 [113280/209539 (54%)]\tAll Loss: 1.7631\tTriple Loss(1): 0.0843\tClassification Loss: 1.5944\r\n",
      "Train Epoch: 9 [113920/209539 (54%)]\tAll Loss: 3.2960\tTriple Loss(0): 0.6732\tClassification Loss: 1.9495\r\n",
      "Train Epoch: 9 [114560/209539 (55%)]\tAll Loss: 2.2441\tTriple Loss(1): 0.2473\tClassification Loss: 1.7494\r\n",
      "Train Epoch: 9 [115200/209539 (55%)]\tAll Loss: 2.2902\tTriple Loss(1): 0.1914\tClassification Loss: 1.9075\r\n",
      "Train Epoch: 9 [115840/209539 (55%)]\tAll Loss: 3.2462\tTriple Loss(0): 0.6660\tClassification Loss: 1.9143\r\n",
      "Train Epoch: 9 [116480/209539 (56%)]\tAll Loss: 3.0352\tTriple Loss(0): 0.6678\tClassification Loss: 1.6996\r\n",
      "Train Epoch: 9 [117120/209539 (56%)]\tAll Loss: 2.5410\tTriple Loss(1): 0.2892\tClassification Loss: 1.9626\r\n",
      "Train Epoch: 9 [117760/209539 (56%)]\tAll Loss: 1.9883\tTriple Loss(1): 0.1254\tClassification Loss: 1.7375\r\n",
      "Train Epoch: 9 [118400/209539 (57%)]\tAll Loss: 1.8967\tTriple Loss(1): 0.1834\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 9 [119040/209539 (57%)]\tAll Loss: 2.4975\tTriple Loss(1): 0.2288\tClassification Loss: 2.0400\r\n",
      "Train Epoch: 9 [119680/209539 (57%)]\tAll Loss: 2.0712\tTriple Loss(1): 0.2557\tClassification Loss: 1.5599\r\n",
      "Train Epoch: 9 [120320/209539 (57%)]\tAll Loss: 2.1420\tTriple Loss(1): 0.1245\tClassification Loss: 1.8930\r\n",
      "Train Epoch: 9 [120960/209539 (58%)]\tAll Loss: 1.7043\tTriple Loss(1): 0.1292\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 9 [121600/209539 (58%)]\tAll Loss: 1.8543\tTriple Loss(1): 0.1736\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 9 [122240/209539 (58%)]\tAll Loss: 2.9660\tTriple Loss(0): 0.6366\tClassification Loss: 1.6928\r\n",
      "Train Epoch: 9 [122880/209539 (59%)]\tAll Loss: 2.0750\tTriple Loss(1): 0.1565\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 9 [123520/209539 (59%)]\tAll Loss: 3.1266\tTriple Loss(0): 0.6991\tClassification Loss: 1.7284\r\n",
      "Train Epoch: 9 [124160/209539 (59%)]\tAll Loss: 2.0797\tTriple Loss(1): 0.0912\tClassification Loss: 1.8973\r\n",
      "Train Epoch: 9 [124800/209539 (60%)]\tAll Loss: 1.9398\tTriple Loss(1): 0.1510\tClassification Loss: 1.6378\r\n",
      "Train Epoch: 9 [125440/209539 (60%)]\tAll Loss: 3.1476\tTriple Loss(0): 0.6474\tClassification Loss: 1.8528\r\n",
      "Train Epoch: 9 [126080/209539 (60%)]\tAll Loss: 2.1038\tTriple Loss(1): 0.1214\tClassification Loss: 1.8610\r\n",
      "Train Epoch: 9 [126720/209539 (60%)]\tAll Loss: 2.0383\tTriple Loss(1): 0.1735\tClassification Loss: 1.6914\r\n",
      "Train Epoch: 9 [127360/209539 (61%)]\tAll Loss: 2.1262\tTriple Loss(1): 0.1670\tClassification Loss: 1.7921\r\n",
      "Train Epoch: 9 [128000/209539 (61%)]\tAll Loss: 3.1140\tTriple Loss(0): 0.6760\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 9 [128640/209539 (61%)]\tAll Loss: 2.0725\tTriple Loss(1): 0.1954\tClassification Loss: 1.6816\r\n",
      "Train Epoch: 9 [129280/209539 (62%)]\tAll Loss: 2.2213\tTriple Loss(1): 0.2470\tClassification Loss: 1.7274\r\n",
      "Train Epoch: 9 [129920/209539 (62%)]\tAll Loss: 2.3140\tTriple Loss(1): 0.1738\tClassification Loss: 1.9663\r\n",
      "Train Epoch: 9 [130560/209539 (62%)]\tAll Loss: 1.8445\tTriple Loss(1): 0.1493\tClassification Loss: 1.5459\r\n",
      "Train Epoch: 9 [131200/209539 (63%)]\tAll Loss: 2.2786\tTriple Loss(1): 0.1296\tClassification Loss: 2.0195\r\n",
      "Train Epoch: 9 [131840/209539 (63%)]\tAll Loss: 1.7984\tTriple Loss(1): 0.2097\tClassification Loss: 1.3791\r\n",
      "Train Epoch: 9 [132480/209539 (63%)]\tAll Loss: 2.1538\tTriple Loss(1): 0.1273\tClassification Loss: 1.8992\r\n",
      "Train Epoch: 9 [133120/209539 (64%)]\tAll Loss: 1.9875\tTriple Loss(1): 0.2033\tClassification Loss: 1.5810\r\n",
      "Train Epoch: 9 [133760/209539 (64%)]\tAll Loss: 1.9220\tTriple Loss(1): 0.2198\tClassification Loss: 1.4825\r\n",
      "Train Epoch: 9 [134400/209539 (64%)]\tAll Loss: 2.2543\tTriple Loss(1): 0.1573\tClassification Loss: 1.9398\r\n",
      "Train Epoch: 9 [135040/209539 (64%)]\tAll Loss: 2.0276\tTriple Loss(1): 0.1035\tClassification Loss: 1.8207\r\n",
      "Train Epoch: 9 [135680/209539 (65%)]\tAll Loss: 2.4628\tTriple Loss(1): 0.1590\tClassification Loss: 2.1447\r\n",
      "Train Epoch: 9 [136320/209539 (65%)]\tAll Loss: 2.4732\tTriple Loss(1): 0.3152\tClassification Loss: 1.8429\r\n",
      "Train Epoch: 9 [136960/209539 (65%)]\tAll Loss: 2.0354\tTriple Loss(1): 0.1543\tClassification Loss: 1.7268\r\n",
      "Train Epoch: 9 [137600/209539 (66%)]\tAll Loss: 2.1486\tTriple Loss(1): 0.1868\tClassification Loss: 1.7749\r\n",
      "Train Epoch: 9 [138240/209539 (66%)]\tAll Loss: 1.9918\tTriple Loss(1): 0.1440\tClassification Loss: 1.7039\r\n",
      "Train Epoch: 9 [138880/209539 (66%)]\tAll Loss: 3.0391\tTriple Loss(0): 0.5925\tClassification Loss: 1.8541\r\n",
      "Train Epoch: 9 [139520/209539 (67%)]\tAll Loss: 3.0015\tTriple Loss(0): 0.5322\tClassification Loss: 1.9371\r\n",
      "Train Epoch: 9 [140160/209539 (67%)]\tAll Loss: 2.2893\tTriple Loss(1): 0.1210\tClassification Loss: 2.0473\r\n",
      "Train Epoch: 9 [140800/209539 (67%)]\tAll Loss: 2.8884\tTriple Loss(0): 0.5730\tClassification Loss: 1.7423\r\n",
      "Train Epoch: 9 [141440/209539 (68%)]\tAll Loss: 2.0637\tTriple Loss(1): 0.1712\tClassification Loss: 1.7213\r\n",
      "Train Epoch: 9 [142080/209539 (68%)]\tAll Loss: 2.0745\tTriple Loss(1): 0.1687\tClassification Loss: 1.7371\r\n",
      "Train Epoch: 9 [142720/209539 (68%)]\tAll Loss: 2.3147\tTriple Loss(1): 0.2650\tClassification Loss: 1.7847\r\n",
      "Train Epoch: 9 [143360/209539 (68%)]\tAll Loss: 2.0582\tTriple Loss(1): 0.2615\tClassification Loss: 1.5351\r\n",
      "Train Epoch: 9 [144000/209539 (69%)]\tAll Loss: 2.4643\tTriple Loss(1): 0.2577\tClassification Loss: 1.9489\r\n",
      "Train Epoch: 9 [144640/209539 (69%)]\tAll Loss: 2.0407\tTriple Loss(1): 0.0762\tClassification Loss: 1.8884\r\n",
      "Train Epoch: 9 [145280/209539 (69%)]\tAll Loss: 2.3384\tTriple Loss(1): 0.2050\tClassification Loss: 1.9285\r\n",
      "Train Epoch: 9 [145920/209539 (70%)]\tAll Loss: 1.6846\tTriple Loss(1): 0.0383\tClassification Loss: 1.6080\r\n",
      "Train Epoch: 9 [146560/209539 (70%)]\tAll Loss: 2.7340\tTriple Loss(1): 0.2392\tClassification Loss: 2.2557\r\n",
      "Train Epoch: 9 [147200/209539 (70%)]\tAll Loss: 3.3499\tTriple Loss(0): 0.6556\tClassification Loss: 2.0387\r\n",
      "Train Epoch: 9 [147840/209539 (71%)]\tAll Loss: 1.9833\tTriple Loss(1): 0.1528\tClassification Loss: 1.6777\r\n",
      "Train Epoch: 9 [148480/209539 (71%)]\tAll Loss: 1.9721\tTriple Loss(1): 0.1145\tClassification Loss: 1.7432\r\n",
      "Train Epoch: 9 [149120/209539 (71%)]\tAll Loss: 2.3061\tTriple Loss(1): 0.2368\tClassification Loss: 1.8326\r\n",
      "Train Epoch: 9 [149760/209539 (71%)]\tAll Loss: 2.5694\tTriple Loss(1): 0.2008\tClassification Loss: 2.1677\r\n",
      "Train Epoch: 9 [150400/209539 (72%)]\tAll Loss: 2.3095\tTriple Loss(1): 0.1876\tClassification Loss: 1.9342\r\n",
      "Train Epoch: 9 [151040/209539 (72%)]\tAll Loss: 2.2024\tTriple Loss(1): 0.2258\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 9 [151680/209539 (72%)]\tAll Loss: 2.1952\tTriple Loss(1): 0.1758\tClassification Loss: 1.8436\r\n",
      "Train Epoch: 9 [152320/209539 (73%)]\tAll Loss: 2.0883\tTriple Loss(1): 0.0908\tClassification Loss: 1.9066\r\n",
      "Train Epoch: 9 [152960/209539 (73%)]\tAll Loss: 3.1189\tTriple Loss(0): 0.5740\tClassification Loss: 1.9709\r\n",
      "Train Epoch: 9 [153600/209539 (73%)]\tAll Loss: 2.1862\tTriple Loss(1): 0.2425\tClassification Loss: 1.7011\r\n",
      "Train Epoch: 9 [154240/209539 (74%)]\tAll Loss: 2.1313\tTriple Loss(1): 0.1863\tClassification Loss: 1.7586\r\n",
      "Train Epoch: 9 [154880/209539 (74%)]\tAll Loss: 3.2575\tTriple Loss(0): 0.7935\tClassification Loss: 1.6704\r\n",
      "Train Epoch: 9 [155520/209539 (74%)]\tAll Loss: 2.0313\tTriple Loss(1): 0.2023\tClassification Loss: 1.6267\r\n",
      "Train Epoch: 9 [156160/209539 (75%)]\tAll Loss: 2.6127\tTriple Loss(1): 0.2079\tClassification Loss: 2.1970\r\n",
      "Train Epoch: 9 [156800/209539 (75%)]\tAll Loss: 2.4446\tTriple Loss(1): 0.1946\tClassification Loss: 2.0553\r\n",
      "Train Epoch: 9 [157440/209539 (75%)]\tAll Loss: 2.3410\tTriple Loss(1): 0.2173\tClassification Loss: 1.9064\r\n",
      "Train Epoch: 9 [158080/209539 (75%)]\tAll Loss: 1.9861\tTriple Loss(1): 0.1369\tClassification Loss: 1.7124\r\n",
      "Train Epoch: 9 [158720/209539 (76%)]\tAll Loss: 1.9232\tTriple Loss(1): 0.1066\tClassification Loss: 1.7100\r\n",
      "Train Epoch: 9 [159360/209539 (76%)]\tAll Loss: 2.1234\tTriple Loss(1): 0.0580\tClassification Loss: 2.0073\r\n",
      "Train Epoch: 9 [160000/209539 (76%)]\tAll Loss: 2.1000\tTriple Loss(1): 0.1317\tClassification Loss: 1.8367\r\n",
      "Train Epoch: 9 [160640/209539 (77%)]\tAll Loss: 2.2018\tTriple Loss(1): 0.1827\tClassification Loss: 1.8364\r\n",
      "Train Epoch: 9 [161280/209539 (77%)]\tAll Loss: 2.2436\tTriple Loss(1): 0.2262\tClassification Loss: 1.7913\r\n",
      "Train Epoch: 9 [161920/209539 (77%)]\tAll Loss: 1.7027\tTriple Loss(1): 0.0996\tClassification Loss: 1.5035\r\n",
      "Train Epoch: 9 [162560/209539 (78%)]\tAll Loss: 1.7221\tTriple Loss(1): 0.0606\tClassification Loss: 1.6009\r\n",
      "Train Epoch: 9 [163200/209539 (78%)]\tAll Loss: 1.8131\tTriple Loss(1): 0.0799\tClassification Loss: 1.6533\r\n",
      "Train Epoch: 9 [163840/209539 (78%)]\tAll Loss: 2.2282\tTriple Loss(1): 0.1693\tClassification Loss: 1.8897\r\n",
      "Train Epoch: 9 [164480/209539 (78%)]\tAll Loss: 3.1854\tTriple Loss(0): 0.7120\tClassification Loss: 1.7615\r\n",
      "Train Epoch: 9 [165120/209539 (79%)]\tAll Loss: 2.0857\tTriple Loss(1): 0.1726\tClassification Loss: 1.7405\r\n",
      "Train Epoch: 9 [165760/209539 (79%)]\tAll Loss: 3.1359\tTriple Loss(0): 0.5717\tClassification Loss: 1.9925\r\n",
      "Train Epoch: 9 [166400/209539 (79%)]\tAll Loss: 2.2150\tTriple Loss(1): 0.1415\tClassification Loss: 1.9320\r\n",
      "Train Epoch: 9 [167040/209539 (80%)]\tAll Loss: 2.1445\tTriple Loss(1): 0.2657\tClassification Loss: 1.6130\r\n",
      "Train Epoch: 9 [167680/209539 (80%)]\tAll Loss: 3.6682\tTriple Loss(0): 0.8744\tClassification Loss: 1.9193\r\n",
      "Train Epoch: 9 [168320/209539 (80%)]\tAll Loss: 2.3793\tTriple Loss(1): 0.2317\tClassification Loss: 1.9158\r\n",
      "Train Epoch: 9 [168960/209539 (81%)]\tAll Loss: 2.4455\tTriple Loss(1): 0.3163\tClassification Loss: 1.8128\r\n",
      "Train Epoch: 9 [169600/209539 (81%)]\tAll Loss: 3.0145\tTriple Loss(0): 0.5687\tClassification Loss: 1.8770\r\n",
      "Train Epoch: 9 [170240/209539 (81%)]\tAll Loss: 2.1226\tTriple Loss(1): 0.2522\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 9 [170880/209539 (82%)]\tAll Loss: 2.0723\tTriple Loss(1): 0.1753\tClassification Loss: 1.7218\r\n",
      "Train Epoch: 9 [171520/209539 (82%)]\tAll Loss: 2.3892\tTriple Loss(1): 0.2444\tClassification Loss: 1.9004\r\n",
      "Train Epoch: 9 [172160/209539 (82%)]\tAll Loss: 1.9938\tTriple Loss(1): 0.0637\tClassification Loss: 1.8664\r\n",
      "Train Epoch: 9 [172800/209539 (82%)]\tAll Loss: 2.4960\tTriple Loss(1): 0.2455\tClassification Loss: 2.0051\r\n",
      "Train Epoch: 9 [173440/209539 (83%)]\tAll Loss: 1.8961\tTriple Loss(1): 0.0881\tClassification Loss: 1.7198\r\n",
      "Train Epoch: 9 [174080/209539 (83%)]\tAll Loss: 3.1118\tTriple Loss(0): 0.7233\tClassification Loss: 1.6653\r\n",
      "Train Epoch: 9 [174720/209539 (83%)]\tAll Loss: 2.0234\tTriple Loss(1): 0.1644\tClassification Loss: 1.6947\r\n",
      "Train Epoch: 9 [175360/209539 (84%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.1927\tClassification Loss: 1.9981\r\n",
      "Train Epoch: 9 [176000/209539 (84%)]\tAll Loss: 2.4947\tTriple Loss(0): 0.4652\tClassification Loss: 1.5642\r\n",
      "Train Epoch: 9 [176640/209539 (84%)]\tAll Loss: 2.3119\tTriple Loss(1): 0.1877\tClassification Loss: 1.9364\r\n",
      "Train Epoch: 9 [177280/209539 (85%)]\tAll Loss: 2.4322\tTriple Loss(1): 0.2097\tClassification Loss: 2.0127\r\n",
      "Train Epoch: 9 [177920/209539 (85%)]\tAll Loss: 3.6199\tTriple Loss(0): 0.8557\tClassification Loss: 1.9085\r\n",
      "Train Epoch: 9 [178560/209539 (85%)]\tAll Loss: 1.8675\tTriple Loss(1): 0.1498\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 9 [179200/209539 (86%)]\tAll Loss: 1.9364\tTriple Loss(1): 0.1568\tClassification Loss: 1.6228\r\n",
      "Train Epoch: 9 [179840/209539 (86%)]\tAll Loss: 1.8166\tTriple Loss(1): 0.1531\tClassification Loss: 1.5104\r\n",
      "Train Epoch: 9 [180480/209539 (86%)]\tAll Loss: 2.8665\tTriple Loss(0): 0.6245\tClassification Loss: 1.6175\r\n",
      "Train Epoch: 9 [181120/209539 (86%)]\tAll Loss: 2.4925\tTriple Loss(1): 0.2093\tClassification Loss: 2.0739\r\n",
      "Train Epoch: 9 [181760/209539 (87%)]\tAll Loss: 2.1880\tTriple Loss(1): 0.1764\tClassification Loss: 1.8352\r\n",
      "Train Epoch: 9 [182400/209539 (87%)]\tAll Loss: 2.2409\tTriple Loss(1): 0.2354\tClassification Loss: 1.7701\r\n",
      "Train Epoch: 9 [183040/209539 (87%)]\tAll Loss: 2.2404\tTriple Loss(1): 0.1804\tClassification Loss: 1.8796\r\n",
      "Train Epoch: 9 [183680/209539 (88%)]\tAll Loss: 3.2512\tTriple Loss(0): 0.8127\tClassification Loss: 1.6257\r\n",
      "Train Epoch: 9 [184320/209539 (88%)]\tAll Loss: 2.0273\tTriple Loss(1): 0.1570\tClassification Loss: 1.7132\r\n",
      "Train Epoch: 9 [184960/209539 (88%)]\tAll Loss: 3.4692\tTriple Loss(0): 0.8449\tClassification Loss: 1.7794\r\n",
      "Train Epoch: 9 [185600/209539 (89%)]\tAll Loss: 2.0400\tTriple Loss(1): 0.1382\tClassification Loss: 1.7636\r\n",
      "Train Epoch: 9 [186240/209539 (89%)]\tAll Loss: 2.1532\tTriple Loss(1): 0.1128\tClassification Loss: 1.9276\r\n",
      "Train Epoch: 9 [186880/209539 (89%)]\tAll Loss: 2.8942\tTriple Loss(0): 0.6764\tClassification Loss: 1.5415\r\n",
      "Train Epoch: 9 [187520/209539 (89%)]\tAll Loss: 2.2337\tTriple Loss(1): 0.2213\tClassification Loss: 1.7911\r\n",
      "Train Epoch: 9 [188160/209539 (90%)]\tAll Loss: 3.7104\tTriple Loss(0): 0.9212\tClassification Loss: 1.8681\r\n",
      "Train Epoch: 9 [188800/209539 (90%)]\tAll Loss: 2.3918\tTriple Loss(1): 0.2660\tClassification Loss: 1.8598\r\n",
      "Train Epoch: 9 [189440/209539 (90%)]\tAll Loss: 2.1208\tTriple Loss(1): 0.1238\tClassification Loss: 1.8732\r\n",
      "Train Epoch: 9 [190080/209539 (91%)]\tAll Loss: 1.9024\tTriple Loss(1): 0.1828\tClassification Loss: 1.5369\r\n",
      "Train Epoch: 9 [190720/209539 (91%)]\tAll Loss: 2.2505\tTriple Loss(1): 0.1712\tClassification Loss: 1.9082\r\n",
      "Train Epoch: 9 [191360/209539 (91%)]\tAll Loss: 3.5910\tTriple Loss(0): 0.8965\tClassification Loss: 1.7980\r\n",
      "Train Epoch: 9 [192000/209539 (92%)]\tAll Loss: 2.4292\tTriple Loss(1): 0.2656\tClassification Loss: 1.8980\r\n",
      "Train Epoch: 9 [192640/209539 (92%)]\tAll Loss: 2.0893\tTriple Loss(1): 0.1081\tClassification Loss: 1.8730\r\n",
      "Train Epoch: 9 [193280/209539 (92%)]\tAll Loss: 2.1391\tTriple Loss(1): 0.1389\tClassification Loss: 1.8613\r\n",
      "Train Epoch: 9 [193920/209539 (93%)]\tAll Loss: 2.2296\tTriple Loss(1): 0.2109\tClassification Loss: 1.8078\r\n",
      "Train Epoch: 9 [194560/209539 (93%)]\tAll Loss: 3.1602\tTriple Loss(0): 0.6969\tClassification Loss: 1.7664\r\n",
      "Train Epoch: 9 [195200/209539 (93%)]\tAll Loss: 3.2703\tTriple Loss(0): 0.7580\tClassification Loss: 1.7544\r\n",
      "Train Epoch: 9 [195840/209539 (93%)]\tAll Loss: 1.8226\tTriple Loss(1): 0.0349\tClassification Loss: 1.7529\r\n",
      "Train Epoch: 9 [196480/209539 (94%)]\tAll Loss: 2.2203\tTriple Loss(1): 0.2632\tClassification Loss: 1.6939\r\n",
      "Train Epoch: 9 [197120/209539 (94%)]\tAll Loss: 2.3749\tTriple Loss(1): 0.2507\tClassification Loss: 1.8735\r\n",
      "Train Epoch: 9 [197760/209539 (94%)]\tAll Loss: 1.9349\tTriple Loss(1): 0.1319\tClassification Loss: 1.6712\r\n",
      "Train Epoch: 9 [198400/209539 (95%)]\tAll Loss: 1.7988\tTriple Loss(1): 0.2081\tClassification Loss: 1.3825\r\n",
      "Train Epoch: 9 [199040/209539 (95%)]\tAll Loss: 2.0569\tTriple Loss(1): 0.1789\tClassification Loss: 1.6991\r\n",
      "Train Epoch: 9 [199680/209539 (95%)]\tAll Loss: 3.7505\tTriple Loss(0): 0.8983\tClassification Loss: 1.9539\r\n",
      "Train Epoch: 9 [200320/209539 (96%)]\tAll Loss: 2.0168\tTriple Loss(1): 0.0811\tClassification Loss: 1.8546\r\n",
      "Train Epoch: 9 [200960/209539 (96%)]\tAll Loss: 1.9941\tTriple Loss(1): 0.1863\tClassification Loss: 1.6216\r\n",
      "Train Epoch: 9 [201600/209539 (96%)]\tAll Loss: 2.3058\tTriple Loss(1): 0.2295\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 9 [202240/209539 (97%)]\tAll Loss: 3.3173\tTriple Loss(0): 0.7419\tClassification Loss: 1.8335\r\n",
      "Train Epoch: 9 [202880/209539 (97%)]\tAll Loss: 2.0574\tTriple Loss(1): 0.2184\tClassification Loss: 1.6206\r\n",
      "Train Epoch: 9 [203520/209539 (97%)]\tAll Loss: 2.3440\tTriple Loss(1): 0.1713\tClassification Loss: 2.0015\r\n",
      "Train Epoch: 9 [204160/209539 (97%)]\tAll Loss: 2.5750\tTriple Loss(1): 0.2257\tClassification Loss: 2.1237\r\n",
      "Train Epoch: 9 [204800/209539 (98%)]\tAll Loss: 1.5794\tTriple Loss(1): 0.0019\tClassification Loss: 1.5755\r\n",
      "Train Epoch: 9 [205440/209539 (98%)]\tAll Loss: 1.7942\tTriple Loss(1): 0.0733\tClassification Loss: 1.6477\r\n",
      "Train Epoch: 9 [206080/209539 (98%)]\tAll Loss: 2.1093\tTriple Loss(1): 0.1492\tClassification Loss: 1.8109\r\n",
      "Train Epoch: 9 [206720/209539 (99%)]\tAll Loss: 3.1331\tTriple Loss(0): 0.6252\tClassification Loss: 1.8827\r\n",
      "Train Epoch: 9 [207360/209539 (99%)]\tAll Loss: 2.0366\tTriple Loss(1): 0.1551\tClassification Loss: 1.7263\r\n",
      "Train Epoch: 9 [208000/209539 (99%)]\tAll Loss: 2.3785\tTriple Loss(1): 0.2781\tClassification Loss: 1.8222\r\n",
      "Train Epoch: 9 [208640/209539 (100%)]\tAll Loss: 3.0995\tTriple Loss(0): 0.6085\tClassification Loss: 1.8824\r\n",
      "Train Epoch: 9 [209280/209539 (100%)]\tAll Loss: 2.1230\tTriple Loss(1): 0.1931\tClassification Loss: 1.7369\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/9_epochs\r\n",
      "Train Epoch: 10 [0/209539 (0%)]\tAll Loss: 2.6592\tTriple Loss(1): 0.3184\tClassification Loss: 2.0223\r\n",
      "\r\n",
      "Test set: Average loss: 1.6210, Accuracy: 42142/80128 (53%)\r\n",
      "\r\n",
      "Train Epoch: 10 [640/209539 (0%)]\tAll Loss: 3.3744\tTriple Loss(0): 0.7133\tClassification Loss: 1.9478\r\n",
      "Train Epoch: 10 [1280/209539 (1%)]\tAll Loss: 2.5223\tTriple Loss(1): 0.2775\tClassification Loss: 1.9672\r\n",
      "Train Epoch: 10 [1920/209539 (1%)]\tAll Loss: 1.9847\tTriple Loss(1): 0.1914\tClassification Loss: 1.6019\r\n",
      "Train Epoch: 10 [2560/209539 (1%)]\tAll Loss: 2.4340\tTriple Loss(1): 0.2966\tClassification Loss: 1.8408\r\n",
      "Train Epoch: 10 [3200/209539 (2%)]\tAll Loss: 2.2092\tTriple Loss(1): 0.1979\tClassification Loss: 1.8133\r\n",
      "Train Epoch: 10 [3840/209539 (2%)]\tAll Loss: 2.1171\tTriple Loss(1): 0.1040\tClassification Loss: 1.9091\r\n",
      "Train Epoch: 10 [4480/209539 (2%)]\tAll Loss: 3.3404\tTriple Loss(0): 0.7424\tClassification Loss: 1.8555\r\n",
      "Train Epoch: 10 [5120/209539 (2%)]\tAll Loss: 2.1455\tTriple Loss(1): 0.1254\tClassification Loss: 1.8948\r\n",
      "Train Epoch: 10 [5760/209539 (3%)]\tAll Loss: 1.9703\tTriple Loss(1): 0.1433\tClassification Loss: 1.6836\r\n",
      "Train Epoch: 10 [6400/209539 (3%)]\tAll Loss: 2.0474\tTriple Loss(1): 0.1585\tClassification Loss: 1.7304\r\n",
      "Train Epoch: 10 [7040/209539 (3%)]\tAll Loss: 3.4447\tTriple Loss(0): 0.7707\tClassification Loss: 1.9033\r\n",
      "Train Epoch: 10 [7680/209539 (4%)]\tAll Loss: 1.8495\tTriple Loss(1): 0.1450\tClassification Loss: 1.5595\r\n",
      "Train Epoch: 10 [8320/209539 (4%)]\tAll Loss: 2.1241\tTriple Loss(1): 0.1624\tClassification Loss: 1.7993\r\n",
      "Train Epoch: 10 [8960/209539 (4%)]\tAll Loss: 1.9668\tTriple Loss(1): 0.0701\tClassification Loss: 1.8267\r\n",
      "Train Epoch: 10 [9600/209539 (5%)]\tAll Loss: 2.1432\tTriple Loss(1): 0.1437\tClassification Loss: 1.8558\r\n",
      "Train Epoch: 10 [10240/209539 (5%)]\tAll Loss: 2.3247\tTriple Loss(1): 0.1488\tClassification Loss: 2.0271\r\n",
      "Train Epoch: 10 [10880/209539 (5%)]\tAll Loss: 2.2680\tTriple Loss(1): 0.1348\tClassification Loss: 1.9983\r\n",
      "Train Epoch: 10 [11520/209539 (5%)]\tAll Loss: 2.0965\tTriple Loss(1): 0.1750\tClassification Loss: 1.7466\r\n",
      "Train Epoch: 10 [12160/209539 (6%)]\tAll Loss: 2.2248\tTriple Loss(1): 0.1682\tClassification Loss: 1.8883\r\n",
      "Train Epoch: 10 [12800/209539 (6%)]\tAll Loss: 2.2747\tTriple Loss(1): 0.2041\tClassification Loss: 1.8665\r\n",
      "Train Epoch: 10 [13440/209539 (6%)]\tAll Loss: 2.0851\tTriple Loss(1): 0.1421\tClassification Loss: 1.8009\r\n",
      "Train Epoch: 10 [14080/209539 (7%)]\tAll Loss: 3.3825\tTriple Loss(0): 0.6554\tClassification Loss: 2.0717\r\n",
      "Train Epoch: 10 [14720/209539 (7%)]\tAll Loss: 3.4810\tTriple Loss(0): 0.7793\tClassification Loss: 1.9224\r\n",
      "Train Epoch: 10 [15360/209539 (7%)]\tAll Loss: 2.5017\tTriple Loss(1): 0.3229\tClassification Loss: 1.8558\r\n",
      "Train Epoch: 10 [16000/209539 (8%)]\tAll Loss: 2.0446\tTriple Loss(1): 0.1059\tClassification Loss: 1.8327\r\n",
      "Train Epoch: 10 [16640/209539 (8%)]\tAll Loss: 2.2194\tTriple Loss(1): 0.1728\tClassification Loss: 1.8737\r\n",
      "Train Epoch: 10 [17280/209539 (8%)]\tAll Loss: 2.1499\tTriple Loss(1): 0.1393\tClassification Loss: 1.8713\r\n",
      "Train Epoch: 10 [17920/209539 (9%)]\tAll Loss: 1.8384\tTriple Loss(1): 0.1642\tClassification Loss: 1.5100\r\n",
      "Train Epoch: 10 [18560/209539 (9%)]\tAll Loss: 2.2386\tTriple Loss(1): 0.1737\tClassification Loss: 1.8912\r\n",
      "Train Epoch: 10 [19200/209539 (9%)]\tAll Loss: 2.1287\tTriple Loss(1): 0.1589\tClassification Loss: 1.8109\r\n",
      "Train Epoch: 10 [19840/209539 (9%)]\tAll Loss: 2.8432\tTriple Loss(0): 0.5943\tClassification Loss: 1.6545\r\n",
      "Train Epoch: 10 [20480/209539 (10%)]\tAll Loss: 2.2315\tTriple Loss(1): 0.1751\tClassification Loss: 1.8813\r\n",
      "Train Epoch: 10 [21120/209539 (10%)]\tAll Loss: 1.7621\tTriple Loss(1): 0.0635\tClassification Loss: 1.6351\r\n",
      "Train Epoch: 10 [21760/209539 (10%)]\tAll Loss: 3.2661\tTriple Loss(0): 0.6775\tClassification Loss: 1.9111\r\n",
      "Train Epoch: 10 [22400/209539 (11%)]\tAll Loss: 2.0749\tTriple Loss(1): 0.1878\tClassification Loss: 1.6993\r\n",
      "Train Epoch: 10 [23040/209539 (11%)]\tAll Loss: 2.1753\tTriple Loss(1): 0.1164\tClassification Loss: 1.9426\r\n",
      "Train Epoch: 10 [23680/209539 (11%)]\tAll Loss: 2.0331\tTriple Loss(1): 0.2463\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 10 [24320/209539 (12%)]\tAll Loss: 1.9788\tTriple Loss(1): 0.1448\tClassification Loss: 1.6892\r\n",
      "Train Epoch: 10 [24960/209539 (12%)]\tAll Loss: 2.0339\tTriple Loss(1): 0.2069\tClassification Loss: 1.6200\r\n",
      "Train Epoch: 10 [25600/209539 (12%)]\tAll Loss: 1.7528\tTriple Loss(1): 0.1156\tClassification Loss: 1.5216\r\n",
      "Train Epoch: 10 [26240/209539 (13%)]\tAll Loss: 2.3057\tTriple Loss(1): 0.1660\tClassification Loss: 1.9738\r\n",
      "Train Epoch: 10 [26880/209539 (13%)]\tAll Loss: 2.1195\tTriple Loss(1): 0.1838\tClassification Loss: 1.7519\r\n",
      "Train Epoch: 10 [27520/209539 (13%)]\tAll Loss: 2.0914\tTriple Loss(1): 0.2351\tClassification Loss: 1.6212\r\n",
      "Train Epoch: 10 [28160/209539 (13%)]\tAll Loss: 2.1008\tTriple Loss(1): 0.1397\tClassification Loss: 1.8214\r\n",
      "Train Epoch: 10 [28800/209539 (14%)]\tAll Loss: 3.4840\tTriple Loss(0): 0.8240\tClassification Loss: 1.8359\r\n",
      "Train Epoch: 10 [29440/209539 (14%)]\tAll Loss: 2.1350\tTriple Loss(1): 0.0905\tClassification Loss: 1.9540\r\n",
      "Train Epoch: 10 [30080/209539 (14%)]\tAll Loss: 2.1143\tTriple Loss(1): 0.1780\tClassification Loss: 1.7582\r\n",
      "Train Epoch: 10 [30720/209539 (15%)]\tAll Loss: 2.2189\tTriple Loss(1): 0.1465\tClassification Loss: 1.9258\r\n",
      "Train Epoch: 10 [31360/209539 (15%)]\tAll Loss: 2.4473\tTriple Loss(1): 0.1756\tClassification Loss: 2.0960\r\n",
      "Train Epoch: 10 [32000/209539 (15%)]\tAll Loss: 2.4706\tTriple Loss(1): 0.1793\tClassification Loss: 2.1120\r\n",
      "Train Epoch: 10 [32640/209539 (16%)]\tAll Loss: 2.5300\tTriple Loss(1): 0.1865\tClassification Loss: 2.1569\r\n",
      "Train Epoch: 10 [33280/209539 (16%)]\tAll Loss: 2.0302\tTriple Loss(1): 0.1203\tClassification Loss: 1.7896\r\n",
      "Train Epoch: 10 [33920/209539 (16%)]\tAll Loss: 3.2196\tTriple Loss(0): 0.6499\tClassification Loss: 1.9199\r\n",
      "Train Epoch: 10 [34560/209539 (16%)]\tAll Loss: 1.9480\tTriple Loss(1): 0.1262\tClassification Loss: 1.6955\r\n",
      "Train Epoch: 10 [35200/209539 (17%)]\tAll Loss: 2.4881\tTriple Loss(1): 0.2034\tClassification Loss: 2.0814\r\n",
      "Train Epoch: 10 [35840/209539 (17%)]\tAll Loss: 3.4013\tTriple Loss(0): 0.8047\tClassification Loss: 1.7919\r\n",
      "Train Epoch: 10 [36480/209539 (17%)]\tAll Loss: 2.0372\tTriple Loss(1): 0.1234\tClassification Loss: 1.7903\r\n",
      "Train Epoch: 10 [37120/209539 (18%)]\tAll Loss: 2.5031\tTriple Loss(1): 0.2644\tClassification Loss: 1.9743\r\n",
      "Train Epoch: 10 [37760/209539 (18%)]\tAll Loss: 2.2218\tTriple Loss(1): 0.1226\tClassification Loss: 1.9766\r\n",
      "Train Epoch: 10 [38400/209539 (18%)]\tAll Loss: 1.6576\tTriple Loss(1): 0.2203\tClassification Loss: 1.2169\r\n",
      "Train Epoch: 10 [39040/209539 (19%)]\tAll Loss: 3.2977\tTriple Loss(0): 0.7275\tClassification Loss: 1.8426\r\n",
      "Train Epoch: 10 [39680/209539 (19%)]\tAll Loss: 3.0539\tTriple Loss(0): 0.7048\tClassification Loss: 1.6444\r\n",
      "Train Epoch: 10 [40320/209539 (19%)]\tAll Loss: 2.6385\tTriple Loss(1): 0.2497\tClassification Loss: 2.1392\r\n",
      "Train Epoch: 10 [40960/209539 (20%)]\tAll Loss: 2.0613\tTriple Loss(1): 0.0910\tClassification Loss: 1.8793\r\n",
      "Train Epoch: 10 [41600/209539 (20%)]\tAll Loss: 2.7357\tTriple Loss(1): 0.2768\tClassification Loss: 2.1820\r\n",
      "Train Epoch: 10 [42240/209539 (20%)]\tAll Loss: 1.9802\tTriple Loss(1): 0.1062\tClassification Loss: 1.7678\r\n",
      "Train Epoch: 10 [42880/209539 (20%)]\tAll Loss: 2.1126\tTriple Loss(1): 0.0958\tClassification Loss: 1.9210\r\n",
      "Train Epoch: 10 [43520/209539 (21%)]\tAll Loss: 2.6217\tTriple Loss(1): 0.2344\tClassification Loss: 2.1529\r\n",
      "Train Epoch: 10 [44160/209539 (21%)]\tAll Loss: 2.0824\tTriple Loss(1): 0.1662\tClassification Loss: 1.7500\r\n",
      "Train Epoch: 10 [44800/209539 (21%)]\tAll Loss: 3.0962\tTriple Loss(0): 0.7060\tClassification Loss: 1.6842\r\n",
      "Train Epoch: 10 [45440/209539 (22%)]\tAll Loss: 1.6922\tTriple Loss(1): 0.1061\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 10 [46080/209539 (22%)]\tAll Loss: 2.3117\tTriple Loss(1): 0.1462\tClassification Loss: 2.0193\r\n",
      "Train Epoch: 10 [46720/209539 (22%)]\tAll Loss: 2.0340\tTriple Loss(1): 0.1332\tClassification Loss: 1.7676\r\n",
      "Train Epoch: 10 [47360/209539 (23%)]\tAll Loss: 2.3770\tTriple Loss(1): 0.2250\tClassification Loss: 1.9269\r\n",
      "Train Epoch: 10 [48000/209539 (23%)]\tAll Loss: 2.5150\tTriple Loss(1): 0.2263\tClassification Loss: 2.0624\r\n",
      "Train Epoch: 10 [48640/209539 (23%)]\tAll Loss: 2.3195\tTriple Loss(1): 0.2290\tClassification Loss: 1.8614\r\n",
      "Train Epoch: 10 [49280/209539 (24%)]\tAll Loss: 2.1755\tTriple Loss(1): 0.2253\tClassification Loss: 1.7249\r\n",
      "Train Epoch: 10 [49920/209539 (24%)]\tAll Loss: 2.2175\tTriple Loss(1): 0.1684\tClassification Loss: 1.8808\r\n",
      "Train Epoch: 10 [50560/209539 (24%)]\tAll Loss: 3.2371\tTriple Loss(0): 0.7422\tClassification Loss: 1.7526\r\n",
      "Train Epoch: 10 [51200/209539 (24%)]\tAll Loss: 2.7720\tTriple Loss(1): 0.4037\tClassification Loss: 1.9645\r\n",
      "Train Epoch: 10 [51840/209539 (25%)]\tAll Loss: 3.4431\tTriple Loss(0): 0.8150\tClassification Loss: 1.8132\r\n",
      "Train Epoch: 10 [52480/209539 (25%)]\tAll Loss: 4.0386\tTriple Loss(0): 1.0234\tClassification Loss: 1.9918\r\n",
      "Train Epoch: 10 [53120/209539 (25%)]\tAll Loss: 2.6757\tTriple Loss(0): 0.5769\tClassification Loss: 1.5219\r\n",
      "Train Epoch: 10 [53760/209539 (26%)]\tAll Loss: 2.3681\tTriple Loss(1): 0.1427\tClassification Loss: 2.0827\r\n",
      "Train Epoch: 10 [54400/209539 (26%)]\tAll Loss: 2.5085\tTriple Loss(1): 0.2424\tClassification Loss: 2.0237\r\n",
      "Train Epoch: 10 [55040/209539 (26%)]\tAll Loss: 2.0546\tTriple Loss(1): 0.2542\tClassification Loss: 1.5462\r\n",
      "Train Epoch: 10 [55680/209539 (27%)]\tAll Loss: 3.4826\tTriple Loss(0): 0.7589\tClassification Loss: 1.9647\r\n",
      "Train Epoch: 10 [56320/209539 (27%)]\tAll Loss: 1.8134\tTriple Loss(1): 0.1061\tClassification Loss: 1.6012\r\n",
      "Train Epoch: 10 [56960/209539 (27%)]\tAll Loss: 2.1637\tTriple Loss(1): 0.2286\tClassification Loss: 1.7064\r\n",
      "Train Epoch: 10 [57600/209539 (27%)]\tAll Loss: 2.5287\tTriple Loss(1): 0.3212\tClassification Loss: 1.8863\r\n",
      "Train Epoch: 10 [58240/209539 (28%)]\tAll Loss: 2.0674\tTriple Loss(1): 0.1433\tClassification Loss: 1.7809\r\n",
      "Train Epoch: 10 [58880/209539 (28%)]\tAll Loss: 1.9087\tTriple Loss(1): 0.0625\tClassification Loss: 1.7838\r\n",
      "Train Epoch: 10 [59520/209539 (28%)]\tAll Loss: 2.2106\tTriple Loss(1): 0.1350\tClassification Loss: 1.9407\r\n",
      "Train Epoch: 10 [60160/209539 (29%)]\tAll Loss: 2.0858\tTriple Loss(1): 0.2242\tClassification Loss: 1.6374\r\n",
      "Train Epoch: 10 [60800/209539 (29%)]\tAll Loss: 2.0247\tTriple Loss(1): 0.1260\tClassification Loss: 1.7726\r\n",
      "Train Epoch: 10 [61440/209539 (29%)]\tAll Loss: 2.4402\tTriple Loss(1): 0.2980\tClassification Loss: 1.8442\r\n",
      "Train Epoch: 10 [62080/209539 (30%)]\tAll Loss: 3.5504\tTriple Loss(0): 0.8767\tClassification Loss: 1.7970\r\n",
      "Train Epoch: 10 [62720/209539 (30%)]\tAll Loss: 3.3030\tTriple Loss(0): 0.5368\tClassification Loss: 2.2294\r\n",
      "Train Epoch: 10 [63360/209539 (30%)]\tAll Loss: 2.1838\tTriple Loss(1): 0.1154\tClassification Loss: 1.9529\r\n",
      "Train Epoch: 10 [64000/209539 (31%)]\tAll Loss: 2.5074\tTriple Loss(1): 0.2281\tClassification Loss: 2.0511\r\n",
      "Train Epoch: 10 [64640/209539 (31%)]\tAll Loss: 2.3555\tTriple Loss(1): 0.2132\tClassification Loss: 1.9290\r\n",
      "Train Epoch: 10 [65280/209539 (31%)]\tAll Loss: 2.3538\tTriple Loss(1): 0.2032\tClassification Loss: 1.9475\r\n",
      "Train Epoch: 10 [65920/209539 (31%)]\tAll Loss: 2.2004\tTriple Loss(1): 0.1234\tClassification Loss: 1.9536\r\n",
      "Train Epoch: 10 [66560/209539 (32%)]\tAll Loss: 1.9455\tTriple Loss(1): 0.1608\tClassification Loss: 1.6239\r\n",
      "Train Epoch: 10 [67200/209539 (32%)]\tAll Loss: 2.3156\tTriple Loss(1): 0.1257\tClassification Loss: 2.0642\r\n",
      "Train Epoch: 10 [67840/209539 (32%)]\tAll Loss: 2.0082\tTriple Loss(1): 0.2042\tClassification Loss: 1.5998\r\n",
      "Train Epoch: 10 [68480/209539 (33%)]\tAll Loss: 2.0252\tTriple Loss(1): 0.1451\tClassification Loss: 1.7349\r\n",
      "Train Epoch: 10 [69120/209539 (33%)]\tAll Loss: 2.9697\tTriple Loss(0): 0.7132\tClassification Loss: 1.5433\r\n",
      "Train Epoch: 10 [69760/209539 (33%)]\tAll Loss: 3.4723\tTriple Loss(0): 0.7245\tClassification Loss: 2.0234\r\n",
      "Train Epoch: 10 [70400/209539 (34%)]\tAll Loss: 2.5188\tTriple Loss(1): 0.3146\tClassification Loss: 1.8896\r\n",
      "Train Epoch: 10 [71040/209539 (34%)]\tAll Loss: 2.1565\tTriple Loss(1): 0.2050\tClassification Loss: 1.7464\r\n",
      "Train Epoch: 10 [71680/209539 (34%)]\tAll Loss: 2.0906\tTriple Loss(1): 0.1675\tClassification Loss: 1.7555\r\n",
      "Train Epoch: 10 [72320/209539 (35%)]\tAll Loss: 2.4032\tTriple Loss(1): 0.3304\tClassification Loss: 1.7425\r\n",
      "Train Epoch: 10 [72960/209539 (35%)]\tAll Loss: 2.2378\tTriple Loss(1): 0.2290\tClassification Loss: 1.7798\r\n",
      "Train Epoch: 10 [73600/209539 (35%)]\tAll Loss: 1.9745\tTriple Loss(1): 0.1595\tClassification Loss: 1.6556\r\n",
      "Train Epoch: 10 [74240/209539 (35%)]\tAll Loss: 2.7633\tTriple Loss(0): 0.4746\tClassification Loss: 1.8141\r\n",
      "Train Epoch: 10 [74880/209539 (36%)]\tAll Loss: 2.0505\tTriple Loss(1): 0.1942\tClassification Loss: 1.6621\r\n",
      "Train Epoch: 10 [75520/209539 (36%)]\tAll Loss: 2.1371\tTriple Loss(1): 0.1632\tClassification Loss: 1.8107\r\n",
      "Train Epoch: 10 [76160/209539 (36%)]\tAll Loss: 3.2601\tTriple Loss(0): 0.6863\tClassification Loss: 1.8874\r\n",
      "Train Epoch: 10 [76800/209539 (37%)]\tAll Loss: 2.3377\tTriple Loss(1): 0.2178\tClassification Loss: 1.9021\r\n",
      "Train Epoch: 10 [77440/209539 (37%)]\tAll Loss: 1.9207\tTriple Loss(1): 0.0532\tClassification Loss: 1.8143\r\n",
      "Train Epoch: 10 [78080/209539 (37%)]\tAll Loss: 2.1943\tTriple Loss(1): 0.1672\tClassification Loss: 1.8599\r\n",
      "Train Epoch: 10 [78720/209539 (38%)]\tAll Loss: 3.5657\tTriple Loss(0): 0.6841\tClassification Loss: 2.1975\r\n",
      "Train Epoch: 10 [79360/209539 (38%)]\tAll Loss: 2.7030\tTriple Loss(1): 0.2807\tClassification Loss: 2.1417\r\n",
      "Train Epoch: 10 [80000/209539 (38%)]\tAll Loss: 2.4807\tTriple Loss(1): 0.1924\tClassification Loss: 2.0959\r\n",
      "Train Epoch: 10 [80640/209539 (38%)]\tAll Loss: 3.3313\tTriple Loss(0): 0.8563\tClassification Loss: 1.6187\r\n",
      "Train Epoch: 10 [81280/209539 (39%)]\tAll Loss: 2.2679\tTriple Loss(1): 0.2307\tClassification Loss: 1.8064\r\n",
      "Train Epoch: 10 [81920/209539 (39%)]\tAll Loss: 2.0257\tTriple Loss(1): 0.1449\tClassification Loss: 1.7359\r\n",
      "Train Epoch: 10 [82560/209539 (39%)]\tAll Loss: 2.2128\tTriple Loss(1): 0.1691\tClassification Loss: 1.8745\r\n",
      "Train Epoch: 10 [83200/209539 (40%)]\tAll Loss: 1.9593\tTriple Loss(1): 0.1439\tClassification Loss: 1.6715\r\n",
      "Train Epoch: 10 [83840/209539 (40%)]\tAll Loss: 3.0231\tTriple Loss(0): 0.5713\tClassification Loss: 1.8805\r\n",
      "Train Epoch: 10 [84480/209539 (40%)]\tAll Loss: 2.1075\tTriple Loss(1): 0.1607\tClassification Loss: 1.7861\r\n",
      "Train Epoch: 10 [85120/209539 (41%)]\tAll Loss: 3.1932\tTriple Loss(0): 0.7169\tClassification Loss: 1.7595\r\n",
      "Train Epoch: 10 [85760/209539 (41%)]\tAll Loss: 2.0593\tTriple Loss(1): 0.1579\tClassification Loss: 1.7435\r\n",
      "Train Epoch: 10 [86400/209539 (41%)]\tAll Loss: 2.3795\tTriple Loss(1): 0.1477\tClassification Loss: 2.0841\r\n",
      "Train Epoch: 10 [87040/209539 (42%)]\tAll Loss: 3.4772\tTriple Loss(0): 0.8721\tClassification Loss: 1.7329\r\n",
      "Train Epoch: 10 [87680/209539 (42%)]\tAll Loss: 2.0963\tTriple Loss(1): 0.1863\tClassification Loss: 1.7236\r\n",
      "Train Epoch: 10 [88320/209539 (42%)]\tAll Loss: 2.0651\tTriple Loss(1): 0.1068\tClassification Loss: 1.8515\r\n",
      "Train Epoch: 10 [88960/209539 (42%)]\tAll Loss: 1.9918\tTriple Loss(1): 0.1290\tClassification Loss: 1.7338\r\n",
      "Train Epoch: 10 [89600/209539 (43%)]\tAll Loss: 2.3547\tTriple Loss(1): 0.2038\tClassification Loss: 1.9471\r\n",
      "Train Epoch: 10 [90240/209539 (43%)]\tAll Loss: 2.1692\tTriple Loss(1): 0.1638\tClassification Loss: 1.8417\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/9_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/9_epochs\r\n",
      "Train Epoch: 10 [0/209539 (0%)]\tAll Loss: 2.6928\tTriple Loss(1): 0.2851\tClassification Loss: 2.1225\r\n",
      "train.py:215: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.7463\r\n",
      "Top 1 Accuracy: 40126/80128 (50%)\r\n",
      "Top 3 Accuracy: 58558/80128 (73%)\r\n",
      "Top 5 Accuracy: 66582/80128 (83%)\r\n",
      " \r\n",
      "Train Epoch: 10 [32000/209539 (15%)]\tAll Loss: 1.9665\tTriple Loss(1): 0.0668\tClassification Loss: 1.8329\r\n",
      "Train Epoch: 10 [64000/209539 (31%)]\tAll Loss: 2.6538\tTriple Loss(1): 0.1690\tClassification Loss: 2.3159\r\n",
      "Train Epoch: 10 [96000/209539 (46%)]\tAll Loss: 2.0659\tTriple Loss(1): 0.1155\tClassification Loss: 1.8348\r\n",
      "Train Epoch: 10 [128000/209539 (61%)]\tAll Loss: 3.2499\tTriple Loss(0): 0.6467\tClassification Loss: 1.9564\r\n",
      "Train Epoch: 10 [160000/209539 (76%)]\tAll Loss: 2.4433\tTriple Loss(1): 0.0861\tClassification Loss: 2.2710\r\n",
      "Train Epoch: 10 [192000/209539 (92%)]\tAll Loss: 3.4463\tTriple Loss(0): 0.7908\tClassification Loss: 1.8647\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/10_epochs\r\n",
      "Train Epoch: 11 [0/209539 (0%)]\tAll Loss: 2.1574\tTriple Loss(1): 0.1797\tClassification Loss: 1.7979\r\n",
      "\r\n",
      "Test set: Average loss: 1.6820\r\n",
      "Top 1 Accuracy: 41373/80128 (52%)\r\n",
      "Top 3 Accuracy: 60083/80128 (75%)\r\n",
      "Top 5 Accuracy: 67850/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 11 [32000/209539 (15%)]\tAll Loss: 3.5837\tTriple Loss(0): 0.9200\tClassification Loss: 1.7437\r\n",
      "Train Epoch: 11 [64000/209539 (31%)]\tAll Loss: 2.1409\tTriple Loss(1): 0.1434\tClassification Loss: 1.8542\r\n",
      "Train Epoch: 11 [96000/209539 (46%)]\tAll Loss: 2.0646\tTriple Loss(1): 0.1454\tClassification Loss: 1.7739\r\n",
      "Train Epoch: 11 [128000/209539 (61%)]\tAll Loss: 2.9160\tTriple Loss(0): 0.5001\tClassification Loss: 1.9157\r\n",
      "Train Epoch: 11 [160000/209539 (76%)]\tAll Loss: 2.1137\tTriple Loss(1): 0.0963\tClassification Loss: 1.9211\r\n",
      "Train Epoch: 11 [192000/209539 (92%)]\tAll Loss: 1.8917\tTriple Loss(1): 0.0927\tClassification Loss: 1.7063\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/11_epochs\r\n",
      "Train Epoch: 12 [0/209539 (0%)]\tAll Loss: 2.3457\tTriple Loss(1): 0.3504\tClassification Loss: 1.6449\r\n",
      "\r\n",
      "Test set: Average loss: 1.6566\r\n",
      "Top 1 Accuracy: 41961/80128 (52%)\r\n",
      "Top 3 Accuracy: 60537/80128 (76%)\r\n",
      "Top 5 Accuracy: 68171/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 12 [32000/209539 (15%)]\tAll Loss: 2.1475\tTriple Loss(1): 0.1720\tClassification Loss: 1.8034\r\n",
      "Train Epoch: 12 [64000/209539 (31%)]\tAll Loss: 3.5380\tTriple Loss(0): 0.7634\tClassification Loss: 2.0113\r\n",
      "Train Epoch: 12 [96000/209539 (46%)]\tAll Loss: 1.8258\tTriple Loss(1): 0.0531\tClassification Loss: 1.7196\r\n"
     ]
    }
   ],
   "source": [
    "# Loading \"freeze=False/lr=0.002/9_epochs\"\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/11_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/11_epochs\r\n",
      "Train Epoch: 12 [0/209539 (0%)]\tAll Loss: 3.0714\tTriple Loss(1): 0.4702\tClassification Loss: 2.1309\r\n",
      "train.py:215: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.6999\r\n",
      "Top 1 Accuracy: 40619/80128 (51%)\r\n",
      "Top 3 Accuracy: 59355/80128 (74%)\r\n",
      "Top 5 Accuracy: 67394/80128 (84%)\r\n",
      " \r\n",
      "Train Epoch: 12 [32000/209539 (15%)]\tAll Loss: 2.2101\tTriple Loss(1): 0.1880\tClassification Loss: 1.8342\r\n",
      "Train Epoch: 12 [64000/209539 (31%)]\tAll Loss: 2.2187\tTriple Loss(1): 0.1383\tClassification Loss: 1.9421\r\n",
      "Train Epoch: 12 [96000/209539 (46%)]\tAll Loss: 2.3713\tTriple Loss(1): 0.1428\tClassification Loss: 2.0857\r\n",
      "Train Epoch: 12 [128000/209539 (61%)]\tAll Loss: 2.0337\tTriple Loss(1): 0.1632\tClassification Loss: 1.7072\r\n",
      "Train Epoch: 12 [160000/209539 (76%)]\tAll Loss: 1.8181\tTriple Loss(1): 0.0953\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 12 [192000/209539 (92%)]\tAll Loss: 2.2550\tTriple Loss(1): 0.1807\tClassification Loss: 1.8937\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/12_epochs\r\n",
      "Train Epoch: 13 [0/209539 (0%)]\tAll Loss: 2.8811\tTriple Loss(1): 0.3207\tClassification Loss: 2.2397\r\n",
      "\r\n",
      "Test set: Average loss: 1.6369\r\n",
      "Top 1 Accuracy: 42553/80128 (53%)\r\n",
      "Top 3 Accuracy: 60943/80128 (76%)\r\n",
      "Top 5 Accuracy: 68269/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 13 [32000/209539 (15%)]\tAll Loss: 2.2180\tTriple Loss(1): 0.2347\tClassification Loss: 1.7485\r\n",
      "Train Epoch: 13 [64000/209539 (31%)]\tAll Loss: 2.3319\tTriple Loss(1): 0.2120\tClassification Loss: 1.9079\r\n",
      "Train Epoch: 13 [96000/209539 (46%)]\tAll Loss: 2.2055\tTriple Loss(1): 0.0824\tClassification Loss: 2.0407\r\n",
      "Train Epoch: 13 [128000/209539 (61%)]\tAll Loss: 1.8828\tTriple Loss(1): 0.1347\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 13 [160000/209539 (76%)]\tAll Loss: 2.7684\tTriple Loss(0): 0.6053\tClassification Loss: 1.5578\r\n",
      "Train Epoch: 13 [192000/209539 (92%)]\tAll Loss: 1.9636\tTriple Loss(1): 0.1527\tClassification Loss: 1.6581\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/13_epochs\r\n",
      "Train Epoch: 14 [0/209539 (0%)]\tAll Loss: 2.7743\tTriple Loss(1): 0.4220\tClassification Loss: 1.9304\r\n",
      "\r\n",
      "Test set: Average loss: 1.6072\r\n",
      "Top 1 Accuracy: 43091/80128 (54%)\r\n",
      "Top 3 Accuracy: 61431/80128 (77%)\r\n",
      "Top 5 Accuracy: 68614/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 14 [32000/209539 (15%)]\tAll Loss: 1.9467\tTriple Loss(1): 0.1889\tClassification Loss: 1.5690\r\n",
      "Train Epoch: 14 [64000/209539 (31%)]\tAll Loss: 2.1794\tTriple Loss(1): 0.1069\tClassification Loss: 1.9655\r\n",
      "Train Epoch: 14 [96000/209539 (46%)]\tAll Loss: 2.2969\tTriple Loss(1): 0.0793\tClassification Loss: 2.1383\r\n",
      "Train Epoch: 14 [128000/209539 (61%)]\tAll Loss: 1.9493\tTriple Loss(1): 0.1594\tClassification Loss: 1.6304\r\n",
      "Train Epoch: 14 [160000/209539 (76%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.1239\tClassification Loss: 1.5398\r\n",
      "Train Epoch: 14 [192000/209539 (92%)]\tAll Loss: 1.9714\tTriple Loss(1): 0.1254\tClassification Loss: 1.7206\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/14_epochs\r\n",
      "Train Epoch: 15 [0/209539 (0%)]\tAll Loss: 2.5430\tTriple Loss(1): 0.2970\tClassification Loss: 1.9489\r\n",
      "\r\n",
      "Test set: Average loss: 1.5898\r\n",
      "Top 1 Accuracy: 43517/80128 (54%)\r\n",
      "Top 3 Accuracy: 61713/80128 (77%)\r\n",
      "Top 5 Accuracy: 68837/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 15 [32000/209539 (15%)]\tAll Loss: 1.9828\tTriple Loss(1): 0.1010\tClassification Loss: 1.7808\r\n",
      "Train Epoch: 15 [64000/209539 (31%)]\tAll Loss: 1.7899\tTriple Loss(1): 0.0056\tClassification Loss: 1.7787\r\n",
      "Train Epoch: 15 [96000/209539 (46%)]\tAll Loss: 2.0923\tTriple Loss(1): 0.0829\tClassification Loss: 1.9265\r\n",
      "Train Epoch: 15 [128000/209539 (61%)]\tAll Loss: 1.7941\tTriple Loss(1): 0.1021\tClassification Loss: 1.5898\r\n",
      "Train Epoch: 15 [160000/209539 (76%)]\tAll Loss: 1.7319\tTriple Loss(1): 0.1032\tClassification Loss: 1.5255\r\n",
      "Train Epoch: 15 [192000/209539 (92%)]\tAll Loss: 2.0528\tTriple Loss(1): 0.1488\tClassification Loss: 1.7551\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/15_epochs\r\n",
      "Train Epoch: 16 [0/209539 (0%)]\tAll Loss: 2.3178\tTriple Loss(1): 0.1969\tClassification Loss: 1.9240\r\n",
      "\r\n",
      "Test set: Average loss: 1.5740\r\n",
      "Top 1 Accuracy: 43668/80128 (54%)\r\n",
      "Top 3 Accuracy: 61879/80128 (77%)\r\n",
      "Top 5 Accuracy: 68994/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 16 [32000/209539 (15%)]\tAll Loss: 1.6209\tTriple Loss(1): 0.0864\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 16 [64000/209539 (31%)]\tAll Loss: 2.3487\tTriple Loss(1): 0.2460\tClassification Loss: 1.8567\r\n",
      "Train Epoch: 16 [96000/209539 (46%)]\tAll Loss: 2.0344\tTriple Loss(1): 0.0741\tClassification Loss: 1.8862\r\n",
      "Train Epoch: 16 [128000/209539 (61%)]\tAll Loss: 1.7443\tTriple Loss(1): 0.1133\tClassification Loss: 1.5177\r\n",
      "Train Epoch: 16 [160000/209539 (76%)]\tAll Loss: 2.0226\tTriple Loss(1): 0.1915\tClassification Loss: 1.6396\r\n",
      "Train Epoch: 16 [192000/209539 (92%)]\tAll Loss: 2.2619\tTriple Loss(1): 0.2143\tClassification Loss: 1.8333\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/16_epochs\r\n",
      "Train Epoch: 17 [0/209539 (0%)]\tAll Loss: 2.3286\tTriple Loss(1): 0.1719\tClassification Loss: 1.9847\r\n",
      "\r\n",
      "Test set: Average loss: 1.5690\r\n",
      "Top 1 Accuracy: 43901/80128 (55%)\r\n",
      "Top 3 Accuracy: 61928/80128 (77%)\r\n",
      "Top 5 Accuracy: 68956/80128 (86%)\r\n",
      " \r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 325, in <module>\r\n",
      "    train(epoch)\r\n",
      "  File \"train.py\", line 128, in train\r\n",
      "    data_tri = torch.cat(data_tri_list, 0)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "# Loading \"freeze=False/lr=0.002/11_epochs\"\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/16_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.002/16_epochs\r\n",
      "Train Epoch: 17 [0/209539 (0%)]\tAll Loss: 2.5540\tTriple Loss(1): 0.2369\tClassification Loss: 2.0802\r\n",
      "train.py:215: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.7091\r\n",
      "Top 1 Accuracy: 40668/80128 (51%)\r\n",
      "Top 3 Accuracy: 59157/80128 (74%)\r\n",
      "Top 5 Accuracy: 67045/80128 (84%)\r\n",
      " \r\n",
      "Train Epoch: 17 [32000/209539 (15%)]\tAll Loss: 3.5690\tTriple Loss(0): 0.8922\tClassification Loss: 1.7846\r\n",
      "Train Epoch: 17 [64000/209539 (31%)]\tAll Loss: 2.1927\tTriple Loss(1): 0.1358\tClassification Loss: 1.9211\r\n",
      "Train Epoch: 17 [96000/209539 (46%)]\tAll Loss: 1.8430\tTriple Loss(1): 0.1409\tClassification Loss: 1.5612\r\n",
      "Train Epoch: 17 [128000/209539 (61%)]\tAll Loss: 3.6079\tTriple Loss(0): 0.9056\tClassification Loss: 1.7966\r\n",
      "Train Epoch: 17 [160000/209539 (76%)]\tAll Loss: 2.2562\tTriple Loss(1): 0.1658\tClassification Loss: 1.9246\r\n",
      "Train Epoch: 17 [192000/209539 (92%)]\tAll Loss: 2.2629\tTriple Loss(1): 0.1828\tClassification Loss: 1.8974\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/17_epochs\r\n",
      "Train Epoch: 18 [0/209539 (0%)]\tAll Loss: 3.1154\tTriple Loss(1): 0.3758\tClassification Loss: 2.3639\r\n",
      "\r\n",
      "Test set: Average loss: 1.5475\r\n",
      "Top 1 Accuracy: 44636/80128 (56%)\r\n",
      "Top 3 Accuracy: 62760/80128 (78%)\r\n",
      "Top 5 Accuracy: 69681/80128 (87%)\r\n",
      " \r\n",
      "Train Epoch: 18 [32000/209539 (15%)]\tAll Loss: 1.8153\tTriple Loss(1): 0.0159\tClassification Loss: 1.7834\r\n",
      "Train Epoch: 18 [64000/209539 (31%)]\tAll Loss: 2.0978\tTriple Loss(1): 0.1189\tClassification Loss: 1.8599\r\n",
      "Train Epoch: 18 [96000/209539 (46%)]\tAll Loss: 1.8453\tTriple Loss(1): 0.1133\tClassification Loss: 1.6186\r\n",
      "Train Epoch: 18 [128000/209539 (61%)]\tAll Loss: 1.9938\tTriple Loss(1): 0.1556\tClassification Loss: 1.6827\r\n",
      "Train Epoch: 18 [160000/209539 (76%)]\tAll Loss: 3.1844\tTriple Loss(0): 0.7017\tClassification Loss: 1.7811\r\n",
      "Train Epoch: 18 [192000/209539 (92%)]\tAll Loss: 2.0660\tTriple Loss(1): 0.0969\tClassification Loss: 1.8723\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/18_epochs\r\n",
      "Train Epoch: 19 [0/209539 (0%)]\tAll Loss: 2.7165\tTriple Loss(1): 0.2808\tClassification Loss: 2.1549\r\n",
      "\r\n",
      "Test set: Average loss: 1.5121\r\n",
      "Top 1 Accuracy: 45292/80128 (57%)\r\n",
      "Top 3 Accuracy: 63269/80128 (79%)\r\n",
      "Top 5 Accuracy: 70035/80128 (87%)\r\n",
      " \r\n",
      "Train Epoch: 19 [32000/209539 (15%)]\tAll Loss: 1.9987\tTriple Loss(1): 0.1041\tClassification Loss: 1.7905\r\n",
      "Train Epoch: 19 [64000/209539 (31%)]\tAll Loss: 1.9664\tTriple Loss(1): 0.0839\tClassification Loss: 1.7986\r\n",
      "Train Epoch: 19 [96000/209539 (46%)]\tAll Loss: 2.6138\tTriple Loss(0): 0.5655\tClassification Loss: 1.4829\r\n",
      "Train Epoch: 19 [128000/209539 (61%)]\tAll Loss: 1.7880\tTriple Loss(1): 0.1049\tClassification Loss: 1.5783\r\n",
      "Train Epoch: 19 [160000/209539 (76%)]\tAll Loss: 2.0864\tTriple Loss(1): 0.1389\tClassification Loss: 1.8087\r\n",
      "Train Epoch: 19 [192000/209539 (92%)]\tAll Loss: 2.9650\tTriple Loss(0): 0.6039\tClassification Loss: 1.7572\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/19_epochs\r\n",
      "Train Epoch: 20 [0/209539 (0%)]\tAll Loss: 2.5464\tTriple Loss(1): 0.1510\tClassification Loss: 2.2445\r\n",
      "\r\n",
      "Test set: Average loss: 1.4899\r\n",
      "Top 1 Accuracy: 45652/80128 (57%)\r\n",
      "Top 3 Accuracy: 63587/80128 (79%)\r\n",
      "Top 5 Accuracy: 70270/80128 (88%)\r\n",
      " \r\n",
      "Train Epoch: 20 [32000/209539 (15%)]\tAll Loss: 2.0140\tTriple Loss(1): 0.2231\tClassification Loss: 1.5678\r\n",
      "Train Epoch: 20 [64000/209539 (31%)]\tAll Loss: 2.0901\tTriple Loss(1): 0.0714\tClassification Loss: 1.9473\r\n",
      "Train Epoch: 20 [96000/209539 (46%)]\tAll Loss: 1.6485\tTriple Loss(1): 0.1008\tClassification Loss: 1.4468\r\n",
      "Train Epoch: 20 [128000/209539 (61%)]\tAll Loss: 2.1389\tTriple Loss(1): 0.1354\tClassification Loss: 1.8681\r\n",
      "Train Epoch: 20 [160000/209539 (76%)]\tAll Loss: 1.9453\tTriple Loss(1): 0.1473\tClassification Loss: 1.6507\r\n",
      "Train Epoch: 20 [192000/209539 (92%)]\tAll Loss: 1.8872\tTriple Loss(1): 0.0749\tClassification Loss: 1.7374\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/20_epochs\r\n",
      "Train Epoch: 21 [0/209539 (0%)]\tAll Loss: 2.6162\tTriple Loss(1): 0.2022\tClassification Loss: 2.2119\r\n",
      "\r\n",
      "Test set: Average loss: 1.4776\r\n",
      "Top 1 Accuracy: 45950/80128 (57%)\r\n",
      "Top 3 Accuracy: 63795/80128 (80%)\r\n",
      "Top 5 Accuracy: 70336/80128 (88%)\r\n",
      " \r\n",
      "Train Epoch: 21 [32000/209539 (15%)]\tAll Loss: 3.1074\tTriple Loss(0): 0.6670\tClassification Loss: 1.7734\r\n",
      "Train Epoch: 21 [64000/209539 (31%)]\tAll Loss: 1.8602\tTriple Loss(1): 0.0737\tClassification Loss: 1.7129\r\n",
      "Train Epoch: 21 [96000/209539 (46%)]\tAll Loss: 1.6790\tTriple Loss(1): 0.0304\tClassification Loss: 1.6182\r\n",
      "Train Epoch: 21 [128000/209539 (61%)]\tAll Loss: 1.8372\tTriple Loss(1): 0.1045\tClassification Loss: 1.6282\r\n",
      "Train Epoch: 21 [160000/209539 (76%)]\tAll Loss: 1.7886\tTriple Loss(1): 0.1198\tClassification Loss: 1.5489\r\n",
      "Train Epoch: 21 [192000/209539 (92%)]\tAll Loss: 1.7952\tTriple Loss(1): 0.0407\tClassification Loss: 1.7138\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/21_epochs\r\n",
      "Train Epoch: 22 [0/209539 (0%)]\tAll Loss: 2.4058\tTriple Loss(1): 0.2024\tClassification Loss: 2.0009\r\n",
      "\r\n",
      "Test set: Average loss: 1.4691\r\n",
      "Top 1 Accuracy: 46059/80128 (57%)\r\n",
      "Top 3 Accuracy: 63833/80128 (80%)\r\n",
      "Top 5 Accuracy: 70384/80128 (88%)\r\n",
      " \r\n",
      "Train Epoch: 22 [32000/209539 (15%)]\tAll Loss: 2.0277\tTriple Loss(1): 0.1736\tClassification Loss: 1.6805\r\n",
      "Train Epoch: 22 [64000/209539 (31%)]\tAll Loss: 1.9258\tTriple Loss(1): 0.0364\tClassification Loss: 1.8531\r\n",
      "Train Epoch: 22 [96000/209539 (46%)]\tAll Loss: 1.6660\tTriple Loss(1): 0.1153\tClassification Loss: 1.4355\r\n",
      "Train Epoch: 22 [128000/209539 (61%)]\tAll Loss: 1.7633\tTriple Loss(1): 0.0513\tClassification Loss: 1.6606\r\n",
      "Train Epoch: 22 [160000/209539 (76%)]\tAll Loss: 2.7058\tTriple Loss(0): 0.5235\tClassification Loss: 1.6589\r\n",
      "Train Epoch: 22 [192000/209539 (92%)]\tAll Loss: 2.0964\tTriple Loss(1): 0.2320\tClassification Loss: 1.6324\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/22_epochs\r\n",
      "Train Epoch: 23 [0/209539 (0%)]\tAll Loss: 2.3356\tTriple Loss(1): 0.1690\tClassification Loss: 1.9977\r\n",
      "\r\n",
      "Test set: Average loss: 1.4604\r\n",
      "Top 1 Accuracy: 46262/80128 (58%)\r\n",
      "Top 3 Accuracy: 63839/80128 (80%)\r\n",
      "Top 5 Accuracy: 70445/80128 (88%)\r\n",
      " \r\n",
      "Train Epoch: 23 [32000/209539 (15%)]\tAll Loss: 1.9993\tTriple Loss(1): 0.0882\tClassification Loss: 1.8229\r\n",
      "Train Epoch: 23 [64000/209539 (31%)]\tAll Loss: 2.9789\tTriple Loss(0): 0.5606\tClassification Loss: 1.8576\r\n",
      "Train Epoch: 23 [96000/209539 (46%)]\tAll Loss: 1.8687\tTriple Loss(1): 0.1174\tClassification Loss: 1.6340\r\n",
      "Train Epoch: 23 [128000/209539 (61%)]\tAll Loss: 1.8470\tTriple Loss(1): 0.1638\tClassification Loss: 1.5194\r\n",
      "Train Epoch: 23 [160000/209539 (76%)]\tAll Loss: 2.0344\tTriple Loss(1): 0.1793\tClassification Loss: 1.6758\r\n",
      "Train Epoch: 23 [192000/209539 (92%)]\tAll Loss: 1.8886\tTriple Loss(1): 0.0921\tClassification Loss: 1.7044\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/freeze=True/lr=0.002/23_epochs\r\n",
      "Train Epoch: 24 [0/209539 (0%)]\tAll Loss: 2.1249\tTriple Loss(1): 0.1110\tClassification Loss: 1.9030\r\n",
      "\r\n",
      "Test set: Average loss: 1.4579\r\n",
      "Top 1 Accuracy: 46311/80128 (58%)\r\n",
      "Top 3 Accuracy: 63925/80128 (80%)\r\n",
      "Top 5 Accuracy: 70468/80128 (88%)\r\n",
      " \r\n",
      "Train Epoch: 24 [32000/209539 (15%)]\tAll Loss: 2.4686\tTriple Loss(0): 0.4801\tClassification Loss: 1.5085\r\n"
     ]
    }
   ],
   "source": [
    "# Loading \"freeze=False/lr=0.002/16_epochs\"\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Train Epoch: 1 [0/209539 (0%)]\tAll Loss: 5.3020\tTriple Loss(1): 0.6663\tClassification Loss: 3.9694\r\n",
      "train.py:215: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 3.9166\r\n",
      "Top 1 Accuracy: 85/80128 (0%)\r\n",
      "Top 3 Accuracy: 1153/80128 (1%)\r\n",
      "Top 5 Accuracy: 3150/80128 (4%)\r\n",
      " \r\n",
      "Train Epoch: 1 [32000/209539 (15%)]\tAll Loss: 2.8669\tTriple Loss(1): 0.2901\tClassification Loss: 2.2868\r\n",
      "Train Epoch: 1 [64000/209539 (31%)]\tAll Loss: 2.6994\tTriple Loss(1): 0.2902\tClassification Loss: 2.1189\r\n",
      "Train Epoch: 1 [96000/209539 (46%)]\tAll Loss: 2.2655\tTriple Loss(1): 0.2296\tClassification Loss: 1.8064\r\n",
      "Train Epoch: 1 [128000/209539 (61%)]\tAll Loss: 3.8827\tTriple Loss(0): 0.9709\tClassification Loss: 1.9408\r\n",
      "Train Epoch: 1 [160000/209539 (76%)]\tAll Loss: 2.6805\tTriple Loss(1): 0.2927\tClassification Loss: 2.0950\r\n",
      "Train Epoch: 1 [192000/209539 (92%)]\tAll Loss: 2.6944\tTriple Loss(1): 0.3440\tClassification Loss: 2.0064\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/1_epochs\r\n",
      "Train Epoch: 2 [0/209539 (0%)]\tAll Loss: 3.1782\tTriple Loss(1): 0.5229\tClassification Loss: 2.1323\r\n",
      "\r\n",
      "Test set: Average loss: 1.7723\r\n",
      "Top 1 Accuracy: 39850/80128 (50%)\r\n",
      "Top 3 Accuracy: 59097/80128 (74%)\r\n",
      "Top 5 Accuracy: 66962/80128 (84%)\r\n",
      " \r\n",
      "Train Epoch: 2 [32000/209539 (15%)]\tAll Loss: 2.4057\tTriple Loss(1): 0.2127\tClassification Loss: 1.9802\r\n",
      "Train Epoch: 2 [64000/209539 (31%)]\tAll Loss: 2.3538\tTriple Loss(1): 0.1969\tClassification Loss: 1.9600\r\n",
      "Train Epoch: 2 [96000/209539 (46%)]\tAll Loss: 2.1087\tTriple Loss(1): 0.1691\tClassification Loss: 1.7704\r\n",
      "Train Epoch: 2 [128000/209539 (61%)]\tAll Loss: 2.4338\tTriple Loss(1): 0.2148\tClassification Loss: 2.0042\r\n",
      "Train Epoch: 2 [160000/209539 (76%)]\tAll Loss: 2.7189\tTriple Loss(1): 0.3718\tClassification Loss: 1.9753\r\n",
      "Train Epoch: 2 [192000/209539 (92%)]\tAll Loss: 2.9492\tTriple Loss(1): 0.4583\tClassification Loss: 2.0326\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/2_epochs\r\n",
      "Train Epoch: 3 [0/209539 (0%)]\tAll Loss: 2.7670\tTriple Loss(1): 0.3518\tClassification Loss: 2.0634\r\n",
      "\r\n",
      "Test set: Average loss: 1.6847\r\n",
      "Top 1 Accuracy: 41485/80128 (52%)\r\n",
      "Top 3 Accuracy: 60454/80128 (75%)\r\n",
      "Top 5 Accuracy: 67997/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 3 [32000/209539 (15%)]\tAll Loss: 2.2197\tTriple Loss(1): 0.2599\tClassification Loss: 1.6998\r\n",
      "Train Epoch: 3 [64000/209539 (31%)]\tAll Loss: 2.3221\tTriple Loss(1): 0.1750\tClassification Loss: 1.9722\r\n",
      "Train Epoch: 3 [96000/209539 (46%)]\tAll Loss: 2.1432\tTriple Loss(1): 0.2662\tClassification Loss: 1.6108\r\n",
      "Train Epoch: 3 [128000/209539 (61%)]\tAll Loss: 2.6313\tTriple Loss(1): 0.3654\tClassification Loss: 1.9005\r\n",
      "Train Epoch: 3 [160000/209539 (76%)]\tAll Loss: 2.2320\tTriple Loss(1): 0.1598\tClassification Loss: 1.9124\r\n",
      "Train Epoch: 3 [192000/209539 (92%)]\tAll Loss: 2.2117\tTriple Loss(1): 0.1086\tClassification Loss: 1.9946\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/3_epochs\r\n",
      "Train Epoch: 4 [0/209539 (0%)]\tAll Loss: 2.2300\tTriple Loss(1): 0.1487\tClassification Loss: 1.9326\r\n",
      "\r\n",
      "Test set: Average loss: 1.6535\r\n",
      "Top 1 Accuracy: 41923/80128 (52%)\r\n",
      "Top 3 Accuracy: 60860/80128 (76%)\r\n",
      "Top 5 Accuracy: 68328/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 4 [32000/209539 (15%)]\tAll Loss: 2.0647\tTriple Loss(1): 0.1525\tClassification Loss: 1.7596\r\n",
      "Train Epoch: 4 [64000/209539 (31%)]\tAll Loss: 2.3214\tTriple Loss(1): 0.2088\tClassification Loss: 1.9037\r\n",
      "Train Epoch: 4 [96000/209539 (46%)]\tAll Loss: 1.7333\tTriple Loss(1): 0.1031\tClassification Loss: 1.5270\r\n",
      "Train Epoch: 4 [128000/209539 (61%)]\tAll Loss: 2.1667\tTriple Loss(1): 0.1733\tClassification Loss: 1.8201\r\n",
      "Train Epoch: 4 [160000/209539 (76%)]\tAll Loss: 2.1690\tTriple Loss(1): 0.1747\tClassification Loss: 1.8195\r\n",
      "Train Epoch: 4 [192000/209539 (92%)]\tAll Loss: 2.4245\tTriple Loss(1): 0.1791\tClassification Loss: 2.0662\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/4_epochs\r\n",
      "Train Epoch: 5 [0/209539 (0%)]\tAll Loss: 2.7406\tTriple Loss(1): 0.4639\tClassification Loss: 1.8129\r\n",
      "\r\n",
      "Test set: Average loss: 1.6397\r\n",
      "Top 1 Accuracy: 42119/80128 (53%)\r\n",
      "Top 3 Accuracy: 61025/80128 (76%)\r\n",
      "Top 5 Accuracy: 68449/80128 (85%)\r\n",
      " \r\n",
      "Train Epoch: 5 [32000/209539 (15%)]\tAll Loss: 2.1848\tTriple Loss(1): 0.1622\tClassification Loss: 1.8603\r\n",
      "Train Epoch: 5 [64000/209539 (31%)]\tAll Loss: 2.1103\tTriple Loss(1): 0.1038\tClassification Loss: 1.9027\r\n",
      "Train Epoch: 5 [96000/209539 (46%)]\tAll Loss: 1.9911\tTriple Loss(1): 0.1727\tClassification Loss: 1.6458\r\n",
      "Train Epoch: 5 [128000/209539 (61%)]\tAll Loss: 2.3990\tTriple Loss(1): 0.2511\tClassification Loss: 1.8969\r\n",
      "Train Epoch: 5 [160000/209539 (76%)]\tAll Loss: 3.0464\tTriple Loss(0): 0.6236\tClassification Loss: 1.7993\r\n",
      "Train Epoch: 5 [192000/209539 (92%)]\tAll Loss: 3.1747\tTriple Loss(0): 0.5858\tClassification Loss: 2.0031\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/5_epochs\r\n",
      "Train Epoch: 6 [0/209539 (0%)]\tAll Loss: 2.5147\tTriple Loss(1): 0.3788\tClassification Loss: 1.7571\r\n",
      "\r\n",
      "Test set: Average loss: 1.6313\r\n",
      "Top 1 Accuracy: 42122/80128 (53%)\r\n",
      "Top 3 Accuracy: 61047/80128 (76%)\r\n",
      "Top 5 Accuracy: 68527/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 6 [32000/209539 (15%)]\tAll Loss: 3.5811\tTriple Loss(0): 0.8769\tClassification Loss: 1.8274\r\n",
      "Train Epoch: 6 [64000/209539 (31%)]\tAll Loss: 2.2214\tTriple Loss(1): 0.2421\tClassification Loss: 1.7373\r\n",
      "Train Epoch: 6 [96000/209539 (46%)]\tAll Loss: 3.4855\tTriple Loss(0): 0.8598\tClassification Loss: 1.7659\r\n",
      "Train Epoch: 6 [128000/209539 (61%)]\tAll Loss: 2.5945\tTriple Loss(1): 0.2925\tClassification Loss: 2.0096\r\n",
      "Train Epoch: 6 [160000/209539 (76%)]\tAll Loss: 2.3098\tTriple Loss(1): 0.2124\tClassification Loss: 1.8851\r\n",
      "Train Epoch: 6 [192000/209539 (92%)]\tAll Loss: 2.2704\tTriple Loss(1): 0.1909\tClassification Loss: 1.8885\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/6_epochs\r\n",
      "Train Epoch: 7 [0/209539 (0%)]\tAll Loss: 2.2356\tTriple Loss(1): 0.2698\tClassification Loss: 1.6959\r\n",
      "\r\n",
      "Test set: Average loss: 1.6292\r\n",
      "Top 1 Accuracy: 42079/80128 (53%)\r\n",
      "Top 3 Accuracy: 61010/80128 (76%)\r\n",
      "Top 5 Accuracy: 68553/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 7 [32000/209539 (15%)]\tAll Loss: 2.1321\tTriple Loss(1): 0.2153\tClassification Loss: 1.7014\r\n",
      "Train Epoch: 7 [64000/209539 (31%)]\tAll Loss: 3.3824\tTriple Loss(0): 0.7331\tClassification Loss: 1.9163\r\n",
      "Train Epoch: 7 [96000/209539 (46%)]\tAll Loss: 2.1187\tTriple Loss(1): 0.1840\tClassification Loss: 1.7508\r\n",
      "Train Epoch: 7 [128000/209539 (61%)]\tAll Loss: 2.1646\tTriple Loss(1): 0.1802\tClassification Loss: 1.8042\r\n",
      "Train Epoch: 7 [160000/209539 (76%)]\tAll Loss: 2.2682\tTriple Loss(1): 0.2336\tClassification Loss: 1.8011\r\n",
      "Train Epoch: 7 [192000/209539 (92%)]\tAll Loss: 2.2652\tTriple Loss(1): 0.1652\tClassification Loss: 1.9348\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/7_epochs\r\n",
      "Train Epoch: 8 [0/209539 (0%)]\tAll Loss: 2.6315\tTriple Loss(1): 0.3773\tClassification Loss: 1.8768\r\n",
      "\r\n",
      "Test set: Average loss: 1.6257\r\n",
      "Top 1 Accuracy: 42089/80128 (53%)\r\n",
      "Top 3 Accuracy: 61087/80128 (76%)\r\n",
      "Top 5 Accuracy: 68652/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 8 [32000/209539 (15%)]\tAll Loss: 2.3945\tTriple Loss(1): 0.2402\tClassification Loss: 1.9141\r\n",
      "Train Epoch: 8 [64000/209539 (31%)]\tAll Loss: 1.9396\tTriple Loss(1): 0.1106\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 8 [96000/209539 (46%)]\tAll Loss: 2.8255\tTriple Loss(0): 0.5738\tClassification Loss: 1.6779\r\n",
      "Train Epoch: 8 [128000/209539 (61%)]\tAll Loss: 2.3662\tTriple Loss(1): 0.2478\tClassification Loss: 1.8707\r\n",
      "Train Epoch: 8 [160000/209539 (76%)]\tAll Loss: 3.3164\tTriple Loss(0): 0.6770\tClassification Loss: 1.9623\r\n",
      "Train Epoch: 8 [192000/209539 (92%)]\tAll Loss: 2.4531\tTriple Loss(1): 0.2736\tClassification Loss: 1.9060\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/8_epochs\r\n",
      "Train Epoch: 9 [0/209539 (0%)]\tAll Loss: 2.6070\tTriple Loss(1): 0.3995\tClassification Loss: 1.8081\r\n",
      "\r\n",
      "Test set: Average loss: 1.6292\r\n",
      "Top 1 Accuracy: 41882/80128 (52%)\r\n",
      "Top 3 Accuracy: 60997/80128 (76%)\r\n",
      "Top 5 Accuracy: 68555/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 9 [32000/209539 (15%)]\tAll Loss: 2.0784\tTriple Loss(1): 0.1302\tClassification Loss: 1.8181\r\n",
      "Train Epoch: 9 [64000/209539 (31%)]\tAll Loss: 2.1891\tTriple Loss(1): 0.1545\tClassification Loss: 1.8800\r\n",
      "Train Epoch: 9 [96000/209539 (46%)]\tAll Loss: 2.0423\tTriple Loss(1): 0.2699\tClassification Loss: 1.5025\r\n",
      "Train Epoch: 9 [128000/209539 (61%)]\tAll Loss: 2.3874\tTriple Loss(1): 0.2340\tClassification Loss: 1.9194\r\n",
      "Train Epoch: 9 [160000/209539 (76%)]\tAll Loss: 2.4140\tTriple Loss(1): 0.2935\tClassification Loss: 1.8269\r\n",
      "Train Epoch: 9 [192000/209539 (92%)]\tAll Loss: 2.5104\tTriple Loss(1): 0.2505\tClassification Loss: 2.0095\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/9_epochs\r\n",
      "Train Epoch: 10 [0/209539 (0%)]\tAll Loss: 2.0287\tTriple Loss(1): 0.1784\tClassification Loss: 1.6719\r\n",
      "\r\n",
      "Test set: Average loss: 1.6192\r\n",
      "Top 1 Accuracy: 42237/80128 (53%)\r\n",
      "Top 3 Accuracy: 61196/80128 (76%)\r\n",
      "Top 5 Accuracy: 68703/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 10 [32000/209539 (15%)]\tAll Loss: 2.1463\tTriple Loss(1): 0.1462\tClassification Loss: 1.8540\r\n",
      "Train Epoch: 10 [64000/209539 (31%)]\tAll Loss: 1.9076\tTriple Loss(1): 0.1074\tClassification Loss: 1.6927\r\n",
      "Train Epoch: 10 [96000/209539 (46%)]\tAll Loss: 1.9386\tTriple Loss(1): 0.1537\tClassification Loss: 1.6311\r\n",
      "Train Epoch: 10 [128000/209539 (61%)]\tAll Loss: 2.2138\tTriple Loss(1): 0.1386\tClassification Loss: 1.9367\r\n",
      "Train Epoch: 10 [160000/209539 (76%)]\tAll Loss: 2.0468\tTriple Loss(1): 0.1402\tClassification Loss: 1.7664\r\n",
      "Train Epoch: 10 [192000/209539 (92%)]\tAll Loss: 3.7137\tTriple Loss(0): 0.9346\tClassification Loss: 1.8445\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/10_epochs\r\n",
      "Train Epoch: 11 [0/209539 (0%)]\tAll Loss: 2.3522\tTriple Loss(1): 0.2940\tClassification Loss: 1.7643\r\n",
      "\r\n",
      "Test set: Average loss: 1.6197\r\n",
      "Top 1 Accuracy: 42262/80128 (53%)\r\n",
      "Top 3 Accuracy: 61195/80128 (76%)\r\n",
      "Top 5 Accuracy: 68639/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 11 [32000/209539 (15%)]\tAll Loss: 2.0057\tTriple Loss(1): 0.1424\tClassification Loss: 1.7208\r\n",
      "Train Epoch: 11 [64000/209539 (31%)]\tAll Loss: 2.2161\tTriple Loss(1): 0.2522\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 11 [96000/209539 (46%)]\tAll Loss: 2.0003\tTriple Loss(1): 0.1789\tClassification Loss: 1.6424\r\n",
      "Train Epoch: 11 [128000/209539 (61%)]\tAll Loss: 2.5025\tTriple Loss(1): 0.3246\tClassification Loss: 1.8533\r\n",
      "Train Epoch: 11 [160000/209539 (76%)]\tAll Loss: 1.9633\tTriple Loss(1): 0.0616\tClassification Loss: 1.8401\r\n",
      "Train Epoch: 11 [192000/209539 (92%)]\tAll Loss: 2.2984\tTriple Loss(1): 0.1142\tClassification Loss: 2.0701\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/11_epochs\r\n",
      "Train Epoch: 12 [0/209539 (0%)]\tAll Loss: 2.1464\tTriple Loss(1): 0.2055\tClassification Loss: 1.7353\r\n",
      "\r\n",
      "Test set: Average loss: 1.6219\r\n",
      "Top 1 Accuracy: 41987/80128 (52%)\r\n",
      "Top 3 Accuracy: 61182/80128 (76%)\r\n",
      "Top 5 Accuracy: 68668/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 12 [32000/209539 (15%)]\tAll Loss: 2.0930\tTriple Loss(1): 0.1923\tClassification Loss: 1.7084\r\n",
      "Train Epoch: 12 [64000/209539 (31%)]\tAll Loss: 2.1805\tTriple Loss(1): 0.1736\tClassification Loss: 1.8333\r\n",
      "Train Epoch: 12 [96000/209539 (46%)]\tAll Loss: 2.1230\tTriple Loss(1): 0.2129\tClassification Loss: 1.6972\r\n",
      "Train Epoch: 12 [128000/209539 (61%)]\tAll Loss: 3.2010\tTriple Loss(0): 0.6457\tClassification Loss: 1.9096\r\n",
      "Train Epoch: 12 [160000/209539 (76%)]\tAll Loss: 2.8775\tTriple Loss(0): 0.5255\tClassification Loss: 1.8266\r\n",
      "Train Epoch: 12 [192000/209539 (92%)]\tAll Loss: 2.3516\tTriple Loss(1): 0.2027\tClassification Loss: 1.9463\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/12_epochs\r\n",
      "Train Epoch: 13 [0/209539 (0%)]\tAll Loss: 2.0841\tTriple Loss(1): 0.1616\tClassification Loss: 1.7608\r\n",
      "\r\n",
      "Test set: Average loss: 1.6174\r\n",
      "Top 1 Accuracy: 42184/80128 (53%)\r\n",
      "Top 3 Accuracy: 61283/80128 (76%)\r\n",
      "Top 5 Accuracy: 68703/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 13 [32000/209539 (15%)]\tAll Loss: 1.9174\tTriple Loss(1): 0.0854\tClassification Loss: 1.7467\r\n",
      "Train Epoch: 13 [64000/209539 (31%)]\tAll Loss: 2.1238\tTriple Loss(1): 0.2332\tClassification Loss: 1.6574\r\n",
      "Train Epoch: 13 [96000/209539 (46%)]\tAll Loss: 2.0712\tTriple Loss(1): 0.2315\tClassification Loss: 1.6081\r\n",
      "Train Epoch: 13 [128000/209539 (61%)]\tAll Loss: 2.0796\tTriple Loss(1): 0.1507\tClassification Loss: 1.7782\r\n",
      "Train Epoch: 13 [160000/209539 (76%)]\tAll Loss: 2.1204\tTriple Loss(1): 0.1516\tClassification Loss: 1.8173\r\n",
      "Train Epoch: 13 [192000/209539 (92%)]\tAll Loss: 2.3764\tTriple Loss(1): 0.1732\tClassification Loss: 2.0299\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/13_epochs\r\n",
      "Train Epoch: 14 [0/209539 (0%)]\tAll Loss: 2.0502\tTriple Loss(1): 0.2222\tClassification Loss: 1.6058\r\n",
      "\r\n",
      "Test set: Average loss: 1.6181\r\n",
      "Top 1 Accuracy: 42223/80128 (53%)\r\n",
      "Top 3 Accuracy: 61262/80128 (76%)\r\n",
      "Top 5 Accuracy: 68693/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 14 [32000/209539 (15%)]\tAll Loss: 3.1911\tTriple Loss(0): 0.7260\tClassification Loss: 1.7392\r\n",
      "Train Epoch: 14 [64000/209539 (31%)]\tAll Loss: 2.2607\tTriple Loss(1): 0.2711\tClassification Loss: 1.7186\r\n",
      "Train Epoch: 14 [96000/209539 (46%)]\tAll Loss: 2.0334\tTriple Loss(1): 0.3098\tClassification Loss: 1.4138\r\n",
      "Train Epoch: 14 [128000/209539 (61%)]\tAll Loss: 3.5066\tTriple Loss(0): 0.7564\tClassification Loss: 1.9939\r\n",
      "Train Epoch: 14 [160000/209539 (76%)]\tAll Loss: 2.0692\tTriple Loss(1): 0.1270\tClassification Loss: 1.8152\r\n",
      "Train Epoch: 14 [192000/209539 (92%)]\tAll Loss: 2.5850\tTriple Loss(1): 0.2329\tClassification Loss: 2.1192\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/14_epochs\r\n",
      "Train Epoch: 15 [0/209539 (0%)]\tAll Loss: 2.4089\tTriple Loss(1): 0.3504\tClassification Loss: 1.7080\r\n",
      "\r\n",
      "Test set: Average loss: 1.6183\r\n",
      "Top 1 Accuracy: 42042/80128 (52%)\r\n",
      "Top 3 Accuracy: 61305/80128 (77%)\r\n",
      "Top 5 Accuracy: 68748/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 15 [32000/209539 (15%)]\tAll Loss: 2.2442\tTriple Loss(1): 0.1670\tClassification Loss: 1.9102\r\n",
      "Train Epoch: 15 [64000/209539 (31%)]\tAll Loss: 3.1663\tTriple Loss(0): 0.7134\tClassification Loss: 1.7395\r\n",
      "Train Epoch: 15 [96000/209539 (46%)]\tAll Loss: 1.7584\tTriple Loss(1): 0.0579\tClassification Loss: 1.6427\r\n",
      "Train Epoch: 15 [128000/209539 (61%)]\tAll Loss: 3.6933\tTriple Loss(0): 0.8878\tClassification Loss: 1.9177\r\n",
      "Train Epoch: 15 [160000/209539 (76%)]\tAll Loss: 2.1195\tTriple Loss(1): 0.1765\tClassification Loss: 1.7666\r\n",
      "Train Epoch: 15 [192000/209539 (92%)]\tAll Loss: 2.3566\tTriple Loss(1): 0.1416\tClassification Loss: 2.0733\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.002/15_epochs\r\n",
      "Train Epoch: 16 [0/209539 (0%)]\tAll Loss: 2.1031\tTriple Loss(1): 0.2135\tClassification Loss: 1.6762\r\n",
      "\r\n",
      "Test set: Average loss: 1.6117\r\n",
      "Top 1 Accuracy: 42253/80128 (53%)\r\n",
      "Top 3 Accuracy: 61383/80128 (77%)\r\n",
      "Top 5 Accuracy: 68773/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 16 [32000/209539 (15%)]\tAll Loss: 2.9430\tTriple Loss(0): 0.6156\tClassification Loss: 1.7118\r\n",
      "Train Epoch: 16 [64000/209539 (31%)]\tAll Loss: 1.9694\tTriple Loss(1): 0.1158\tClassification Loss: 1.7378\r\n",
      "Train Epoch: 16 [96000/209539 (46%)]\tAll Loss: 2.0622\tTriple Loss(1): 0.1247\tClassification Loss: 1.8128\r\n",
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "# with_eastern. Freeze=True. lr=0.02\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model with_eastern=False/lr=0.002/15_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "Loading model with_eastern=False/lr=0.002/15_epochs\r\n",
      "Train Epoch: 16 [0/209539 (0%)]\tAll Loss: 2.3847\tTriple Loss(1): 0.1915\tClassification Loss: 2.0018\r\n",
      "train.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "\r\n",
      "Test set: Average loss: 1.6240\r\n",
      "Top 1 Accuracy: 41954/80128 (52%)\r\n",
      "Top 3 Accuracy: 60993/80128 (76%)\r\n",
      "Top 5 Accuracy: 68565/80128 (86%)\r\n",
      " \r\n",
      "Train Epoch: 16 [640/209539 (0%)]\tAll Loss: 2.5882\tTriple Loss(1): 0.2308\tClassification Loss: 2.1265\r\n",
      "Train Epoch: 16 [1280/209539 (1%)]\tAll Loss: 2.3690\tTriple Loss(1): 0.2761\tClassification Loss: 1.8167\r\n",
      "Train Epoch: 16 [1920/209539 (1%)]\tAll Loss: 3.6575\tTriple Loss(0): 0.8179\tClassification Loss: 2.0216\r\n",
      "Train Epoch: 16 [2560/209539 (1%)]\tAll Loss: 2.3522\tTriple Loss(1): 0.1524\tClassification Loss: 2.0474\r\n",
      "Train Epoch: 16 [3200/209539 (2%)]\tAll Loss: 2.4778\tTriple Loss(1): 0.2271\tClassification Loss: 2.0237\r\n",
      "Train Epoch: 16 [3840/209539 (2%)]\tAll Loss: 3.5817\tTriple Loss(0): 0.8650\tClassification Loss: 1.8518\r\n",
      "Train Epoch: 16 [4480/209539 (2%)]\tAll Loss: 2.1787\tTriple Loss(1): 0.2103\tClassification Loss: 1.7581\r\n",
      "Train Epoch: 16 [5120/209539 (2%)]\tAll Loss: 2.2372\tTriple Loss(1): 0.1717\tClassification Loss: 1.8938\r\n",
      "Train Epoch: 16 [5760/209539 (3%)]\tAll Loss: 2.4405\tTriple Loss(1): 0.2584\tClassification Loss: 1.9237\r\n",
      "Train Epoch: 16 [6400/209539 (3%)]\tAll Loss: 1.7902\tTriple Loss(1): 0.2217\tClassification Loss: 1.3468\r\n",
      "Train Epoch: 16 [7040/209539 (3%)]\tAll Loss: 2.2098\tTriple Loss(1): 0.2520\tClassification Loss: 1.7057\r\n",
      "Train Epoch: 16 [7680/209539 (4%)]\tAll Loss: 2.0274\tTriple Loss(1): 0.1352\tClassification Loss: 1.7571\r\n",
      "Train Epoch: 16 [8320/209539 (4%)]\tAll Loss: 2.4178\tTriple Loss(1): 0.2380\tClassification Loss: 1.9418\r\n",
      "Train Epoch: 16 [8960/209539 (4%)]\tAll Loss: 1.7492\tTriple Loss(1): 0.0991\tClassification Loss: 1.5511\r\n",
      "Train Epoch: 16 [9600/209539 (5%)]\tAll Loss: 2.3054\tTriple Loss(1): 0.1971\tClassification Loss: 1.9111\r\n",
      "Train Epoch: 16 [10240/209539 (5%)]\tAll Loss: 1.8818\tTriple Loss(1): 0.1246\tClassification Loss: 1.6325\r\n",
      "Train Epoch: 16 [10880/209539 (5%)]\tAll Loss: 3.1063\tTriple Loss(0): 0.7723\tClassification Loss: 1.5618\r\n",
      "Train Epoch: 16 [11520/209539 (5%)]\tAll Loss: 2.2907\tTriple Loss(1): 0.3106\tClassification Loss: 1.6696\r\n",
      "Train Epoch: 16 [12160/209539 (6%)]\tAll Loss: 1.9617\tTriple Loss(1): 0.2163\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 16 [12800/209539 (6%)]\tAll Loss: 1.3263\tTriple Loss(1): 0.0981\tClassification Loss: 1.1302\r\n",
      "Train Epoch: 16 [13440/209539 (6%)]\tAll Loss: 2.0722\tTriple Loss(1): 0.2101\tClassification Loss: 1.6519\r\n",
      "Train Epoch: 16 [14080/209539 (7%)]\tAll Loss: 2.0007\tTriple Loss(1): 0.1557\tClassification Loss: 1.6892\r\n",
      "Train Epoch: 16 [14720/209539 (7%)]\tAll Loss: 2.9045\tTriple Loss(0): 0.5588\tClassification Loss: 1.7869\r\n",
      "Train Epoch: 16 [15360/209539 (7%)]\tAll Loss: 3.0103\tTriple Loss(0): 0.7107\tClassification Loss: 1.5890\r\n",
      "Train Epoch: 16 [16000/209539 (8%)]\tAll Loss: 2.1937\tTriple Loss(1): 0.1472\tClassification Loss: 1.8993\r\n",
      "Train Epoch: 16 [16640/209539 (8%)]\tAll Loss: 2.4821\tTriple Loss(1): 0.1876\tClassification Loss: 2.1068\r\n",
      "Train Epoch: 16 [17280/209539 (8%)]\tAll Loss: 1.9091\tTriple Loss(1): 0.0884\tClassification Loss: 1.7323\r\n",
      "Train Epoch: 16 [17920/209539 (9%)]\tAll Loss: 2.2387\tTriple Loss(1): 0.2651\tClassification Loss: 1.7085\r\n",
      "Train Epoch: 16 [18560/209539 (9%)]\tAll Loss: 2.1792\tTriple Loss(1): 0.2045\tClassification Loss: 1.7702\r\n",
      "Train Epoch: 16 [19200/209539 (9%)]\tAll Loss: 1.7209\tTriple Loss(1): 0.0561\tClassification Loss: 1.6086\r\n",
      "Train Epoch: 16 [19840/209539 (9%)]\tAll Loss: 1.9431\tTriple Loss(1): 0.1015\tClassification Loss: 1.7401\r\n",
      "Train Epoch: 16 [20480/209539 (10%)]\tAll Loss: 1.9359\tTriple Loss(1): 0.1630\tClassification Loss: 1.6100\r\n",
      "Train Epoch: 16 [21120/209539 (10%)]\tAll Loss: 3.0755\tTriple Loss(0): 0.6253\tClassification Loss: 1.8250\r\n",
      "Train Epoch: 16 [21760/209539 (10%)]\tAll Loss: 1.8377\tTriple Loss(1): 0.1256\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 16 [22400/209539 (11%)]\tAll Loss: 1.7692\tTriple Loss(1): 0.1475\tClassification Loss: 1.4742\r\n",
      "Train Epoch: 16 [23040/209539 (11%)]\tAll Loss: 1.7834\tTriple Loss(1): 0.1046\tClassification Loss: 1.5742\r\n",
      "Train Epoch: 16 [23680/209539 (11%)]\tAll Loss: 1.5204\tTriple Loss(1): 0.0656\tClassification Loss: 1.3891\r\n",
      "Train Epoch: 16 [24320/209539 (12%)]\tAll Loss: 2.0159\tTriple Loss(1): 0.2056\tClassification Loss: 1.6048\r\n",
      "Train Epoch: 16 [24960/209539 (12%)]\tAll Loss: 1.7496\tTriple Loss(1): 0.0982\tClassification Loss: 1.5532\r\n",
      "Train Epoch: 16 [25600/209539 (12%)]\tAll Loss: 1.7639\tTriple Loss(1): 0.1469\tClassification Loss: 1.4701\r\n",
      "Train Epoch: 16 [26240/209539 (13%)]\tAll Loss: 3.0289\tTriple Loss(0): 0.7651\tClassification Loss: 1.4987\r\n",
      "Train Epoch: 16 [26880/209539 (13%)]\tAll Loss: 2.5591\tTriple Loss(0): 0.3481\tClassification Loss: 1.8629\r\n",
      "Train Epoch: 16 [27520/209539 (13%)]\tAll Loss: 1.6341\tTriple Loss(1): 0.0388\tClassification Loss: 1.5564\r\n",
      "Train Epoch: 16 [28160/209539 (13%)]\tAll Loss: 1.7959\tTriple Loss(1): 0.1187\tClassification Loss: 1.5586\r\n",
      "Train Epoch: 16 [28800/209539 (14%)]\tAll Loss: 3.5724\tTriple Loss(0): 0.9670\tClassification Loss: 1.6384\r\n",
      "Train Epoch: 16 [29440/209539 (14%)]\tAll Loss: 2.1216\tTriple Loss(1): 0.1164\tClassification Loss: 1.8889\r\n",
      "Train Epoch: 16 [30080/209539 (14%)]\tAll Loss: 2.6728\tTriple Loss(0): 0.5794\tClassification Loss: 1.5140\r\n",
      "Train Epoch: 16 [30720/209539 (15%)]\tAll Loss: 1.7411\tTriple Loss(1): 0.1338\tClassification Loss: 1.4736\r\n",
      "Train Epoch: 16 [31360/209539 (15%)]\tAll Loss: 1.7578\tTriple Loss(1): 0.1141\tClassification Loss: 1.5296\r\n",
      "Train Epoch: 16 [32000/209539 (15%)]\tAll Loss: 2.7911\tTriple Loss(0): 0.6049\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 16 [32640/209539 (16%)]\tAll Loss: 2.1366\tTriple Loss(1): 0.2517\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 16 [33280/209539 (16%)]\tAll Loss: 2.1171\tTriple Loss(1): 0.1560\tClassification Loss: 1.8050\r\n",
      "Train Epoch: 16 [33920/209539 (16%)]\tAll Loss: 1.8520\tTriple Loss(1): 0.1693\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 16 [34560/209539 (16%)]\tAll Loss: 1.7505\tTriple Loss(1): 0.0650\tClassification Loss: 1.6206\r\n",
      "Train Epoch: 16 [35200/209539 (17%)]\tAll Loss: 1.8548\tTriple Loss(1): 0.1615\tClassification Loss: 1.5318\r\n",
      "Train Epoch: 16 [35840/209539 (17%)]\tAll Loss: 2.3875\tTriple Loss(0): 0.4119\tClassification Loss: 1.5637\r\n",
      "Train Epoch: 16 [36480/209539 (17%)]\tAll Loss: 2.7228\tTriple Loss(0): 0.5752\tClassification Loss: 1.5723\r\n",
      "Train Epoch: 16 [37120/209539 (18%)]\tAll Loss: 2.9315\tTriple Loss(0): 0.4885\tClassification Loss: 1.9546\r\n",
      "Train Epoch: 16 [37760/209539 (18%)]\tAll Loss: 2.9888\tTriple Loss(0): 0.6615\tClassification Loss: 1.6658\r\n",
      "Train Epoch: 16 [38400/209539 (18%)]\tAll Loss: 3.1373\tTriple Loss(0): 0.6802\tClassification Loss: 1.7770\r\n",
      "Train Epoch: 16 [39040/209539 (19%)]\tAll Loss: 1.6310\tTriple Loss(1): 0.1431\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 16 [39680/209539 (19%)]\tAll Loss: 1.5201\tTriple Loss(1): 0.0504\tClassification Loss: 1.4193\r\n",
      "Train Epoch: 16 [40320/209539 (19%)]\tAll Loss: 3.0463\tTriple Loss(0): 0.7176\tClassification Loss: 1.6110\r\n",
      "Train Epoch: 16 [40960/209539 (20%)]\tAll Loss: 1.5947\tTriple Loss(1): 0.0738\tClassification Loss: 1.4471\r\n",
      "Train Epoch: 16 [41600/209539 (20%)]\tAll Loss: 2.1086\tTriple Loss(1): 0.2011\tClassification Loss: 1.7063\r\n",
      "Train Epoch: 16 [42240/209539 (20%)]\tAll Loss: 1.7284\tTriple Loss(1): 0.1331\tClassification Loss: 1.4622\r\n",
      "Train Epoch: 16 [42880/209539 (20%)]\tAll Loss: 2.0496\tTriple Loss(1): 0.1816\tClassification Loss: 1.6863\r\n",
      "Train Epoch: 16 [43520/209539 (21%)]\tAll Loss: 2.1710\tTriple Loss(1): 0.1266\tClassification Loss: 1.9177\r\n",
      "Train Epoch: 16 [44160/209539 (21%)]\tAll Loss: 2.0260\tTriple Loss(1): 0.0476\tClassification Loss: 1.9308\r\n",
      "Train Epoch: 16 [44800/209539 (21%)]\tAll Loss: 2.1446\tTriple Loss(1): 0.1631\tClassification Loss: 1.8185\r\n",
      "Train Epoch: 16 [45440/209539 (22%)]\tAll Loss: 2.0898\tTriple Loss(1): 0.0942\tClassification Loss: 1.9015\r\n",
      "Train Epoch: 16 [46080/209539 (22%)]\tAll Loss: 1.5553\tTriple Loss(1): 0.0663\tClassification Loss: 1.4226\r\n",
      "Train Epoch: 16 [46720/209539 (22%)]\tAll Loss: 3.0556\tTriple Loss(0): 0.5756\tClassification Loss: 1.9044\r\n",
      "Train Epoch: 16 [47360/209539 (23%)]\tAll Loss: 2.4577\tTriple Loss(0): 0.5688\tClassification Loss: 1.3202\r\n",
      "Train Epoch: 16 [48000/209539 (23%)]\tAll Loss: 1.9480\tTriple Loss(1): 0.1326\tClassification Loss: 1.6829\r\n",
      "Train Epoch: 16 [48640/209539 (23%)]\tAll Loss: 2.0823\tTriple Loss(1): 0.0575\tClassification Loss: 1.9674\r\n",
      "Train Epoch: 16 [49280/209539 (24%)]\tAll Loss: 1.7442\tTriple Loss(1): 0.1021\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 16 [49920/209539 (24%)]\tAll Loss: 1.9685\tTriple Loss(1): 0.1116\tClassification Loss: 1.7453\r\n",
      "Train Epoch: 16 [50560/209539 (24%)]\tAll Loss: 2.1468\tTriple Loss(1): 0.1100\tClassification Loss: 1.9268\r\n",
      "Train Epoch: 16 [51200/209539 (24%)]\tAll Loss: 1.8762\tTriple Loss(1): 0.0822\tClassification Loss: 1.7119\r\n",
      "Train Epoch: 16 [51840/209539 (25%)]\tAll Loss: 1.9739\tTriple Loss(1): 0.1994\tClassification Loss: 1.5751\r\n",
      "Train Epoch: 16 [52480/209539 (25%)]\tAll Loss: 2.6716\tTriple Loss(0): 0.5623\tClassification Loss: 1.5469\r\n",
      "Train Epoch: 16 [53120/209539 (25%)]\tAll Loss: 1.5464\tTriple Loss(1): 0.0414\tClassification Loss: 1.4636\r\n",
      "Train Epoch: 16 [53760/209539 (26%)]\tAll Loss: 2.0586\tTriple Loss(1): 0.0213\tClassification Loss: 2.0159\r\n",
      "Train Epoch: 16 [54400/209539 (26%)]\tAll Loss: 3.1693\tTriple Loss(0): 0.6437\tClassification Loss: 1.8820\r\n",
      "Train Epoch: 16 [55040/209539 (26%)]\tAll Loss: 1.7683\tTriple Loss(1): 0.1638\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 16 [55680/209539 (27%)]\tAll Loss: 1.8542\tTriple Loss(1): 0.0869\tClassification Loss: 1.6803\r\n",
      "Train Epoch: 16 [56320/209539 (27%)]\tAll Loss: 1.3924\tTriple Loss(1): 0.0573\tClassification Loss: 1.2779\r\n",
      "Train Epoch: 16 [56960/209539 (27%)]\tAll Loss: 1.8354\tTriple Loss(1): 0.1527\tClassification Loss: 1.5300\r\n",
      "Train Epoch: 16 [57600/209539 (27%)]\tAll Loss: 1.7968\tTriple Loss(1): 0.1186\tClassification Loss: 1.5597\r\n",
      "Train Epoch: 16 [58240/209539 (28%)]\tAll Loss: 1.9631\tTriple Loss(1): 0.2395\tClassification Loss: 1.4840\r\n",
      "Train Epoch: 16 [58880/209539 (28%)]\tAll Loss: 1.9076\tTriple Loss(1): 0.1600\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 16 [59520/209539 (28%)]\tAll Loss: 1.7388\tTriple Loss(1): 0.1167\tClassification Loss: 1.5054\r\n",
      "Train Epoch: 16 [60160/209539 (29%)]\tAll Loss: 1.9915\tTriple Loss(1): 0.1154\tClassification Loss: 1.7607\r\n",
      "Train Epoch: 16 [60800/209539 (29%)]\tAll Loss: 1.9438\tTriple Loss(1): 0.1364\tClassification Loss: 1.6710\r\n",
      "Train Epoch: 16 [61440/209539 (29%)]\tAll Loss: 2.6057\tTriple Loss(0): 0.4841\tClassification Loss: 1.6375\r\n",
      "Train Epoch: 16 [62080/209539 (30%)]\tAll Loss: 3.1590\tTriple Loss(0): 0.6988\tClassification Loss: 1.7613\r\n",
      "Train Epoch: 16 [62720/209539 (30%)]\tAll Loss: 1.8726\tTriple Loss(1): 0.1006\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 16 [63360/209539 (30%)]\tAll Loss: 3.0056\tTriple Loss(0): 0.6055\tClassification Loss: 1.7945\r\n",
      "Train Epoch: 16 [64000/209539 (31%)]\tAll Loss: 1.9906\tTriple Loss(1): 0.1277\tClassification Loss: 1.7352\r\n",
      "Train Epoch: 16 [64640/209539 (31%)]\tAll Loss: 2.3707\tTriple Loss(1): 0.2006\tClassification Loss: 1.9695\r\n",
      "Train Epoch: 16 [65280/209539 (31%)]\tAll Loss: 1.8572\tTriple Loss(1): 0.0919\tClassification Loss: 1.6735\r\n",
      "Train Epoch: 16 [65920/209539 (31%)]\tAll Loss: 2.1636\tTriple Loss(1): 0.1206\tClassification Loss: 1.9224\r\n",
      "Train Epoch: 16 [66560/209539 (32%)]\tAll Loss: 1.5930\tTriple Loss(1): 0.1178\tClassification Loss: 1.3574\r\n",
      "Train Epoch: 16 [67200/209539 (32%)]\tAll Loss: 2.0247\tTriple Loss(1): 0.0695\tClassification Loss: 1.8857\r\n",
      "Train Epoch: 16 [67840/209539 (32%)]\tAll Loss: 1.5434\tTriple Loss(1): 0.0547\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 16 [68480/209539 (33%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.0914\tClassification Loss: 1.8377\r\n",
      "Train Epoch: 16 [69120/209539 (33%)]\tAll Loss: 2.1142\tTriple Loss(1): 0.1785\tClassification Loss: 1.7572\r\n",
      "Train Epoch: 16 [69760/209539 (33%)]\tAll Loss: 1.4345\tTriple Loss(1): 0.0094\tClassification Loss: 1.4156\r\n",
      "Train Epoch: 16 [70400/209539 (34%)]\tAll Loss: 1.9175\tTriple Loss(1): 0.1611\tClassification Loss: 1.5953\r\n",
      "Train Epoch: 16 [71040/209539 (34%)]\tAll Loss: 2.7331\tTriple Loss(0): 0.4003\tClassification Loss: 1.9324\r\n",
      "Train Epoch: 16 [71680/209539 (34%)]\tAll Loss: 2.0031\tTriple Loss(1): 0.0780\tClassification Loss: 1.8472\r\n",
      "Train Epoch: 16 [72320/209539 (35%)]\tAll Loss: 2.8625\tTriple Loss(0): 0.6392\tClassification Loss: 1.5841\r\n",
      "Train Epoch: 16 [72960/209539 (35%)]\tAll Loss: 1.7538\tTriple Loss(1): 0.1798\tClassification Loss: 1.3941\r\n",
      "Train Epoch: 16 [73600/209539 (35%)]\tAll Loss: 2.0357\tTriple Loss(1): 0.2166\tClassification Loss: 1.6025\r\n",
      "Train Epoch: 16 [74240/209539 (35%)]\tAll Loss: 2.0197\tTriple Loss(1): 0.1088\tClassification Loss: 1.8020\r\n",
      "Train Epoch: 16 [74880/209539 (36%)]\tAll Loss: 2.4689\tTriple Loss(1): 0.1778\tClassification Loss: 2.1132\r\n",
      "Train Epoch: 16 [75520/209539 (36%)]\tAll Loss: 2.2790\tTriple Loss(0): 0.5424\tClassification Loss: 1.1941\r\n",
      "Train Epoch: 16 [76160/209539 (36%)]\tAll Loss: 1.6794\tTriple Loss(1): 0.0557\tClassification Loss: 1.5680\r\n",
      "Train Epoch: 16 [76800/209539 (37%)]\tAll Loss: 1.7683\tTriple Loss(1): 0.0803\tClassification Loss: 1.6078\r\n",
      "Train Epoch: 16 [77440/209539 (37%)]\tAll Loss: 1.9375\tTriple Loss(1): 0.0873\tClassification Loss: 1.7629\r\n",
      "Train Epoch: 16 [78080/209539 (37%)]\tAll Loss: 1.6183\tTriple Loss(1): 0.0509\tClassification Loss: 1.5164\r\n",
      "Train Epoch: 16 [78720/209539 (38%)]\tAll Loss: 1.8230\tTriple Loss(1): 0.0386\tClassification Loss: 1.7458\r\n",
      "Train Epoch: 16 [79360/209539 (38%)]\tAll Loss: 2.2927\tTriple Loss(0): 0.3784\tClassification Loss: 1.5358\r\n",
      "Train Epoch: 16 [80000/209539 (38%)]\tAll Loss: 2.2012\tTriple Loss(0): 0.3212\tClassification Loss: 1.5589\r\n",
      "Train Epoch: 16 [80640/209539 (38%)]\tAll Loss: 1.7307\tTriple Loss(1): 0.1519\tClassification Loss: 1.4269\r\n",
      "Train Epoch: 16 [81280/209539 (39%)]\tAll Loss: 1.7198\tTriple Loss(1): 0.0675\tClassification Loss: 1.5847\r\n",
      "Train Epoch: 16 [81920/209539 (39%)]\tAll Loss: 1.8677\tTriple Loss(1): 0.1636\tClassification Loss: 1.5405\r\n",
      "Train Epoch: 16 [82560/209539 (39%)]\tAll Loss: 2.6906\tTriple Loss(0): 0.5542\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 16 [83200/209539 (40%)]\tAll Loss: 1.7011\tTriple Loss(1): 0.0877\tClassification Loss: 1.5256\r\n",
      "Train Epoch: 16 [83840/209539 (40%)]\tAll Loss: 2.6008\tTriple Loss(0): 0.4581\tClassification Loss: 1.6845\r\n",
      "Train Epoch: 16 [84480/209539 (40%)]\tAll Loss: 1.6348\tTriple Loss(1): 0.1150\tClassification Loss: 1.4049\r\n",
      "Train Epoch: 16 [85120/209539 (41%)]\tAll Loss: 2.3473\tTriple Loss(1): 0.1150\tClassification Loss: 2.1174\r\n",
      "Train Epoch: 16 [85760/209539 (41%)]\tAll Loss: 1.7446\tTriple Loss(1): 0.0893\tClassification Loss: 1.5660\r\n",
      "Train Epoch: 16 [86400/209539 (41%)]\tAll Loss: 1.5851\tTriple Loss(1): 0.0625\tClassification Loss: 1.4601\r\n",
      "Train Epoch: 16 [87040/209539 (42%)]\tAll Loss: 2.3359\tTriple Loss(0): 0.4700\tClassification Loss: 1.3960\r\n",
      "Train Epoch: 16 [87680/209539 (42%)]\tAll Loss: 1.9416\tTriple Loss(1): 0.1696\tClassification Loss: 1.6023\r\n",
      "Train Epoch: 16 [88320/209539 (42%)]\tAll Loss: 1.7687\tTriple Loss(1): 0.0823\tClassification Loss: 1.6042\r\n",
      "Train Epoch: 16 [88960/209539 (42%)]\tAll Loss: 1.7648\tTriple Loss(1): 0.0927\tClassification Loss: 1.5794\r\n",
      "Train Epoch: 16 [89600/209539 (43%)]\tAll Loss: 2.0174\tTriple Loss(1): 0.1387\tClassification Loss: 1.7400\r\n",
      "Train Epoch: 16 [90240/209539 (43%)]\tAll Loss: 1.7776\tTriple Loss(1): 0.1187\tClassification Loss: 1.5402\r\n",
      "Train Epoch: 16 [90880/209539 (43%)]\tAll Loss: 2.1834\tTriple Loss(1): 0.0759\tClassification Loss: 2.0316\r\n",
      "Train Epoch: 16 [91520/209539 (44%)]\tAll Loss: 1.6189\tTriple Loss(1): 0.0430\tClassification Loss: 1.5328\r\n",
      "Train Epoch: 16 [92160/209539 (44%)]\tAll Loss: 1.7885\tTriple Loss(1): 0.0779\tClassification Loss: 1.6328\r\n",
      "Train Epoch: 16 [92800/209539 (44%)]\tAll Loss: 1.5914\tTriple Loss(1): 0.0582\tClassification Loss: 1.4750\r\n",
      "Train Epoch: 16 [93440/209539 (45%)]\tAll Loss: 1.9465\tTriple Loss(1): 0.0837\tClassification Loss: 1.7791\r\n",
      "Train Epoch: 16 [94080/209539 (45%)]\tAll Loss: 1.9714\tTriple Loss(1): 0.0964\tClassification Loss: 1.7786\r\n",
      "Train Epoch: 16 [94720/209539 (45%)]\tAll Loss: 2.9892\tTriple Loss(0): 0.6315\tClassification Loss: 1.7263\r\n",
      "Train Epoch: 16 [95360/209539 (46%)]\tAll Loss: 1.7950\tTriple Loss(1): 0.0706\tClassification Loss: 1.6539\r\n",
      "Train Epoch: 16 [96000/209539 (46%)]\tAll Loss: 1.7685\tTriple Loss(1): 0.1206\tClassification Loss: 1.5274\r\n",
      "Train Epoch: 16 [96640/209539 (46%)]\tAll Loss: 2.0357\tTriple Loss(1): 0.0808\tClassification Loss: 1.8740\r\n",
      "Train Epoch: 16 [97280/209539 (46%)]\tAll Loss: 1.5200\tTriple Loss(1): 0.0689\tClassification Loss: 1.3821\r\n",
      "Train Epoch: 16 [97920/209539 (47%)]\tAll Loss: 2.4383\tTriple Loss(0): 0.3894\tClassification Loss: 1.6596\r\n",
      "Train Epoch: 16 [98560/209539 (47%)]\tAll Loss: 1.9367\tTriple Loss(1): 0.0651\tClassification Loss: 1.8064\r\n",
      "Train Epoch: 16 [99200/209539 (47%)]\tAll Loss: 1.8849\tTriple Loss(1): 0.1347\tClassification Loss: 1.6155\r\n",
      "Train Epoch: 16 [99840/209539 (48%)]\tAll Loss: 2.9666\tTriple Loss(0): 0.6063\tClassification Loss: 1.7541\r\n",
      "Train Epoch: 16 [100480/209539 (48%)]\tAll Loss: 1.6404\tTriple Loss(1): 0.0650\tClassification Loss: 1.5105\r\n",
      "Train Epoch: 16 [101120/209539 (48%)]\tAll Loss: 2.5002\tTriple Loss(0): 0.4872\tClassification Loss: 1.5257\r\n",
      "Train Epoch: 16 [101760/209539 (49%)]\tAll Loss: 1.6283\tTriple Loss(1): 0.0391\tClassification Loss: 1.5501\r\n",
      "Train Epoch: 16 [102400/209539 (49%)]\tAll Loss: 1.8616\tTriple Loss(1): 0.1003\tClassification Loss: 1.6609\r\n",
      "Train Epoch: 16 [103040/209539 (49%)]\tAll Loss: 1.8855\tTriple Loss(1): 0.0950\tClassification Loss: 1.6955\r\n",
      "Train Epoch: 16 [103680/209539 (49%)]\tAll Loss: 2.0357\tTriple Loss(1): 0.0639\tClassification Loss: 1.9079\r\n",
      "Train Epoch: 16 [104320/209539 (50%)]\tAll Loss: 1.7967\tTriple Loss(1): 0.1115\tClassification Loss: 1.5736\r\n",
      "Train Epoch: 16 [104960/209539 (50%)]\tAll Loss: 1.5592\tTriple Loss(1): 0.0831\tClassification Loss: 1.3929\r\n",
      "Train Epoch: 16 [105600/209539 (50%)]\tAll Loss: 1.6768\tTriple Loss(1): 0.0584\tClassification Loss: 1.5600\r\n",
      "Train Epoch: 16 [106240/209539 (51%)]\tAll Loss: 1.6277\tTriple Loss(1): 0.0680\tClassification Loss: 1.4916\r\n",
      "Train Epoch: 16 [106880/209539 (51%)]\tAll Loss: 2.7772\tTriple Loss(0): 0.5186\tClassification Loss: 1.7399\r\n",
      "Train Epoch: 16 [107520/209539 (51%)]\tAll Loss: 1.4850\tTriple Loss(1): 0.0508\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 16 [108160/209539 (52%)]\tAll Loss: 2.8893\tTriple Loss(0): 0.6156\tClassification Loss: 1.6580\r\n",
      "Train Epoch: 16 [108800/209539 (52%)]\tAll Loss: 2.8525\tTriple Loss(0): 0.6137\tClassification Loss: 1.6251\r\n",
      "Train Epoch: 16 [109440/209539 (52%)]\tAll Loss: 3.1315\tTriple Loss(0): 0.6037\tClassification Loss: 1.9241\r\n",
      "Train Epoch: 16 [110080/209539 (53%)]\tAll Loss: 1.9952\tTriple Loss(1): 0.1723\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 16 [110720/209539 (53%)]\tAll Loss: 1.6666\tTriple Loss(1): 0.0419\tClassification Loss: 1.5828\r\n",
      "Train Epoch: 16 [111360/209539 (53%)]\tAll Loss: 1.5247\tTriple Loss(1): 0.0952\tClassification Loss: 1.3343\r\n",
      "Train Epoch: 16 [112000/209539 (53%)]\tAll Loss: 1.5115\tTriple Loss(1): 0.0603\tClassification Loss: 1.3909\r\n",
      "Train Epoch: 16 [112640/209539 (54%)]\tAll Loss: 1.7393\tTriple Loss(1): 0.0616\tClassification Loss: 1.6161\r\n",
      "Train Epoch: 16 [113280/209539 (54%)]\tAll Loss: 2.5253\tTriple Loss(0): 0.5188\tClassification Loss: 1.4876\r\n",
      "Train Epoch: 16 [113920/209539 (54%)]\tAll Loss: 2.7492\tTriple Loss(0): 0.4983\tClassification Loss: 1.7526\r\n",
      "Train Epoch: 16 [114560/209539 (55%)]\tAll Loss: 1.8233\tTriple Loss(1): 0.0986\tClassification Loss: 1.6260\r\n",
      "Train Epoch: 16 [115200/209539 (55%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.0372\tClassification Loss: 1.6864\r\n",
      "Train Epoch: 16 [115840/209539 (55%)]\tAll Loss: 1.7779\tTriple Loss(1): 0.1200\tClassification Loss: 1.5379\r\n",
      "Train Epoch: 16 [116480/209539 (56%)]\tAll Loss: 1.8082\tTriple Loss(1): 0.0922\tClassification Loss: 1.6238\r\n",
      "Train Epoch: 16 [117120/209539 (56%)]\tAll Loss: 1.8777\tTriple Loss(1): 0.2126\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 16 [117760/209539 (56%)]\tAll Loss: 2.5838\tTriple Loss(0): 0.5255\tClassification Loss: 1.5327\r\n",
      "Train Epoch: 16 [118400/209539 (57%)]\tAll Loss: 2.0105\tTriple Loss(0): 0.3557\tClassification Loss: 1.2992\r\n",
      "Train Epoch: 16 [119040/209539 (57%)]\tAll Loss: 1.8442\tTriple Loss(1): 0.0664\tClassification Loss: 1.7114\r\n",
      "Train Epoch: 16 [119680/209539 (57%)]\tAll Loss: 1.7442\tTriple Loss(1): 0.0768\tClassification Loss: 1.5906\r\n",
      "Train Epoch: 16 [120320/209539 (57%)]\tAll Loss: 1.6766\tTriple Loss(1): 0.0456\tClassification Loss: 1.5855\r\n",
      "Train Epoch: 16 [120960/209539 (58%)]\tAll Loss: 1.5038\tTriple Loss(1): 0.0828\tClassification Loss: 1.3383\r\n",
      "Train Epoch: 16 [121600/209539 (58%)]\tAll Loss: 1.6810\tTriple Loss(1): 0.1754\tClassification Loss: 1.3302\r\n",
      "Train Epoch: 16 [122240/209539 (58%)]\tAll Loss: 1.6614\tTriple Loss(1): 0.0947\tClassification Loss: 1.4720\r\n",
      "Train Epoch: 16 [122880/209539 (59%)]\tAll Loss: 1.6479\tTriple Loss(1): 0.1139\tClassification Loss: 1.4202\r\n",
      "Train Epoch: 16 [123520/209539 (59%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.1094\tClassification Loss: 1.6350\r\n",
      "Train Epoch: 16 [124160/209539 (59%)]\tAll Loss: 1.7974\tTriple Loss(1): 0.0747\tClassification Loss: 1.6480\r\n",
      "Train Epoch: 16 [124800/209539 (60%)]\tAll Loss: 1.2296\tTriple Loss(1): 0.0284\tClassification Loss: 1.1727\r\n",
      "Train Epoch: 16 [125440/209539 (60%)]\tAll Loss: 1.7224\tTriple Loss(1): 0.0411\tClassification Loss: 1.6402\r\n",
      "Train Epoch: 16 [126080/209539 (60%)]\tAll Loss: 2.4201\tTriple Loss(0): 0.3806\tClassification Loss: 1.6588\r\n",
      "Train Epoch: 16 [126720/209539 (60%)]\tAll Loss: 1.9697\tTriple Loss(1): 0.0941\tClassification Loss: 1.7814\r\n",
      "Train Epoch: 16 [127360/209539 (61%)]\tAll Loss: 1.7215\tTriple Loss(1): 0.0741\tClassification Loss: 1.5734\r\n",
      "Train Epoch: 16 [128000/209539 (61%)]\tAll Loss: 1.9397\tTriple Loss(1): 0.0679\tClassification Loss: 1.8038\r\n",
      "Train Epoch: 16 [128640/209539 (61%)]\tAll Loss: 1.5890\tTriple Loss(1): 0.1082\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 16 [129280/209539 (62%)]\tAll Loss: 1.9032\tTriple Loss(1): 0.1113\tClassification Loss: 1.6806\r\n",
      "Train Epoch: 16 [129920/209539 (62%)]\tAll Loss: 2.3043\tTriple Loss(0): 0.5036\tClassification Loss: 1.2970\r\n",
      "Train Epoch: 16 [130560/209539 (62%)]\tAll Loss: 2.3690\tTriple Loss(0): 0.5291\tClassification Loss: 1.3108\r\n",
      "Train Epoch: 16 [131200/209539 (63%)]\tAll Loss: 1.5211\tTriple Loss(1): 0.0843\tClassification Loss: 1.3524\r\n",
      "Train Epoch: 16 [131840/209539 (63%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.2031\tClassification Loss: 1.6144\r\n",
      "Train Epoch: 16 [132480/209539 (63%)]\tAll Loss: 1.6218\tTriple Loss(1): 0.1299\tClassification Loss: 1.3621\r\n",
      "Train Epoch: 16 [133120/209539 (64%)]\tAll Loss: 1.7310\tTriple Loss(1): 0.0967\tClassification Loss: 1.5376\r\n",
      "Train Epoch: 16 [133760/209539 (64%)]\tAll Loss: 1.8587\tTriple Loss(1): 0.1278\tClassification Loss: 1.6031\r\n",
      "Train Epoch: 16 [134400/209539 (64%)]\tAll Loss: 2.4656\tTriple Loss(0): 0.5068\tClassification Loss: 1.4520\r\n",
      "Train Epoch: 16 [135040/209539 (64%)]\tAll Loss: 2.0357\tTriple Loss(1): 0.2144\tClassification Loss: 1.6070\r\n",
      "Train Epoch: 16 [135680/209539 (65%)]\tAll Loss: 1.9324\tTriple Loss(1): 0.0925\tClassification Loss: 1.7475\r\n",
      "Train Epoch: 16 [136320/209539 (65%)]\tAll Loss: 1.9369\tTriple Loss(1): 0.1302\tClassification Loss: 1.6765\r\n",
      "Train Epoch: 16 [136960/209539 (65%)]\tAll Loss: 1.5110\tTriple Loss(1): 0.0882\tClassification Loss: 1.3347\r\n",
      "Train Epoch: 16 [137600/209539 (66%)]\tAll Loss: 1.9152\tTriple Loss(0): 0.2696\tClassification Loss: 1.3760\r\n",
      "Train Epoch: 16 [138240/209539 (66%)]\tAll Loss: 1.9360\tTriple Loss(1): 0.1937\tClassification Loss: 1.5487\r\n",
      "Train Epoch: 16 [138880/209539 (66%)]\tAll Loss: 1.5303\tTriple Loss(1): 0.0274\tClassification Loss: 1.4756\r\n",
      "Train Epoch: 16 [139520/209539 (67%)]\tAll Loss: 1.8146\tTriple Loss(1): 0.0410\tClassification Loss: 1.7327\r\n",
      "Train Epoch: 16 [140160/209539 (67%)]\tAll Loss: 1.9183\tTriple Loss(1): 0.1569\tClassification Loss: 1.6044\r\n",
      "Train Epoch: 16 [140800/209539 (67%)]\tAll Loss: 1.9927\tTriple Loss(1): 0.1090\tClassification Loss: 1.7747\r\n",
      "Train Epoch: 16 [141440/209539 (68%)]\tAll Loss: 1.9560\tTriple Loss(1): 0.1169\tClassification Loss: 1.7222\r\n",
      "Train Epoch: 16 [142080/209539 (68%)]\tAll Loss: 2.3012\tTriple Loss(0): 0.4525\tClassification Loss: 1.3962\r\n",
      "Train Epoch: 16 [142720/209539 (68%)]\tAll Loss: 2.0981\tTriple Loss(1): 0.0645\tClassification Loss: 1.9691\r\n",
      "Train Epoch: 16 [143360/209539 (68%)]\tAll Loss: 1.4703\tTriple Loss(1): 0.0825\tClassification Loss: 1.3053\r\n",
      "Train Epoch: 16 [144000/209539 (69%)]\tAll Loss: 2.7368\tTriple Loss(0): 0.5767\tClassification Loss: 1.5834\r\n",
      "Train Epoch: 16 [144640/209539 (69%)]\tAll Loss: 2.4450\tTriple Loss(0): 0.4690\tClassification Loss: 1.5071\r\n",
      "Train Epoch: 16 [145280/209539 (69%)]\tAll Loss: 1.9824\tTriple Loss(1): 0.1130\tClassification Loss: 1.7564\r\n",
      "Train Epoch: 16 [145920/209539 (70%)]\tAll Loss: 2.7916\tTriple Loss(0): 0.5264\tClassification Loss: 1.7388\r\n",
      "Train Epoch: 16 [146560/209539 (70%)]\tAll Loss: 1.4826\tTriple Loss(1): 0.1292\tClassification Loss: 1.2242\r\n",
      "Train Epoch: 16 [147200/209539 (70%)]\tAll Loss: 1.5257\tTriple Loss(1): 0.0419\tClassification Loss: 1.4419\r\n",
      "Train Epoch: 16 [147840/209539 (71%)]\tAll Loss: 2.2328\tTriple Loss(0): 0.4779\tClassification Loss: 1.2769\r\n",
      "Train Epoch: 16 [148480/209539 (71%)]\tAll Loss: 1.4715\tTriple Loss(1): 0.0473\tClassification Loss: 1.3769\r\n",
      "Train Epoch: 16 [149120/209539 (71%)]\tAll Loss: 1.7672\tTriple Loss(1): 0.0977\tClassification Loss: 1.5718\r\n",
      "Train Epoch: 16 [149760/209539 (71%)]\tAll Loss: 1.6145\tTriple Loss(1): 0.0927\tClassification Loss: 1.4292\r\n",
      "Train Epoch: 16 [150400/209539 (72%)]\tAll Loss: 1.7164\tTriple Loss(1): 0.0869\tClassification Loss: 1.5426\r\n",
      "Train Epoch: 16 [151040/209539 (72%)]\tAll Loss: 1.4647\tTriple Loss(1): 0.0416\tClassification Loss: 1.3815\r\n",
      "Train Epoch: 16 [151680/209539 (72%)]\tAll Loss: 1.9823\tTriple Loss(1): 0.0895\tClassification Loss: 1.8034\r\n",
      "Train Epoch: 16 [152320/209539 (73%)]\tAll Loss: 1.8473\tTriple Loss(1): 0.1653\tClassification Loss: 1.5167\r\n",
      "Train Epoch: 16 [152960/209539 (73%)]\tAll Loss: 1.7346\tTriple Loss(1): 0.1738\tClassification Loss: 1.3869\r\n",
      "Train Epoch: 16 [153600/209539 (73%)]\tAll Loss: 2.5809\tTriple Loss(0): 0.5203\tClassification Loss: 1.5403\r\n",
      "Train Epoch: 16 [154240/209539 (74%)]\tAll Loss: 1.6765\tTriple Loss(1): 0.1046\tClassification Loss: 1.4674\r\n",
      "Train Epoch: 16 [154880/209539 (74%)]\tAll Loss: 3.2282\tTriple Loss(0): 0.7067\tClassification Loss: 1.8149\r\n",
      "Train Epoch: 16 [155520/209539 (74%)]\tAll Loss: 1.5223\tTriple Loss(1): 0.0935\tClassification Loss: 1.3354\r\n",
      "Train Epoch: 16 [156160/209539 (75%)]\tAll Loss: 1.8435\tTriple Loss(1): 0.0000\tClassification Loss: 1.8435\r\n",
      "Train Epoch: 16 [156800/209539 (75%)]\tAll Loss: 2.3451\tTriple Loss(1): 0.1247\tClassification Loss: 2.0957\r\n",
      "Train Epoch: 16 [157440/209539 (75%)]\tAll Loss: 1.8490\tTriple Loss(1): 0.0864\tClassification Loss: 1.6763\r\n",
      "Train Epoch: 16 [158080/209539 (75%)]\tAll Loss: 1.5114\tTriple Loss(1): 0.0482\tClassification Loss: 1.4151\r\n",
      "Train Epoch: 16 [158720/209539 (76%)]\tAll Loss: 1.6117\tTriple Loss(1): 0.0859\tClassification Loss: 1.4399\r\n",
      "Train Epoch: 16 [159360/209539 (76%)]\tAll Loss: 1.4402\tTriple Loss(1): 0.1084\tClassification Loss: 1.2234\r\n",
      "Train Epoch: 16 [160000/209539 (76%)]\tAll Loss: 2.0495\tTriple Loss(1): 0.0798\tClassification Loss: 1.8899\r\n",
      "Train Epoch: 16 [160640/209539 (77%)]\tAll Loss: 1.6894\tTriple Loss(1): 0.0787\tClassification Loss: 1.5320\r\n",
      "Train Epoch: 16 [161280/209539 (77%)]\tAll Loss: 1.5554\tTriple Loss(1): 0.1226\tClassification Loss: 1.3102\r\n",
      "Train Epoch: 16 [161920/209539 (77%)]\tAll Loss: 1.7868\tTriple Loss(1): 0.0910\tClassification Loss: 1.6049\r\n",
      "Train Epoch: 16 [162560/209539 (78%)]\tAll Loss: 1.5871\tTriple Loss(1): 0.0375\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 16 [163200/209539 (78%)]\tAll Loss: 1.6180\tTriple Loss(1): 0.1224\tClassification Loss: 1.3732\r\n",
      "Train Epoch: 16 [163840/209539 (78%)]\tAll Loss: 1.7592\tTriple Loss(1): 0.1283\tClassification Loss: 1.5026\r\n",
      "Train Epoch: 16 [164480/209539 (78%)]\tAll Loss: 1.7465\tTriple Loss(1): 0.2127\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 16 [165120/209539 (79%)]\tAll Loss: 1.6103\tTriple Loss(1): 0.0432\tClassification Loss: 1.5239\r\n",
      "Train Epoch: 16 [165760/209539 (79%)]\tAll Loss: 1.9144\tTriple Loss(1): 0.1876\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 16 [166400/209539 (79%)]\tAll Loss: 1.7721\tTriple Loss(1): 0.0863\tClassification Loss: 1.5995\r\n",
      "Train Epoch: 16 [167040/209539 (80%)]\tAll Loss: 1.6236\tTriple Loss(1): 0.0528\tClassification Loss: 1.5181\r\n",
      "Train Epoch: 16 [167680/209539 (80%)]\tAll Loss: 1.9593\tTriple Loss(1): 0.0565\tClassification Loss: 1.8463\r\n",
      "Train Epoch: 16 [168320/209539 (80%)]\tAll Loss: 1.7968\tTriple Loss(1): 0.1157\tClassification Loss: 1.5655\r\n",
      "Train Epoch: 16 [168960/209539 (81%)]\tAll Loss: 1.5854\tTriple Loss(1): 0.1127\tClassification Loss: 1.3599\r\n",
      "Train Epoch: 16 [169600/209539 (81%)]\tAll Loss: 1.6101\tTriple Loss(1): 0.0651\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 16 [170240/209539 (81%)]\tAll Loss: 1.8888\tTriple Loss(1): 0.0865\tClassification Loss: 1.7159\r\n",
      "Train Epoch: 16 [170880/209539 (82%)]\tAll Loss: 1.7059\tTriple Loss(1): 0.0849\tClassification Loss: 1.5361\r\n",
      "Train Epoch: 16 [171520/209539 (82%)]\tAll Loss: 1.3713\tTriple Loss(1): 0.0545\tClassification Loss: 1.2622\r\n",
      "Train Epoch: 16 [172160/209539 (82%)]\tAll Loss: 1.6196\tTriple Loss(1): 0.0421\tClassification Loss: 1.5355\r\n",
      "Train Epoch: 16 [172800/209539 (82%)]\tAll Loss: 2.3942\tTriple Loss(0): 0.3253\tClassification Loss: 1.7436\r\n",
      "Train Epoch: 16 [173440/209539 (83%)]\tAll Loss: 1.9380\tTriple Loss(1): 0.0202\tClassification Loss: 1.8977\r\n",
      "Train Epoch: 16 [174080/209539 (83%)]\tAll Loss: 2.6222\tTriple Loss(0): 0.5601\tClassification Loss: 1.5020\r\n",
      "Train Epoch: 16 [174720/209539 (83%)]\tAll Loss: 1.8121\tTriple Loss(1): 0.1176\tClassification Loss: 1.5769\r\n",
      "Train Epoch: 16 [175360/209539 (84%)]\tAll Loss: 1.8786\tTriple Loss(1): 0.0649\tClassification Loss: 1.7488\r\n",
      "Train Epoch: 16 [176000/209539 (84%)]\tAll Loss: 1.9810\tTriple Loss(1): 0.2384\tClassification Loss: 1.5041\r\n",
      "Train Epoch: 16 [176640/209539 (84%)]\tAll Loss: 1.9313\tTriple Loss(0): 0.4197\tClassification Loss: 1.0918\r\n",
      "Train Epoch: 16 [177280/209539 (85%)]\tAll Loss: 1.7737\tTriple Loss(1): 0.0359\tClassification Loss: 1.7020\r\n",
      "Train Epoch: 16 [177920/209539 (85%)]\tAll Loss: 1.8518\tTriple Loss(1): 0.0711\tClassification Loss: 1.7097\r\n",
      "Train Epoch: 16 [178560/209539 (85%)]\tAll Loss: 1.5173\tTriple Loss(1): 0.0366\tClassification Loss: 1.4442\r\n",
      "Train Epoch: 16 [179200/209539 (86%)]\tAll Loss: 1.5082\tTriple Loss(1): 0.0111\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 16 [179840/209539 (86%)]\tAll Loss: 1.5737\tTriple Loss(1): 0.0552\tClassification Loss: 1.4632\r\n",
      "Train Epoch: 16 [180480/209539 (86%)]\tAll Loss: 2.3122\tTriple Loss(0): 0.4006\tClassification Loss: 1.5110\r\n",
      "Train Epoch: 16 [181120/209539 (86%)]\tAll Loss: 2.8591\tTriple Loss(0): 0.5818\tClassification Loss: 1.6954\r\n",
      "Train Epoch: 16 [181760/209539 (87%)]\tAll Loss: 1.4101\tTriple Loss(1): 0.0841\tClassification Loss: 1.2419\r\n",
      "Train Epoch: 16 [182400/209539 (87%)]\tAll Loss: 1.8923\tTriple Loss(1): 0.1466\tClassification Loss: 1.5991\r\n",
      "Train Epoch: 16 [183040/209539 (87%)]\tAll Loss: 1.5396\tTriple Loss(1): 0.0647\tClassification Loss: 1.4102\r\n",
      "Train Epoch: 16 [183680/209539 (88%)]\tAll Loss: 1.4457\tTriple Loss(1): 0.0749\tClassification Loss: 1.2960\r\n",
      "Train Epoch: 16 [184320/209539 (88%)]\tAll Loss: 1.6433\tTriple Loss(1): 0.0577\tClassification Loss: 1.5280\r\n",
      "Train Epoch: 16 [184960/209539 (88%)]\tAll Loss: 2.1688\tTriple Loss(0): 0.4616\tClassification Loss: 1.2455\r\n",
      "Train Epoch: 16 [185600/209539 (89%)]\tAll Loss: 1.8834\tTriple Loss(1): 0.0980\tClassification Loss: 1.6873\r\n",
      "Train Epoch: 16 [186240/209539 (89%)]\tAll Loss: 1.8064\tTriple Loss(1): 0.0942\tClassification Loss: 1.6181\r\n",
      "Train Epoch: 16 [186880/209539 (89%)]\tAll Loss: 1.7761\tTriple Loss(1): 0.1178\tClassification Loss: 1.5404\r\n",
      "Train Epoch: 16 [187520/209539 (89%)]\tAll Loss: 2.1979\tTriple Loss(1): 0.1197\tClassification Loss: 1.9585\r\n",
      "Train Epoch: 16 [188160/209539 (90%)]\tAll Loss: 1.3493\tTriple Loss(1): 0.0509\tClassification Loss: 1.2475\r\n",
      "Train Epoch: 16 [188800/209539 (90%)]\tAll Loss: 1.5434\tTriple Loss(1): 0.0813\tClassification Loss: 1.3807\r\n",
      "Train Epoch: 16 [189440/209539 (90%)]\tAll Loss: 1.6042\tTriple Loss(1): 0.0820\tClassification Loss: 1.4403\r\n",
      "Train Epoch: 16 [190080/209539 (91%)]\tAll Loss: 2.1691\tTriple Loss(0): 0.4044\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 16 [190720/209539 (91%)]\tAll Loss: 1.7352\tTriple Loss(1): 0.0575\tClassification Loss: 1.6201\r\n",
      "Train Epoch: 16 [191360/209539 (91%)]\tAll Loss: 2.2232\tTriple Loss(0): 0.4499\tClassification Loss: 1.3235\r\n",
      "Train Epoch: 16 [192000/209539 (92%)]\tAll Loss: 2.1494\tTriple Loss(1): 0.1012\tClassification Loss: 1.9469\r\n",
      "Train Epoch: 16 [192640/209539 (92%)]\tAll Loss: 1.7373\tTriple Loss(1): 0.0640\tClassification Loss: 1.6092\r\n",
      "Train Epoch: 16 [193280/209539 (92%)]\tAll Loss: 2.4651\tTriple Loss(0): 0.5090\tClassification Loss: 1.4472\r\n",
      "Train Epoch: 16 [193920/209539 (93%)]\tAll Loss: 1.8062\tTriple Loss(1): 0.1215\tClassification Loss: 1.5632\r\n",
      "Train Epoch: 16 [194560/209539 (93%)]\tAll Loss: 2.4464\tTriple Loss(0): 0.5663\tClassification Loss: 1.3139\r\n",
      "Train Epoch: 16 [195200/209539 (93%)]\tAll Loss: 2.4122\tTriple Loss(0): 0.4264\tClassification Loss: 1.5594\r\n",
      "Train Epoch: 16 [195840/209539 (93%)]\tAll Loss: 2.2935\tTriple Loss(0): 0.5196\tClassification Loss: 1.2543\r\n",
      "Train Epoch: 16 [196480/209539 (94%)]\tAll Loss: 1.8856\tTriple Loss(1): 0.0857\tClassification Loss: 1.7141\r\n",
      "Train Epoch: 16 [197120/209539 (94%)]\tAll Loss: 2.0258\tTriple Loss(1): 0.0825\tClassification Loss: 1.8607\r\n",
      "Train Epoch: 16 [197760/209539 (94%)]\tAll Loss: 2.4958\tTriple Loss(0): 0.5256\tClassification Loss: 1.4447\r\n",
      "Train Epoch: 16 [198400/209539 (95%)]\tAll Loss: 1.7201\tTriple Loss(1): 0.0978\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 16 [199040/209539 (95%)]\tAll Loss: 1.7895\tTriple Loss(1): 0.0519\tClassification Loss: 1.6857\r\n",
      "Train Epoch: 16 [199680/209539 (95%)]\tAll Loss: 1.3490\tTriple Loss(1): 0.0217\tClassification Loss: 1.3057\r\n",
      "Train Epoch: 16 [200320/209539 (96%)]\tAll Loss: 1.6028\tTriple Loss(1): 0.0362\tClassification Loss: 1.5303\r\n",
      "Train Epoch: 16 [200960/209539 (96%)]\tAll Loss: 1.3481\tTriple Loss(1): 0.0444\tClassification Loss: 1.2592\r\n",
      "Train Epoch: 16 [201600/209539 (96%)]\tAll Loss: 2.5219\tTriple Loss(0): 0.6292\tClassification Loss: 1.2636\r\n",
      "Train Epoch: 16 [202240/209539 (97%)]\tAll Loss: 1.5284\tTriple Loss(1): 0.0225\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 16 [202880/209539 (97%)]\tAll Loss: 1.5442\tTriple Loss(1): 0.1057\tClassification Loss: 1.3328\r\n",
      "Train Epoch: 16 [203520/209539 (97%)]\tAll Loss: 1.7473\tTriple Loss(1): 0.0563\tClassification Loss: 1.6347\r\n",
      "Train Epoch: 16 [204160/209539 (97%)]\tAll Loss: 1.9896\tTriple Loss(1): 0.0463\tClassification Loss: 1.8970\r\n",
      "Train Epoch: 16 [204800/209539 (98%)]\tAll Loss: 1.9368\tTriple Loss(1): 0.1068\tClassification Loss: 1.7232\r\n",
      "Train Epoch: 16 [205440/209539 (98%)]\tAll Loss: 1.4374\tTriple Loss(1): 0.0897\tClassification Loss: 1.2580\r\n",
      "Train Epoch: 16 [206080/209539 (98%)]\tAll Loss: 1.5507\tTriple Loss(1): 0.0032\tClassification Loss: 1.5442\r\n",
      "Train Epoch: 16 [206720/209539 (99%)]\tAll Loss: 1.4373\tTriple Loss(1): 0.0247\tClassification Loss: 1.3879\r\n",
      "Train Epoch: 16 [207360/209539 (99%)]\tAll Loss: 2.5395\tTriple Loss(0): 0.6123\tClassification Loss: 1.3150\r\n",
      "Train Epoch: 16 [208000/209539 (99%)]\tAll Loss: 1.3047\tTriple Loss(1): 0.0000\tClassification Loss: 1.3047\r\n",
      "Train Epoch: 16 [208640/209539 (100%)]\tAll Loss: 2.3777\tTriple Loss(0): 0.4503\tClassification Loss: 1.4770\r\n",
      "Train Epoch: 16 [209280/209539 (100%)]\tAll Loss: 1.8080\tTriple Loss(1): 0.1612\tClassification Loss: 1.4856\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/16_epochs\r\n",
      "Train Epoch: 17 [0/209539 (0%)]\tAll Loss: 3.0055\tTriple Loss(0): 0.5982\tClassification Loss: 1.8091\r\n",
      "\r\n",
      "Test set: Average loss: 1.3279\r\n",
      "Top 1 Accuracy: 49161/80128 (61%)\r\n",
      "Top 3 Accuracy: 66324/80128 (83%)\r\n",
      "Top 5 Accuracy: 72300/80128 (90%)\r\n",
      " \r\n",
      "Train Epoch: 17 [640/209539 (0%)]\tAll Loss: 1.6930\tTriple Loss(1): 0.0620\tClassification Loss: 1.5690\r\n",
      "Train Epoch: 17 [1280/209539 (1%)]\tAll Loss: 1.3869\tTriple Loss(1): 0.0236\tClassification Loss: 1.3396\r\n",
      "Train Epoch: 17 [1920/209539 (1%)]\tAll Loss: 2.9046\tTriple Loss(0): 0.6508\tClassification Loss: 1.6031\r\n",
      "Train Epoch: 17 [2560/209539 (1%)]\tAll Loss: 2.0194\tTriple Loss(1): 0.0390\tClassification Loss: 1.9415\r\n",
      "Train Epoch: 17 [3200/209539 (2%)]\tAll Loss: 1.8764\tTriple Loss(1): 0.0367\tClassification Loss: 1.8031\r\n",
      "Train Epoch: 17 [3840/209539 (2%)]\tAll Loss: 1.7599\tTriple Loss(1): 0.1212\tClassification Loss: 1.5176\r\n",
      "Train Epoch: 17 [4480/209539 (2%)]\tAll Loss: 2.2902\tTriple Loss(0): 0.3858\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 17 [5120/209539 (2%)]\tAll Loss: 1.4528\tTriple Loss(1): 0.0922\tClassification Loss: 1.2684\r\n",
      "Train Epoch: 17 [5760/209539 (3%)]\tAll Loss: 2.9393\tTriple Loss(0): 0.6444\tClassification Loss: 1.6505\r\n",
      "Train Epoch: 17 [6400/209539 (3%)]\tAll Loss: 1.2577\tTriple Loss(1): 0.0838\tClassification Loss: 1.0901\r\n",
      "Train Epoch: 17 [7040/209539 (3%)]\tAll Loss: 1.5994\tTriple Loss(1): 0.0989\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 17 [7680/209539 (4%)]\tAll Loss: 2.1330\tTriple Loss(0): 0.4327\tClassification Loss: 1.2676\r\n",
      "Train Epoch: 17 [8320/209539 (4%)]\tAll Loss: 1.6832\tTriple Loss(1): 0.0428\tClassification Loss: 1.5977\r\n",
      "Train Epoch: 17 [8960/209539 (4%)]\tAll Loss: 1.5391\tTriple Loss(1): 0.0792\tClassification Loss: 1.3807\r\n",
      "Train Epoch: 17 [9600/209539 (5%)]\tAll Loss: 1.8083\tTriple Loss(1): 0.0687\tClassification Loss: 1.6708\r\n",
      "Train Epoch: 17 [10240/209539 (5%)]\tAll Loss: 1.4306\tTriple Loss(1): 0.0000\tClassification Loss: 1.4306\r\n",
      "Train Epoch: 17 [10880/209539 (5%)]\tAll Loss: 1.7260\tTriple Loss(1): 0.0875\tClassification Loss: 1.5510\r\n",
      "Train Epoch: 17 [11520/209539 (5%)]\tAll Loss: 1.7137\tTriple Loss(1): 0.1101\tClassification Loss: 1.4936\r\n",
      "Train Epoch: 17 [12160/209539 (6%)]\tAll Loss: 1.3779\tTriple Loss(1): 0.0183\tClassification Loss: 1.3412\r\n",
      "Train Epoch: 17 [12800/209539 (6%)]\tAll Loss: 1.2414\tTriple Loss(1): 0.1011\tClassification Loss: 1.0391\r\n",
      "Train Epoch: 17 [13440/209539 (6%)]\tAll Loss: 1.5756\tTriple Loss(1): 0.0390\tClassification Loss: 1.4975\r\n",
      "Train Epoch: 17 [14080/209539 (7%)]\tAll Loss: 2.2705\tTriple Loss(0): 0.4036\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 17 [14720/209539 (7%)]\tAll Loss: 2.5207\tTriple Loss(0): 0.5150\tClassification Loss: 1.4907\r\n",
      "Train Epoch: 17 [15360/209539 (7%)]\tAll Loss: 2.2725\tTriple Loss(0): 0.4042\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 17 [16000/209539 (8%)]\tAll Loss: 2.7223\tTriple Loss(0): 0.3875\tClassification Loss: 1.9472\r\n",
      "Train Epoch: 17 [16640/209539 (8%)]\tAll Loss: 1.9871\tTriple Loss(1): 0.0780\tClassification Loss: 1.8311\r\n",
      "Train Epoch: 17 [17280/209539 (8%)]\tAll Loss: 1.6293\tTriple Loss(1): 0.0318\tClassification Loss: 1.5657\r\n",
      "Train Epoch: 17 [17920/209539 (9%)]\tAll Loss: 2.3092\tTriple Loss(0): 0.3964\tClassification Loss: 1.5164\r\n",
      "Train Epoch: 17 [18560/209539 (9%)]\tAll Loss: 2.5983\tTriple Loss(0): 0.4831\tClassification Loss: 1.6320\r\n",
      "Train Epoch: 17 [19200/209539 (9%)]\tAll Loss: 2.4147\tTriple Loss(0): 0.4511\tClassification Loss: 1.5125\r\n",
      "Train Epoch: 17 [19840/209539 (9%)]\tAll Loss: 2.3475\tTriple Loss(0): 0.4275\tClassification Loss: 1.4925\r\n",
      "Train Epoch: 17 [20480/209539 (10%)]\tAll Loss: 1.3323\tTriple Loss(1): 0.0292\tClassification Loss: 1.2739\r\n",
      "Train Epoch: 17 [21120/209539 (10%)]\tAll Loss: 1.8708\tTriple Loss(1): 0.0548\tClassification Loss: 1.7612\r\n",
      "Train Epoch: 17 [21760/209539 (10%)]\tAll Loss: 2.5220\tTriple Loss(0): 0.5339\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 17 [22400/209539 (11%)]\tAll Loss: 2.3821\tTriple Loss(0): 0.5051\tClassification Loss: 1.3719\r\n",
      "Train Epoch: 17 [23040/209539 (11%)]\tAll Loss: 1.7546\tTriple Loss(1): 0.0902\tClassification Loss: 1.5742\r\n",
      "Train Epoch: 17 [23680/209539 (11%)]\tAll Loss: 1.3969\tTriple Loss(1): 0.0468\tClassification Loss: 1.3032\r\n",
      "Train Epoch: 17 [24320/209539 (12%)]\tAll Loss: 1.8343\tTriple Loss(1): 0.1156\tClassification Loss: 1.6032\r\n",
      "Train Epoch: 17 [24960/209539 (12%)]\tAll Loss: 1.5309\tTriple Loss(1): 0.0780\tClassification Loss: 1.3748\r\n",
      "Train Epoch: 17 [25600/209539 (12%)]\tAll Loss: 1.8628\tTriple Loss(1): 0.2297\tClassification Loss: 1.4033\r\n",
      "Train Epoch: 17 [26240/209539 (13%)]\tAll Loss: 1.5282\tTriple Loss(1): 0.0702\tClassification Loss: 1.3879\r\n",
      "Train Epoch: 17 [26880/209539 (13%)]\tAll Loss: 1.5818\tTriple Loss(1): 0.0089\tClassification Loss: 1.5640\r\n",
      "Train Epoch: 17 [27520/209539 (13%)]\tAll Loss: 1.8464\tTriple Loss(1): 0.1337\tClassification Loss: 1.5790\r\n",
      "Train Epoch: 17 [28160/209539 (13%)]\tAll Loss: 2.7348\tTriple Loss(0): 0.5242\tClassification Loss: 1.6864\r\n",
      "Train Epoch: 17 [28800/209539 (14%)]\tAll Loss: 1.8756\tTriple Loss(1): 0.0316\tClassification Loss: 1.8123\r\n",
      "Train Epoch: 17 [29440/209539 (14%)]\tAll Loss: 1.6921\tTriple Loss(1): 0.0597\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 17 [30080/209539 (14%)]\tAll Loss: 1.5028\tTriple Loss(1): 0.0692\tClassification Loss: 1.3644\r\n",
      "Train Epoch: 17 [30720/209539 (15%)]\tAll Loss: 1.4769\tTriple Loss(1): 0.0811\tClassification Loss: 1.3146\r\n",
      "Train Epoch: 17 [31360/209539 (15%)]\tAll Loss: 1.5139\tTriple Loss(1): 0.1356\tClassification Loss: 1.2428\r\n",
      "Train Epoch: 17 [32000/209539 (15%)]\tAll Loss: 2.1858\tTriple Loss(0): 0.4162\tClassification Loss: 1.3535\r\n",
      "Train Epoch: 17 [32640/209539 (16%)]\tAll Loss: 2.4338\tTriple Loss(0): 0.4133\tClassification Loss: 1.6071\r\n",
      "Train Epoch: 17 [33280/209539 (16%)]\tAll Loss: 2.5924\tTriple Loss(0): 0.4958\tClassification Loss: 1.6008\r\n",
      "Train Epoch: 17 [33920/209539 (16%)]\tAll Loss: 1.7207\tTriple Loss(1): 0.1098\tClassification Loss: 1.5012\r\n",
      "Train Epoch: 17 [34560/209539 (16%)]\tAll Loss: 1.5554\tTriple Loss(1): 0.0488\tClassification Loss: 1.4579\r\n",
      "Train Epoch: 17 [35200/209539 (17%)]\tAll Loss: 1.5995\tTriple Loss(1): 0.1044\tClassification Loss: 1.3906\r\n",
      "Train Epoch: 17 [35840/209539 (17%)]\tAll Loss: 1.5220\tTriple Loss(1): 0.1502\tClassification Loss: 1.2216\r\n",
      "Train Epoch: 17 [36480/209539 (17%)]\tAll Loss: 1.4784\tTriple Loss(1): 0.0382\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 17 [37120/209539 (18%)]\tAll Loss: 1.7009\tTriple Loss(1): 0.0389\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 17 [37760/209539 (18%)]\tAll Loss: 1.8459\tTriple Loss(1): 0.0372\tClassification Loss: 1.7715\r\n",
      "Train Epoch: 17 [38400/209539 (18%)]\tAll Loss: 1.6708\tTriple Loss(1): 0.0613\tClassification Loss: 1.5482\r\n",
      "Train Epoch: 17 [39040/209539 (19%)]\tAll Loss: 1.1096\tTriple Loss(1): 0.0255\tClassification Loss: 1.0587\r\n",
      "Train Epoch: 17 [39680/209539 (19%)]\tAll Loss: 2.3748\tTriple Loss(0): 0.5260\tClassification Loss: 1.3229\r\n",
      "Train Epoch: 17 [40320/209539 (19%)]\tAll Loss: 2.7907\tTriple Loss(0): 0.5925\tClassification Loss: 1.6056\r\n",
      "Train Epoch: 17 [40960/209539 (20%)]\tAll Loss: 1.5417\tTriple Loss(1): 0.1092\tClassification Loss: 1.3233\r\n",
      "Train Epoch: 17 [41600/209539 (20%)]\tAll Loss: 1.5644\tTriple Loss(1): 0.0762\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 17 [42240/209539 (20%)]\tAll Loss: 2.2012\tTriple Loss(0): 0.4650\tClassification Loss: 1.2713\r\n",
      "Train Epoch: 17 [42880/209539 (20%)]\tAll Loss: 1.5205\tTriple Loss(1): 0.0828\tClassification Loss: 1.3548\r\n",
      "Train Epoch: 17 [43520/209539 (21%)]\tAll Loss: 1.7625\tTriple Loss(1): 0.0691\tClassification Loss: 1.6243\r\n",
      "Train Epoch: 17 [44160/209539 (21%)]\tAll Loss: 1.9664\tTriple Loss(1): 0.0574\tClassification Loss: 1.8516\r\n",
      "Train Epoch: 17 [44800/209539 (21%)]\tAll Loss: 1.8705\tTriple Loss(1): 0.1044\tClassification Loss: 1.6618\r\n",
      "Train Epoch: 17 [45440/209539 (22%)]\tAll Loss: 2.3032\tTriple Loss(1): 0.1118\tClassification Loss: 2.0796\r\n",
      "Train Epoch: 17 [46080/209539 (22%)]\tAll Loss: 1.4037\tTriple Loss(1): 0.0325\tClassification Loss: 1.3388\r\n",
      "Train Epoch: 17 [46720/209539 (22%)]\tAll Loss: 2.2181\tTriple Loss(1): 0.1084\tClassification Loss: 2.0013\r\n",
      "Train Epoch: 17 [47360/209539 (23%)]\tAll Loss: 1.3432\tTriple Loss(1): 0.0258\tClassification Loss: 1.2916\r\n",
      "Train Epoch: 17 [48000/209539 (23%)]\tAll Loss: 1.7148\tTriple Loss(1): 0.0971\tClassification Loss: 1.5206\r\n",
      "Train Epoch: 17 [48640/209539 (23%)]\tAll Loss: 2.0820\tTriple Loss(1): 0.0576\tClassification Loss: 1.9668\r\n",
      "Train Epoch: 17 [49280/209539 (24%)]\tAll Loss: 2.2564\tTriple Loss(0): 0.4195\tClassification Loss: 1.4174\r\n",
      "Train Epoch: 17 [49920/209539 (24%)]\tAll Loss: 1.8055\tTriple Loss(1): 0.0563\tClassification Loss: 1.6930\r\n",
      "Train Epoch: 17 [50560/209539 (24%)]\tAll Loss: 1.8541\tTriple Loss(1): 0.1165\tClassification Loss: 1.6211\r\n",
      "Train Epoch: 17 [51200/209539 (24%)]\tAll Loss: 1.8504\tTriple Loss(1): 0.0954\tClassification Loss: 1.6596\r\n",
      "Train Epoch: 17 [51840/209539 (25%)]\tAll Loss: 1.5785\tTriple Loss(1): 0.0876\tClassification Loss: 1.4033\r\n",
      "Train Epoch: 17 [52480/209539 (25%)]\tAll Loss: 2.1017\tTriple Loss(0): 0.3988\tClassification Loss: 1.3041\r\n",
      "Train Epoch: 17 [53120/209539 (25%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0770\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 17 [53760/209539 (26%)]\tAll Loss: 2.9108\tTriple Loss(0): 0.5345\tClassification Loss: 1.8418\r\n",
      "Train Epoch: 17 [54400/209539 (26%)]\tAll Loss: 1.9862\tTriple Loss(1): 0.0398\tClassification Loss: 1.9066\r\n",
      "Train Epoch: 17 [55040/209539 (26%)]\tAll Loss: 1.3421\tTriple Loss(1): 0.0237\tClassification Loss: 1.2948\r\n",
      "Train Epoch: 17 [55680/209539 (27%)]\tAll Loss: 2.1116\tTriple Loss(1): 0.1511\tClassification Loss: 1.8094\r\n",
      "Train Epoch: 17 [56320/209539 (27%)]\tAll Loss: 1.3558\tTriple Loss(1): 0.0577\tClassification Loss: 1.2405\r\n",
      "Train Epoch: 17 [56960/209539 (27%)]\tAll Loss: 1.6404\tTriple Loss(1): 0.0258\tClassification Loss: 1.5887\r\n",
      "Train Epoch: 17 [57600/209539 (27%)]\tAll Loss: 1.9060\tTriple Loss(1): 0.2173\tClassification Loss: 1.4715\r\n",
      "Train Epoch: 17 [58240/209539 (28%)]\tAll Loss: 1.3821\tTriple Loss(1): 0.0580\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 17 [58880/209539 (28%)]\tAll Loss: 1.5084\tTriple Loss(1): 0.0580\tClassification Loss: 1.3924\r\n",
      "Train Epoch: 17 [59520/209539 (28%)]\tAll Loss: 1.5570\tTriple Loss(1): 0.0899\tClassification Loss: 1.3772\r\n",
      "Train Epoch: 17 [60160/209539 (29%)]\tAll Loss: 1.8538\tTriple Loss(1): 0.1112\tClassification Loss: 1.6314\r\n",
      "Train Epoch: 17 [60800/209539 (29%)]\tAll Loss: 1.8206\tTriple Loss(1): 0.1242\tClassification Loss: 1.5722\r\n",
      "Train Epoch: 17 [61440/209539 (29%)]\tAll Loss: 2.5493\tTriple Loss(0): 0.5602\tClassification Loss: 1.4290\r\n",
      "Train Epoch: 17 [62080/209539 (30%)]\tAll Loss: 1.8763\tTriple Loss(1): 0.0416\tClassification Loss: 1.7931\r\n",
      "Train Epoch: 17 [62720/209539 (30%)]\tAll Loss: 1.5543\tTriple Loss(1): 0.0972\tClassification Loss: 1.3599\r\n",
      "Train Epoch: 17 [63360/209539 (30%)]\tAll Loss: 1.4814\tTriple Loss(1): 0.0345\tClassification Loss: 1.4123\r\n",
      "Train Epoch: 17 [64000/209539 (31%)]\tAll Loss: 1.8249\tTriple Loss(1): 0.0665\tClassification Loss: 1.6919\r\n",
      "Train Epoch: 17 [64640/209539 (31%)]\tAll Loss: 1.9176\tTriple Loss(1): 0.0491\tClassification Loss: 1.8194\r\n",
      "Train Epoch: 17 [65280/209539 (31%)]\tAll Loss: 1.7727\tTriple Loss(1): 0.0989\tClassification Loss: 1.5750\r\n",
      "Train Epoch: 17 [65920/209539 (31%)]\tAll Loss: 2.9140\tTriple Loss(0): 0.5274\tClassification Loss: 1.8592\r\n",
      "Train Epoch: 17 [66560/209539 (32%)]\tAll Loss: 2.2891\tTriple Loss(0): 0.4342\tClassification Loss: 1.4208\r\n",
      "Train Epoch: 17 [67200/209539 (32%)]\tAll Loss: 1.6664\tTriple Loss(1): 0.0697\tClassification Loss: 1.5269\r\n",
      "Train Epoch: 17 [67840/209539 (32%)]\tAll Loss: 1.7639\tTriple Loss(1): 0.1276\tClassification Loss: 1.5086\r\n",
      "Train Epoch: 17 [68480/209539 (33%)]\tAll Loss: 1.9541\tTriple Loss(1): 0.0680\tClassification Loss: 1.8182\r\n",
      "Train Epoch: 17 [69120/209539 (33%)]\tAll Loss: 2.1477\tTriple Loss(1): 0.1658\tClassification Loss: 1.8161\r\n",
      "Train Epoch: 17 [69760/209539 (33%)]\tAll Loss: 1.6020\tTriple Loss(1): 0.0676\tClassification Loss: 1.4667\r\n",
      "Train Epoch: 17 [70400/209539 (34%)]\tAll Loss: 2.1257\tTriple Loss(0): 0.3187\tClassification Loss: 1.4882\r\n",
      "Train Epoch: 17 [71040/209539 (34%)]\tAll Loss: 1.9294\tTriple Loss(1): 0.0417\tClassification Loss: 1.8459\r\n",
      "Train Epoch: 17 [71680/209539 (34%)]\tAll Loss: 1.8595\tTriple Loss(1): 0.0211\tClassification Loss: 1.8174\r\n",
      "Train Epoch: 17 [72320/209539 (35%)]\tAll Loss: 2.6336\tTriple Loss(0): 0.5930\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 17 [72960/209539 (35%)]\tAll Loss: 1.5836\tTriple Loss(1): 0.0259\tClassification Loss: 1.5317\r\n",
      "Train Epoch: 17 [73600/209539 (35%)]\tAll Loss: 2.8574\tTriple Loss(0): 0.6073\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 17 [74240/209539 (35%)]\tAll Loss: 2.4129\tTriple Loss(0): 0.3754\tClassification Loss: 1.6621\r\n",
      "Train Epoch: 17 [74880/209539 (36%)]\tAll Loss: 2.0880\tTriple Loss(1): 0.0868\tClassification Loss: 1.9143\r\n",
      "Train Epoch: 17 [75520/209539 (36%)]\tAll Loss: 1.2653\tTriple Loss(1): 0.0130\tClassification Loss: 1.2392\r\n",
      "Train Epoch: 17 [76160/209539 (36%)]\tAll Loss: 1.6635\tTriple Loss(1): 0.1396\tClassification Loss: 1.3843\r\n",
      "Train Epoch: 17 [76800/209539 (37%)]\tAll Loss: 1.4707\tTriple Loss(1): 0.0143\tClassification Loss: 1.4422\r\n",
      "Train Epoch: 17 [77440/209539 (37%)]\tAll Loss: 1.6386\tTriple Loss(1): 0.0610\tClassification Loss: 1.5166\r\n",
      "Train Epoch: 17 [78080/209539 (37%)]\tAll Loss: 1.7402\tTriple Loss(1): 0.0970\tClassification Loss: 1.5463\r\n",
      "Train Epoch: 17 [78720/209539 (38%)]\tAll Loss: 1.8087\tTriple Loss(1): 0.0927\tClassification Loss: 1.6234\r\n",
      "Train Epoch: 17 [79360/209539 (38%)]\tAll Loss: 1.6315\tTriple Loss(1): 0.1028\tClassification Loss: 1.4259\r\n",
      "Train Epoch: 17 [80000/209539 (38%)]\tAll Loss: 1.5947\tTriple Loss(1): 0.1055\tClassification Loss: 1.3837\r\n",
      "Train Epoch: 17 [80640/209539 (38%)]\tAll Loss: 1.5931\tTriple Loss(1): 0.0605\tClassification Loss: 1.4721\r\n",
      "Train Epoch: 17 [81280/209539 (39%)]\tAll Loss: 2.1960\tTriple Loss(0): 0.3738\tClassification Loss: 1.4484\r\n",
      "Train Epoch: 17 [81920/209539 (39%)]\tAll Loss: 2.2864\tTriple Loss(0): 0.4000\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 17 [82560/209539 (39%)]\tAll Loss: 1.6803\tTriple Loss(1): 0.0063\tClassification Loss: 1.6677\r\n",
      "Train Epoch: 17 [83200/209539 (40%)]\tAll Loss: 1.8855\tTriple Loss(1): 0.1067\tClassification Loss: 1.6721\r\n",
      "Train Epoch: 17 [83840/209539 (40%)]\tAll Loss: 1.5334\tTriple Loss(1): 0.0907\tClassification Loss: 1.3520\r\n",
      "Train Epoch: 17 [84480/209539 (40%)]\tAll Loss: 1.4111\tTriple Loss(1): 0.0077\tClassification Loss: 1.3958\r\n",
      "Train Epoch: 17 [85120/209539 (41%)]\tAll Loss: 2.8112\tTriple Loss(0): 0.3686\tClassification Loss: 2.0740\r\n",
      "Train Epoch: 17 [85760/209539 (41%)]\tAll Loss: 2.4842\tTriple Loss(0): 0.5724\tClassification Loss: 1.3394\r\n",
      "Train Epoch: 17 [86400/209539 (41%)]\tAll Loss: 1.5443\tTriple Loss(1): 0.0327\tClassification Loss: 1.4789\r\n",
      "Train Epoch: 17 [87040/209539 (42%)]\tAll Loss: 1.8137\tTriple Loss(1): 0.1860\tClassification Loss: 1.4418\r\n",
      "Train Epoch: 17 [87680/209539 (42%)]\tAll Loss: 1.6907\tTriple Loss(1): 0.0399\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 17 [88320/209539 (42%)]\tAll Loss: 1.5853\tTriple Loss(1): 0.0421\tClassification Loss: 1.5011\r\n",
      "Train Epoch: 17 [88960/209539 (42%)]\tAll Loss: 1.8510\tTriple Loss(1): 0.1199\tClassification Loss: 1.6112\r\n",
      "Train Epoch: 17 [89600/209539 (43%)]\tAll Loss: 1.6641\tTriple Loss(1): 0.0306\tClassification Loss: 1.6029\r\n",
      "Train Epoch: 17 [90240/209539 (43%)]\tAll Loss: 1.6286\tTriple Loss(1): 0.0914\tClassification Loss: 1.4458\r\n",
      "Train Epoch: 17 [90880/209539 (43%)]\tAll Loss: 1.9247\tTriple Loss(1): 0.0819\tClassification Loss: 1.7609\r\n",
      "Train Epoch: 17 [91520/209539 (44%)]\tAll Loss: 1.6403\tTriple Loss(1): 0.0917\tClassification Loss: 1.4568\r\n",
      "Train Epoch: 17 [92160/209539 (44%)]\tAll Loss: 1.6686\tTriple Loss(1): 0.1117\tClassification Loss: 1.4452\r\n",
      "Train Epoch: 17 [92800/209539 (44%)]\tAll Loss: 1.4927\tTriple Loss(1): 0.0646\tClassification Loss: 1.3634\r\n",
      "Train Epoch: 17 [93440/209539 (45%)]\tAll Loss: 1.5788\tTriple Loss(1): 0.0000\tClassification Loss: 1.5788\r\n",
      "Train Epoch: 17 [94080/209539 (45%)]\tAll Loss: 1.8024\tTriple Loss(1): 0.0241\tClassification Loss: 1.7542\r\n",
      "Train Epoch: 17 [94720/209539 (45%)]\tAll Loss: 1.6353\tTriple Loss(1): 0.0497\tClassification Loss: 1.5358\r\n",
      "Train Epoch: 17 [95360/209539 (46%)]\tAll Loss: 1.7899\tTriple Loss(1): 0.0885\tClassification Loss: 1.6129\r\n",
      "Train Epoch: 17 [96000/209539 (46%)]\tAll Loss: 2.4765\tTriple Loss(0): 0.4873\tClassification Loss: 1.5019\r\n",
      "Train Epoch: 17 [96640/209539 (46%)]\tAll Loss: 1.7499\tTriple Loss(1): 0.0402\tClassification Loss: 1.6695\r\n",
      "Train Epoch: 17 [97280/209539 (46%)]\tAll Loss: 1.3438\tTriple Loss(1): 0.0298\tClassification Loss: 1.2843\r\n",
      "Train Epoch: 17 [97920/209539 (47%)]\tAll Loss: 1.7095\tTriple Loss(1): 0.0692\tClassification Loss: 1.5710\r\n",
      "Train Epoch: 17 [98560/209539 (47%)]\tAll Loss: 2.0685\tTriple Loss(1): 0.1317\tClassification Loss: 1.8051\r\n",
      "Train Epoch: 17 [99200/209539 (47%)]\tAll Loss: 1.5131\tTriple Loss(1): 0.0534\tClassification Loss: 1.4062\r\n",
      "Train Epoch: 17 [99840/209539 (48%)]\tAll Loss: 2.5760\tTriple Loss(0): 0.4657\tClassification Loss: 1.6445\r\n",
      "Train Epoch: 17 [100480/209539 (48%)]\tAll Loss: 1.5056\tTriple Loss(1): 0.0452\tClassification Loss: 1.4153\r\n",
      "Train Epoch: 17 [101120/209539 (48%)]\tAll Loss: 1.7008\tTriple Loss(1): 0.1637\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 17 [101760/209539 (49%)]\tAll Loss: 1.5539\tTriple Loss(1): 0.0863\tClassification Loss: 1.3814\r\n",
      "Train Epoch: 17 [102400/209539 (49%)]\tAll Loss: 1.8995\tTriple Loss(1): 0.1273\tClassification Loss: 1.6448\r\n",
      "Train Epoch: 17 [103040/209539 (49%)]\tAll Loss: 1.5560\tTriple Loss(1): 0.0959\tClassification Loss: 1.3641\r\n",
      "Train Epoch: 17 [103680/209539 (49%)]\tAll Loss: 1.7085\tTriple Loss(1): 0.0472\tClassification Loss: 1.6141\r\n",
      "Train Epoch: 17 [104320/209539 (50%)]\tAll Loss: 1.2337\tTriple Loss(1): 0.0104\tClassification Loss: 1.2128\r\n",
      "Train Epoch: 17 [104960/209539 (50%)]\tAll Loss: 1.7720\tTriple Loss(1): 0.1663\tClassification Loss: 1.4393\r\n",
      "Train Epoch: 17 [105600/209539 (50%)]\tAll Loss: 1.7158\tTriple Loss(1): 0.0629\tClassification Loss: 1.5899\r\n",
      "Train Epoch: 17 [106240/209539 (51%)]\tAll Loss: 2.1894\tTriple Loss(0): 0.4822\tClassification Loss: 1.2249\r\n",
      "Train Epoch: 17 [106880/209539 (51%)]\tAll Loss: 1.5445\tTriple Loss(1): 0.0389\tClassification Loss: 1.4666\r\n",
      "Train Epoch: 17 [107520/209539 (51%)]\tAll Loss: 2.3984\tTriple Loss(0): 0.5313\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 17 [108160/209539 (52%)]\tAll Loss: 2.1429\tTriple Loss(0): 0.3471\tClassification Loss: 1.4486\r\n",
      "Train Epoch: 17 [108800/209539 (52%)]\tAll Loss: 1.5241\tTriple Loss(1): 0.0855\tClassification Loss: 1.3530\r\n",
      "Train Epoch: 17 [109440/209539 (52%)]\tAll Loss: 1.8434\tTriple Loss(1): 0.0419\tClassification Loss: 1.7596\r\n",
      "Train Epoch: 17 [110080/209539 (53%)]\tAll Loss: 2.8929\tTriple Loss(0): 0.6775\tClassification Loss: 1.5380\r\n",
      "Train Epoch: 17 [110720/209539 (53%)]\tAll Loss: 1.7211\tTriple Loss(1): 0.0872\tClassification Loss: 1.5468\r\n",
      "Train Epoch: 17 [111360/209539 (53%)]\tAll Loss: 1.3000\tTriple Loss(1): 0.0627\tClassification Loss: 1.1745\r\n",
      "Train Epoch: 17 [112000/209539 (53%)]\tAll Loss: 1.3287\tTriple Loss(1): 0.0135\tClassification Loss: 1.3017\r\n",
      "Train Epoch: 17 [112640/209539 (54%)]\tAll Loss: 1.6092\tTriple Loss(1): 0.0262\tClassification Loss: 1.5567\r\n",
      "Train Epoch: 17 [113280/209539 (54%)]\tAll Loss: 1.6313\tTriple Loss(1): 0.0918\tClassification Loss: 1.4477\r\n",
      "Train Epoch: 17 [113920/209539 (54%)]\tAll Loss: 1.6339\tTriple Loss(1): 0.1061\tClassification Loss: 1.4217\r\n",
      "Train Epoch: 17 [114560/209539 (55%)]\tAll Loss: 1.6460\tTriple Loss(1): 0.0964\tClassification Loss: 1.4532\r\n",
      "Train Epoch: 17 [115200/209539 (55%)]\tAll Loss: 1.6834\tTriple Loss(1): 0.0289\tClassification Loss: 1.6256\r\n",
      "Train Epoch: 17 [115840/209539 (55%)]\tAll Loss: 2.4690\tTriple Loss(0): 0.4772\tClassification Loss: 1.5147\r\n",
      "Train Epoch: 17 [116480/209539 (56%)]\tAll Loss: 1.8667\tTriple Loss(1): 0.1276\tClassification Loss: 1.6115\r\n",
      "Train Epoch: 17 [117120/209539 (56%)]\tAll Loss: 1.4471\tTriple Loss(1): 0.0148\tClassification Loss: 1.4175\r\n",
      "Train Epoch: 17 [117760/209539 (56%)]\tAll Loss: 1.6175\tTriple Loss(1): 0.0312\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 17 [118400/209539 (57%)]\tAll Loss: 2.0138\tTriple Loss(0): 0.3323\tClassification Loss: 1.3492\r\n",
      "Train Epoch: 17 [119040/209539 (57%)]\tAll Loss: 1.8399\tTriple Loss(1): 0.0404\tClassification Loss: 1.7591\r\n",
      "Train Epoch: 17 [119680/209539 (57%)]\tAll Loss: 1.4296\tTriple Loss(1): 0.0588\tClassification Loss: 1.3121\r\n",
      "Train Epoch: 17 [120320/209539 (57%)]\tAll Loss: 2.1355\tTriple Loss(1): 0.3108\tClassification Loss: 1.5139\r\n",
      "Train Epoch: 17 [120960/209539 (58%)]\tAll Loss: 1.3492\tTriple Loss(1): 0.0709\tClassification Loss: 1.2074\r\n",
      "Train Epoch: 17 [121600/209539 (58%)]\tAll Loss: 1.8975\tTriple Loss(0): 0.3570\tClassification Loss: 1.1834\r\n",
      "Train Epoch: 17 [122240/209539 (58%)]\tAll Loss: 1.4778\tTriple Loss(1): 0.1002\tClassification Loss: 1.2774\r\n",
      "Train Epoch: 17 [122880/209539 (59%)]\tAll Loss: 1.3479\tTriple Loss(1): 0.0469\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 17 [123520/209539 (59%)]\tAll Loss: 1.6865\tTriple Loss(1): 0.0194\tClassification Loss: 1.6477\r\n",
      "Train Epoch: 17 [124160/209539 (59%)]\tAll Loss: 1.6902\tTriple Loss(1): 0.0606\tClassification Loss: 1.5690\r\n",
      "Train Epoch: 17 [124800/209539 (60%)]\tAll Loss: 1.5381\tTriple Loss(1): 0.0844\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 17 [125440/209539 (60%)]\tAll Loss: 2.0014\tTriple Loss(1): 0.1090\tClassification Loss: 1.7834\r\n",
      "Train Epoch: 17 [126080/209539 (60%)]\tAll Loss: 1.8689\tTriple Loss(1): 0.0601\tClassification Loss: 1.7487\r\n",
      "Train Epoch: 17 [126720/209539 (60%)]\tAll Loss: 2.2277\tTriple Loss(0): 0.3633\tClassification Loss: 1.5010\r\n",
      "Train Epoch: 17 [127360/209539 (61%)]\tAll Loss: 1.6770\tTriple Loss(1): 0.0899\tClassification Loss: 1.4972\r\n",
      "Train Epoch: 17 [128000/209539 (61%)]\tAll Loss: 2.6949\tTriple Loss(0): 0.4637\tClassification Loss: 1.7675\r\n",
      "Train Epoch: 17 [128640/209539 (61%)]\tAll Loss: 1.2492\tTriple Loss(1): 0.0298\tClassification Loss: 1.1897\r\n",
      "Train Epoch: 17 [129280/209539 (62%)]\tAll Loss: 2.6619\tTriple Loss(0): 0.5332\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 17 [129920/209539 (62%)]\tAll Loss: 1.5123\tTriple Loss(1): 0.0628\tClassification Loss: 1.3867\r\n",
      "Train Epoch: 17 [130560/209539 (62%)]\tAll Loss: 1.4115\tTriple Loss(1): 0.0957\tClassification Loss: 1.2201\r\n",
      "Train Epoch: 17 [131200/209539 (63%)]\tAll Loss: 1.4887\tTriple Loss(1): 0.0395\tClassification Loss: 1.4097\r\n",
      "Train Epoch: 17 [131840/209539 (63%)]\tAll Loss: 1.7044\tTriple Loss(1): 0.0810\tClassification Loss: 1.5424\r\n",
      "Train Epoch: 17 [132480/209539 (63%)]\tAll Loss: 1.5025\tTriple Loss(1): 0.1005\tClassification Loss: 1.3015\r\n",
      "Train Epoch: 17 [133120/209539 (64%)]\tAll Loss: 1.3760\tTriple Loss(1): 0.0068\tClassification Loss: 1.3624\r\n",
      "Train Epoch: 17 [133760/209539 (64%)]\tAll Loss: 2.2826\tTriple Loss(0): 0.3848\tClassification Loss: 1.5129\r\n",
      "Train Epoch: 17 [134400/209539 (64%)]\tAll Loss: 1.4319\tTriple Loss(1): 0.0380\tClassification Loss: 1.3559\r\n",
      "Train Epoch: 17 [135040/209539 (64%)]\tAll Loss: 1.6517\tTriple Loss(1): 0.0406\tClassification Loss: 1.5705\r\n",
      "Train Epoch: 17 [135680/209539 (65%)]\tAll Loss: 1.8250\tTriple Loss(1): 0.0614\tClassification Loss: 1.7021\r\n",
      "Train Epoch: 17 [136320/209539 (65%)]\tAll Loss: 1.7324\tTriple Loss(1): 0.0709\tClassification Loss: 1.5906\r\n",
      "Train Epoch: 17 [136960/209539 (65%)]\tAll Loss: 1.5312\tTriple Loss(1): 0.0615\tClassification Loss: 1.4082\r\n",
      "Train Epoch: 17 [137600/209539 (66%)]\tAll Loss: 1.3578\tTriple Loss(1): 0.0794\tClassification Loss: 1.1991\r\n",
      "Train Epoch: 17 [138240/209539 (66%)]\tAll Loss: 1.8772\tTriple Loss(1): 0.0861\tClassification Loss: 1.7049\r\n",
      "Train Epoch: 17 [138880/209539 (66%)]\tAll Loss: 2.4672\tTriple Loss(0): 0.5572\tClassification Loss: 1.3529\r\n",
      "Train Epoch: 17 [139520/209539 (67%)]\tAll Loss: 2.1334\tTriple Loss(0): 0.2641\tClassification Loss: 1.6052\r\n",
      "Train Epoch: 17 [140160/209539 (67%)]\tAll Loss: 1.5824\tTriple Loss(1): 0.0599\tClassification Loss: 1.4626\r\n",
      "Train Epoch: 17 [140800/209539 (67%)]\tAll Loss: 1.8609\tTriple Loss(1): 0.1051\tClassification Loss: 1.6507\r\n",
      "Train Epoch: 17 [141440/209539 (68%)]\tAll Loss: 1.5920\tTriple Loss(1): 0.0611\tClassification Loss: 1.4698\r\n",
      "Train Epoch: 17 [142080/209539 (68%)]\tAll Loss: 1.6049\tTriple Loss(1): 0.0386\tClassification Loss: 1.5276\r\n",
      "Train Epoch: 17 [142720/209539 (68%)]\tAll Loss: 2.8779\tTriple Loss(0): 0.5616\tClassification Loss: 1.7547\r\n",
      "Train Epoch: 17 [143360/209539 (68%)]\tAll Loss: 1.5297\tTriple Loss(1): 0.1101\tClassification Loss: 1.3096\r\n",
      "Train Epoch: 17 [144000/209539 (69%)]\tAll Loss: 1.3900\tTriple Loss(1): 0.0554\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 17 [144640/209539 (69%)]\tAll Loss: 1.9012\tTriple Loss(0): 0.2766\tClassification Loss: 1.3480\r\n",
      "Train Epoch: 17 [145280/209539 (69%)]\tAll Loss: 1.5204\tTriple Loss(1): 0.0387\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 17 [145920/209539 (70%)]\tAll Loss: 1.6013\tTriple Loss(1): 0.0787\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 17 [146560/209539 (70%)]\tAll Loss: 1.3724\tTriple Loss(1): 0.0805\tClassification Loss: 1.2114\r\n",
      "Train Epoch: 17 [147200/209539 (70%)]\tAll Loss: 2.8437\tTriple Loss(0): 0.6679\tClassification Loss: 1.5078\r\n",
      "Train Epoch: 17 [147840/209539 (71%)]\tAll Loss: 2.0606\tTriple Loss(0): 0.4034\tClassification Loss: 1.2537\r\n",
      "Train Epoch: 17 [148480/209539 (71%)]\tAll Loss: 1.4920\tTriple Loss(1): 0.1111\tClassification Loss: 1.2699\r\n",
      "Train Epoch: 17 [149120/209539 (71%)]\tAll Loss: 2.3472\tTriple Loss(0): 0.4039\tClassification Loss: 1.5394\r\n",
      "Train Epoch: 17 [149760/209539 (71%)]\tAll Loss: 2.0758\tTriple Loss(0): 0.3558\tClassification Loss: 1.3641\r\n",
      "Train Epoch: 17 [150400/209539 (72%)]\tAll Loss: 1.8400\tTriple Loss(1): 0.1062\tClassification Loss: 1.6276\r\n",
      "Train Epoch: 17 [151040/209539 (72%)]\tAll Loss: 1.6039\tTriple Loss(1): 0.0494\tClassification Loss: 1.5052\r\n",
      "Train Epoch: 17 [151680/209539 (72%)]\tAll Loss: 1.7249\tTriple Loss(1): 0.0974\tClassification Loss: 1.5302\r\n",
      "Train Epoch: 17 [152320/209539 (73%)]\tAll Loss: 2.0980\tTriple Loss(0): 0.3334\tClassification Loss: 1.4311\r\n",
      "Train Epoch: 17 [152960/209539 (73%)]\tAll Loss: 1.6108\tTriple Loss(1): 0.1199\tClassification Loss: 1.3711\r\n",
      "Train Epoch: 17 [153600/209539 (73%)]\tAll Loss: 1.5201\tTriple Loss(1): 0.0411\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 17 [154240/209539 (74%)]\tAll Loss: 1.4341\tTriple Loss(1): 0.0413\tClassification Loss: 1.3515\r\n",
      "Train Epoch: 17 [154880/209539 (74%)]\tAll Loss: 1.8162\tTriple Loss(1): 0.0371\tClassification Loss: 1.7419\r\n",
      "Train Epoch: 17 [155520/209539 (74%)]\tAll Loss: 1.6189\tTriple Loss(1): 0.0604\tClassification Loss: 1.4980\r\n",
      "Train Epoch: 17 [156160/209539 (75%)]\tAll Loss: 2.4148\tTriple Loss(0): 0.3982\tClassification Loss: 1.6183\r\n",
      "Train Epoch: 17 [156800/209539 (75%)]\tAll Loss: 2.7839\tTriple Loss(0): 0.4632\tClassification Loss: 1.8576\r\n",
      "Train Epoch: 17 [157440/209539 (75%)]\tAll Loss: 1.6993\tTriple Loss(1): 0.0397\tClassification Loss: 1.6199\r\n",
      "Train Epoch: 17 [158080/209539 (75%)]\tAll Loss: 1.7603\tTriple Loss(1): 0.1063\tClassification Loss: 1.5478\r\n",
      "Train Epoch: 17 [158720/209539 (76%)]\tAll Loss: 1.2819\tTriple Loss(1): 0.0443\tClassification Loss: 1.1932\r\n",
      "Train Epoch: 17 [159360/209539 (76%)]\tAll Loss: 1.3755\tTriple Loss(1): 0.0809\tClassification Loss: 1.2137\r\n",
      "Train Epoch: 17 [160000/209539 (76%)]\tAll Loss: 2.4464\tTriple Loss(0): 0.3438\tClassification Loss: 1.7587\r\n",
      "Train Epoch: 17 [160640/209539 (77%)]\tAll Loss: 1.6459\tTriple Loss(1): 0.0368\tClassification Loss: 1.5722\r\n",
      "Train Epoch: 17 [161280/209539 (77%)]\tAll Loss: 2.1745\tTriple Loss(0): 0.4238\tClassification Loss: 1.3269\r\n",
      "Train Epoch: 17 [161920/209539 (77%)]\tAll Loss: 1.6195\tTriple Loss(1): 0.0880\tClassification Loss: 1.4435\r\n",
      "Train Epoch: 17 [162560/209539 (78%)]\tAll Loss: 1.5283\tTriple Loss(1): 0.0385\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 17 [163200/209539 (78%)]\tAll Loss: 1.4263\tTriple Loss(1): 0.0005\tClassification Loss: 1.4253\r\n",
      "Train Epoch: 17 [163840/209539 (78%)]\tAll Loss: 1.7668\tTriple Loss(1): 0.1199\tClassification Loss: 1.5271\r\n",
      "Train Epoch: 17 [164480/209539 (78%)]\tAll Loss: 1.2547\tTriple Loss(1): 0.0239\tClassification Loss: 1.2070\r\n",
      "Train Epoch: 17 [165120/209539 (79%)]\tAll Loss: 1.3149\tTriple Loss(1): 0.0546\tClassification Loss: 1.2058\r\n",
      "Train Epoch: 17 [165760/209539 (79%)]\tAll Loss: 1.5251\tTriple Loss(1): 0.0219\tClassification Loss: 1.4813\r\n",
      "Train Epoch: 17 [166400/209539 (79%)]\tAll Loss: 1.7748\tTriple Loss(1): 0.0793\tClassification Loss: 1.6163\r\n",
      "Train Epoch: 17 [167040/209539 (80%)]\tAll Loss: 2.9671\tTriple Loss(0): 0.6367\tClassification Loss: 1.6936\r\n",
      "Train Epoch: 17 [167680/209539 (80%)]\tAll Loss: 1.6698\tTriple Loss(1): 0.0672\tClassification Loss: 1.5354\r\n",
      "Train Epoch: 17 [168320/209539 (80%)]\tAll Loss: 2.2006\tTriple Loss(0): 0.3946\tClassification Loss: 1.4115\r\n",
      "Train Epoch: 17 [168960/209539 (81%)]\tAll Loss: 1.6770\tTriple Loss(1): 0.1536\tClassification Loss: 1.3697\r\n",
      "Train Epoch: 17 [169600/209539 (81%)]\tAll Loss: 1.5742\tTriple Loss(1): 0.0591\tClassification Loss: 1.4561\r\n",
      "Train Epoch: 17 [170240/209539 (81%)]\tAll Loss: 1.7635\tTriple Loss(1): 0.1021\tClassification Loss: 1.5592\r\n",
      "Train Epoch: 17 [170880/209539 (82%)]\tAll Loss: 1.5427\tTriple Loss(1): 0.0417\tClassification Loss: 1.4592\r\n",
      "Train Epoch: 17 [171520/209539 (82%)]\tAll Loss: 1.7837\tTriple Loss(0): 0.3486\tClassification Loss: 1.0865\r\n",
      "Train Epoch: 17 [172160/209539 (82%)]\tAll Loss: 1.5971\tTriple Loss(1): 0.0004\tClassification Loss: 1.5962\r\n",
      "Train Epoch: 17 [172800/209539 (82%)]\tAll Loss: 1.4914\tTriple Loss(1): 0.0612\tClassification Loss: 1.3690\r\n",
      "Train Epoch: 17 [173440/209539 (83%)]\tAll Loss: 2.7269\tTriple Loss(0): 0.5287\tClassification Loss: 1.6695\r\n",
      "Train Epoch: 17 [174080/209539 (83%)]\tAll Loss: 1.5762\tTriple Loss(1): 0.1176\tClassification Loss: 1.3409\r\n",
      "Train Epoch: 17 [174720/209539 (83%)]\tAll Loss: 1.6684\tTriple Loss(1): 0.1064\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 17 [175360/209539 (84%)]\tAll Loss: 1.5723\tTriple Loss(1): 0.0375\tClassification Loss: 1.4973\r\n",
      "Train Epoch: 17 [176000/209539 (84%)]\tAll Loss: 1.3409\tTriple Loss(1): 0.0120\tClassification Loss: 1.3169\r\n",
      "Train Epoch: 17 [176640/209539 (84%)]\tAll Loss: 1.2038\tTriple Loss(1): 0.0193\tClassification Loss: 1.1651\r\n",
      "Train Epoch: 17 [177280/209539 (85%)]\tAll Loss: 1.9238\tTriple Loss(1): 0.1238\tClassification Loss: 1.6762\r\n",
      "Train Epoch: 17 [177920/209539 (85%)]\tAll Loss: 2.8818\tTriple Loss(0): 0.6037\tClassification Loss: 1.6743\r\n",
      "Train Epoch: 17 [178560/209539 (85%)]\tAll Loss: 1.6076\tTriple Loss(1): 0.0681\tClassification Loss: 1.4713\r\n",
      "Train Epoch: 17 [179200/209539 (86%)]\tAll Loss: 1.7480\tTriple Loss(1): 0.0619\tClassification Loss: 1.6242\r\n",
      "Train Epoch: 17 [179840/209539 (86%)]\tAll Loss: 1.5213\tTriple Loss(1): 0.0442\tClassification Loss: 1.4330\r\n",
      "Train Epoch: 17 [180480/209539 (86%)]\tAll Loss: 1.5640\tTriple Loss(1): 0.1458\tClassification Loss: 1.2723\r\n",
      "Train Epoch: 17 [181120/209539 (86%)]\tAll Loss: 2.3905\tTriple Loss(0): 0.3855\tClassification Loss: 1.6194\r\n",
      "Train Epoch: 17 [181760/209539 (87%)]\tAll Loss: 1.5197\tTriple Loss(1): 0.1179\tClassification Loss: 1.2839\r\n",
      "Train Epoch: 17 [182400/209539 (87%)]\tAll Loss: 2.3785\tTriple Loss(0): 0.3636\tClassification Loss: 1.6514\r\n",
      "Train Epoch: 17 [183040/209539 (87%)]\tAll Loss: 1.5860\tTriple Loss(1): 0.0230\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 17 [183680/209539 (88%)]\tAll Loss: 1.9914\tTriple Loss(0): 0.3990\tClassification Loss: 1.1934\r\n",
      "Train Epoch: 17 [184320/209539 (88%)]\tAll Loss: 2.2752\tTriple Loss(0): 0.4548\tClassification Loss: 1.3655\r\n",
      "Train Epoch: 17 [184960/209539 (88%)]\tAll Loss: 1.4522\tTriple Loss(1): 0.0693\tClassification Loss: 1.3136\r\n",
      "Train Epoch: 17 [185600/209539 (89%)]\tAll Loss: 2.1715\tTriple Loss(0): 0.2722\tClassification Loss: 1.6271\r\n",
      "Train Epoch: 17 [186240/209539 (89%)]\tAll Loss: 2.4138\tTriple Loss(0): 0.4369\tClassification Loss: 1.5400\r\n",
      "Train Epoch: 17 [186880/209539 (89%)]\tAll Loss: 2.4933\tTriple Loss(0): 0.4880\tClassification Loss: 1.5172\r\n",
      "Train Epoch: 17 [187520/209539 (89%)]\tAll Loss: 2.3578\tTriple Loss(0): 0.3957\tClassification Loss: 1.5665\r\n",
      "Train Epoch: 17 [188160/209539 (90%)]\tAll Loss: 1.1708\tTriple Loss(1): 0.0181\tClassification Loss: 1.1346\r\n",
      "Train Epoch: 17 [188800/209539 (90%)]\tAll Loss: 1.6063\tTriple Loss(1): 0.0612\tClassification Loss: 1.4839\r\n",
      "Train Epoch: 17 [189440/209539 (90%)]\tAll Loss: 2.3600\tTriple Loss(0): 0.4559\tClassification Loss: 1.4483\r\n",
      "Train Epoch: 17 [190080/209539 (91%)]\tAll Loss: 2.1630\tTriple Loss(0): 0.3795\tClassification Loss: 1.4039\r\n",
      "Train Epoch: 17 [190720/209539 (91%)]\tAll Loss: 1.6835\tTriple Loss(1): 0.0057\tClassification Loss: 1.6721\r\n",
      "Train Epoch: 17 [191360/209539 (91%)]\tAll Loss: 1.3178\tTriple Loss(1): 0.0090\tClassification Loss: 1.2998\r\n",
      "Train Epoch: 17 [192000/209539 (92%)]\tAll Loss: 2.0416\tTriple Loss(1): 0.1131\tClassification Loss: 1.8154\r\n",
      "Train Epoch: 17 [192640/209539 (92%)]\tAll Loss: 1.3965\tTriple Loss(1): 0.0351\tClassification Loss: 1.3264\r\n",
      "Train Epoch: 17 [193280/209539 (92%)]\tAll Loss: 1.4522\tTriple Loss(1): 0.0614\tClassification Loss: 1.3294\r\n",
      "Train Epoch: 17 [193920/209539 (93%)]\tAll Loss: 1.4233\tTriple Loss(1): 0.0026\tClassification Loss: 1.4181\r\n",
      "Train Epoch: 17 [194560/209539 (93%)]\tAll Loss: 1.6872\tTriple Loss(0): 0.2392\tClassification Loss: 1.2088\r\n",
      "Train Epoch: 17 [195200/209539 (93%)]\tAll Loss: 1.6948\tTriple Loss(1): 0.0677\tClassification Loss: 1.5593\r\n",
      "Train Epoch: 17 [195840/209539 (93%)]\tAll Loss: 1.3439\tTriple Loss(1): 0.1100\tClassification Loss: 1.1239\r\n",
      "Train Epoch: 17 [196480/209539 (94%)]\tAll Loss: 1.5802\tTriple Loss(1): 0.0343\tClassification Loss: 1.5115\r\n",
      "Train Epoch: 17 [197120/209539 (94%)]\tAll Loss: 1.7644\tTriple Loss(1): 0.0779\tClassification Loss: 1.6086\r\n",
      "Train Epoch: 17 [197760/209539 (94%)]\tAll Loss: 1.6085\tTriple Loss(1): 0.0614\tClassification Loss: 1.4857\r\n",
      "Train Epoch: 17 [198400/209539 (95%)]\tAll Loss: 1.7047\tTriple Loss(1): 0.1099\tClassification Loss: 1.4849\r\n",
      "Train Epoch: 17 [199040/209539 (95%)]\tAll Loss: 1.9576\tTriple Loss(1): 0.1201\tClassification Loss: 1.7173\r\n",
      "Train Epoch: 17 [199680/209539 (95%)]\tAll Loss: 1.3834\tTriple Loss(1): 0.0307\tClassification Loss: 1.3221\r\n",
      "Train Epoch: 17 [200320/209539 (96%)]\tAll Loss: 1.4407\tTriple Loss(1): 0.0731\tClassification Loss: 1.2945\r\n",
      "Train Epoch: 17 [200960/209539 (96%)]\tAll Loss: 1.2685\tTriple Loss(1): 0.0349\tClassification Loss: 1.1986\r\n",
      "Train Epoch: 17 [201600/209539 (96%)]\tAll Loss: 1.4330\tTriple Loss(1): 0.0869\tClassification Loss: 1.2591\r\n",
      "Train Epoch: 17 [202240/209539 (97%)]\tAll Loss: 1.6057\tTriple Loss(1): 0.1076\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 17 [202880/209539 (97%)]\tAll Loss: 1.4402\tTriple Loss(1): 0.1161\tClassification Loss: 1.2079\r\n",
      "Train Epoch: 17 [203520/209539 (97%)]\tAll Loss: 1.8385\tTriple Loss(1): 0.1824\tClassification Loss: 1.4738\r\n",
      "Train Epoch: 17 [204160/209539 (97%)]\tAll Loss: 2.0091\tTriple Loss(1): 0.0885\tClassification Loss: 1.8321\r\n",
      "Train Epoch: 17 [204800/209539 (98%)]\tAll Loss: 1.7652\tTriple Loss(1): 0.0631\tClassification Loss: 1.6389\r\n",
      "Train Epoch: 17 [205440/209539 (98%)]\tAll Loss: 1.5590\tTriple Loss(1): 0.1008\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 17 [206080/209539 (98%)]\tAll Loss: 1.6086\tTriple Loss(1): 0.1436\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 17 [206720/209539 (99%)]\tAll Loss: 1.3341\tTriple Loss(1): 0.0610\tClassification Loss: 1.2122\r\n",
      "Train Epoch: 17 [207360/209539 (99%)]\tAll Loss: 1.3281\tTriple Loss(1): 0.0105\tClassification Loss: 1.3072\r\n",
      "Train Epoch: 17 [208000/209539 (99%)]\tAll Loss: 1.4722\tTriple Loss(1): 0.1041\tClassification Loss: 1.2641\r\n",
      "Train Epoch: 17 [208640/209539 (100%)]\tAll Loss: 1.6284\tTriple Loss(1): 0.0756\tClassification Loss: 1.4772\r\n",
      "Train Epoch: 17 [209280/209539 (100%)]\tAll Loss: 1.6574\tTriple Loss(1): 0.0504\tClassification Loss: 1.5567\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/17_epochs\r\n",
      "Train Epoch: 18 [0/209539 (0%)]\tAll Loss: 2.2599\tTriple Loss(1): 0.2639\tClassification Loss: 1.7320\r\n",
      "\r\n",
      "Test set: Average loss: 1.2728\r\n",
      "Top 1 Accuracy: 50596/80128 (63%)\r\n",
      "Top 3 Accuracy: 67318/80128 (84%)\r\n",
      "Top 5 Accuracy: 72790/80128 (91%)\r\n",
      " \r\n",
      "Train Epoch: 18 [640/209539 (0%)]\tAll Loss: 3.2079\tTriple Loss(0): 0.8155\tClassification Loss: 1.5769\r\n",
      "Train Epoch: 18 [1280/209539 (1%)]\tAll Loss: 1.3578\tTriple Loss(1): 0.0635\tClassification Loss: 1.2309\r\n",
      "Train Epoch: 18 [1920/209539 (1%)]\tAll Loss: 1.4783\tTriple Loss(1): 0.0533\tClassification Loss: 1.3717\r\n",
      "Train Epoch: 18 [2560/209539 (1%)]\tAll Loss: 2.0013\tTriple Loss(1): 0.1197\tClassification Loss: 1.7619\r\n",
      "Train Epoch: 18 [3200/209539 (2%)]\tAll Loss: 1.7250\tTriple Loss(1): 0.0908\tClassification Loss: 1.5435\r\n",
      "Train Epoch: 18 [3840/209539 (2%)]\tAll Loss: 2.3697\tTriple Loss(0): 0.4905\tClassification Loss: 1.3886\r\n",
      "Train Epoch: 18 [4480/209539 (2%)]\tAll Loss: 2.3710\tTriple Loss(0): 0.4898\tClassification Loss: 1.3914\r\n",
      "Train Epoch: 18 [5120/209539 (2%)]\tAll Loss: 1.5369\tTriple Loss(1): 0.0754\tClassification Loss: 1.3861\r\n",
      "Train Epoch: 18 [5760/209539 (3%)]\tAll Loss: 1.4870\tTriple Loss(1): 0.0194\tClassification Loss: 1.4483\r\n",
      "Train Epoch: 18 [6400/209539 (3%)]\tAll Loss: 1.1575\tTriple Loss(1): 0.0733\tClassification Loss: 1.0108\r\n",
      "Train Epoch: 18 [7040/209539 (3%)]\tAll Loss: 1.5602\tTriple Loss(1): 0.0330\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 18 [7680/209539 (4%)]\tAll Loss: 1.4583\tTriple Loss(1): 0.0853\tClassification Loss: 1.2877\r\n",
      "Train Epoch: 18 [8320/209539 (4%)]\tAll Loss: 1.6216\tTriple Loss(1): 0.0972\tClassification Loss: 1.4272\r\n",
      "Train Epoch: 18 [8960/209539 (4%)]\tAll Loss: 1.5067\tTriple Loss(1): 0.0479\tClassification Loss: 1.4109\r\n",
      "Train Epoch: 18 [9600/209539 (5%)]\tAll Loss: 1.9058\tTriple Loss(1): 0.0386\tClassification Loss: 1.8287\r\n",
      "Train Epoch: 18 [10240/209539 (5%)]\tAll Loss: 2.1072\tTriple Loss(0): 0.4609\tClassification Loss: 1.1855\r\n",
      "Train Epoch: 18 [10880/209539 (5%)]\tAll Loss: 1.3269\tTriple Loss(1): 0.0578\tClassification Loss: 1.2114\r\n",
      "Train Epoch: 18 [11520/209539 (5%)]\tAll Loss: 1.5578\tTriple Loss(1): 0.0066\tClassification Loss: 1.5446\r\n",
      "Train Epoch: 18 [12160/209539 (6%)]\tAll Loss: 1.4645\tTriple Loss(1): 0.0538\tClassification Loss: 1.3569\r\n",
      "Train Epoch: 18 [12800/209539 (6%)]\tAll Loss: 1.3689\tTriple Loss(1): 0.1223\tClassification Loss: 1.1243\r\n",
      "Train Epoch: 18 [13440/209539 (6%)]\tAll Loss: 1.4809\tTriple Loss(1): 0.0328\tClassification Loss: 1.4153\r\n",
      "Train Epoch: 18 [14080/209539 (7%)]\tAll Loss: 1.7684\tTriple Loss(1): 0.1569\tClassification Loss: 1.4546\r\n",
      "Train Epoch: 18 [14720/209539 (7%)]\tAll Loss: 2.3463\tTriple Loss(0): 0.4351\tClassification Loss: 1.4761\r\n",
      "Train Epoch: 18 [15360/209539 (7%)]\tAll Loss: 1.4825\tTriple Loss(1): 0.0285\tClassification Loss: 1.4255\r\n",
      "Train Epoch: 18 [16000/209539 (8%)]\tAll Loss: 1.6321\tTriple Loss(1): 0.0276\tClassification Loss: 1.5769\r\n",
      "Train Epoch: 18 [16640/209539 (8%)]\tAll Loss: 1.7976\tTriple Loss(1): 0.0046\tClassification Loss: 1.7884\r\n",
      "Train Epoch: 18 [17280/209539 (8%)]\tAll Loss: 1.6615\tTriple Loss(1): 0.1074\tClassification Loss: 1.4467\r\n",
      "Train Epoch: 18 [17920/209539 (9%)]\tAll Loss: 1.6001\tTriple Loss(1): 0.0885\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 18 [18560/209539 (9%)]\tAll Loss: 1.6395\tTriple Loss(1): 0.0316\tClassification Loss: 1.5763\r\n",
      "Train Epoch: 18 [19200/209539 (9%)]\tAll Loss: 1.7887\tTriple Loss(1): 0.0645\tClassification Loss: 1.6598\r\n",
      "Train Epoch: 18 [19840/209539 (9%)]\tAll Loss: 1.5130\tTriple Loss(1): 0.0932\tClassification Loss: 1.3266\r\n",
      "Train Epoch: 18 [20480/209539 (10%)]\tAll Loss: 2.5339\tTriple Loss(0): 0.6570\tClassification Loss: 1.2199\r\n",
      "Train Epoch: 18 [21120/209539 (10%)]\tAll Loss: 1.7988\tTriple Loss(1): 0.0573\tClassification Loss: 1.6842\r\n",
      "Train Epoch: 18 [21760/209539 (10%)]\tAll Loss: 1.3797\tTriple Loss(1): 0.0333\tClassification Loss: 1.3132\r\n",
      "Train Epoch: 18 [22400/209539 (11%)]\tAll Loss: 1.5111\tTriple Loss(1): 0.0738\tClassification Loss: 1.3634\r\n",
      "Train Epoch: 18 [23040/209539 (11%)]\tAll Loss: 1.5517\tTriple Loss(1): 0.0346\tClassification Loss: 1.4825\r\n",
      "Train Epoch: 18 [23680/209539 (11%)]\tAll Loss: 1.2159\tTriple Loss(1): 0.0200\tClassification Loss: 1.1759\r\n",
      "Train Epoch: 18 [24320/209539 (12%)]\tAll Loss: 1.5634\tTriple Loss(1): 0.0476\tClassification Loss: 1.4683\r\n",
      "Train Epoch: 18 [24960/209539 (12%)]\tAll Loss: 1.7475\tTriple Loss(0): 0.2482\tClassification Loss: 1.2510\r\n",
      "Train Epoch: 18 [25600/209539 (12%)]\tAll Loss: 1.5489\tTriple Loss(1): 0.0404\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 18 [26240/209539 (13%)]\tAll Loss: 1.6110\tTriple Loss(1): 0.0969\tClassification Loss: 1.4172\r\n",
      "Train Epoch: 18 [26880/209539 (13%)]\tAll Loss: 1.8156\tTriple Loss(1): 0.0963\tClassification Loss: 1.6230\r\n",
      "Train Epoch: 18 [27520/209539 (13%)]\tAll Loss: 2.3654\tTriple Loss(0): 0.3783\tClassification Loss: 1.6089\r\n",
      "Train Epoch: 18 [28160/209539 (13%)]\tAll Loss: 1.6276\tTriple Loss(1): 0.0238\tClassification Loss: 1.5800\r\n",
      "Train Epoch: 18 [28800/209539 (14%)]\tAll Loss: 1.6966\tTriple Loss(1): 0.0463\tClassification Loss: 1.6039\r\n",
      "Train Epoch: 18 [29440/209539 (14%)]\tAll Loss: 1.5444\tTriple Loss(1): 0.0090\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 18 [30080/209539 (14%)]\tAll Loss: 1.5161\tTriple Loss(1): 0.1124\tClassification Loss: 1.2912\r\n",
      "Train Epoch: 18 [30720/209539 (15%)]\tAll Loss: 1.3260\tTriple Loss(1): 0.0061\tClassification Loss: 1.3138\r\n",
      "Train Epoch: 18 [31360/209539 (15%)]\tAll Loss: 2.1028\tTriple Loss(0): 0.4695\tClassification Loss: 1.1637\r\n",
      "Train Epoch: 18 [32000/209539 (15%)]\tAll Loss: 2.4646\tTriple Loss(0): 0.4974\tClassification Loss: 1.4699\r\n",
      "Train Epoch: 18 [32640/209539 (16%)]\tAll Loss: 1.3568\tTriple Loss(1): 0.0366\tClassification Loss: 1.2836\r\n",
      "Train Epoch: 18 [33280/209539 (16%)]\tAll Loss: 1.7140\tTriple Loss(1): 0.0264\tClassification Loss: 1.6612\r\n",
      "Train Epoch: 18 [33920/209539 (16%)]\tAll Loss: 1.7435\tTriple Loss(1): 0.1048\tClassification Loss: 1.5339\r\n",
      "Train Epoch: 18 [34560/209539 (16%)]\tAll Loss: 1.4881\tTriple Loss(1): 0.0457\tClassification Loss: 1.3967\r\n",
      "Train Epoch: 18 [35200/209539 (17%)]\tAll Loss: 1.4987\tTriple Loss(1): 0.1246\tClassification Loss: 1.2494\r\n",
      "Train Epoch: 18 [35840/209539 (17%)]\tAll Loss: 1.5678\tTriple Loss(1): 0.0755\tClassification Loss: 1.4168\r\n",
      "Train Epoch: 18 [36480/209539 (17%)]\tAll Loss: 1.3326\tTriple Loss(1): 0.0199\tClassification Loss: 1.2927\r\n",
      "Train Epoch: 18 [37120/209539 (18%)]\tAll Loss: 1.7173\tTriple Loss(1): 0.0259\tClassification Loss: 1.6656\r\n",
      "Train Epoch: 18 [37760/209539 (18%)]\tAll Loss: 3.1368\tTriple Loss(0): 0.8562\tClassification Loss: 1.4243\r\n",
      "Train Epoch: 18 [38400/209539 (18%)]\tAll Loss: 1.5928\tTriple Loss(1): 0.0422\tClassification Loss: 1.5084\r\n",
      "Train Epoch: 18 [39040/209539 (19%)]\tAll Loss: 1.3375\tTriple Loss(1): 0.0994\tClassification Loss: 1.1387\r\n",
      "Train Epoch: 18 [39680/209539 (19%)]\tAll Loss: 1.3380\tTriple Loss(1): 0.1032\tClassification Loss: 1.1317\r\n",
      "Train Epoch: 18 [40320/209539 (19%)]\tAll Loss: 1.9188\tTriple Loss(0): 0.2868\tClassification Loss: 1.3453\r\n",
      "Train Epoch: 18 [40960/209539 (20%)]\tAll Loss: 1.1967\tTriple Loss(1): 0.0466\tClassification Loss: 1.1035\r\n",
      "Train Epoch: 18 [41600/209539 (20%)]\tAll Loss: 2.2382\tTriple Loss(0): 0.4247\tClassification Loss: 1.3889\r\n",
      "Train Epoch: 18 [42240/209539 (20%)]\tAll Loss: 1.4558\tTriple Loss(1): 0.0291\tClassification Loss: 1.3976\r\n",
      "Train Epoch: 18 [42880/209539 (20%)]\tAll Loss: 2.1627\tTriple Loss(0): 0.5316\tClassification Loss: 1.0994\r\n",
      "Train Epoch: 18 [43520/209539 (21%)]\tAll Loss: 1.7118\tTriple Loss(1): 0.0132\tClassification Loss: 1.6853\r\n",
      "Train Epoch: 18 [44160/209539 (21%)]\tAll Loss: 1.6504\tTriple Loss(1): 0.0242\tClassification Loss: 1.6021\r\n",
      "Train Epoch: 18 [44800/209539 (21%)]\tAll Loss: 2.0945\tTriple Loss(0): 0.2634\tClassification Loss: 1.5677\r\n",
      "Train Epoch: 18 [45440/209539 (22%)]\tAll Loss: 1.9235\tTriple Loss(1): 0.0884\tClassification Loss: 1.7467\r\n",
      "Train Epoch: 18 [46080/209539 (22%)]\tAll Loss: 2.0204\tTriple Loss(0): 0.3835\tClassification Loss: 1.2534\r\n",
      "Train Epoch: 18 [46720/209539 (22%)]\tAll Loss: 1.8993\tTriple Loss(1): 0.0660\tClassification Loss: 1.7673\r\n",
      "Train Epoch: 18 [47360/209539 (23%)]\tAll Loss: 1.1429\tTriple Loss(1): 0.0228\tClassification Loss: 1.0973\r\n",
      "Train Epoch: 18 [48000/209539 (23%)]\tAll Loss: 1.5403\tTriple Loss(1): 0.0565\tClassification Loss: 1.4274\r\n",
      "Train Epoch: 18 [48640/209539 (23%)]\tAll Loss: 1.8201\tTriple Loss(1): 0.0419\tClassification Loss: 1.7363\r\n",
      "Train Epoch: 18 [49280/209539 (24%)]\tAll Loss: 1.5659\tTriple Loss(1): 0.0972\tClassification Loss: 1.3714\r\n",
      "Train Epoch: 18 [49920/209539 (24%)]\tAll Loss: 1.6711\tTriple Loss(1): 0.0440\tClassification Loss: 1.5830\r\n",
      "Train Epoch: 18 [50560/209539 (24%)]\tAll Loss: 1.8793\tTriple Loss(1): 0.0673\tClassification Loss: 1.7446\r\n",
      "Train Epoch: 18 [51200/209539 (24%)]\tAll Loss: 1.4759\tTriple Loss(1): 0.0095\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 18 [51840/209539 (25%)]\tAll Loss: 1.8050\tTriple Loss(0): 0.2602\tClassification Loss: 1.2846\r\n",
      "Train Epoch: 18 [52480/209539 (25%)]\tAll Loss: 1.4693\tTriple Loss(1): 0.1239\tClassification Loss: 1.2214\r\n",
      "Train Epoch: 18 [53120/209539 (25%)]\tAll Loss: 2.2818\tTriple Loss(0): 0.4039\tClassification Loss: 1.4740\r\n",
      "Train Epoch: 18 [53760/209539 (26%)]\tAll Loss: 2.4034\tTriple Loss(0): 0.3667\tClassification Loss: 1.6700\r\n",
      "Train Epoch: 18 [54400/209539 (26%)]\tAll Loss: 1.8161\tTriple Loss(1): 0.0641\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 18 [55040/209539 (26%)]\tAll Loss: 1.4422\tTriple Loss(1): 0.0970\tClassification Loss: 1.2481\r\n",
      "Train Epoch: 18 [55680/209539 (27%)]\tAll Loss: 1.7343\tTriple Loss(1): 0.0623\tClassification Loss: 1.6096\r\n",
      "Train Epoch: 18 [56320/209539 (27%)]\tAll Loss: 1.2945\tTriple Loss(1): 0.0115\tClassification Loss: 1.2716\r\n",
      "Train Epoch: 18 [56960/209539 (27%)]\tAll Loss: 1.3665\tTriple Loss(1): 0.0138\tClassification Loss: 1.3389\r\n",
      "Train Epoch: 18 [57600/209539 (27%)]\tAll Loss: 2.0716\tTriple Loss(0): 0.3801\tClassification Loss: 1.3115\r\n",
      "Train Epoch: 18 [58240/209539 (28%)]\tAll Loss: 1.3342\tTriple Loss(1): 0.0252\tClassification Loss: 1.2839\r\n",
      "Train Epoch: 18 [58880/209539 (28%)]\tAll Loss: 1.3089\tTriple Loss(1): 0.0860\tClassification Loss: 1.1368\r\n",
      "Train Epoch: 18 [59520/209539 (28%)]\tAll Loss: 1.3253\tTriple Loss(1): 0.0661\tClassification Loss: 1.1931\r\n",
      "Train Epoch: 18 [60160/209539 (29%)]\tAll Loss: 1.8921\tTriple Loss(1): 0.1534\tClassification Loss: 1.5854\r\n",
      "Train Epoch: 18 [60800/209539 (29%)]\tAll Loss: 2.5744\tTriple Loss(0): 0.5173\tClassification Loss: 1.5397\r\n",
      "Train Epoch: 18 [61440/209539 (29%)]\tAll Loss: 1.3738\tTriple Loss(1): 0.0288\tClassification Loss: 1.3161\r\n",
      "Train Epoch: 18 [62080/209539 (30%)]\tAll Loss: 1.6230\tTriple Loss(1): 0.0040\tClassification Loss: 1.6149\r\n",
      "Train Epoch: 18 [62720/209539 (30%)]\tAll Loss: 1.3037\tTriple Loss(1): 0.0163\tClassification Loss: 1.2711\r\n",
      "Train Epoch: 18 [63360/209539 (30%)]\tAll Loss: 1.3796\tTriple Loss(1): 0.0137\tClassification Loss: 1.3522\r\n",
      "Train Epoch: 18 [64000/209539 (31%)]\tAll Loss: 1.8738\tTriple Loss(1): 0.0379\tClassification Loss: 1.7979\r\n",
      "Train Epoch: 18 [64640/209539 (31%)]\tAll Loss: 1.7290\tTriple Loss(1): 0.0032\tClassification Loss: 1.7227\r\n",
      "Train Epoch: 18 [65280/209539 (31%)]\tAll Loss: 2.4321\tTriple Loss(0): 0.4629\tClassification Loss: 1.5064\r\n",
      "Train Epoch: 18 [65920/209539 (31%)]\tAll Loss: 2.0119\tTriple Loss(1): 0.1277\tClassification Loss: 1.7565\r\n",
      "Train Epoch: 18 [66560/209539 (32%)]\tAll Loss: 1.4524\tTriple Loss(1): 0.0497\tClassification Loss: 1.3531\r\n",
      "Train Epoch: 18 [67200/209539 (32%)]\tAll Loss: 1.4302\tTriple Loss(1): 0.0277\tClassification Loss: 1.3748\r\n",
      "Train Epoch: 18 [67840/209539 (32%)]\tAll Loss: 1.4189\tTriple Loss(1): 0.1035\tClassification Loss: 1.2119\r\n",
      "Train Epoch: 18 [68480/209539 (33%)]\tAll Loss: 1.7333\tTriple Loss(1): 0.0638\tClassification Loss: 1.6057\r\n",
      "Train Epoch: 18 [69120/209539 (33%)]\tAll Loss: 1.6959\tTriple Loss(1): 0.0281\tClassification Loss: 1.6397\r\n",
      "Train Epoch: 18 [69760/209539 (33%)]\tAll Loss: 1.2795\tTriple Loss(1): 0.0620\tClassification Loss: 1.1555\r\n",
      "Train Epoch: 18 [70400/209539 (34%)]\tAll Loss: 1.4818\tTriple Loss(1): 0.0911\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 18 [71040/209539 (34%)]\tAll Loss: 1.7624\tTriple Loss(1): 0.0860\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 18 [71680/209539 (34%)]\tAll Loss: 2.3157\tTriple Loss(0): 0.3916\tClassification Loss: 1.5326\r\n",
      "Train Epoch: 18 [72320/209539 (35%)]\tAll Loss: 1.6607\tTriple Loss(1): 0.1098\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 18 [72960/209539 (35%)]\tAll Loss: 1.3834\tTriple Loss(1): 0.0712\tClassification Loss: 1.2411\r\n",
      "Train Epoch: 18 [73600/209539 (35%)]\tAll Loss: 1.5938\tTriple Loss(1): 0.0254\tClassification Loss: 1.5429\r\n",
      "Train Epoch: 18 [74240/209539 (35%)]\tAll Loss: 2.3199\tTriple Loss(0): 0.3742\tClassification Loss: 1.5716\r\n",
      "Train Epoch: 18 [74880/209539 (36%)]\tAll Loss: 2.0813\tTriple Loss(1): 0.0905\tClassification Loss: 1.9003\r\n",
      "Train Epoch: 18 [75520/209539 (36%)]\tAll Loss: 1.1013\tTriple Loss(1): 0.0129\tClassification Loss: 1.0754\r\n",
      "Train Epoch: 18 [76160/209539 (36%)]\tAll Loss: 1.5273\tTriple Loss(1): 0.0865\tClassification Loss: 1.3542\r\n",
      "Train Epoch: 18 [76800/209539 (37%)]\tAll Loss: 1.5060\tTriple Loss(1): 0.0127\tClassification Loss: 1.4805\r\n",
      "Train Epoch: 18 [77440/209539 (37%)]\tAll Loss: 1.5326\tTriple Loss(1): 0.0412\tClassification Loss: 1.4502\r\n",
      "Train Epoch: 18 [78080/209539 (37%)]\tAll Loss: 1.7356\tTriple Loss(1): 0.1374\tClassification Loss: 1.4608\r\n",
      "Train Epoch: 18 [78720/209539 (38%)]\tAll Loss: 1.7159\tTriple Loss(1): 0.0738\tClassification Loss: 1.5683\r\n",
      "Train Epoch: 18 [79360/209539 (38%)]\tAll Loss: 1.9213\tTriple Loss(0): 0.2414\tClassification Loss: 1.4385\r\n",
      "Train Epoch: 18 [80000/209539 (38%)]\tAll Loss: 1.3948\tTriple Loss(1): 0.0332\tClassification Loss: 1.3285\r\n",
      "Train Epoch: 18 [80640/209539 (38%)]\tAll Loss: 1.8532\tTriple Loss(1): 0.1392\tClassification Loss: 1.5749\r\n",
      "Train Epoch: 18 [81280/209539 (39%)]\tAll Loss: 1.6025\tTriple Loss(1): 0.0346\tClassification Loss: 1.5334\r\n",
      "Train Epoch: 18 [81920/209539 (39%)]\tAll Loss: 1.4762\tTriple Loss(1): 0.0786\tClassification Loss: 1.3190\r\n",
      "Train Epoch: 18 [82560/209539 (39%)]\tAll Loss: 2.6657\tTriple Loss(0): 0.5518\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 18 [83200/209539 (40%)]\tAll Loss: 1.6959\tTriple Loss(1): 0.0936\tClassification Loss: 1.5087\r\n",
      "Train Epoch: 18 [83840/209539 (40%)]\tAll Loss: 2.4225\tTriple Loss(0): 0.4498\tClassification Loss: 1.5229\r\n",
      "Train Epoch: 18 [84480/209539 (40%)]\tAll Loss: 1.4254\tTriple Loss(1): 0.0588\tClassification Loss: 1.3077\r\n",
      "Train Epoch: 18 [85120/209539 (41%)]\tAll Loss: 2.1012\tTriple Loss(1): 0.0122\tClassification Loss: 2.0768\r\n",
      "Train Epoch: 18 [85760/209539 (41%)]\tAll Loss: 1.6611\tTriple Loss(1): 0.1034\tClassification Loss: 1.4543\r\n",
      "Train Epoch: 18 [86400/209539 (41%)]\tAll Loss: 2.5550\tTriple Loss(0): 0.5589\tClassification Loss: 1.4372\r\n",
      "Train Epoch: 18 [87040/209539 (42%)]\tAll Loss: 2.1071\tTriple Loss(0): 0.3754\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 18 [87680/209539 (42%)]\tAll Loss: 1.5069\tTriple Loss(1): 0.0535\tClassification Loss: 1.4000\r\n",
      "Train Epoch: 18 [88320/209539 (42%)]\tAll Loss: 1.5588\tTriple Loss(1): 0.0658\tClassification Loss: 1.4272\r\n",
      "Train Epoch: 18 [88960/209539 (42%)]\tAll Loss: 1.5387\tTriple Loss(1): 0.0205\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 18 [89600/209539 (43%)]\tAll Loss: 2.6437\tTriple Loss(0): 0.5101\tClassification Loss: 1.6236\r\n",
      "Train Epoch: 18 [90240/209539 (43%)]\tAll Loss: 1.2918\tTriple Loss(1): 0.0173\tClassification Loss: 1.2571\r\n",
      "Train Epoch: 18 [90880/209539 (43%)]\tAll Loss: 2.3892\tTriple Loss(0): 0.4154\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 18 [91520/209539 (44%)]\tAll Loss: 2.6330\tTriple Loss(0): 0.5375\tClassification Loss: 1.5579\r\n",
      "Train Epoch: 18 [92160/209539 (44%)]\tAll Loss: 1.4746\tTriple Loss(1): 0.0819\tClassification Loss: 1.3109\r\n",
      "Train Epoch: 18 [92800/209539 (44%)]\tAll Loss: 2.0157\tTriple Loss(0): 0.2778\tClassification Loss: 1.4600\r\n",
      "Train Epoch: 18 [93440/209539 (45%)]\tAll Loss: 1.4603\tTriple Loss(1): 0.0145\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 18 [94080/209539 (45%)]\tAll Loss: 1.3656\tTriple Loss(1): 0.0394\tClassification Loss: 1.2869\r\n",
      "Train Epoch: 18 [94720/209539 (45%)]\tAll Loss: 1.8890\tTriple Loss(1): 0.0741\tClassification Loss: 1.7408\r\n",
      "Train Epoch: 18 [95360/209539 (46%)]\tAll Loss: 1.6417\tTriple Loss(1): 0.0290\tClassification Loss: 1.5838\r\n",
      "Train Epoch: 18 [96000/209539 (46%)]\tAll Loss: 1.7435\tTriple Loss(1): 0.0889\tClassification Loss: 1.5657\r\n",
      "Train Epoch: 18 [96640/209539 (46%)]\tAll Loss: 1.8127\tTriple Loss(1): 0.1058\tClassification Loss: 1.6012\r\n",
      "Train Epoch: 18 [97280/209539 (46%)]\tAll Loss: 1.8577\tTriple Loss(0): 0.2546\tClassification Loss: 1.3485\r\n",
      "Train Epoch: 18 [97920/209539 (47%)]\tAll Loss: 1.5565\tTriple Loss(1): 0.0852\tClassification Loss: 1.3862\r\n",
      "Train Epoch: 18 [98560/209539 (47%)]\tAll Loss: 1.8889\tTriple Loss(1): 0.0732\tClassification Loss: 1.7425\r\n",
      "Train Epoch: 18 [99200/209539 (47%)]\tAll Loss: 1.5861\tTriple Loss(1): 0.0674\tClassification Loss: 1.4512\r\n",
      "Train Epoch: 18 [99840/209539 (48%)]\tAll Loss: 1.4793\tTriple Loss(1): 0.0012\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 18 [100480/209539 (48%)]\tAll Loss: 2.2296\tTriple Loss(0): 0.3994\tClassification Loss: 1.4308\r\n",
      "Train Epoch: 18 [101120/209539 (48%)]\tAll Loss: 1.5148\tTriple Loss(1): 0.0571\tClassification Loss: 1.4006\r\n",
      "Train Epoch: 18 [101760/209539 (49%)]\tAll Loss: 1.3843\tTriple Loss(1): 0.0460\tClassification Loss: 1.2923\r\n",
      "Train Epoch: 18 [102400/209539 (49%)]\tAll Loss: 1.3259\tTriple Loss(1): 0.0099\tClassification Loss: 1.3060\r\n",
      "Train Epoch: 18 [103040/209539 (49%)]\tAll Loss: 2.5075\tTriple Loss(0): 0.5242\tClassification Loss: 1.4590\r\n",
      "Train Epoch: 18 [103680/209539 (49%)]\tAll Loss: 1.6592\tTriple Loss(1): 0.0231\tClassification Loss: 1.6129\r\n",
      "Train Epoch: 18 [104320/209539 (50%)]\tAll Loss: 1.3369\tTriple Loss(1): 0.0419\tClassification Loss: 1.2530\r\n",
      "Train Epoch: 18 [104960/209539 (50%)]\tAll Loss: 2.1777\tTriple Loss(0): 0.3786\tClassification Loss: 1.4205\r\n",
      "Train Epoch: 18 [105600/209539 (50%)]\tAll Loss: 1.4924\tTriple Loss(1): 0.0250\tClassification Loss: 1.4425\r\n",
      "Train Epoch: 18 [106240/209539 (51%)]\tAll Loss: 1.3986\tTriple Loss(1): 0.0621\tClassification Loss: 1.2744\r\n",
      "Train Epoch: 18 [106880/209539 (51%)]\tAll Loss: 1.3832\tTriple Loss(1): 0.0165\tClassification Loss: 1.3503\r\n",
      "Train Epoch: 18 [107520/209539 (51%)]\tAll Loss: 1.4031\tTriple Loss(1): 0.0496\tClassification Loss: 1.3040\r\n",
      "Train Epoch: 18 [108160/209539 (52%)]\tAll Loss: 1.4490\tTriple Loss(1): 0.0343\tClassification Loss: 1.3805\r\n",
      "Train Epoch: 18 [108800/209539 (52%)]\tAll Loss: 1.5524\tTriple Loss(1): 0.0284\tClassification Loss: 1.4956\r\n",
      "Train Epoch: 18 [109440/209539 (52%)]\tAll Loss: 1.9128\tTriple Loss(1): 0.0749\tClassification Loss: 1.7631\r\n",
      "Train Epoch: 18 [110080/209539 (53%)]\tAll Loss: 1.6129\tTriple Loss(1): 0.0557\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 18 [110720/209539 (53%)]\tAll Loss: 1.3792\tTriple Loss(1): 0.0224\tClassification Loss: 1.3345\r\n",
      "Train Epoch: 18 [111360/209539 (53%)]\tAll Loss: 1.3886\tTriple Loss(1): 0.0713\tClassification Loss: 1.2460\r\n",
      "Train Epoch: 18 [112000/209539 (53%)]\tAll Loss: 1.5106\tTriple Loss(1): 0.0577\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 18 [112640/209539 (54%)]\tAll Loss: 1.5272\tTriple Loss(1): 0.0360\tClassification Loss: 1.4552\r\n",
      "Train Epoch: 18 [113280/209539 (54%)]\tAll Loss: 2.0675\tTriple Loss(0): 0.4015\tClassification Loss: 1.2644\r\n",
      "Train Epoch: 18 [113920/209539 (54%)]\tAll Loss: 1.6407\tTriple Loss(1): 0.0473\tClassification Loss: 1.5462\r\n",
      "Train Epoch: 18 [114560/209539 (55%)]\tAll Loss: 1.6944\tTriple Loss(1): 0.1060\tClassification Loss: 1.4823\r\n",
      "Train Epoch: 18 [115200/209539 (55%)]\tAll Loss: 2.6761\tTriple Loss(0): 0.5613\tClassification Loss: 1.5536\r\n",
      "Train Epoch: 18 [115840/209539 (55%)]\tAll Loss: 1.5606\tTriple Loss(1): 0.0364\tClassification Loss: 1.4878\r\n",
      "Train Epoch: 18 [116480/209539 (56%)]\tAll Loss: 1.4458\tTriple Loss(1): 0.0197\tClassification Loss: 1.4064\r\n",
      "Train Epoch: 18 [117120/209539 (56%)]\tAll Loss: 1.4359\tTriple Loss(1): 0.0777\tClassification Loss: 1.2805\r\n",
      "Train Epoch: 18 [117760/209539 (56%)]\tAll Loss: 1.3121\tTriple Loss(1): 0.0271\tClassification Loss: 1.2578\r\n",
      "Train Epoch: 18 [118400/209539 (57%)]\tAll Loss: 1.5047\tTriple Loss(1): 0.1778\tClassification Loss: 1.1492\r\n",
      "Train Epoch: 18 [119040/209539 (57%)]\tAll Loss: 1.6467\tTriple Loss(1): 0.0508\tClassification Loss: 1.5452\r\n",
      "Train Epoch: 18 [119680/209539 (57%)]\tAll Loss: 1.5649\tTriple Loss(1): 0.1112\tClassification Loss: 1.3426\r\n",
      "Train Epoch: 18 [120320/209539 (57%)]\tAll Loss: 1.5647\tTriple Loss(1): 0.0713\tClassification Loss: 1.4220\r\n",
      "Train Epoch: 18 [120960/209539 (58%)]\tAll Loss: 1.4260\tTriple Loss(1): 0.0520\tClassification Loss: 1.3221\r\n",
      "Train Epoch: 18 [121600/209539 (58%)]\tAll Loss: 1.3633\tTriple Loss(1): 0.0162\tClassification Loss: 1.3309\r\n",
      "Train Epoch: 18 [122240/209539 (58%)]\tAll Loss: 1.2943\tTriple Loss(1): 0.0304\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 18 [122880/209539 (59%)]\tAll Loss: 2.5110\tTriple Loss(0): 0.4638\tClassification Loss: 1.5833\r\n",
      "Train Epoch: 18 [123520/209539 (59%)]\tAll Loss: 1.6651\tTriple Loss(1): 0.0360\tClassification Loss: 1.5930\r\n",
      "Train Epoch: 18 [124160/209539 (59%)]\tAll Loss: 1.5672\tTriple Loss(1): 0.0634\tClassification Loss: 1.4403\r\n",
      "Train Epoch: 18 [124800/209539 (60%)]\tAll Loss: 1.1966\tTriple Loss(1): 0.0382\tClassification Loss: 1.1203\r\n",
      "Train Epoch: 18 [125440/209539 (60%)]\tAll Loss: 1.6575\tTriple Loss(1): 0.0625\tClassification Loss: 1.5324\r\n",
      "Train Epoch: 18 [126080/209539 (60%)]\tAll Loss: 1.6762\tTriple Loss(1): 0.0864\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 18 [126720/209539 (60%)]\tAll Loss: 1.7583\tTriple Loss(1): 0.0805\tClassification Loss: 1.5973\r\n",
      "Train Epoch: 18 [127360/209539 (61%)]\tAll Loss: 1.5161\tTriple Loss(1): 0.0359\tClassification Loss: 1.4444\r\n",
      "Train Epoch: 18 [128000/209539 (61%)]\tAll Loss: 1.6866\tTriple Loss(1): 0.0507\tClassification Loss: 1.5853\r\n",
      "Train Epoch: 18 [128640/209539 (61%)]\tAll Loss: 1.2205\tTriple Loss(1): 0.0439\tClassification Loss: 1.1326\r\n",
      "Train Epoch: 18 [129280/209539 (62%)]\tAll Loss: 1.6923\tTriple Loss(1): 0.0228\tClassification Loss: 1.6467\r\n",
      "Train Epoch: 18 [129920/209539 (62%)]\tAll Loss: 1.4039\tTriple Loss(1): 0.0597\tClassification Loss: 1.2845\r\n",
      "Train Epoch: 18 [130560/209539 (62%)]\tAll Loss: 1.2970\tTriple Loss(1): 0.0170\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 18 [131200/209539 (63%)]\tAll Loss: 1.3389\tTriple Loss(1): 0.0297\tClassification Loss: 1.2795\r\n",
      "Train Epoch: 18 [131840/209539 (63%)]\tAll Loss: 1.3996\tTriple Loss(1): 0.0473\tClassification Loss: 1.3050\r\n",
      "Train Epoch: 18 [132480/209539 (63%)]\tAll Loss: 1.3880\tTriple Loss(1): 0.0977\tClassification Loss: 1.1926\r\n",
      "Train Epoch: 18 [133120/209539 (64%)]\tAll Loss: 1.4429\tTriple Loss(1): 0.0964\tClassification Loss: 1.2501\r\n",
      "Train Epoch: 18 [133760/209539 (64%)]\tAll Loss: 1.5798\tTriple Loss(1): 0.0529\tClassification Loss: 1.4741\r\n",
      "Train Epoch: 18 [134400/209539 (64%)]\tAll Loss: 1.3541\tTriple Loss(1): 0.0818\tClassification Loss: 1.1905\r\n",
      "Train Epoch: 18 [135040/209539 (64%)]\tAll Loss: 1.7943\tTriple Loss(1): 0.0713\tClassification Loss: 1.6518\r\n",
      "Train Epoch: 18 [135680/209539 (65%)]\tAll Loss: 1.6136\tTriple Loss(1): 0.0000\tClassification Loss: 1.6136\r\n",
      "Train Epoch: 18 [136320/209539 (65%)]\tAll Loss: 1.6872\tTriple Loss(1): 0.0123\tClassification Loss: 1.6627\r\n",
      "Train Epoch: 18 [136960/209539 (65%)]\tAll Loss: 1.9264\tTriple Loss(0): 0.3413\tClassification Loss: 1.2438\r\n",
      "Train Epoch: 18 [137600/209539 (66%)]\tAll Loss: 1.4083\tTriple Loss(1): 0.0530\tClassification Loss: 1.3023\r\n",
      "Train Epoch: 18 [138240/209539 (66%)]\tAll Loss: 1.5603\tTriple Loss(1): 0.0000\tClassification Loss: 1.5603\r\n",
      "Train Epoch: 18 [138880/209539 (66%)]\tAll Loss: 1.3731\tTriple Loss(1): 0.0116\tClassification Loss: 1.3498\r\n",
      "Train Epoch: 18 [139520/209539 (67%)]\tAll Loss: 1.6201\tTriple Loss(1): 0.0170\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 18 [140160/209539 (67%)]\tAll Loss: 1.7735\tTriple Loss(1): 0.2118\tClassification Loss: 1.3498\r\n",
      "Train Epoch: 18 [140800/209539 (67%)]\tAll Loss: 1.6326\tTriple Loss(1): 0.0950\tClassification Loss: 1.4426\r\n",
      "Train Epoch: 18 [141440/209539 (68%)]\tAll Loss: 1.5939\tTriple Loss(1): 0.0004\tClassification Loss: 1.5930\r\n",
      "Train Epoch: 18 [142080/209539 (68%)]\tAll Loss: 1.5714\tTriple Loss(1): 0.0724\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 18 [142720/209539 (68%)]\tAll Loss: 1.7495\tTriple Loss(1): 0.0137\tClassification Loss: 1.7222\r\n",
      "Train Epoch: 18 [143360/209539 (68%)]\tAll Loss: 1.4743\tTriple Loss(1): 0.0953\tClassification Loss: 1.2836\r\n",
      "Train Epoch: 18 [144000/209539 (69%)]\tAll Loss: 2.3201\tTriple Loss(0): 0.4269\tClassification Loss: 1.4663\r\n",
      "Train Epoch: 18 [144640/209539 (69%)]\tAll Loss: 1.3078\tTriple Loss(1): 0.0093\tClassification Loss: 1.2891\r\n",
      "Train Epoch: 18 [145280/209539 (69%)]\tAll Loss: 1.5422\tTriple Loss(1): 0.0448\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 18 [145920/209539 (70%)]\tAll Loss: 2.0231\tTriple Loss(0): 0.3516\tClassification Loss: 1.3199\r\n",
      "Train Epoch: 18 [146560/209539 (70%)]\tAll Loss: 2.2465\tTriple Loss(0): 0.4445\tClassification Loss: 1.3575\r\n",
      "Train Epoch: 18 [147200/209539 (70%)]\tAll Loss: 1.5929\tTriple Loss(1): 0.0274\tClassification Loss: 1.5382\r\n",
      "Train Epoch: 18 [147840/209539 (71%)]\tAll Loss: 1.2201\tTriple Loss(1): 0.0000\tClassification Loss: 1.2201\r\n",
      "Train Epoch: 18 [148480/209539 (71%)]\tAll Loss: 1.9860\tTriple Loss(0): 0.3706\tClassification Loss: 1.2449\r\n",
      "Train Epoch: 18 [149120/209539 (71%)]\tAll Loss: 1.5937\tTriple Loss(1): 0.0640\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 18 [149760/209539 (71%)]\tAll Loss: 1.2103\tTriple Loss(1): 0.0344\tClassification Loss: 1.1414\r\n",
      "Train Epoch: 18 [150400/209539 (72%)]\tAll Loss: 2.4107\tTriple Loss(0): 0.5219\tClassification Loss: 1.3669\r\n",
      "Train Epoch: 18 [151040/209539 (72%)]\tAll Loss: 2.1162\tTriple Loss(0): 0.3755\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 18 [151680/209539 (72%)]\tAll Loss: 1.7242\tTriple Loss(1): 0.0831\tClassification Loss: 1.5581\r\n",
      "Train Epoch: 18 [152320/209539 (73%)]\tAll Loss: 1.7192\tTriple Loss(1): 0.1869\tClassification Loss: 1.3454\r\n",
      "Train Epoch: 18 [152960/209539 (73%)]\tAll Loss: 1.2414\tTriple Loss(1): 0.0504\tClassification Loss: 1.1406\r\n",
      "Train Epoch: 18 [153600/209539 (73%)]\tAll Loss: 1.4920\tTriple Loss(1): 0.0494\tClassification Loss: 1.3933\r\n",
      "Train Epoch: 18 [154240/209539 (74%)]\tAll Loss: 1.4464\tTriple Loss(1): 0.0496\tClassification Loss: 1.3471\r\n",
      "Train Epoch: 18 [154880/209539 (74%)]\tAll Loss: 2.4369\tTriple Loss(0): 0.4769\tClassification Loss: 1.4832\r\n",
      "Train Epoch: 18 [155520/209539 (74%)]\tAll Loss: 2.3628\tTriple Loss(0): 0.5481\tClassification Loss: 1.2665\r\n",
      "Train Epoch: 18 [156160/209539 (75%)]\tAll Loss: 1.8690\tTriple Loss(1): 0.0897\tClassification Loss: 1.6896\r\n",
      "Train Epoch: 18 [156800/209539 (75%)]\tAll Loss: 1.8714\tTriple Loss(1): 0.0375\tClassification Loss: 1.7963\r\n",
      "Train Epoch: 18 [157440/209539 (75%)]\tAll Loss: 1.6590\tTriple Loss(1): 0.0223\tClassification Loss: 1.6144\r\n",
      "Train Epoch: 18 [158080/209539 (75%)]\tAll Loss: 1.3607\tTriple Loss(1): 0.0556\tClassification Loss: 1.2496\r\n",
      "Train Epoch: 18 [158720/209539 (76%)]\tAll Loss: 1.9141\tTriple Loss(0): 0.3697\tClassification Loss: 1.1746\r\n",
      "Train Epoch: 18 [159360/209539 (76%)]\tAll Loss: 1.2877\tTriple Loss(1): 0.0539\tClassification Loss: 1.1799\r\n",
      "Train Epoch: 18 [160000/209539 (76%)]\tAll Loss: 2.4636\tTriple Loss(0): 0.4224\tClassification Loss: 1.6188\r\n",
      "Train Epoch: 18 [160640/209539 (77%)]\tAll Loss: 1.3189\tTriple Loss(1): 0.0527\tClassification Loss: 1.2136\r\n",
      "Train Epoch: 18 [161280/209539 (77%)]\tAll Loss: 1.2817\tTriple Loss(1): 0.0773\tClassification Loss: 1.1271\r\n",
      "Train Epoch: 18 [161920/209539 (77%)]\tAll Loss: 1.5720\tTriple Loss(1): 0.0270\tClassification Loss: 1.5180\r\n",
      "Train Epoch: 18 [162560/209539 (78%)]\tAll Loss: 1.4912\tTriple Loss(1): 0.0206\tClassification Loss: 1.4501\r\n",
      "Train Epoch: 18 [163200/209539 (78%)]\tAll Loss: 2.3281\tTriple Loss(0): 0.4975\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 18 [163840/209539 (78%)]\tAll Loss: 1.3033\tTriple Loss(1): 0.0056\tClassification Loss: 1.2921\r\n",
      "Train Epoch: 18 [164480/209539 (78%)]\tAll Loss: 1.2413\tTriple Loss(1): 0.0560\tClassification Loss: 1.1293\r\n",
      "Train Epoch: 18 [165120/209539 (79%)]\tAll Loss: 1.2615\tTriple Loss(1): 0.0431\tClassification Loss: 1.1754\r\n",
      "Train Epoch: 18 [165760/209539 (79%)]\tAll Loss: 2.3805\tTriple Loss(0): 0.5308\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 18 [166400/209539 (79%)]\tAll Loss: 2.2184\tTriple Loss(0): 0.3987\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 18 [167040/209539 (80%)]\tAll Loss: 1.5183\tTriple Loss(1): 0.0351\tClassification Loss: 1.4481\r\n",
      "Train Epoch: 18 [167680/209539 (80%)]\tAll Loss: 2.6879\tTriple Loss(0): 0.5777\tClassification Loss: 1.5324\r\n",
      "Train Epoch: 18 [168320/209539 (80%)]\tAll Loss: 1.5034\tTriple Loss(1): 0.0825\tClassification Loss: 1.3385\r\n",
      "Train Epoch: 18 [168960/209539 (81%)]\tAll Loss: 1.4429\tTriple Loss(1): 0.0216\tClassification Loss: 1.3997\r\n",
      "Train Epoch: 18 [169600/209539 (81%)]\tAll Loss: 2.8172\tTriple Loss(0): 0.7188\tClassification Loss: 1.3795\r\n",
      "Train Epoch: 18 [170240/209539 (81%)]\tAll Loss: 1.5934\tTriple Loss(1): 0.0595\tClassification Loss: 1.4744\r\n",
      "Train Epoch: 18 [170880/209539 (82%)]\tAll Loss: 1.4936\tTriple Loss(1): 0.0374\tClassification Loss: 1.4188\r\n",
      "Train Epoch: 18 [171520/209539 (82%)]\tAll Loss: 1.7635\tTriple Loss(0): 0.2189\tClassification Loss: 1.3257\r\n",
      "Train Epoch: 18 [172160/209539 (82%)]\tAll Loss: 1.6156\tTriple Loss(1): 0.1019\tClassification Loss: 1.4118\r\n",
      "Train Epoch: 18 [172800/209539 (82%)]\tAll Loss: 1.4407\tTriple Loss(1): 0.0788\tClassification Loss: 1.2831\r\n",
      "Train Epoch: 18 [173440/209539 (83%)]\tAll Loss: 1.7488\tTriple Loss(1): 0.0337\tClassification Loss: 1.6813\r\n",
      "Train Epoch: 18 [174080/209539 (83%)]\tAll Loss: 1.3164\tTriple Loss(1): 0.0429\tClassification Loss: 1.2305\r\n",
      "Train Epoch: 18 [174720/209539 (83%)]\tAll Loss: 1.5900\tTriple Loss(1): 0.0533\tClassification Loss: 1.4834\r\n",
      "Train Epoch: 18 [175360/209539 (84%)]\tAll Loss: 1.5449\tTriple Loss(1): 0.0485\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 18 [176000/209539 (84%)]\tAll Loss: 1.5526\tTriple Loss(1): 0.0217\tClassification Loss: 1.5092\r\n",
      "Train Epoch: 18 [176640/209539 (84%)]\tAll Loss: 1.0269\tTriple Loss(1): 0.0190\tClassification Loss: 0.9890\r\n",
      "Train Epoch: 18 [177280/209539 (85%)]\tAll Loss: 2.2185\tTriple Loss(0): 0.2961\tClassification Loss: 1.6262\r\n",
      "Train Epoch: 18 [177920/209539 (85%)]\tAll Loss: 1.7565\tTriple Loss(1): 0.0580\tClassification Loss: 1.6405\r\n",
      "Train Epoch: 18 [178560/209539 (85%)]\tAll Loss: 1.5584\tTriple Loss(1): 0.0939\tClassification Loss: 1.3706\r\n",
      "Train Epoch: 18 [179200/209539 (86%)]\tAll Loss: 1.5700\tTriple Loss(1): 0.0313\tClassification Loss: 1.5074\r\n",
      "Train Epoch: 18 [179840/209539 (86%)]\tAll Loss: 2.3023\tTriple Loss(0): 0.4297\tClassification Loss: 1.4430\r\n",
      "Train Epoch: 18 [180480/209539 (86%)]\tAll Loss: 1.4429\tTriple Loss(1): 0.0428\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 18 [181120/209539 (86%)]\tAll Loss: 1.6443\tTriple Loss(1): 0.0429\tClassification Loss: 1.5585\r\n",
      "Train Epoch: 18 [181760/209539 (87%)]\tAll Loss: 1.2374\tTriple Loss(1): 0.0107\tClassification Loss: 1.2161\r\n",
      "Train Epoch: 18 [182400/209539 (87%)]\tAll Loss: 1.7091\tTriple Loss(1): 0.0149\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 18 [183040/209539 (87%)]\tAll Loss: 1.6046\tTriple Loss(1): 0.0221\tClassification Loss: 1.5605\r\n",
      "Train Epoch: 18 [183680/209539 (88%)]\tAll Loss: 1.3147\tTriple Loss(1): 0.0963\tClassification Loss: 1.1222\r\n",
      "Train Epoch: 18 [184320/209539 (88%)]\tAll Loss: 1.2764\tTriple Loss(1): 0.0408\tClassification Loss: 1.1947\r\n",
      "Train Epoch: 18 [184960/209539 (88%)]\tAll Loss: 1.2868\tTriple Loss(1): 0.0154\tClassification Loss: 1.2559\r\n",
      "Train Epoch: 18 [185600/209539 (89%)]\tAll Loss: 1.6521\tTriple Loss(1): 0.0786\tClassification Loss: 1.4949\r\n",
      "Train Epoch: 18 [186240/209539 (89%)]\tAll Loss: 1.6827\tTriple Loss(1): 0.0803\tClassification Loss: 1.5220\r\n",
      "Train Epoch: 18 [186880/209539 (89%)]\tAll Loss: 1.4859\tTriple Loss(1): 0.0805\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 18 [187520/209539 (89%)]\tAll Loss: 1.8295\tTriple Loss(1): 0.0876\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 18 [188160/209539 (90%)]\tAll Loss: 1.2359\tTriple Loss(1): 0.0463\tClassification Loss: 1.1434\r\n",
      "Train Epoch: 18 [188800/209539 (90%)]\tAll Loss: 1.4057\tTriple Loss(1): 0.0973\tClassification Loss: 1.2111\r\n",
      "Train Epoch: 18 [189440/209539 (90%)]\tAll Loss: 1.4654\tTriple Loss(1): 0.0898\tClassification Loss: 1.2858\r\n",
      "Train Epoch: 18 [190080/209539 (91%)]\tAll Loss: 1.7482\tTriple Loss(0): 0.2583\tClassification Loss: 1.2315\r\n",
      "Train Epoch: 18 [190720/209539 (91%)]\tAll Loss: 1.5810\tTriple Loss(1): 0.0697\tClassification Loss: 1.4415\r\n",
      "Train Epoch: 18 [191360/209539 (91%)]\tAll Loss: 1.3004\tTriple Loss(1): 0.0352\tClassification Loss: 1.2300\r\n",
      "Train Epoch: 18 [192000/209539 (92%)]\tAll Loss: 1.7308\tTriple Loss(1): 0.0326\tClassification Loss: 1.6655\r\n",
      "Train Epoch: 18 [192640/209539 (92%)]\tAll Loss: 1.9742\tTriple Loss(0): 0.3457\tClassification Loss: 1.2828\r\n",
      "Train Epoch: 18 [193280/209539 (92%)]\tAll Loss: 1.5501\tTriple Loss(1): 0.0544\tClassification Loss: 1.4413\r\n",
      "Train Epoch: 18 [193920/209539 (93%)]\tAll Loss: 1.5885\tTriple Loss(1): 0.0383\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 18 [194560/209539 (93%)]\tAll Loss: 1.4761\tTriple Loss(1): 0.0357\tClassification Loss: 1.4047\r\n",
      "Train Epoch: 18 [195200/209539 (93%)]\tAll Loss: 1.4699\tTriple Loss(1): 0.0448\tClassification Loss: 1.3803\r\n",
      "Train Epoch: 18 [195840/209539 (93%)]\tAll Loss: 1.2999\tTriple Loss(1): 0.0579\tClassification Loss: 1.1841\r\n",
      "Train Epoch: 18 [196480/209539 (94%)]\tAll Loss: 1.7073\tTriple Loss(1): 0.0232\tClassification Loss: 1.6610\r\n",
      "Train Epoch: 18 [197120/209539 (94%)]\tAll Loss: 2.1406\tTriple Loss(0): 0.3427\tClassification Loss: 1.4553\r\n",
      "Train Epoch: 18 [197760/209539 (94%)]\tAll Loss: 1.5709\tTriple Loss(1): 0.0921\tClassification Loss: 1.3868\r\n",
      "Train Epoch: 18 [198400/209539 (95%)]\tAll Loss: 1.5861\tTriple Loss(1): 0.1298\tClassification Loss: 1.3265\r\n",
      "Train Epoch: 18 [199040/209539 (95%)]\tAll Loss: 1.6253\tTriple Loss(1): 0.0245\tClassification Loss: 1.5763\r\n",
      "Train Epoch: 18 [199680/209539 (95%)]\tAll Loss: 2.1371\tTriple Loss(0): 0.4246\tClassification Loss: 1.2880\r\n",
      "Train Epoch: 18 [200320/209539 (96%)]\tAll Loss: 1.6021\tTriple Loss(1): 0.0149\tClassification Loss: 1.5723\r\n",
      "Train Epoch: 18 [200960/209539 (96%)]\tAll Loss: 1.2385\tTriple Loss(1): 0.0323\tClassification Loss: 1.1739\r\n",
      "Train Epoch: 18 [201600/209539 (96%)]\tAll Loss: 1.4285\tTriple Loss(1): 0.0240\tClassification Loss: 1.3804\r\n",
      "Train Epoch: 18 [202240/209539 (97%)]\tAll Loss: 2.2534\tTriple Loss(0): 0.4365\tClassification Loss: 1.3804\r\n",
      "Train Epoch: 18 [202880/209539 (97%)]\tAll Loss: 1.0565\tTriple Loss(1): 0.0264\tClassification Loss: 1.0037\r\n",
      "Train Epoch: 18 [203520/209539 (97%)]\tAll Loss: 1.5332\tTriple Loss(1): 0.0393\tClassification Loss: 1.4545\r\n",
      "Train Epoch: 18 [204160/209539 (97%)]\tAll Loss: 1.7394\tTriple Loss(1): 0.0000\tClassification Loss: 1.7394\r\n",
      "Train Epoch: 18 [204800/209539 (98%)]\tAll Loss: 1.4656\tTriple Loss(1): 0.0321\tClassification Loss: 1.4015\r\n",
      "Train Epoch: 18 [205440/209539 (98%)]\tAll Loss: 1.3621\tTriple Loss(1): 0.1139\tClassification Loss: 1.1343\r\n",
      "Train Epoch: 18 [206080/209539 (98%)]\tAll Loss: 1.4086\tTriple Loss(1): 0.0048\tClassification Loss: 1.3990\r\n",
      "Train Epoch: 18 [206720/209539 (99%)]\tAll Loss: 1.9339\tTriple Loss(0): 0.3283\tClassification Loss: 1.2774\r\n",
      "Train Epoch: 18 [207360/209539 (99%)]\tAll Loss: 1.2663\tTriple Loss(1): 0.0123\tClassification Loss: 1.2417\r\n",
      "Train Epoch: 18 [208000/209539 (99%)]\tAll Loss: 1.2083\tTriple Loss(1): 0.0389\tClassification Loss: 1.1305\r\n",
      "Train Epoch: 18 [208640/209539 (100%)]\tAll Loss: 1.4895\tTriple Loss(1): 0.0081\tClassification Loss: 1.4732\r\n",
      "Train Epoch: 18 [209280/209539 (100%)]\tAll Loss: 1.4508\tTriple Loss(1): 0.0093\tClassification Loss: 1.4322\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/18_epochs\r\n",
      "Train Epoch: 19 [0/209539 (0%)]\tAll Loss: 1.8462\tTriple Loss(1): 0.0616\tClassification Loss: 1.7231\r\n",
      "\r\n",
      "Test set: Average loss: 1.2297\r\n",
      "Top 1 Accuracy: 51505/80128 (64%)\r\n",
      "Top 3 Accuracy: 67964/80128 (85%)\r\n",
      "Top 5 Accuracy: 73157/80128 (91%)\r\n",
      " \r\n",
      "Train Epoch: 19 [640/209539 (0%)]\tAll Loss: 1.6418\tTriple Loss(1): 0.0568\tClassification Loss: 1.5283\r\n",
      "Train Epoch: 19 [1280/209539 (1%)]\tAll Loss: 2.0223\tTriple Loss(0): 0.5009\tClassification Loss: 1.0206\r\n",
      "Train Epoch: 19 [1920/209539 (1%)]\tAll Loss: 1.5698\tTriple Loss(1): 0.0597\tClassification Loss: 1.4503\r\n",
      "Train Epoch: 19 [2560/209539 (1%)]\tAll Loss: 2.4280\tTriple Loss(0): 0.4052\tClassification Loss: 1.6175\r\n",
      "Train Epoch: 19 [3200/209539 (2%)]\tAll Loss: 1.2544\tTriple Loss(1): 0.0074\tClassification Loss: 1.2396\r\n",
      "Train Epoch: 19 [3840/209539 (2%)]\tAll Loss: 1.4608\tTriple Loss(1): 0.0221\tClassification Loss: 1.4167\r\n",
      "Train Epoch: 19 [4480/209539 (2%)]\tAll Loss: 1.5818\tTriple Loss(1): 0.0287\tClassification Loss: 1.5243\r\n",
      "Train Epoch: 19 [5120/209539 (2%)]\tAll Loss: 1.4851\tTriple Loss(1): 0.0000\tClassification Loss: 1.4851\r\n",
      "Train Epoch: 19 [5760/209539 (3%)]\tAll Loss: 1.7416\tTriple Loss(1): 0.0573\tClassification Loss: 1.6270\r\n",
      "Train Epoch: 19 [6400/209539 (3%)]\tAll Loss: 1.5639\tTriple Loss(0): 0.3042\tClassification Loss: 0.9554\r\n",
      "Train Epoch: 19 [7040/209539 (3%)]\tAll Loss: 1.4309\tTriple Loss(1): 0.0451\tClassification Loss: 1.3406\r\n",
      "Train Epoch: 19 [7680/209539 (4%)]\tAll Loss: 1.8642\tTriple Loss(0): 0.3904\tClassification Loss: 1.0835\r\n",
      "Train Epoch: 19 [8320/209539 (4%)]\tAll Loss: 1.3801\tTriple Loss(1): 0.0438\tClassification Loss: 1.2924\r\n",
      "Train Epoch: 19 [8960/209539 (4%)]\tAll Loss: 1.4629\tTriple Loss(1): 0.0495\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 19 [9600/209539 (5%)]\tAll Loss: 1.5739\tTriple Loss(1): 0.0467\tClassification Loss: 1.4804\r\n",
      "Train Epoch: 19 [10240/209539 (5%)]\tAll Loss: 1.3823\tTriple Loss(1): 0.0543\tClassification Loss: 1.2738\r\n",
      "Train Epoch: 19 [10880/209539 (5%)]\tAll Loss: 1.3170\tTriple Loss(1): 0.0177\tClassification Loss: 1.2815\r\n",
      "Train Epoch: 19 [11520/209539 (5%)]\tAll Loss: 1.4828\tTriple Loss(1): 0.0818\tClassification Loss: 1.3192\r\n",
      "Train Epoch: 19 [12160/209539 (6%)]\tAll Loss: 1.6036\tTriple Loss(0): 0.3030\tClassification Loss: 0.9977\r\n",
      "Train Epoch: 19 [12800/209539 (6%)]\tAll Loss: 1.2492\tTriple Loss(1): 0.0580\tClassification Loss: 1.1332\r\n",
      "Train Epoch: 19 [13440/209539 (6%)]\tAll Loss: 1.3170\tTriple Loss(1): 0.0403\tClassification Loss: 1.2365\r\n",
      "Train Epoch: 19 [14080/209539 (7%)]\tAll Loss: 2.0366\tTriple Loss(0): 0.4010\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 19 [14720/209539 (7%)]\tAll Loss: 1.5455\tTriple Loss(1): 0.0398\tClassification Loss: 1.4659\r\n",
      "Train Epoch: 19 [15360/209539 (7%)]\tAll Loss: 1.4410\tTriple Loss(1): 0.0646\tClassification Loss: 1.3119\r\n",
      "Train Epoch: 19 [16000/209539 (8%)]\tAll Loss: 1.8189\tTriple Loss(1): 0.0349\tClassification Loss: 1.7491\r\n",
      "Train Epoch: 19 [16640/209539 (8%)]\tAll Loss: 1.8735\tTriple Loss(1): 0.0485\tClassification Loss: 1.7765\r\n",
      "Train Epoch: 19 [17280/209539 (8%)]\tAll Loss: 1.6297\tTriple Loss(1): 0.0619\tClassification Loss: 1.5058\r\n",
      "Train Epoch: 19 [17920/209539 (9%)]\tAll Loss: 1.3688\tTriple Loss(1): 0.0123\tClassification Loss: 1.3443\r\n",
      "Train Epoch: 19 [18560/209539 (9%)]\tAll Loss: 2.2526\tTriple Loss(0): 0.4209\tClassification Loss: 1.4107\r\n",
      "Train Epoch: 19 [19200/209539 (9%)]\tAll Loss: 2.4211\tTriple Loss(0): 0.4913\tClassification Loss: 1.4385\r\n",
      "Train Epoch: 19 [19840/209539 (9%)]\tAll Loss: 2.2931\tTriple Loss(0): 0.5430\tClassification Loss: 1.2070\r\n",
      "Train Epoch: 19 [20480/209539 (10%)]\tAll Loss: 1.3627\tTriple Loss(1): 0.1210\tClassification Loss: 1.1207\r\n",
      "Train Epoch: 19 [21120/209539 (10%)]\tAll Loss: 1.7293\tTriple Loss(1): 0.0840\tClassification Loss: 1.5613\r\n",
      "Train Epoch: 19 [21760/209539 (10%)]\tAll Loss: 1.6564\tTriple Loss(0): 0.1631\tClassification Loss: 1.3303\r\n",
      "Train Epoch: 19 [22400/209539 (11%)]\tAll Loss: 1.4040\tTriple Loss(1): 0.0848\tClassification Loss: 1.2345\r\n",
      "Train Epoch: 19 [23040/209539 (11%)]\tAll Loss: 1.4140\tTriple Loss(1): 0.0253\tClassification Loss: 1.3635\r\n",
      "Train Epoch: 19 [23680/209539 (11%)]\tAll Loss: 1.1602\tTriple Loss(1): 0.0038\tClassification Loss: 1.1525\r\n",
      "Train Epoch: 19 [24320/209539 (12%)]\tAll Loss: 1.4949\tTriple Loss(1): 0.0713\tClassification Loss: 1.3522\r\n",
      "Train Epoch: 19 [24960/209539 (12%)]\tAll Loss: 1.3507\tTriple Loss(1): 0.0245\tClassification Loss: 1.3018\r\n",
      "Train Epoch: 19 [25600/209539 (12%)]\tAll Loss: 1.5799\tTriple Loss(1): 0.0429\tClassification Loss: 1.4941\r\n",
      "Train Epoch: 19 [26240/209539 (13%)]\tAll Loss: 1.3893\tTriple Loss(1): 0.0318\tClassification Loss: 1.3257\r\n",
      "Train Epoch: 19 [26880/209539 (13%)]\tAll Loss: 1.6340\tTriple Loss(1): 0.0692\tClassification Loss: 1.4957\r\n",
      "Train Epoch: 19 [27520/209539 (13%)]\tAll Loss: 1.3964\tTriple Loss(1): 0.0460\tClassification Loss: 1.3044\r\n",
      "Train Epoch: 19 [28160/209539 (13%)]\tAll Loss: 1.6354\tTriple Loss(1): 0.0289\tClassification Loss: 1.5776\r\n",
      "Train Epoch: 19 [28800/209539 (14%)]\tAll Loss: 1.7711\tTriple Loss(1): 0.0757\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 19 [29440/209539 (14%)]\tAll Loss: 1.6273\tTriple Loss(1): 0.1270\tClassification Loss: 1.3732\r\n",
      "Train Epoch: 19 [30080/209539 (14%)]\tAll Loss: 2.0542\tTriple Loss(0): 0.3701\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 19 [30720/209539 (15%)]\tAll Loss: 1.3790\tTriple Loss(1): 0.0376\tClassification Loss: 1.3037\r\n",
      "Train Epoch: 19 [31360/209539 (15%)]\tAll Loss: 2.0995\tTriple Loss(0): 0.4329\tClassification Loss: 1.2338\r\n",
      "Train Epoch: 19 [32000/209539 (15%)]\tAll Loss: 1.3452\tTriple Loss(1): 0.0523\tClassification Loss: 1.2406\r\n",
      "Train Epoch: 19 [32640/209539 (16%)]\tAll Loss: 1.4575\tTriple Loss(1): 0.0202\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 19 [33280/209539 (16%)]\tAll Loss: 1.5587\tTriple Loss(1): 0.0291\tClassification Loss: 1.5005\r\n",
      "Train Epoch: 19 [33920/209539 (16%)]\tAll Loss: 2.1084\tTriple Loss(0): 0.3163\tClassification Loss: 1.4758\r\n",
      "Train Epoch: 19 [34560/209539 (16%)]\tAll Loss: 1.3480\tTriple Loss(1): 0.0253\tClassification Loss: 1.2975\r\n",
      "Train Epoch: 19 [35200/209539 (17%)]\tAll Loss: 1.2224\tTriple Loss(1): 0.0000\tClassification Loss: 1.2224\r\n",
      "Train Epoch: 19 [35840/209539 (17%)]\tAll Loss: 1.2619\tTriple Loss(1): 0.0221\tClassification Loss: 1.2178\r\n",
      "Train Epoch: 19 [36480/209539 (17%)]\tAll Loss: 1.1845\tTriple Loss(1): 0.0205\tClassification Loss: 1.1435\r\n",
      "Train Epoch: 19 [37120/209539 (18%)]\tAll Loss: 1.6107\tTriple Loss(1): 0.0002\tClassification Loss: 1.6102\r\n",
      "Train Epoch: 19 [37760/209539 (18%)]\tAll Loss: 1.5994\tTriple Loss(1): 0.0208\tClassification Loss: 1.5577\r\n",
      "Train Epoch: 19 [38400/209539 (18%)]\tAll Loss: 1.4496\tTriple Loss(1): 0.0381\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 19 [39040/209539 (19%)]\tAll Loss: 1.2405\tTriple Loss(1): 0.0676\tClassification Loss: 1.1053\r\n",
      "Train Epoch: 19 [39680/209539 (19%)]\tAll Loss: 1.1666\tTriple Loss(1): 0.0197\tClassification Loss: 1.1273\r\n",
      "Train Epoch: 19 [40320/209539 (19%)]\tAll Loss: 1.4573\tTriple Loss(1): 0.0379\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 19 [40960/209539 (20%)]\tAll Loss: 1.1947\tTriple Loss(1): 0.0499\tClassification Loss: 1.0949\r\n",
      "Train Epoch: 19 [41600/209539 (20%)]\tAll Loss: 1.5631\tTriple Loss(1): 0.0750\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 19 [42240/209539 (20%)]\tAll Loss: 1.3390\tTriple Loss(1): 0.0381\tClassification Loss: 1.2627\r\n",
      "Train Epoch: 19 [42880/209539 (20%)]\tAll Loss: 1.0465\tTriple Loss(1): 0.0147\tClassification Loss: 1.0171\r\n",
      "Train Epoch: 19 [43520/209539 (21%)]\tAll Loss: 2.2383\tTriple Loss(0): 0.3555\tClassification Loss: 1.5273\r\n",
      "Train Epoch: 19 [44160/209539 (21%)]\tAll Loss: 1.8273\tTriple Loss(1): 0.0067\tClassification Loss: 1.8138\r\n",
      "Train Epoch: 19 [44800/209539 (21%)]\tAll Loss: 1.6620\tTriple Loss(1): 0.0290\tClassification Loss: 1.6040\r\n",
      "Train Epoch: 19 [45440/209539 (22%)]\tAll Loss: 1.8246\tTriple Loss(1): 0.0000\tClassification Loss: 1.8246\r\n",
      "Train Epoch: 19 [46080/209539 (22%)]\tAll Loss: 1.3191\tTriple Loss(1): 0.0161\tClassification Loss: 1.2869\r\n",
      "Train Epoch: 19 [46720/209539 (22%)]\tAll Loss: 1.9004\tTriple Loss(1): 0.0652\tClassification Loss: 1.7701\r\n",
      "Train Epoch: 19 [47360/209539 (23%)]\tAll Loss: 1.6579\tTriple Loss(0): 0.2853\tClassification Loss: 1.0874\r\n",
      "Train Epoch: 19 [48000/209539 (23%)]\tAll Loss: 1.4127\tTriple Loss(1): 0.0251\tClassification Loss: 1.3625\r\n",
      "Train Epoch: 19 [48640/209539 (23%)]\tAll Loss: 1.7403\tTriple Loss(1): 0.0300\tClassification Loss: 1.6803\r\n",
      "Train Epoch: 19 [49280/209539 (24%)]\tAll Loss: 2.1641\tTriple Loss(0): 0.3489\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 19 [49920/209539 (24%)]\tAll Loss: 1.5668\tTriple Loss(1): 0.0695\tClassification Loss: 1.4278\r\n",
      "Train Epoch: 19 [50560/209539 (24%)]\tAll Loss: 1.7736\tTriple Loss(1): 0.1241\tClassification Loss: 1.5254\r\n",
      "Train Epoch: 19 [51200/209539 (24%)]\tAll Loss: 1.5099\tTriple Loss(1): 0.0420\tClassification Loss: 1.4260\r\n",
      "Train Epoch: 19 [51840/209539 (25%)]\tAll Loss: 1.3963\tTriple Loss(1): 0.0344\tClassification Loss: 1.3275\r\n",
      "Train Epoch: 19 [52480/209539 (25%)]\tAll Loss: 1.6947\tTriple Loss(0): 0.2721\tClassification Loss: 1.1505\r\n",
      "Train Epoch: 19 [53120/209539 (25%)]\tAll Loss: 1.4342\tTriple Loss(1): 0.0551\tClassification Loss: 1.3239\r\n",
      "Train Epoch: 19 [53760/209539 (26%)]\tAll Loss: 1.9090\tTriple Loss(1): 0.0969\tClassification Loss: 1.7152\r\n",
      "Train Epoch: 19 [54400/209539 (26%)]\tAll Loss: 1.6791\tTriple Loss(1): 0.0392\tClassification Loss: 1.6007\r\n",
      "Train Epoch: 19 [55040/209539 (26%)]\tAll Loss: 1.0807\tTriple Loss(1): 0.0127\tClassification Loss: 1.0553\r\n",
      "Train Epoch: 19 [55680/209539 (27%)]\tAll Loss: 1.6497\tTriple Loss(1): 0.0298\tClassification Loss: 1.5901\r\n",
      "Train Epoch: 19 [56320/209539 (27%)]\tAll Loss: 1.1546\tTriple Loss(1): 0.0353\tClassification Loss: 1.0841\r\n",
      "Train Epoch: 19 [56960/209539 (27%)]\tAll Loss: 1.4302\tTriple Loss(1): 0.0055\tClassification Loss: 1.4191\r\n",
      "Train Epoch: 19 [57600/209539 (27%)]\tAll Loss: 1.3782\tTriple Loss(1): 0.0537\tClassification Loss: 1.2709\r\n",
      "Train Epoch: 19 [58240/209539 (28%)]\tAll Loss: 1.2197\tTriple Loss(1): 0.0800\tClassification Loss: 1.0597\r\n",
      "Train Epoch: 19 [58880/209539 (28%)]\tAll Loss: 1.6344\tTriple Loss(1): 0.0389\tClassification Loss: 1.5566\r\n",
      "Train Epoch: 19 [59520/209539 (28%)]\tAll Loss: 2.0396\tTriple Loss(0): 0.3628\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 19 [60160/209539 (29%)]\tAll Loss: 1.6785\tTriple Loss(1): 0.0034\tClassification Loss: 1.6717\r\n",
      "Train Epoch: 19 [60800/209539 (29%)]\tAll Loss: 1.5634\tTriple Loss(1): 0.0317\tClassification Loss: 1.5000\r\n",
      "Train Epoch: 19 [61440/209539 (29%)]\tAll Loss: 1.4921\tTriple Loss(1): 0.0309\tClassification Loss: 1.4303\r\n",
      "Train Epoch: 19 [62080/209539 (30%)]\tAll Loss: 1.5756\tTriple Loss(1): 0.0140\tClassification Loss: 1.5475\r\n",
      "Train Epoch: 19 [62720/209539 (30%)]\tAll Loss: 1.6053\tTriple Loss(1): 0.0000\tClassification Loss: 1.6053\r\n",
      "Train Epoch: 19 [63360/209539 (30%)]\tAll Loss: 1.4806\tTriple Loss(1): 0.0216\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 19 [64000/209539 (31%)]\tAll Loss: 2.6324\tTriple Loss(0): 0.4915\tClassification Loss: 1.6493\r\n",
      "Train Epoch: 19 [64640/209539 (31%)]\tAll Loss: 1.9753\tTriple Loss(1): 0.1434\tClassification Loss: 1.6886\r\n",
      "Train Epoch: 19 [65280/209539 (31%)]\tAll Loss: 1.4201\tTriple Loss(1): 0.0414\tClassification Loss: 1.3373\r\n",
      "Train Epoch: 19 [65920/209539 (31%)]\tAll Loss: 2.1010\tTriple Loss(0): 0.2172\tClassification Loss: 1.6666\r\n",
      "Train Epoch: 19 [66560/209539 (32%)]\tAll Loss: 1.4622\tTriple Loss(1): 0.0528\tClassification Loss: 1.3566\r\n",
      "Train Epoch: 19 [67200/209539 (32%)]\tAll Loss: 1.7155\tTriple Loss(1): 0.0707\tClassification Loss: 1.5741\r\n",
      "Train Epoch: 19 [67840/209539 (32%)]\tAll Loss: 1.9795\tTriple Loss(0): 0.2973\tClassification Loss: 1.3849\r\n",
      "Train Epoch: 19 [68480/209539 (33%)]\tAll Loss: 1.7079\tTriple Loss(1): 0.0601\tClassification Loss: 1.5878\r\n",
      "Train Epoch: 19 [69120/209539 (33%)]\tAll Loss: 1.7358\tTriple Loss(1): 0.0611\tClassification Loss: 1.6135\r\n",
      "Train Epoch: 19 [69760/209539 (33%)]\tAll Loss: 1.1714\tTriple Loss(1): 0.0068\tClassification Loss: 1.1578\r\n",
      "Train Epoch: 19 [70400/209539 (34%)]\tAll Loss: 1.4100\tTriple Loss(1): 0.1004\tClassification Loss: 1.2093\r\n",
      "Train Epoch: 19 [71040/209539 (34%)]\tAll Loss: 2.6711\tTriple Loss(0): 0.4957\tClassification Loss: 1.6796\r\n",
      "Train Epoch: 19 [71680/209539 (34%)]\tAll Loss: 1.7844\tTriple Loss(1): 0.0672\tClassification Loss: 1.6501\r\n",
      "Train Epoch: 19 [72320/209539 (35%)]\tAll Loss: 1.9522\tTriple Loss(0): 0.3464\tClassification Loss: 1.2595\r\n",
      "Train Epoch: 19 [72960/209539 (35%)]\tAll Loss: 1.2467\tTriple Loss(1): 0.0136\tClassification Loss: 1.2195\r\n",
      "Train Epoch: 19 [73600/209539 (35%)]\tAll Loss: 2.2849\tTriple Loss(0): 0.3312\tClassification Loss: 1.6224\r\n",
      "Train Epoch: 19 [74240/209539 (35%)]\tAll Loss: 2.8994\tTriple Loss(0): 0.5237\tClassification Loss: 1.8519\r\n",
      "Train Epoch: 19 [74880/209539 (36%)]\tAll Loss: 1.9653\tTriple Loss(1): 0.0594\tClassification Loss: 1.8464\r\n",
      "Train Epoch: 19 [75520/209539 (36%)]\tAll Loss: 1.7723\tTriple Loss(0): 0.3294\tClassification Loss: 1.1134\r\n",
      "Train Epoch: 19 [76160/209539 (36%)]\tAll Loss: 1.3384\tTriple Loss(1): 0.0172\tClassification Loss: 1.3040\r\n",
      "Train Epoch: 19 [76800/209539 (37%)]\tAll Loss: 1.3685\tTriple Loss(1): 0.0236\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 19 [77440/209539 (37%)]\tAll Loss: 1.4570\tTriple Loss(1): 0.0216\tClassification Loss: 1.4138\r\n",
      "Train Epoch: 19 [78080/209539 (37%)]\tAll Loss: 1.4789\tTriple Loss(1): 0.0467\tClassification Loss: 1.3856\r\n",
      "Train Epoch: 19 [78720/209539 (38%)]\tAll Loss: 2.1909\tTriple Loss(0): 0.3389\tClassification Loss: 1.5132\r\n",
      "Train Epoch: 19 [79360/209539 (38%)]\tAll Loss: 1.2875\tTriple Loss(1): 0.0294\tClassification Loss: 1.2287\r\n",
      "Train Epoch: 19 [80000/209539 (38%)]\tAll Loss: 1.7309\tTriple Loss(0): 0.1800\tClassification Loss: 1.3709\r\n",
      "Train Epoch: 19 [80640/209539 (38%)]\tAll Loss: 1.3897\tTriple Loss(1): 0.0019\tClassification Loss: 1.3859\r\n",
      "Train Epoch: 19 [81280/209539 (39%)]\tAll Loss: 1.6250\tTriple Loss(1): 0.0494\tClassification Loss: 1.5261\r\n",
      "Train Epoch: 19 [81920/209539 (39%)]\tAll Loss: 1.9440\tTriple Loss(0): 0.2519\tClassification Loss: 1.4402\r\n",
      "Train Epoch: 19 [82560/209539 (39%)]\tAll Loss: 1.5178\tTriple Loss(1): 0.0351\tClassification Loss: 1.4476\r\n",
      "Train Epoch: 19 [83200/209539 (40%)]\tAll Loss: 1.6131\tTriple Loss(1): 0.0420\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 19 [83840/209539 (40%)]\tAll Loss: 1.5273\tTriple Loss(1): 0.0576\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 19 [84480/209539 (40%)]\tAll Loss: 1.3935\tTriple Loss(1): 0.0744\tClassification Loss: 1.2448\r\n",
      "Train Epoch: 19 [85120/209539 (41%)]\tAll Loss: 2.0803\tTriple Loss(1): 0.0457\tClassification Loss: 1.9889\r\n",
      "Train Epoch: 19 [85760/209539 (41%)]\tAll Loss: 1.4809\tTriple Loss(1): 0.0424\tClassification Loss: 1.3961\r\n",
      "Train Epoch: 19 [86400/209539 (41%)]\tAll Loss: 1.5983\tTriple Loss(1): 0.0584\tClassification Loss: 1.4815\r\n",
      "Train Epoch: 19 [87040/209539 (42%)]\tAll Loss: 1.3092\tTriple Loss(1): 0.0672\tClassification Loss: 1.1747\r\n",
      "Train Epoch: 19 [87680/209539 (42%)]\tAll Loss: 2.1460\tTriple Loss(0): 0.3285\tClassification Loss: 1.4889\r\n",
      "Train Epoch: 19 [88320/209539 (42%)]\tAll Loss: 1.4077\tTriple Loss(1): 0.0539\tClassification Loss: 1.3000\r\n",
      "Train Epoch: 19 [88960/209539 (42%)]\tAll Loss: 1.4617\tTriple Loss(1): 0.0386\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 19 [89600/209539 (43%)]\tAll Loss: 1.6757\tTriple Loss(1): 0.0877\tClassification Loss: 1.5002\r\n",
      "Train Epoch: 19 [90240/209539 (43%)]\tAll Loss: 1.4657\tTriple Loss(1): 0.0237\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 19 [90880/209539 (43%)]\tAll Loss: 1.9366\tTriple Loss(1): 0.1070\tClassification Loss: 1.7226\r\n",
      "Train Epoch: 19 [91520/209539 (44%)]\tAll Loss: 1.5992\tTriple Loss(1): 0.0983\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 19 [92160/209539 (44%)]\tAll Loss: 1.5157\tTriple Loss(1): 0.0375\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 19 [92800/209539 (44%)]\tAll Loss: 1.3845\tTriple Loss(1): 0.0591\tClassification Loss: 1.2663\r\n",
      "Train Epoch: 19 [93440/209539 (45%)]\tAll Loss: 2.1894\tTriple Loss(0): 0.3179\tClassification Loss: 1.5536\r\n",
      "Train Epoch: 19 [94080/209539 (45%)]\tAll Loss: 1.4405\tTriple Loss(1): 0.0000\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 19 [94720/209539 (45%)]\tAll Loss: 1.6873\tTriple Loss(1): 0.1432\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 19 [95360/209539 (46%)]\tAll Loss: 1.4755\tTriple Loss(1): 0.0000\tClassification Loss: 1.4755\r\n",
      "Train Epoch: 19 [96000/209539 (46%)]\tAll Loss: 2.2658\tTriple Loss(0): 0.4510\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 19 [96640/209539 (46%)]\tAll Loss: 1.5410\tTriple Loss(1): 0.0752\tClassification Loss: 1.3906\r\n",
      "Train Epoch: 19 [97280/209539 (46%)]\tAll Loss: 2.2597\tTriple Loss(0): 0.4284\tClassification Loss: 1.4029\r\n",
      "Train Epoch: 19 [97920/209539 (47%)]\tAll Loss: 1.6739\tTriple Loss(1): 0.0643\tClassification Loss: 1.5454\r\n",
      "Train Epoch: 19 [98560/209539 (47%)]\tAll Loss: 1.7710\tTriple Loss(1): 0.0560\tClassification Loss: 1.6590\r\n",
      "Train Epoch: 19 [99200/209539 (47%)]\tAll Loss: 1.5591\tTriple Loss(1): 0.1407\tClassification Loss: 1.2777\r\n",
      "Train Epoch: 19 [99840/209539 (48%)]\tAll Loss: 1.7442\tTriple Loss(1): 0.1338\tClassification Loss: 1.4767\r\n",
      "Train Epoch: 19 [100480/209539 (48%)]\tAll Loss: 2.3164\tTriple Loss(0): 0.4133\tClassification Loss: 1.4897\r\n",
      "Train Epoch: 19 [101120/209539 (48%)]\tAll Loss: 2.2845\tTriple Loss(0): 0.5218\tClassification Loss: 1.2408\r\n",
      "Train Epoch: 19 [101760/209539 (49%)]\tAll Loss: 2.0696\tTriple Loss(0): 0.4336\tClassification Loss: 1.2023\r\n",
      "Train Epoch: 19 [102400/209539 (49%)]\tAll Loss: 1.2887\tTriple Loss(1): 0.0293\tClassification Loss: 1.2301\r\n",
      "Train Epoch: 19 [103040/209539 (49%)]\tAll Loss: 2.2223\tTriple Loss(0): 0.4975\tClassification Loss: 1.2274\r\n",
      "Train Epoch: 19 [103680/209539 (49%)]\tAll Loss: 1.9467\tTriple Loss(1): 0.1357\tClassification Loss: 1.6752\r\n",
      "Train Epoch: 19 [104320/209539 (50%)]\tAll Loss: 1.2950\tTriple Loss(1): 0.0516\tClassification Loss: 1.1919\r\n",
      "Train Epoch: 19 [104960/209539 (50%)]\tAll Loss: 2.2506\tTriple Loss(0): 0.4739\tClassification Loss: 1.3028\r\n",
      "Train Epoch: 19 [105600/209539 (50%)]\tAll Loss: 1.3435\tTriple Loss(1): 0.0366\tClassification Loss: 1.2702\r\n",
      "Train Epoch: 19 [106240/209539 (51%)]\tAll Loss: 1.4235\tTriple Loss(1): 0.0225\tClassification Loss: 1.3785\r\n",
      "Train Epoch: 19 [106880/209539 (51%)]\tAll Loss: 1.2332\tTriple Loss(1): 0.0075\tClassification Loss: 1.2182\r\n",
      "Train Epoch: 19 [107520/209539 (51%)]\tAll Loss: 1.3250\tTriple Loss(1): 0.0083\tClassification Loss: 1.3084\r\n",
      "Train Epoch: 19 [108160/209539 (52%)]\tAll Loss: 1.3727\tTriple Loss(1): 0.0000\tClassification Loss: 1.3727\r\n",
      "Train Epoch: 19 [108800/209539 (52%)]\tAll Loss: 1.4996\tTriple Loss(1): 0.0690\tClassification Loss: 1.3617\r\n",
      "Train Epoch: 19 [109440/209539 (52%)]\tAll Loss: 1.8605\tTriple Loss(1): 0.0538\tClassification Loss: 1.7529\r\n",
      "Train Epoch: 19 [110080/209539 (53%)]\tAll Loss: 1.8662\tTriple Loss(1): 0.0682\tClassification Loss: 1.7299\r\n",
      "Train Epoch: 19 [110720/209539 (53%)]\tAll Loss: 1.5567\tTriple Loss(1): 0.0729\tClassification Loss: 1.4108\r\n",
      "Train Epoch: 19 [111360/209539 (53%)]\tAll Loss: 1.2149\tTriple Loss(1): 0.0535\tClassification Loss: 1.1080\r\n",
      "Train Epoch: 19 [112000/209539 (53%)]\tAll Loss: 1.2505\tTriple Loss(1): 0.0618\tClassification Loss: 1.1269\r\n",
      "Train Epoch: 19 [112640/209539 (54%)]\tAll Loss: 1.3906\tTriple Loss(1): 0.0000\tClassification Loss: 1.3906\r\n",
      "Train Epoch: 19 [113280/209539 (54%)]\tAll Loss: 2.0950\tTriple Loss(0): 0.4042\tClassification Loss: 1.2866\r\n",
      "Train Epoch: 19 [113920/209539 (54%)]\tAll Loss: 1.5251\tTriple Loss(1): 0.0371\tClassification Loss: 1.4509\r\n",
      "Train Epoch: 19 [114560/209539 (55%)]\tAll Loss: 1.6610\tTriple Loss(1): 0.1318\tClassification Loss: 1.3975\r\n",
      "Train Epoch: 19 [115200/209539 (55%)]\tAll Loss: 1.5941\tTriple Loss(1): 0.0482\tClassification Loss: 1.4976\r\n",
      "Train Epoch: 19 [115840/209539 (55%)]\tAll Loss: 1.5790\tTriple Loss(1): 0.0328\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 19 [116480/209539 (56%)]\tAll Loss: 1.5822\tTriple Loss(1): 0.0026\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 19 [117120/209539 (56%)]\tAll Loss: 1.7248\tTriple Loss(1): 0.1936\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 19 [117760/209539 (56%)]\tAll Loss: 1.4419\tTriple Loss(1): 0.0337\tClassification Loss: 1.3745\r\n",
      "Train Epoch: 19 [118400/209539 (57%)]\tAll Loss: 2.5070\tTriple Loss(0): 0.7016\tClassification Loss: 1.1037\r\n",
      "Train Epoch: 19 [119040/209539 (57%)]\tAll Loss: 1.8264\tTriple Loss(1): 0.0783\tClassification Loss: 1.6698\r\n",
      "Train Epoch: 19 [119680/209539 (57%)]\tAll Loss: 1.4454\tTriple Loss(1): 0.0548\tClassification Loss: 1.3359\r\n",
      "Train Epoch: 19 [120320/209539 (57%)]\tAll Loss: 1.3319\tTriple Loss(1): 0.0171\tClassification Loss: 1.2977\r\n",
      "Train Epoch: 19 [120960/209539 (58%)]\tAll Loss: 1.1460\tTriple Loss(1): 0.0000\tClassification Loss: 1.1460\r\n",
      "Train Epoch: 19 [121600/209539 (58%)]\tAll Loss: 1.2141\tTriple Loss(1): 0.0196\tClassification Loss: 1.1749\r\n",
      "Train Epoch: 19 [122240/209539 (58%)]\tAll Loss: 2.1816\tTriple Loss(0): 0.4240\tClassification Loss: 1.3336\r\n",
      "Train Epoch: 19 [122880/209539 (59%)]\tAll Loss: 1.2121\tTriple Loss(1): 0.0267\tClassification Loss: 1.1587\r\n",
      "Train Epoch: 19 [123520/209539 (59%)]\tAll Loss: 2.3631\tTriple Loss(0): 0.4727\tClassification Loss: 1.4176\r\n",
      "Train Epoch: 19 [124160/209539 (59%)]\tAll Loss: 1.6942\tTriple Loss(1): 0.1047\tClassification Loss: 1.4847\r\n",
      "Train Epoch: 19 [124800/209539 (60%)]\tAll Loss: 1.2204\tTriple Loss(1): 0.0219\tClassification Loss: 1.1766\r\n",
      "Train Epoch: 19 [125440/209539 (60%)]\tAll Loss: 1.5461\tTriple Loss(1): 0.0411\tClassification Loss: 1.4638\r\n",
      "Train Epoch: 19 [126080/209539 (60%)]\tAll Loss: 1.6088\tTriple Loss(1): 0.0656\tClassification Loss: 1.4776\r\n",
      "Train Epoch: 19 [126720/209539 (60%)]\tAll Loss: 1.7492\tTriple Loss(1): 0.0226\tClassification Loss: 1.7039\r\n",
      "Train Epoch: 19 [127360/209539 (61%)]\tAll Loss: 1.6065\tTriple Loss(1): 0.0878\tClassification Loss: 1.4308\r\n",
      "Train Epoch: 19 [128000/209539 (61%)]\tAll Loss: 1.6088\tTriple Loss(1): 0.0222\tClassification Loss: 1.5645\r\n",
      "Train Epoch: 19 [128640/209539 (61%)]\tAll Loss: 1.2061\tTriple Loss(1): 0.0349\tClassification Loss: 1.1363\r\n",
      "Train Epoch: 19 [129280/209539 (62%)]\tAll Loss: 1.6411\tTriple Loss(1): 0.0632\tClassification Loss: 1.5147\r\n",
      "Train Epoch: 19 [129920/209539 (62%)]\tAll Loss: 1.3624\tTriple Loss(1): 0.0716\tClassification Loss: 1.2192\r\n",
      "Train Epoch: 19 [130560/209539 (62%)]\tAll Loss: 1.1379\tTriple Loss(1): 0.0368\tClassification Loss: 1.0644\r\n",
      "Train Epoch: 19 [131200/209539 (63%)]\tAll Loss: 1.3368\tTriple Loss(1): 0.0383\tClassification Loss: 1.2601\r\n",
      "Train Epoch: 19 [131840/209539 (63%)]\tAll Loss: 1.5173\tTriple Loss(1): 0.0785\tClassification Loss: 1.3603\r\n",
      "Train Epoch: 19 [132480/209539 (63%)]\tAll Loss: 1.9947\tTriple Loss(0): 0.4255\tClassification Loss: 1.1438\r\n",
      "Train Epoch: 19 [133120/209539 (64%)]\tAll Loss: 1.9587\tTriple Loss(0): 0.3521\tClassification Loss: 1.2545\r\n",
      "Train Epoch: 19 [133760/209539 (64%)]\tAll Loss: 2.4803\tTriple Loss(0): 0.5750\tClassification Loss: 1.3303\r\n",
      "Train Epoch: 19 [134400/209539 (64%)]\tAll Loss: 1.4614\tTriple Loss(1): 0.0141\tClassification Loss: 1.4332\r\n",
      "Train Epoch: 19 [135040/209539 (64%)]\tAll Loss: 1.4062\tTriple Loss(1): 0.0125\tClassification Loss: 1.3812\r\n",
      "Train Epoch: 19 [135680/209539 (65%)]\tAll Loss: 1.7015\tTriple Loss(1): 0.0421\tClassification Loss: 1.6173\r\n",
      "Train Epoch: 19 [136320/209539 (65%)]\tAll Loss: 1.6215\tTriple Loss(1): 0.0272\tClassification Loss: 1.5672\r\n",
      "Train Epoch: 19 [136960/209539 (65%)]\tAll Loss: 1.4533\tTriple Loss(1): 0.0001\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 19 [137600/209539 (66%)]\tAll Loss: 1.4683\tTriple Loss(1): 0.0000\tClassification Loss: 1.4683\r\n",
      "Train Epoch: 19 [138240/209539 (66%)]\tAll Loss: 2.9326\tTriple Loss(0): 0.6147\tClassification Loss: 1.7032\r\n",
      "Train Epoch: 19 [138880/209539 (66%)]\tAll Loss: 1.6144\tTriple Loss(1): 0.0386\tClassification Loss: 1.5371\r\n",
      "Train Epoch: 19 [139520/209539 (67%)]\tAll Loss: 1.6187\tTriple Loss(1): 0.1865\tClassification Loss: 1.2457\r\n",
      "Train Epoch: 19 [140160/209539 (67%)]\tAll Loss: 1.2069\tTriple Loss(1): 0.0171\tClassification Loss: 1.1726\r\n",
      "Train Epoch: 19 [140800/209539 (67%)]\tAll Loss: 1.8078\tTriple Loss(1): 0.0836\tClassification Loss: 1.6407\r\n",
      "Train Epoch: 19 [141440/209539 (68%)]\tAll Loss: 1.5819\tTriple Loss(1): 0.0618\tClassification Loss: 1.4583\r\n",
      "Train Epoch: 19 [142080/209539 (68%)]\tAll Loss: 1.4600\tTriple Loss(1): 0.1120\tClassification Loss: 1.2360\r\n",
      "Train Epoch: 19 [142720/209539 (68%)]\tAll Loss: 1.7741\tTriple Loss(1): 0.0000\tClassification Loss: 1.7741\r\n",
      "Train Epoch: 19 [143360/209539 (68%)]\tAll Loss: 1.8277\tTriple Loss(0): 0.3156\tClassification Loss: 1.1964\r\n",
      "Train Epoch: 19 [144000/209539 (69%)]\tAll Loss: 1.4557\tTriple Loss(1): 0.0141\tClassification Loss: 1.4275\r\n",
      "Train Epoch: 19 [144640/209539 (69%)]\tAll Loss: 1.3073\tTriple Loss(1): 0.0247\tClassification Loss: 1.2578\r\n",
      "Train Epoch: 19 [145280/209539 (69%)]\tAll Loss: 1.4747\tTriple Loss(1): 0.0937\tClassification Loss: 1.2873\r\n",
      "Train Epoch: 19 [145920/209539 (70%)]\tAll Loss: 1.4975\tTriple Loss(1): 0.0626\tClassification Loss: 1.3724\r\n",
      "Train Epoch: 19 [146560/209539 (70%)]\tAll Loss: 1.1440\tTriple Loss(1): 0.0222\tClassification Loss: 1.0996\r\n",
      "Train Epoch: 19 [147200/209539 (70%)]\tAll Loss: 1.8307\tTriple Loss(1): 0.1252\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 19 [147840/209539 (71%)]\tAll Loss: 1.7034\tTriple Loss(0): 0.2949\tClassification Loss: 1.1136\r\n",
      "Train Epoch: 19 [148480/209539 (71%)]\tAll Loss: 1.4149\tTriple Loss(0): 0.1523\tClassification Loss: 1.1103\r\n",
      "Train Epoch: 19 [149120/209539 (71%)]\tAll Loss: 1.4892\tTriple Loss(1): 0.0057\tClassification Loss: 1.4779\r\n",
      "Train Epoch: 19 [149760/209539 (71%)]\tAll Loss: 1.7234\tTriple Loss(0): 0.3236\tClassification Loss: 1.0763\r\n",
      "Train Epoch: 19 [150400/209539 (72%)]\tAll Loss: 1.6356\tTriple Loss(1): 0.1056\tClassification Loss: 1.4245\r\n",
      "Train Epoch: 19 [151040/209539 (72%)]\tAll Loss: 1.3539\tTriple Loss(1): 0.0038\tClassification Loss: 1.3463\r\n",
      "Train Epoch: 19 [151680/209539 (72%)]\tAll Loss: 2.4687\tTriple Loss(0): 0.4417\tClassification Loss: 1.5852\r\n",
      "Train Epoch: 19 [152320/209539 (73%)]\tAll Loss: 1.2025\tTriple Loss(1): 0.0538\tClassification Loss: 1.0950\r\n",
      "Train Epoch: 19 [152960/209539 (73%)]\tAll Loss: 1.5882\tTriple Loss(1): 0.0937\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 19 [153600/209539 (73%)]\tAll Loss: 2.4727\tTriple Loss(0): 0.4724\tClassification Loss: 1.5278\r\n",
      "Train Epoch: 19 [154240/209539 (74%)]\tAll Loss: 1.5200\tTriple Loss(1): 0.0598\tClassification Loss: 1.4004\r\n",
      "Train Epoch: 19 [154880/209539 (74%)]\tAll Loss: 1.4330\tTriple Loss(1): 0.0087\tClassification Loss: 1.4157\r\n",
      "Train Epoch: 19 [155520/209539 (74%)]\tAll Loss: 1.6058\tTriple Loss(1): 0.1004\tClassification Loss: 1.4049\r\n",
      "Train Epoch: 19 [156160/209539 (75%)]\tAll Loss: 1.7599\tTriple Loss(1): 0.0252\tClassification Loss: 1.7094\r\n",
      "Train Epoch: 19 [156800/209539 (75%)]\tAll Loss: 2.9737\tTriple Loss(0): 0.5634\tClassification Loss: 1.8468\r\n",
      "Train Epoch: 19 [157440/209539 (75%)]\tAll Loss: 1.6968\tTriple Loss(1): 0.0688\tClassification Loss: 1.5593\r\n",
      "Train Epoch: 19 [158080/209539 (75%)]\tAll Loss: 1.4612\tTriple Loss(1): 0.1023\tClassification Loss: 1.2565\r\n",
      "Train Epoch: 19 [158720/209539 (76%)]\tAll Loss: 1.1884\tTriple Loss(1): 0.0264\tClassification Loss: 1.1356\r\n",
      "Train Epoch: 19 [159360/209539 (76%)]\tAll Loss: 1.3225\tTriple Loss(1): 0.0578\tClassification Loss: 1.2069\r\n",
      "Train Epoch: 19 [160000/209539 (76%)]\tAll Loss: 1.7421\tTriple Loss(1): 0.0604\tClassification Loss: 1.6213\r\n",
      "Train Epoch: 19 [160640/209539 (77%)]\tAll Loss: 1.3368\tTriple Loss(1): 0.0415\tClassification Loss: 1.2539\r\n",
      "Train Epoch: 19 [161280/209539 (77%)]\tAll Loss: 1.3819\tTriple Loss(1): 0.0670\tClassification Loss: 1.2479\r\n",
      "Train Epoch: 19 [161920/209539 (77%)]\tAll Loss: 1.4398\tTriple Loss(1): 0.0176\tClassification Loss: 1.4046\r\n",
      "Train Epoch: 19 [162560/209539 (78%)]\tAll Loss: 1.4586\tTriple Loss(1): 0.0648\tClassification Loss: 1.3290\r\n",
      "Train Epoch: 19 [163200/209539 (78%)]\tAll Loss: 1.4880\tTriple Loss(1): 0.0923\tClassification Loss: 1.3035\r\n",
      "Train Epoch: 19 [163840/209539 (78%)]\tAll Loss: 1.2823\tTriple Loss(1): 0.0175\tClassification Loss: 1.2474\r\n",
      "Train Epoch: 19 [164480/209539 (78%)]\tAll Loss: 1.2453\tTriple Loss(1): 0.0191\tClassification Loss: 1.2070\r\n",
      "Train Epoch: 19 [165120/209539 (79%)]\tAll Loss: 1.3729\tTriple Loss(1): 0.0830\tClassification Loss: 1.2068\r\n",
      "Train Epoch: 19 [165760/209539 (79%)]\tAll Loss: 1.4861\tTriple Loss(1): 0.0162\tClassification Loss: 1.4537\r\n",
      "Train Epoch: 19 [166400/209539 (79%)]\tAll Loss: 1.5949\tTriple Loss(1): 0.0199\tClassification Loss: 1.5551\r\n",
      "Train Epoch: 19 [167040/209539 (80%)]\tAll Loss: 1.6595\tTriple Loss(1): 0.0504\tClassification Loss: 1.5586\r\n",
      "Train Epoch: 19 [167680/209539 (80%)]\tAll Loss: 2.0528\tTriple Loss(0): 0.3124\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 19 [168320/209539 (80%)]\tAll Loss: 2.2797\tTriple Loss(0): 0.4697\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 19 [168960/209539 (81%)]\tAll Loss: 1.4140\tTriple Loss(1): 0.0546\tClassification Loss: 1.3048\r\n",
      "Train Epoch: 19 [169600/209539 (81%)]\tAll Loss: 1.3560\tTriple Loss(1): 0.0588\tClassification Loss: 1.2384\r\n",
      "Train Epoch: 19 [170240/209539 (81%)]\tAll Loss: 1.3722\tTriple Loss(1): 0.0000\tClassification Loss: 1.3722\r\n",
      "Train Epoch: 19 [170880/209539 (82%)]\tAll Loss: 1.4906\tTriple Loss(1): 0.0350\tClassification Loss: 1.4205\r\n",
      "Train Epoch: 19 [171520/209539 (82%)]\tAll Loss: 1.3804\tTriple Loss(1): 0.1024\tClassification Loss: 1.1756\r\n",
      "Train Epoch: 19 [172160/209539 (82%)]\tAll Loss: 1.4856\tTriple Loss(1): 0.0046\tClassification Loss: 1.4763\r\n",
      "Train Epoch: 19 [172800/209539 (82%)]\tAll Loss: 1.6341\tTriple Loss(1): 0.1341\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 19 [173440/209539 (83%)]\tAll Loss: 1.6000\tTriple Loss(1): 0.0510\tClassification Loss: 1.4980\r\n",
      "Train Epoch: 19 [174080/209539 (83%)]\tAll Loss: 1.3842\tTriple Loss(1): 0.0586\tClassification Loss: 1.2669\r\n",
      "Train Epoch: 19 [174720/209539 (83%)]\tAll Loss: 1.4825\tTriple Loss(1): 0.0225\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 19 [175360/209539 (84%)]\tAll Loss: 1.4023\tTriple Loss(1): 0.0066\tClassification Loss: 1.3890\r\n",
      "Train Epoch: 19 [176000/209539 (84%)]\tAll Loss: 1.3532\tTriple Loss(1): 0.0406\tClassification Loss: 1.2721\r\n",
      "Train Epoch: 19 [176640/209539 (84%)]\tAll Loss: 1.0523\tTriple Loss(1): 0.0724\tClassification Loss: 0.9074\r\n",
      "Train Epoch: 19 [177280/209539 (85%)]\tAll Loss: 1.5176\tTriple Loss(1): 0.0083\tClassification Loss: 1.5010\r\n",
      "Train Epoch: 19 [177920/209539 (85%)]\tAll Loss: 1.7008\tTriple Loss(1): 0.0953\tClassification Loss: 1.5102\r\n",
      "Train Epoch: 19 [178560/209539 (85%)]\tAll Loss: 1.2964\tTriple Loss(1): 0.0838\tClassification Loss: 1.1288\r\n",
      "Train Epoch: 19 [179200/209539 (86%)]\tAll Loss: 1.6850\tTriple Loss(1): 0.0555\tClassification Loss: 1.5740\r\n",
      "Train Epoch: 19 [179840/209539 (86%)]\tAll Loss: 2.2148\tTriple Loss(0): 0.4488\tClassification Loss: 1.3172\r\n",
      "Train Epoch: 19 [180480/209539 (86%)]\tAll Loss: 1.5191\tTriple Loss(1): 0.0538\tClassification Loss: 1.4115\r\n",
      "Train Epoch: 19 [181120/209539 (86%)]\tAll Loss: 1.6389\tTriple Loss(1): 0.0819\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 19 [181760/209539 (87%)]\tAll Loss: 1.2620\tTriple Loss(1): 0.0103\tClassification Loss: 1.2414\r\n",
      "Train Epoch: 19 [182400/209539 (87%)]\tAll Loss: 2.4119\tTriple Loss(0): 0.4502\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 19 [183040/209539 (87%)]\tAll Loss: 2.3284\tTriple Loss(0): 0.3964\tClassification Loss: 1.5355\r\n",
      "Train Epoch: 19 [183680/209539 (88%)]\tAll Loss: 1.4076\tTriple Loss(1): 0.1190\tClassification Loss: 1.1695\r\n",
      "Train Epoch: 19 [184320/209539 (88%)]\tAll Loss: 1.4384\tTriple Loss(1): 0.0476\tClassification Loss: 1.3431\r\n",
      "Train Epoch: 19 [184960/209539 (88%)]\tAll Loss: 1.3707\tTriple Loss(1): 0.1086\tClassification Loss: 1.1534\r\n",
      "Train Epoch: 19 [185600/209539 (89%)]\tAll Loss: 1.5306\tTriple Loss(1): 0.0304\tClassification Loss: 1.4698\r\n",
      "Train Epoch: 19 [186240/209539 (89%)]\tAll Loss: 1.7037\tTriple Loss(1): 0.0210\tClassification Loss: 1.6617\r\n",
      "Train Epoch: 19 [186880/209539 (89%)]\tAll Loss: 1.5591\tTriple Loss(1): 0.0173\tClassification Loss: 1.5244\r\n",
      "Train Epoch: 19 [187520/209539 (89%)]\tAll Loss: 1.5721\tTriple Loss(1): 0.0543\tClassification Loss: 1.4635\r\n",
      "Train Epoch: 19 [188160/209539 (90%)]\tAll Loss: 1.2227\tTriple Loss(1): 0.0722\tClassification Loss: 1.0784\r\n",
      "Train Epoch: 19 [188800/209539 (90%)]\tAll Loss: 1.2040\tTriple Loss(1): 0.0088\tClassification Loss: 1.1865\r\n",
      "Train Epoch: 19 [189440/209539 (90%)]\tAll Loss: 1.3753\tTriple Loss(1): 0.0174\tClassification Loss: 1.3404\r\n",
      "Train Epoch: 19 [190080/209539 (91%)]\tAll Loss: 1.4557\tTriple Loss(1): 0.0804\tClassification Loss: 1.2949\r\n",
      "Train Epoch: 19 [190720/209539 (91%)]\tAll Loss: 2.5108\tTriple Loss(0): 0.5529\tClassification Loss: 1.4050\r\n",
      "Train Epoch: 19 [191360/209539 (91%)]\tAll Loss: 1.6926\tTriple Loss(0): 0.2927\tClassification Loss: 1.1071\r\n",
      "Train Epoch: 19 [192000/209539 (92%)]\tAll Loss: 1.8763\tTriple Loss(1): 0.0656\tClassification Loss: 1.7451\r\n",
      "Train Epoch: 19 [192640/209539 (92%)]\tAll Loss: 1.3602\tTriple Loss(1): 0.0140\tClassification Loss: 1.3322\r\n",
      "Train Epoch: 19 [193280/209539 (92%)]\tAll Loss: 1.4987\tTriple Loss(1): 0.0113\tClassification Loss: 1.4762\r\n",
      "Train Epoch: 19 [193920/209539 (93%)]\tAll Loss: 1.4268\tTriple Loss(1): 0.0119\tClassification Loss: 1.4030\r\n",
      "Train Epoch: 19 [194560/209539 (93%)]\tAll Loss: 1.3574\tTriple Loss(1): 0.0348\tClassification Loss: 1.2879\r\n",
      "Train Epoch: 19 [195200/209539 (93%)]\tAll Loss: 1.4504\tTriple Loss(1): 0.0530\tClassification Loss: 1.3444\r\n",
      "Train Epoch: 19 [195840/209539 (93%)]\tAll Loss: 1.0614\tTriple Loss(1): 0.0107\tClassification Loss: 1.0400\r\n",
      "Train Epoch: 19 [196480/209539 (94%)]\tAll Loss: 1.6979\tTriple Loss(1): 0.0681\tClassification Loss: 1.5617\r\n",
      "Train Epoch: 19 [197120/209539 (94%)]\tAll Loss: 1.5935\tTriple Loss(1): 0.0464\tClassification Loss: 1.5006\r\n",
      "Train Epoch: 19 [197760/209539 (94%)]\tAll Loss: 1.6262\tTriple Loss(1): 0.0941\tClassification Loss: 1.4381\r\n",
      "Train Epoch: 19 [198400/209539 (95%)]\tAll Loss: 1.4295\tTriple Loss(1): 0.0259\tClassification Loss: 1.3777\r\n",
      "Train Epoch: 19 [199040/209539 (95%)]\tAll Loss: 1.5355\tTriple Loss(1): 0.0326\tClassification Loss: 1.4702\r\n",
      "Train Epoch: 19 [199680/209539 (95%)]\tAll Loss: 1.3778\tTriple Loss(1): 0.0374\tClassification Loss: 1.3029\r\n",
      "Train Epoch: 19 [200320/209539 (96%)]\tAll Loss: 1.4316\tTriple Loss(1): 0.0114\tClassification Loss: 1.4089\r\n",
      "Train Epoch: 19 [200960/209539 (96%)]\tAll Loss: 1.2360\tTriple Loss(1): 0.0245\tClassification Loss: 1.1869\r\n",
      "Train Epoch: 19 [201600/209539 (96%)]\tAll Loss: 1.3596\tTriple Loss(1): 0.0568\tClassification Loss: 1.2461\r\n",
      "Train Epoch: 19 [202240/209539 (97%)]\tAll Loss: 1.3318\tTriple Loss(1): 0.0027\tClassification Loss: 1.3264\r\n",
      "Train Epoch: 19 [202880/209539 (97%)]\tAll Loss: 1.6977\tTriple Loss(0): 0.3428\tClassification Loss: 1.0121\r\n",
      "Train Epoch: 19 [203520/209539 (97%)]\tAll Loss: 2.1760\tTriple Loss(0): 0.2916\tClassification Loss: 1.5928\r\n",
      "Train Epoch: 19 [204160/209539 (97%)]\tAll Loss: 1.7234\tTriple Loss(1): 0.0296\tClassification Loss: 1.6642\r\n",
      "Train Epoch: 19 [204800/209539 (98%)]\tAll Loss: 1.8730\tTriple Loss(1): 0.0947\tClassification Loss: 1.6836\r\n",
      "Train Epoch: 19 [205440/209539 (98%)]\tAll Loss: 1.2813\tTriple Loss(1): 0.0621\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 19 [206080/209539 (98%)]\tAll Loss: 1.5691\tTriple Loss(1): 0.0981\tClassification Loss: 1.3729\r\n",
      "Train Epoch: 19 [206720/209539 (99%)]\tAll Loss: 1.3136\tTriple Loss(1): 0.0534\tClassification Loss: 1.2069\r\n",
      "Train Epoch: 19 [207360/209539 (99%)]\tAll Loss: 1.4605\tTriple Loss(1): 0.0817\tClassification Loss: 1.2970\r\n",
      "Train Epoch: 19 [208000/209539 (99%)]\tAll Loss: 1.9224\tTriple Loss(0): 0.3025\tClassification Loss: 1.3175\r\n",
      "Train Epoch: 19 [208640/209539 (100%)]\tAll Loss: 1.6528\tTriple Loss(1): 0.1637\tClassification Loss: 1.3255\r\n",
      "Train Epoch: 19 [209280/209539 (100%)]\tAll Loss: 1.4090\tTriple Loss(1): 0.0157\tClassification Loss: 1.3775\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/19_epochs\r\n",
      "Train Epoch: 20 [0/209539 (0%)]\tAll Loss: 1.8055\tTriple Loss(1): 0.0488\tClassification Loss: 1.7079\r\n",
      "\r\n",
      "Test set: Average loss: 1.1912\r\n",
      "Top 1 Accuracy: 52464/80128 (65%)\r\n",
      "Top 3 Accuracy: 68461/80128 (85%)\r\n",
      "Top 5 Accuracy: 73617/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 20 [640/209539 (0%)]\tAll Loss: 1.5726\tTriple Loss(1): 0.0566\tClassification Loss: 1.4595\r\n",
      "Train Epoch: 20 [1280/209539 (1%)]\tAll Loss: 1.3290\tTriple Loss(1): 0.0525\tClassification Loss: 1.2240\r\n",
      "Train Epoch: 20 [1920/209539 (1%)]\tAll Loss: 1.3015\tTriple Loss(1): 0.0008\tClassification Loss: 1.3000\r\n",
      "Train Epoch: 20 [2560/209539 (1%)]\tAll Loss: 2.2920\tTriple Loss(0): 0.3256\tClassification Loss: 1.6408\r\n",
      "Train Epoch: 20 [3200/209539 (2%)]\tAll Loss: 1.4790\tTriple Loss(1): 0.0454\tClassification Loss: 1.3882\r\n",
      "Train Epoch: 20 [3840/209539 (2%)]\tAll Loss: 1.9946\tTriple Loss(0): 0.4112\tClassification Loss: 1.1722\r\n",
      "Train Epoch: 20 [4480/209539 (2%)]\tAll Loss: 1.5275\tTriple Loss(1): 0.0065\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 20 [5120/209539 (2%)]\tAll Loss: 1.5468\tTriple Loss(1): 0.0474\tClassification Loss: 1.4521\r\n",
      "Train Epoch: 20 [5760/209539 (3%)]\tAll Loss: 1.4764\tTriple Loss(1): 0.0151\tClassification Loss: 1.4462\r\n",
      "Train Epoch: 20 [6400/209539 (3%)]\tAll Loss: 1.2468\tTriple Loss(1): 0.0941\tClassification Loss: 1.0586\r\n",
      "Train Epoch: 20 [7040/209539 (3%)]\tAll Loss: 1.2602\tTriple Loss(1): 0.0455\tClassification Loss: 1.1692\r\n",
      "Train Epoch: 20 [7680/209539 (4%)]\tAll Loss: 1.4061\tTriple Loss(1): 0.0363\tClassification Loss: 1.3334\r\n",
      "Train Epoch: 20 [8320/209539 (4%)]\tAll Loss: 1.4526\tTriple Loss(1): 0.0806\tClassification Loss: 1.2913\r\n",
      "Train Epoch: 20 [8960/209539 (4%)]\tAll Loss: 1.2969\tTriple Loss(1): 0.0602\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 20 [9600/209539 (5%)]\tAll Loss: 2.1553\tTriple Loss(0): 0.3139\tClassification Loss: 1.5274\r\n",
      "Train Epoch: 20 [10240/209539 (5%)]\tAll Loss: 1.1071\tTriple Loss(1): 0.0000\tClassification Loss: 1.1071\r\n",
      "Train Epoch: 20 [10880/209539 (5%)]\tAll Loss: 1.9143\tTriple Loss(0): 0.2911\tClassification Loss: 1.3321\r\n",
      "Train Epoch: 20 [11520/209539 (5%)]\tAll Loss: 1.5479\tTriple Loss(1): 0.0251\tClassification Loss: 1.4978\r\n",
      "Train Epoch: 20 [12160/209539 (6%)]\tAll Loss: 1.2222\tTriple Loss(1): 0.0483\tClassification Loss: 1.1256\r\n",
      "Train Epoch: 20 [12800/209539 (6%)]\tAll Loss: 0.9441\tTriple Loss(1): 0.0100\tClassification Loss: 0.9241\r\n",
      "Train Epoch: 20 [13440/209539 (6%)]\tAll Loss: 1.2929\tTriple Loss(1): 0.0073\tClassification Loss: 1.2783\r\n",
      "Train Epoch: 20 [14080/209539 (7%)]\tAll Loss: 1.4256\tTriple Loss(1): 0.0099\tClassification Loss: 1.4059\r\n",
      "Train Epoch: 20 [14720/209539 (7%)]\tAll Loss: 1.2844\tTriple Loss(1): 0.0421\tClassification Loss: 1.2001\r\n",
      "Train Epoch: 20 [15360/209539 (7%)]\tAll Loss: 1.4178\tTriple Loss(1): 0.0846\tClassification Loss: 1.2486\r\n",
      "Train Epoch: 20 [16000/209539 (8%)]\tAll Loss: 1.6632\tTriple Loss(1): 0.0381\tClassification Loss: 1.5871\r\n",
      "Train Epoch: 20 [16640/209539 (8%)]\tAll Loss: 1.6357\tTriple Loss(1): 0.0088\tClassification Loss: 1.6180\r\n",
      "Train Epoch: 20 [17280/209539 (8%)]\tAll Loss: 1.4904\tTriple Loss(1): 0.0632\tClassification Loss: 1.3640\r\n",
      "Train Epoch: 20 [17920/209539 (9%)]\tAll Loss: 1.3329\tTriple Loss(1): 0.0191\tClassification Loss: 1.2948\r\n",
      "Train Epoch: 20 [18560/209539 (9%)]\tAll Loss: 1.3053\tTriple Loss(1): 0.0298\tClassification Loss: 1.2456\r\n",
      "Train Epoch: 20 [19200/209539 (9%)]\tAll Loss: 1.4136\tTriple Loss(1): 0.0313\tClassification Loss: 1.3511\r\n",
      "Train Epoch: 20 [19840/209539 (9%)]\tAll Loss: 1.3927\tTriple Loss(1): 0.0489\tClassification Loss: 1.2949\r\n",
      "Train Epoch: 20 [20480/209539 (10%)]\tAll Loss: 1.2773\tTriple Loss(1): 0.0796\tClassification Loss: 1.1180\r\n",
      "Train Epoch: 20 [21120/209539 (10%)]\tAll Loss: 2.6080\tTriple Loss(0): 0.5722\tClassification Loss: 1.4636\r\n",
      "Train Epoch: 20 [21760/209539 (10%)]\tAll Loss: 1.1957\tTriple Loss(1): 0.0274\tClassification Loss: 1.1410\r\n",
      "Train Epoch: 20 [22400/209539 (11%)]\tAll Loss: 1.2695\tTriple Loss(1): 0.0546\tClassification Loss: 1.1603\r\n",
      "Train Epoch: 20 [23040/209539 (11%)]\tAll Loss: 1.4587\tTriple Loss(1): 0.0552\tClassification Loss: 1.3484\r\n",
      "Train Epoch: 20 [23680/209539 (11%)]\tAll Loss: 1.2208\tTriple Loss(1): 0.0547\tClassification Loss: 1.1114\r\n",
      "Train Epoch: 20 [24320/209539 (12%)]\tAll Loss: 1.5055\tTriple Loss(1): 0.0431\tClassification Loss: 1.4193\r\n",
      "Train Epoch: 20 [24960/209539 (12%)]\tAll Loss: 1.3006\tTriple Loss(1): 0.0369\tClassification Loss: 1.2269\r\n",
      "Train Epoch: 20 [25600/209539 (12%)]\tAll Loss: 1.2975\tTriple Loss(1): 0.0060\tClassification Loss: 1.2855\r\n",
      "Train Epoch: 20 [26240/209539 (13%)]\tAll Loss: 1.4689\tTriple Loss(1): 0.0517\tClassification Loss: 1.3656\r\n",
      "Train Epoch: 20 [26880/209539 (13%)]\tAll Loss: 1.4595\tTriple Loss(1): 0.0280\tClassification Loss: 1.4034\r\n",
      "Train Epoch: 20 [27520/209539 (13%)]\tAll Loss: 1.4051\tTriple Loss(1): 0.0198\tClassification Loss: 1.3656\r\n",
      "Train Epoch: 20 [28160/209539 (13%)]\tAll Loss: 1.5890\tTriple Loss(1): 0.0530\tClassification Loss: 1.4830\r\n",
      "Train Epoch: 20 [28800/209539 (14%)]\tAll Loss: 1.5741\tTriple Loss(1): 0.0213\tClassification Loss: 1.5314\r\n",
      "Train Epoch: 20 [29440/209539 (14%)]\tAll Loss: 1.5607\tTriple Loss(1): 0.0375\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 20 [30080/209539 (14%)]\tAll Loss: 1.1914\tTriple Loss(1): 0.0428\tClassification Loss: 1.1059\r\n",
      "Train Epoch: 20 [30720/209539 (15%)]\tAll Loss: 1.2596\tTriple Loss(1): 0.0197\tClassification Loss: 1.2201\r\n",
      "Train Epoch: 20 [31360/209539 (15%)]\tAll Loss: 1.2372\tTriple Loss(1): 0.0527\tClassification Loss: 1.1317\r\n",
      "Train Epoch: 20 [32000/209539 (15%)]\tAll Loss: 1.4216\tTriple Loss(1): 0.0305\tClassification Loss: 1.3605\r\n",
      "Train Epoch: 20 [32640/209539 (16%)]\tAll Loss: 1.3962\tTriple Loss(1): 0.0218\tClassification Loss: 1.3527\r\n",
      "Train Epoch: 20 [33280/209539 (16%)]\tAll Loss: 1.5766\tTriple Loss(1): 0.0465\tClassification Loss: 1.4837\r\n",
      "Train Epoch: 20 [33920/209539 (16%)]\tAll Loss: 1.3781\tTriple Loss(1): 0.0072\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 20 [34560/209539 (16%)]\tAll Loss: 1.3313\tTriple Loss(1): 0.0455\tClassification Loss: 1.2403\r\n",
      "Train Epoch: 20 [35200/209539 (17%)]\tAll Loss: 1.2847\tTriple Loss(1): 0.0208\tClassification Loss: 1.2431\r\n",
      "Train Epoch: 20 [35840/209539 (17%)]\tAll Loss: 1.2627\tTriple Loss(1): 0.0168\tClassification Loss: 1.2291\r\n",
      "Train Epoch: 20 [36480/209539 (17%)]\tAll Loss: 2.1317\tTriple Loss(0): 0.4638\tClassification Loss: 1.2041\r\n",
      "Train Epoch: 20 [37120/209539 (18%)]\tAll Loss: 2.2854\tTriple Loss(0): 0.3105\tClassification Loss: 1.6643\r\n",
      "Train Epoch: 20 [37760/209539 (18%)]\tAll Loss: 2.1150\tTriple Loss(0): 0.3233\tClassification Loss: 1.4685\r\n",
      "Train Epoch: 20 [38400/209539 (18%)]\tAll Loss: 1.4559\tTriple Loss(1): 0.0052\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 20 [39040/209539 (19%)]\tAll Loss: 0.9574\tTriple Loss(1): 0.0019\tClassification Loss: 0.9535\r\n",
      "Train Epoch: 20 [39680/209539 (19%)]\tAll Loss: 1.4446\tTriple Loss(1): 0.0926\tClassification Loss: 1.2593\r\n",
      "Train Epoch: 20 [40320/209539 (19%)]\tAll Loss: 1.2400\tTriple Loss(1): 0.0000\tClassification Loss: 1.2400\r\n",
      "Train Epoch: 20 [40960/209539 (20%)]\tAll Loss: 1.5053\tTriple Loss(1): 0.0511\tClassification Loss: 1.4031\r\n",
      "Train Epoch: 20 [41600/209539 (20%)]\tAll Loss: 1.2895\tTriple Loss(1): 0.0052\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 20 [42240/209539 (20%)]\tAll Loss: 1.3651\tTriple Loss(1): 0.0721\tClassification Loss: 1.2209\r\n",
      "Train Epoch: 20 [42880/209539 (20%)]\tAll Loss: 1.2094\tTriple Loss(1): 0.0161\tClassification Loss: 1.1772\r\n",
      "Train Epoch: 20 [43520/209539 (21%)]\tAll Loss: 1.5177\tTriple Loss(1): 0.0255\tClassification Loss: 1.4667\r\n",
      "Train Epoch: 20 [44160/209539 (21%)]\tAll Loss: 1.7395\tTriple Loss(1): 0.0559\tClassification Loss: 1.6277\r\n",
      "Train Epoch: 20 [44800/209539 (21%)]\tAll Loss: 1.7194\tTriple Loss(1): 0.0557\tClassification Loss: 1.6080\r\n",
      "Train Epoch: 20 [45440/209539 (22%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0014\tClassification Loss: 1.5876\r\n",
      "Train Epoch: 20 [46080/209539 (22%)]\tAll Loss: 1.9893\tTriple Loss(0): 0.3809\tClassification Loss: 1.2275\r\n",
      "Train Epoch: 20 [46720/209539 (22%)]\tAll Loss: 1.7319\tTriple Loss(1): 0.0012\tClassification Loss: 1.7294\r\n",
      "Train Epoch: 20 [47360/209539 (23%)]\tAll Loss: 1.2264\tTriple Loss(1): 0.0347\tClassification Loss: 1.1569\r\n",
      "Train Epoch: 20 [48000/209539 (23%)]\tAll Loss: 2.4268\tTriple Loss(0): 0.6591\tClassification Loss: 1.1085\r\n",
      "Train Epoch: 20 [48640/209539 (23%)]\tAll Loss: 1.7192\tTriple Loss(1): 0.1035\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 20 [49280/209539 (24%)]\tAll Loss: 1.4053\tTriple Loss(1): 0.0379\tClassification Loss: 1.3295\r\n",
      "Train Epoch: 20 [49920/209539 (24%)]\tAll Loss: 1.5307\tTriple Loss(1): 0.0740\tClassification Loss: 1.3828\r\n",
      "Train Epoch: 20 [50560/209539 (24%)]\tAll Loss: 1.6976\tTriple Loss(1): 0.0696\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 20 [51200/209539 (24%)]\tAll Loss: 1.4718\tTriple Loss(1): 0.0234\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 20 [51840/209539 (25%)]\tAll Loss: 1.2695\tTriple Loss(1): 0.0444\tClassification Loss: 1.1808\r\n",
      "Train Epoch: 20 [52480/209539 (25%)]\tAll Loss: 1.3852\tTriple Loss(1): 0.0376\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 20 [53120/209539 (25%)]\tAll Loss: 1.4435\tTriple Loss(1): 0.0537\tClassification Loss: 1.3361\r\n",
      "Train Epoch: 20 [53760/209539 (26%)]\tAll Loss: 1.6947\tTriple Loss(1): 0.0477\tClassification Loss: 1.5992\r\n",
      "Train Epoch: 20 [54400/209539 (26%)]\tAll Loss: 1.8290\tTriple Loss(1): 0.0343\tClassification Loss: 1.7604\r\n",
      "Train Epoch: 20 [55040/209539 (26%)]\tAll Loss: 1.9399\tTriple Loss(0): 0.4433\tClassification Loss: 1.0533\r\n",
      "Train Epoch: 20 [55680/209539 (27%)]\tAll Loss: 2.2462\tTriple Loss(0): 0.3203\tClassification Loss: 1.6056\r\n",
      "Train Epoch: 20 [56320/209539 (27%)]\tAll Loss: 2.1302\tTriple Loss(0): 0.4848\tClassification Loss: 1.1607\r\n",
      "Train Epoch: 20 [56960/209539 (27%)]\tAll Loss: 1.8294\tTriple Loss(0): 0.2873\tClassification Loss: 1.2547\r\n",
      "Train Epoch: 20 [57600/209539 (27%)]\tAll Loss: 1.3511\tTriple Loss(1): 0.0539\tClassification Loss: 1.2432\r\n",
      "Train Epoch: 20 [58240/209539 (28%)]\tAll Loss: 1.4065\tTriple Loss(1): 0.0333\tClassification Loss: 1.3399\r\n",
      "Train Epoch: 20 [58880/209539 (28%)]\tAll Loss: 1.4423\tTriple Loss(1): 0.0311\tClassification Loss: 1.3800\r\n",
      "Train Epoch: 20 [59520/209539 (28%)]\tAll Loss: 1.1397\tTriple Loss(1): 0.0000\tClassification Loss: 1.1397\r\n",
      "Train Epoch: 20 [60160/209539 (29%)]\tAll Loss: 1.6033\tTriple Loss(1): 0.0332\tClassification Loss: 1.5369\r\n",
      "Train Epoch: 20 [60800/209539 (29%)]\tAll Loss: 1.9653\tTriple Loss(0): 0.3127\tClassification Loss: 1.3400\r\n",
      "Train Epoch: 20 [61440/209539 (29%)]\tAll Loss: 1.3825\tTriple Loss(1): 0.0038\tClassification Loss: 1.3749\r\n",
      "Train Epoch: 20 [62080/209539 (30%)]\tAll Loss: 1.6025\tTriple Loss(1): 0.0346\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 20 [62720/209539 (30%)]\tAll Loss: 1.9760\tTriple Loss(0): 0.2898\tClassification Loss: 1.3964\r\n",
      "Train Epoch: 20 [63360/209539 (30%)]\tAll Loss: 1.4928\tTriple Loss(1): 0.0276\tClassification Loss: 1.4375\r\n",
      "Train Epoch: 20 [64000/209539 (31%)]\tAll Loss: 1.5789\tTriple Loss(1): 0.0128\tClassification Loss: 1.5533\r\n",
      "Train Epoch: 20 [64640/209539 (31%)]\tAll Loss: 1.9621\tTriple Loss(1): 0.0779\tClassification Loss: 1.8063\r\n",
      "Train Epoch: 20 [65280/209539 (31%)]\tAll Loss: 2.0841\tTriple Loss(0): 0.3785\tClassification Loss: 1.3271\r\n",
      "Train Epoch: 20 [65920/209539 (31%)]\tAll Loss: 1.5745\tTriple Loss(1): 0.0206\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 20 [66560/209539 (32%)]\tAll Loss: 1.9140\tTriple Loss(0): 0.3646\tClassification Loss: 1.1848\r\n",
      "Train Epoch: 20 [67200/209539 (32%)]\tAll Loss: 2.0219\tTriple Loss(0): 0.3198\tClassification Loss: 1.3823\r\n",
      "Train Epoch: 20 [67840/209539 (32%)]\tAll Loss: 1.4599\tTriple Loss(1): 0.0379\tClassification Loss: 1.3841\r\n",
      "Train Epoch: 20 [68480/209539 (33%)]\tAll Loss: 1.7016\tTriple Loss(1): 0.0864\tClassification Loss: 1.5287\r\n",
      "Train Epoch: 20 [69120/209539 (33%)]\tAll Loss: 1.7850\tTriple Loss(1): 0.0836\tClassification Loss: 1.6178\r\n",
      "Train Epoch: 20 [69760/209539 (33%)]\tAll Loss: 1.5556\tTriple Loss(0): 0.2107\tClassification Loss: 1.1341\r\n",
      "Train Epoch: 20 [70400/209539 (34%)]\tAll Loss: 1.3279\tTriple Loss(1): 0.0637\tClassification Loss: 1.2005\r\n",
      "Train Epoch: 20 [71040/209539 (34%)]\tAll Loss: 1.7952\tTriple Loss(1): 0.0836\tClassification Loss: 1.6281\r\n",
      "Train Epoch: 20 [71680/209539 (34%)]\tAll Loss: 1.7651\tTriple Loss(1): 0.0347\tClassification Loss: 1.6958\r\n",
      "Train Epoch: 20 [72320/209539 (35%)]\tAll Loss: 1.3476\tTriple Loss(1): 0.0619\tClassification Loss: 1.2238\r\n",
      "Train Epoch: 20 [72960/209539 (35%)]\tAll Loss: 1.4088\tTriple Loss(1): 0.0409\tClassification Loss: 1.3270\r\n",
      "Train Epoch: 20 [73600/209539 (35%)]\tAll Loss: 1.4397\tTriple Loss(1): 0.0000\tClassification Loss: 1.4397\r\n",
      "Train Epoch: 20 [74240/209539 (35%)]\tAll Loss: 1.7280\tTriple Loss(1): 0.0275\tClassification Loss: 1.6730\r\n",
      "Train Epoch: 20 [74880/209539 (36%)]\tAll Loss: 2.7598\tTriple Loss(0): 0.4178\tClassification Loss: 1.9243\r\n",
      "Train Epoch: 20 [75520/209539 (36%)]\tAll Loss: 1.3136\tTriple Loss(1): 0.0910\tClassification Loss: 1.1317\r\n",
      "Train Epoch: 20 [76160/209539 (36%)]\tAll Loss: 2.2696\tTriple Loss(0): 0.4295\tClassification Loss: 1.4106\r\n",
      "Train Epoch: 20 [76800/209539 (37%)]\tAll Loss: 1.6025\tTriple Loss(1): 0.0680\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 20 [77440/209539 (37%)]\tAll Loss: 1.5574\tTriple Loss(1): 0.0405\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 20 [78080/209539 (37%)]\tAll Loss: 2.3720\tTriple Loss(0): 0.4863\tClassification Loss: 1.3994\r\n",
      "Train Epoch: 20 [78720/209539 (38%)]\tAll Loss: 1.7983\tTriple Loss(1): 0.0777\tClassification Loss: 1.6428\r\n",
      "Train Epoch: 20 [79360/209539 (38%)]\tAll Loss: 1.3626\tTriple Loss(1): 0.0099\tClassification Loss: 1.3428\r\n",
      "Train Epoch: 20 [80000/209539 (38%)]\tAll Loss: 1.4816\tTriple Loss(1): 0.0886\tClassification Loss: 1.3043\r\n",
      "Train Epoch: 20 [80640/209539 (38%)]\tAll Loss: 1.6002\tTriple Loss(1): 0.1519\tClassification Loss: 1.2964\r\n",
      "Train Epoch: 20 [81280/209539 (39%)]\tAll Loss: 1.5258\tTriple Loss(1): 0.0929\tClassification Loss: 1.3399\r\n",
      "Train Epoch: 20 [81920/209539 (39%)]\tAll Loss: 1.4753\tTriple Loss(1): 0.0884\tClassification Loss: 1.2984\r\n",
      "Train Epoch: 20 [82560/209539 (39%)]\tAll Loss: 1.5322\tTriple Loss(1): 0.0159\tClassification Loss: 1.5004\r\n",
      "Train Epoch: 20 [83200/209539 (40%)]\tAll Loss: 1.6378\tTriple Loss(1): 0.0051\tClassification Loss: 1.6276\r\n",
      "Train Epoch: 20 [83840/209539 (40%)]\tAll Loss: 1.4189\tTriple Loss(1): 0.0798\tClassification Loss: 1.2592\r\n",
      "Train Epoch: 20 [84480/209539 (40%)]\tAll Loss: 1.1250\tTriple Loss(1): 0.0117\tClassification Loss: 1.1015\r\n",
      "Train Epoch: 20 [85120/209539 (41%)]\tAll Loss: 1.8973\tTriple Loss(1): 0.0520\tClassification Loss: 1.7933\r\n",
      "Train Epoch: 20 [85760/209539 (41%)]\tAll Loss: 1.5536\tTriple Loss(1): 0.0387\tClassification Loss: 1.4763\r\n",
      "Train Epoch: 20 [86400/209539 (41%)]\tAll Loss: 2.5625\tTriple Loss(0): 0.5748\tClassification Loss: 1.4128\r\n",
      "Train Epoch: 20 [87040/209539 (42%)]\tAll Loss: 1.4611\tTriple Loss(1): 0.0206\tClassification Loss: 1.4199\r\n",
      "Train Epoch: 20 [87680/209539 (42%)]\tAll Loss: 1.5496\tTriple Loss(1): 0.0929\tClassification Loss: 1.3638\r\n",
      "Train Epoch: 20 [88320/209539 (42%)]\tAll Loss: 1.3748\tTriple Loss(1): 0.0047\tClassification Loss: 1.3655\r\n",
      "Train Epoch: 20 [88960/209539 (42%)]\tAll Loss: 1.5106\tTriple Loss(1): 0.0083\tClassification Loss: 1.4939\r\n",
      "Train Epoch: 20 [89600/209539 (43%)]\tAll Loss: 1.7813\tTriple Loss(1): 0.0209\tClassification Loss: 1.7395\r\n",
      "Train Epoch: 20 [90240/209539 (43%)]\tAll Loss: 1.7522\tTriple Loss(0): 0.2591\tClassification Loss: 1.2339\r\n",
      "Train Epoch: 20 [90880/209539 (43%)]\tAll Loss: 1.8360\tTriple Loss(1): 0.0806\tClassification Loss: 1.6748\r\n",
      "Train Epoch: 20 [91520/209539 (44%)]\tAll Loss: 1.4959\tTriple Loss(1): 0.0384\tClassification Loss: 1.4191\r\n",
      "Train Epoch: 20 [92160/209539 (44%)]\tAll Loss: 1.3608\tTriple Loss(1): 0.0185\tClassification Loss: 1.3238\r\n",
      "Train Epoch: 20 [92800/209539 (44%)]\tAll Loss: 1.3257\tTriple Loss(1): 0.0378\tClassification Loss: 1.2502\r\n",
      "Train Epoch: 20 [93440/209539 (45%)]\tAll Loss: 1.6246\tTriple Loss(1): 0.0421\tClassification Loss: 1.5403\r\n",
      "Train Epoch: 20 [94080/209539 (45%)]\tAll Loss: 1.4790\tTriple Loss(1): 0.0440\tClassification Loss: 1.3910\r\n",
      "Train Epoch: 20 [94720/209539 (45%)]\tAll Loss: 1.6931\tTriple Loss(1): 0.1123\tClassification Loss: 1.4684\r\n",
      "Train Epoch: 20 [95360/209539 (46%)]\tAll Loss: 2.1397\tTriple Loss(0): 0.3201\tClassification Loss: 1.4995\r\n",
      "Train Epoch: 20 [96000/209539 (46%)]\tAll Loss: 1.3707\tTriple Loss(1): 0.0024\tClassification Loss: 1.3659\r\n",
      "Train Epoch: 20 [96640/209539 (46%)]\tAll Loss: 1.5265\tTriple Loss(1): 0.0223\tClassification Loss: 1.4818\r\n",
      "Train Epoch: 20 [97280/209539 (46%)]\tAll Loss: 1.9966\tTriple Loss(0): 0.4183\tClassification Loss: 1.1600\r\n",
      "Train Epoch: 20 [97920/209539 (47%)]\tAll Loss: 1.5733\tTriple Loss(1): 0.0295\tClassification Loss: 1.5143\r\n",
      "Train Epoch: 20 [98560/209539 (47%)]\tAll Loss: 1.6925\tTriple Loss(1): 0.0066\tClassification Loss: 1.6794\r\n",
      "Train Epoch: 20 [99200/209539 (47%)]\tAll Loss: 1.4668\tTriple Loss(1): 0.0653\tClassification Loss: 1.3361\r\n",
      "Train Epoch: 20 [99840/209539 (48%)]\tAll Loss: 1.4261\tTriple Loss(1): 0.0280\tClassification Loss: 1.3700\r\n",
      "Train Epoch: 20 [100480/209539 (48%)]\tAll Loss: 2.4622\tTriple Loss(0): 0.5236\tClassification Loss: 1.4150\r\n",
      "Train Epoch: 20 [101120/209539 (48%)]\tAll Loss: 1.5007\tTriple Loss(1): 0.0545\tClassification Loss: 1.3917\r\n",
      "Train Epoch: 20 [101760/209539 (49%)]\tAll Loss: 1.5203\tTriple Loss(1): 0.0454\tClassification Loss: 1.4295\r\n",
      "Train Epoch: 20 [102400/209539 (49%)]\tAll Loss: 1.4354\tTriple Loss(1): 0.0709\tClassification Loss: 1.2936\r\n",
      "Train Epoch: 20 [103040/209539 (49%)]\tAll Loss: 1.3716\tTriple Loss(1): 0.0840\tClassification Loss: 1.2036\r\n",
      "Train Epoch: 20 [103680/209539 (49%)]\tAll Loss: 1.4961\tTriple Loss(1): 0.0473\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 20 [104320/209539 (50%)]\tAll Loss: 1.1855\tTriple Loss(1): 0.0178\tClassification Loss: 1.1499\r\n",
      "Train Epoch: 20 [104960/209539 (50%)]\tAll Loss: 1.6521\tTriple Loss(1): 0.0478\tClassification Loss: 1.5566\r\n",
      "Train Epoch: 20 [105600/209539 (50%)]\tAll Loss: 2.0194\tTriple Loss(0): 0.3520\tClassification Loss: 1.3155\r\n",
      "Train Epoch: 20 [106240/209539 (51%)]\tAll Loss: 1.3684\tTriple Loss(1): 0.0196\tClassification Loss: 1.3292\r\n",
      "Train Epoch: 20 [106880/209539 (51%)]\tAll Loss: 1.4100\tTriple Loss(1): 0.0450\tClassification Loss: 1.3199\r\n",
      "Train Epoch: 20 [107520/209539 (51%)]\tAll Loss: 1.3810\tTriple Loss(1): 0.0493\tClassification Loss: 1.2825\r\n",
      "Train Epoch: 20 [108160/209539 (52%)]\tAll Loss: 1.4073\tTriple Loss(1): 0.0313\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 20 [108800/209539 (52%)]\tAll Loss: 2.2166\tTriple Loss(0): 0.5183\tClassification Loss: 1.1799\r\n",
      "Train Epoch: 20 [109440/209539 (52%)]\tAll Loss: 1.8504\tTriple Loss(1): 0.0624\tClassification Loss: 1.7257\r\n",
      "Train Epoch: 20 [110080/209539 (53%)]\tAll Loss: 1.6400\tTriple Loss(1): 0.0037\tClassification Loss: 1.6327\r\n",
      "Train Epoch: 20 [110720/209539 (53%)]\tAll Loss: 1.9597\tTriple Loss(1): 0.2646\tClassification Loss: 1.4304\r\n",
      "Train Epoch: 20 [111360/209539 (53%)]\tAll Loss: 1.4989\tTriple Loss(1): 0.1008\tClassification Loss: 1.2973\r\n",
      "Train Epoch: 20 [112000/209539 (53%)]\tAll Loss: 1.2285\tTriple Loss(1): 0.0333\tClassification Loss: 1.1619\r\n",
      "Train Epoch: 20 [112640/209539 (54%)]\tAll Loss: 2.5187\tTriple Loss(0): 0.6316\tClassification Loss: 1.2556\r\n",
      "Train Epoch: 20 [113280/209539 (54%)]\tAll Loss: 1.3381\tTriple Loss(1): 0.0511\tClassification Loss: 1.2359\r\n",
      "Train Epoch: 20 [113920/209539 (54%)]\tAll Loss: 1.5056\tTriple Loss(1): 0.0380\tClassification Loss: 1.4296\r\n",
      "Train Epoch: 20 [114560/209539 (55%)]\tAll Loss: 1.1664\tTriple Loss(1): 0.0205\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 20 [115200/209539 (55%)]\tAll Loss: 1.6870\tTriple Loss(1): 0.0502\tClassification Loss: 1.5865\r\n",
      "Train Epoch: 20 [115840/209539 (55%)]\tAll Loss: 1.6493\tTriple Loss(1): 0.0940\tClassification Loss: 1.4613\r\n",
      "Train Epoch: 20 [116480/209539 (56%)]\tAll Loss: 1.5595\tTriple Loss(1): 0.0042\tClassification Loss: 1.5510\r\n",
      "Train Epoch: 20 [117120/209539 (56%)]\tAll Loss: 1.5251\tTriple Loss(1): 0.1232\tClassification Loss: 1.2787\r\n",
      "Train Epoch: 20 [117760/209539 (56%)]\tAll Loss: 1.3488\tTriple Loss(1): 0.0413\tClassification Loss: 1.2662\r\n",
      "Train Epoch: 20 [118400/209539 (57%)]\tAll Loss: 1.5167\tTriple Loss(1): 0.2128\tClassification Loss: 1.0912\r\n",
      "Train Epoch: 20 [119040/209539 (57%)]\tAll Loss: 1.6720\tTriple Loss(1): 0.0592\tClassification Loss: 1.5536\r\n",
      "Train Epoch: 20 [119680/209539 (57%)]\tAll Loss: 1.4315\tTriple Loss(1): 0.0405\tClassification Loss: 1.3505\r\n",
      "Train Epoch: 20 [120320/209539 (57%)]\tAll Loss: 1.3126\tTriple Loss(1): 0.0075\tClassification Loss: 1.2976\r\n",
      "Train Epoch: 20 [120960/209539 (58%)]\tAll Loss: 1.4443\tTriple Loss(1): 0.0872\tClassification Loss: 1.2699\r\n",
      "Train Epoch: 20 [121600/209539 (58%)]\tAll Loss: 1.2918\tTriple Loss(1): 0.0375\tClassification Loss: 1.2167\r\n",
      "Train Epoch: 20 [122240/209539 (58%)]\tAll Loss: 1.2692\tTriple Loss(1): 0.0403\tClassification Loss: 1.1887\r\n",
      "Train Epoch: 20 [122880/209539 (59%)]\tAll Loss: 1.4111\tTriple Loss(1): 0.0986\tClassification Loss: 1.2140\r\n",
      "Train Epoch: 20 [123520/209539 (59%)]\tAll Loss: 1.5127\tTriple Loss(1): 0.0298\tClassification Loss: 1.4530\r\n",
      "Train Epoch: 20 [124160/209539 (59%)]\tAll Loss: 2.2708\tTriple Loss(0): 0.4485\tClassification Loss: 1.3739\r\n",
      "Train Epoch: 20 [124800/209539 (60%)]\tAll Loss: 1.2245\tTriple Loss(1): 0.0495\tClassification Loss: 1.1256\r\n",
      "Train Epoch: 20 [125440/209539 (60%)]\tAll Loss: 1.5310\tTriple Loss(1): 0.0781\tClassification Loss: 1.3748\r\n",
      "Train Epoch: 20 [126080/209539 (60%)]\tAll Loss: 1.5230\tTriple Loss(1): 0.0251\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 20 [126720/209539 (60%)]\tAll Loss: 1.5506\tTriple Loss(1): 0.0153\tClassification Loss: 1.5200\r\n",
      "Train Epoch: 20 [127360/209539 (61%)]\tAll Loss: 1.4367\tTriple Loss(1): 0.0039\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 20 [128000/209539 (61%)]\tAll Loss: 1.3930\tTriple Loss(1): 0.0392\tClassification Loss: 1.3147\r\n",
      "Train Epoch: 20 [128640/209539 (61%)]\tAll Loss: 0.9828\tTriple Loss(1): 0.0239\tClassification Loss: 0.9350\r\n",
      "Train Epoch: 20 [129280/209539 (62%)]\tAll Loss: 1.4189\tTriple Loss(1): 0.0044\tClassification Loss: 1.4100\r\n",
      "Train Epoch: 20 [129920/209539 (62%)]\tAll Loss: 2.0605\tTriple Loss(0): 0.4268\tClassification Loss: 1.2068\r\n",
      "Train Epoch: 20 [130560/209539 (62%)]\tAll Loss: 1.7866\tTriple Loss(0): 0.3779\tClassification Loss: 1.0307\r\n",
      "Train Epoch: 20 [131200/209539 (63%)]\tAll Loss: 1.4043\tTriple Loss(1): 0.0363\tClassification Loss: 1.3316\r\n",
      "Train Epoch: 20 [131840/209539 (63%)]\tAll Loss: 1.5468\tTriple Loss(1): 0.1007\tClassification Loss: 1.3454\r\n",
      "Train Epoch: 20 [132480/209539 (63%)]\tAll Loss: 1.9488\tTriple Loss(0): 0.4245\tClassification Loss: 1.0998\r\n",
      "Train Epoch: 20 [133120/209539 (64%)]\tAll Loss: 1.3693\tTriple Loss(1): 0.0787\tClassification Loss: 1.2118\r\n",
      "Train Epoch: 20 [133760/209539 (64%)]\tAll Loss: 1.3183\tTriple Loss(1): 0.0264\tClassification Loss: 1.2654\r\n",
      "Train Epoch: 20 [134400/209539 (64%)]\tAll Loss: 1.2927\tTriple Loss(1): 0.0639\tClassification Loss: 1.1650\r\n",
      "Train Epoch: 20 [135040/209539 (64%)]\tAll Loss: 1.3535\tTriple Loss(1): 0.0497\tClassification Loss: 1.2540\r\n",
      "Train Epoch: 20 [135680/209539 (65%)]\tAll Loss: 1.6541\tTriple Loss(1): 0.0037\tClassification Loss: 1.6468\r\n",
      "Train Epoch: 20 [136320/209539 (65%)]\tAll Loss: 1.4978\tTriple Loss(1): 0.0085\tClassification Loss: 1.4807\r\n",
      "Train Epoch: 20 [136960/209539 (65%)]\tAll Loss: 1.5002\tTriple Loss(1): 0.1073\tClassification Loss: 1.2856\r\n",
      "Train Epoch: 20 [137600/209539 (66%)]\tAll Loss: 1.3502\tTriple Loss(1): 0.0518\tClassification Loss: 1.2465\r\n",
      "Train Epoch: 20 [138240/209539 (66%)]\tAll Loss: 1.9223\tTriple Loss(1): 0.0927\tClassification Loss: 1.7368\r\n",
      "Train Epoch: 20 [138880/209539 (66%)]\tAll Loss: 1.3426\tTriple Loss(1): 0.0414\tClassification Loss: 1.2599\r\n",
      "Train Epoch: 20 [139520/209539 (67%)]\tAll Loss: 1.3323\tTriple Loss(1): 0.0155\tClassification Loss: 1.3013\r\n",
      "Train Epoch: 20 [140160/209539 (67%)]\tAll Loss: 1.5511\tTriple Loss(1): 0.1284\tClassification Loss: 1.2944\r\n",
      "Train Epoch: 20 [140800/209539 (67%)]\tAll Loss: 1.6867\tTriple Loss(1): 0.0184\tClassification Loss: 1.6499\r\n",
      "Train Epoch: 20 [141440/209539 (68%)]\tAll Loss: 1.6505\tTriple Loss(1): 0.0645\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 20 [142080/209539 (68%)]\tAll Loss: 1.3898\tTriple Loss(1): 0.0490\tClassification Loss: 1.2919\r\n",
      "Train Epoch: 20 [142720/209539 (68%)]\tAll Loss: 2.7105\tTriple Loss(0): 0.5309\tClassification Loss: 1.6487\r\n",
      "Train Epoch: 20 [143360/209539 (68%)]\tAll Loss: 1.2702\tTriple Loss(1): 0.0321\tClassification Loss: 1.2060\r\n",
      "Train Epoch: 20 [144000/209539 (69%)]\tAll Loss: 1.5179\tTriple Loss(1): 0.0323\tClassification Loss: 1.4534\r\n",
      "Train Epoch: 20 [144640/209539 (69%)]\tAll Loss: 1.5007\tTriple Loss(1): 0.0931\tClassification Loss: 1.3146\r\n",
      "Train Epoch: 20 [145280/209539 (69%)]\tAll Loss: 2.2592\tTriple Loss(0): 0.4023\tClassification Loss: 1.4546\r\n",
      "Train Epoch: 20 [145920/209539 (70%)]\tAll Loss: 2.1687\tTriple Loss(0): 0.4152\tClassification Loss: 1.3383\r\n",
      "Train Epoch: 20 [146560/209539 (70%)]\tAll Loss: 1.3207\tTriple Loss(1): 0.0828\tClassification Loss: 1.1550\r\n",
      "Train Epoch: 20 [147200/209539 (70%)]\tAll Loss: 1.5396\tTriple Loss(1): 0.0391\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 20 [147840/209539 (71%)]\tAll Loss: 1.6897\tTriple Loss(0): 0.2910\tClassification Loss: 1.1078\r\n",
      "Train Epoch: 20 [148480/209539 (71%)]\tAll Loss: 1.2747\tTriple Loss(1): 0.0332\tClassification Loss: 1.2084\r\n",
      "Train Epoch: 20 [149120/209539 (71%)]\tAll Loss: 1.4866\tTriple Loss(1): 0.0261\tClassification Loss: 1.4345\r\n",
      "Train Epoch: 20 [149760/209539 (71%)]\tAll Loss: 1.2469\tTriple Loss(1): 0.0457\tClassification Loss: 1.1555\r\n",
      "Train Epoch: 20 [150400/209539 (72%)]\tAll Loss: 2.6975\tTriple Loss(0): 0.6344\tClassification Loss: 1.4286\r\n",
      "Train Epoch: 20 [151040/209539 (72%)]\tAll Loss: 1.4544\tTriple Loss(1): 0.1165\tClassification Loss: 1.2214\r\n",
      "Train Epoch: 20 [151680/209539 (72%)]\tAll Loss: 2.7389\tTriple Loss(0): 0.5240\tClassification Loss: 1.6909\r\n",
      "Train Epoch: 20 [152320/209539 (73%)]\tAll Loss: 1.9541\tTriple Loss(0): 0.3729\tClassification Loss: 1.2083\r\n",
      "Train Epoch: 20 [152960/209539 (73%)]\tAll Loss: 1.5567\tTriple Loss(1): 0.1213\tClassification Loss: 1.3142\r\n",
      "Train Epoch: 20 [153600/209539 (73%)]\tAll Loss: 1.3135\tTriple Loss(1): 0.0121\tClassification Loss: 1.2893\r\n",
      "Train Epoch: 20 [154240/209539 (74%)]\tAll Loss: 1.3773\tTriple Loss(1): 0.0591\tClassification Loss: 1.2591\r\n",
      "Train Epoch: 20 [154880/209539 (74%)]\tAll Loss: 1.7852\tTriple Loss(1): 0.0127\tClassification Loss: 1.7599\r\n",
      "Train Epoch: 20 [155520/209539 (74%)]\tAll Loss: 1.4475\tTriple Loss(1): 0.1018\tClassification Loss: 1.2438\r\n",
      "Train Epoch: 20 [156160/209539 (75%)]\tAll Loss: 1.4763\tTriple Loss(1): 0.0881\tClassification Loss: 1.3001\r\n",
      "Train Epoch: 20 [156800/209539 (75%)]\tAll Loss: 2.5460\tTriple Loss(0): 0.3595\tClassification Loss: 1.8270\r\n",
      "Train Epoch: 20 [157440/209539 (75%)]\tAll Loss: 1.6020\tTriple Loss(1): 0.0732\tClassification Loss: 1.4555\r\n",
      "Train Epoch: 20 [158080/209539 (75%)]\tAll Loss: 1.2215\tTriple Loss(1): 0.0849\tClassification Loss: 1.0517\r\n",
      "Train Epoch: 20 [158720/209539 (76%)]\tAll Loss: 1.1444\tTriple Loss(1): 0.0023\tClassification Loss: 1.1399\r\n",
      "Train Epoch: 20 [159360/209539 (76%)]\tAll Loss: 1.1137\tTriple Loss(1): 0.0347\tClassification Loss: 1.0443\r\n",
      "Train Epoch: 20 [160000/209539 (76%)]\tAll Loss: 1.5307\tTriple Loss(1): 0.0579\tClassification Loss: 1.4148\r\n",
      "Train Epoch: 20 [160640/209539 (77%)]\tAll Loss: 1.2845\tTriple Loss(1): 0.0129\tClassification Loss: 1.2588\r\n",
      "Train Epoch: 20 [161280/209539 (77%)]\tAll Loss: 2.2150\tTriple Loss(0): 0.5129\tClassification Loss: 1.1892\r\n",
      "Train Epoch: 20 [161920/209539 (77%)]\tAll Loss: 1.5302\tTriple Loss(1): 0.0290\tClassification Loss: 1.4721\r\n",
      "Train Epoch: 20 [162560/209539 (78%)]\tAll Loss: 1.2647\tTriple Loss(1): 0.0155\tClassification Loss: 1.2336\r\n",
      "Train Epoch: 20 [163200/209539 (78%)]\tAll Loss: 1.2811\tTriple Loss(1): 0.0097\tClassification Loss: 1.2616\r\n",
      "Train Epoch: 20 [163840/209539 (78%)]\tAll Loss: 1.3689\tTriple Loss(1): 0.0288\tClassification Loss: 1.3114\r\n",
      "Train Epoch: 20 [164480/209539 (78%)]\tAll Loss: 1.1183\tTriple Loss(1): 0.0247\tClassification Loss: 1.0689\r\n",
      "Train Epoch: 20 [165120/209539 (79%)]\tAll Loss: 1.2174\tTriple Loss(1): 0.0640\tClassification Loss: 1.0894\r\n",
      "Train Epoch: 20 [165760/209539 (79%)]\tAll Loss: 2.2072\tTriple Loss(0): 0.4558\tClassification Loss: 1.2955\r\n",
      "Train Epoch: 20 [166400/209539 (79%)]\tAll Loss: 1.3721\tTriple Loss(1): 0.0000\tClassification Loss: 1.3721\r\n",
      "Train Epoch: 20 [167040/209539 (80%)]\tAll Loss: 1.9382\tTriple Loss(0): 0.3037\tClassification Loss: 1.3309\r\n",
      "Train Epoch: 20 [167680/209539 (80%)]\tAll Loss: 1.4675\tTriple Loss(1): 0.0433\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 20 [168320/209539 (80%)]\tAll Loss: 1.2473\tTriple Loss(1): 0.0604\tClassification Loss: 1.1264\r\n",
      "Train Epoch: 20 [168960/209539 (81%)]\tAll Loss: 1.3807\tTriple Loss(0): 0.1667\tClassification Loss: 1.0473\r\n",
      "Train Epoch: 20 [169600/209539 (81%)]\tAll Loss: 1.4866\tTriple Loss(1): 0.1033\tClassification Loss: 1.2800\r\n",
      "Train Epoch: 20 [170240/209539 (81%)]\tAll Loss: 1.5102\tTriple Loss(1): 0.0049\tClassification Loss: 1.5004\r\n",
      "Train Epoch: 20 [170880/209539 (82%)]\tAll Loss: 1.5296\tTriple Loss(1): 0.0069\tClassification Loss: 1.5157\r\n",
      "Train Epoch: 20 [171520/209539 (82%)]\tAll Loss: 1.1318\tTriple Loss(1): 0.0249\tClassification Loss: 1.0820\r\n",
      "Train Epoch: 20 [172160/209539 (82%)]\tAll Loss: 2.0312\tTriple Loss(0): 0.3293\tClassification Loss: 1.3725\r\n",
      "Train Epoch: 20 [172800/209539 (82%)]\tAll Loss: 1.5436\tTriple Loss(1): 0.1172\tClassification Loss: 1.3092\r\n",
      "Train Epoch: 20 [173440/209539 (83%)]\tAll Loss: 1.6604\tTriple Loss(1): 0.0390\tClassification Loss: 1.5824\r\n",
      "Train Epoch: 20 [174080/209539 (83%)]\tAll Loss: 1.2429\tTriple Loss(1): 0.0637\tClassification Loss: 1.1155\r\n",
      "Train Epoch: 20 [174720/209539 (83%)]\tAll Loss: 1.7297\tTriple Loss(1): 0.0889\tClassification Loss: 1.5520\r\n",
      "Train Epoch: 20 [175360/209539 (84%)]\tAll Loss: 1.4508\tTriple Loss(1): 0.0568\tClassification Loss: 1.3372\r\n",
      "Train Epoch: 20 [176000/209539 (84%)]\tAll Loss: 1.7987\tTriple Loss(0): 0.2481\tClassification Loss: 1.3026\r\n",
      "Train Epoch: 20 [176640/209539 (84%)]\tAll Loss: 1.1107\tTriple Loss(1): 0.0802\tClassification Loss: 0.9503\r\n",
      "Train Epoch: 20 [177280/209539 (85%)]\tAll Loss: 1.7160\tTriple Loss(1): 0.0459\tClassification Loss: 1.6241\r\n",
      "Train Epoch: 20 [177920/209539 (85%)]\tAll Loss: 1.6218\tTriple Loss(1): 0.0385\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 20 [178560/209539 (85%)]\tAll Loss: 1.4569\tTriple Loss(1): 0.0375\tClassification Loss: 1.3820\r\n",
      "Train Epoch: 20 [179200/209539 (86%)]\tAll Loss: 1.4664\tTriple Loss(1): 0.0432\tClassification Loss: 1.3800\r\n",
      "Train Epoch: 20 [179840/209539 (86%)]\tAll Loss: 1.3716\tTriple Loss(1): 0.0000\tClassification Loss: 1.3716\r\n",
      "Train Epoch: 20 [180480/209539 (86%)]\tAll Loss: 2.0856\tTriple Loss(0): 0.4120\tClassification Loss: 1.2616\r\n",
      "Train Epoch: 20 [181120/209539 (86%)]\tAll Loss: 1.5175\tTriple Loss(1): 0.0677\tClassification Loss: 1.3820\r\n",
      "Train Epoch: 20 [181760/209539 (87%)]\tAll Loss: 1.7557\tTriple Loss(0): 0.2965\tClassification Loss: 1.1627\r\n",
      "Train Epoch: 20 [182400/209539 (87%)]\tAll Loss: 1.4626\tTriple Loss(1): 0.0229\tClassification Loss: 1.4168\r\n",
      "Train Epoch: 20 [183040/209539 (87%)]\tAll Loss: 1.4323\tTriple Loss(1): 0.0190\tClassification Loss: 1.3943\r\n",
      "Train Epoch: 20 [183680/209539 (88%)]\tAll Loss: 1.2063\tTriple Loss(1): 0.0559\tClassification Loss: 1.0945\r\n",
      "Train Epoch: 20 [184320/209539 (88%)]\tAll Loss: 1.3312\tTriple Loss(1): 0.0117\tClassification Loss: 1.3078\r\n",
      "Train Epoch: 20 [184960/209539 (88%)]\tAll Loss: 1.1636\tTriple Loss(1): 0.0239\tClassification Loss: 1.1158\r\n",
      "Train Epoch: 20 [185600/209539 (89%)]\tAll Loss: 1.5148\tTriple Loss(1): 0.0321\tClassification Loss: 1.4505\r\n",
      "Train Epoch: 20 [186240/209539 (89%)]\tAll Loss: 1.5997\tTriple Loss(1): 0.0472\tClassification Loss: 1.5054\r\n",
      "Train Epoch: 20 [186880/209539 (89%)]\tAll Loss: 1.5254\tTriple Loss(1): 0.0566\tClassification Loss: 1.4122\r\n",
      "Train Epoch: 20 [187520/209539 (89%)]\tAll Loss: 1.6638\tTriple Loss(1): 0.0341\tClassification Loss: 1.5955\r\n",
      "Train Epoch: 20 [188160/209539 (90%)]\tAll Loss: 1.3955\tTriple Loss(1): 0.0892\tClassification Loss: 1.2171\r\n",
      "Train Epoch: 20 [188800/209539 (90%)]\tAll Loss: 1.8198\tTriple Loss(0): 0.2456\tClassification Loss: 1.3286\r\n",
      "Train Epoch: 20 [189440/209539 (90%)]\tAll Loss: 1.4424\tTriple Loss(1): 0.0190\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 20 [190080/209539 (91%)]\tAll Loss: 1.2519\tTriple Loss(1): 0.0364\tClassification Loss: 1.1791\r\n",
      "Train Epoch: 20 [190720/209539 (91%)]\tAll Loss: 1.3515\tTriple Loss(1): 0.0027\tClassification Loss: 1.3461\r\n",
      "Train Epoch: 20 [191360/209539 (91%)]\tAll Loss: 1.2256\tTriple Loss(1): 0.0766\tClassification Loss: 1.0724\r\n",
      "Train Epoch: 20 [192000/209539 (92%)]\tAll Loss: 1.8631\tTriple Loss(1): 0.0688\tClassification Loss: 1.7255\r\n",
      "Train Epoch: 20 [192640/209539 (92%)]\tAll Loss: 1.3169\tTriple Loss(1): 0.0205\tClassification Loss: 1.2759\r\n",
      "Train Epoch: 20 [193280/209539 (92%)]\tAll Loss: 1.3651\tTriple Loss(1): 0.0915\tClassification Loss: 1.1821\r\n",
      "Train Epoch: 20 [193920/209539 (93%)]\tAll Loss: 1.4708\tTriple Loss(1): 0.0306\tClassification Loss: 1.4095\r\n",
      "Train Epoch: 20 [194560/209539 (93%)]\tAll Loss: 1.2931\tTriple Loss(1): 0.0292\tClassification Loss: 1.2347\r\n",
      "Train Epoch: 20 [195200/209539 (93%)]\tAll Loss: 1.4915\tTriple Loss(1): 0.0000\tClassification Loss: 1.4915\r\n",
      "Train Epoch: 20 [195840/209539 (93%)]\tAll Loss: 1.0703\tTriple Loss(1): 0.0570\tClassification Loss: 0.9562\r\n",
      "Train Epoch: 20 [196480/209539 (94%)]\tAll Loss: 1.5377\tTriple Loss(1): 0.0095\tClassification Loss: 1.5187\r\n",
      "Train Epoch: 20 [197120/209539 (94%)]\tAll Loss: 1.4469\tTriple Loss(1): 0.0000\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 20 [197760/209539 (94%)]\tAll Loss: 1.5649\tTriple Loss(1): 0.0421\tClassification Loss: 1.4808\r\n",
      "Train Epoch: 20 [198400/209539 (95%)]\tAll Loss: 1.4407\tTriple Loss(1): 0.0000\tClassification Loss: 1.4407\r\n",
      "Train Epoch: 20 [199040/209539 (95%)]\tAll Loss: 2.3550\tTriple Loss(0): 0.4069\tClassification Loss: 1.5413\r\n",
      "Train Epoch: 20 [199680/209539 (95%)]\tAll Loss: 1.6251\tTriple Loss(1): 0.0791\tClassification Loss: 1.4669\r\n",
      "Train Epoch: 20 [200320/209539 (96%)]\tAll Loss: 2.0513\tTriple Loss(0): 0.3298\tClassification Loss: 1.3916\r\n",
      "Train Epoch: 20 [200960/209539 (96%)]\tAll Loss: 1.4485\tTriple Loss(1): 0.1014\tClassification Loss: 1.2457\r\n",
      "Train Epoch: 20 [201600/209539 (96%)]\tAll Loss: 1.1887\tTriple Loss(1): 0.0010\tClassification Loss: 1.1867\r\n",
      "Train Epoch: 20 [202240/209539 (97%)]\tAll Loss: 1.3967\tTriple Loss(1): 0.0080\tClassification Loss: 1.3807\r\n",
      "Train Epoch: 20 [202880/209539 (97%)]\tAll Loss: 1.1973\tTriple Loss(1): 0.0733\tClassification Loss: 1.0508\r\n",
      "Train Epoch: 20 [203520/209539 (97%)]\tAll Loss: 1.3274\tTriple Loss(1): 0.0000\tClassification Loss: 1.3274\r\n",
      "Train Epoch: 20 [204160/209539 (97%)]\tAll Loss: 1.9459\tTriple Loss(1): 0.0553\tClassification Loss: 1.8353\r\n",
      "Train Epoch: 20 [204800/209539 (98%)]\tAll Loss: 1.6814\tTriple Loss(1): 0.0904\tClassification Loss: 1.5006\r\n",
      "Train Epoch: 20 [205440/209539 (98%)]\tAll Loss: 1.1804\tTriple Loss(1): 0.0761\tClassification Loss: 1.0281\r\n",
      "Train Epoch: 20 [206080/209539 (98%)]\tAll Loss: 1.5673\tTriple Loss(1): 0.0663\tClassification Loss: 1.4346\r\n",
      "Train Epoch: 20 [206720/209539 (99%)]\tAll Loss: 1.4533\tTriple Loss(1): 0.0280\tClassification Loss: 1.3974\r\n",
      "Train Epoch: 20 [207360/209539 (99%)]\tAll Loss: 1.2433\tTriple Loss(1): 0.0073\tClassification Loss: 1.2287\r\n",
      "Train Epoch: 20 [208000/209539 (99%)]\tAll Loss: 1.0983\tTriple Loss(1): 0.0000\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 20 [208640/209539 (100%)]\tAll Loss: 1.4675\tTriple Loss(1): 0.0251\tClassification Loss: 1.4172\r\n",
      "Train Epoch: 20 [209280/209539 (100%)]\tAll Loss: 1.5716\tTriple Loss(1): 0.0304\tClassification Loss: 1.5108\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/20_epochs\r\n",
      "Train Epoch: 21 [0/209539 (0%)]\tAll Loss: 2.4188\tTriple Loss(0): 0.4179\tClassification Loss: 1.5830\r\n",
      "\r\n",
      "Test set: Average loss: 1.1948\r\n",
      "Top 1 Accuracy: 52407/80128 (65%)\r\n",
      "Top 3 Accuracy: 68522/80128 (86%)\r\n",
      "Top 5 Accuracy: 73672/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 21 [640/209539 (0%)]\tAll Loss: 1.8088\tTriple Loss(1): 0.0673\tClassification Loss: 1.6742\r\n",
      "Train Epoch: 21 [1280/209539 (1%)]\tAll Loss: 1.3323\tTriple Loss(1): 0.0347\tClassification Loss: 1.2629\r\n",
      "Train Epoch: 21 [1920/209539 (1%)]\tAll Loss: 1.9904\tTriple Loss(0): 0.2588\tClassification Loss: 1.4728\r\n",
      "Train Epoch: 21 [2560/209539 (1%)]\tAll Loss: 1.5951\tTriple Loss(1): 0.0207\tClassification Loss: 1.5537\r\n",
      "Train Epoch: 21 [3200/209539 (2%)]\tAll Loss: 1.8485\tTriple Loss(1): 0.2169\tClassification Loss: 1.4147\r\n",
      "Train Epoch: 21 [3840/209539 (2%)]\tAll Loss: 1.4752\tTriple Loss(1): 0.0868\tClassification Loss: 1.3015\r\n",
      "Train Epoch: 21 [4480/209539 (2%)]\tAll Loss: 1.3706\tTriple Loss(1): 0.0214\tClassification Loss: 1.3278\r\n",
      "Train Epoch: 21 [5120/209539 (2%)]\tAll Loss: 1.4890\tTriple Loss(1): 0.0272\tClassification Loss: 1.4346\r\n",
      "Train Epoch: 21 [5760/209539 (3%)]\tAll Loss: 1.4901\tTriple Loss(1): 0.0161\tClassification Loss: 1.4580\r\n",
      "Train Epoch: 21 [6400/209539 (3%)]\tAll Loss: 1.0617\tTriple Loss(1): 0.0000\tClassification Loss: 1.0617\r\n",
      "Train Epoch: 21 [7040/209539 (3%)]\tAll Loss: 1.5541\tTriple Loss(1): 0.0588\tClassification Loss: 1.4366\r\n",
      "Train Epoch: 21 [7680/209539 (4%)]\tAll Loss: 1.0627\tTriple Loss(1): 0.0046\tClassification Loss: 1.0535\r\n",
      "Train Epoch: 21 [8320/209539 (4%)]\tAll Loss: 1.4681\tTriple Loss(1): 0.0542\tClassification Loss: 1.3598\r\n",
      "Train Epoch: 21 [8960/209539 (4%)]\tAll Loss: 2.1671\tTriple Loss(0): 0.3903\tClassification Loss: 1.3865\r\n",
      "Train Epoch: 21 [9600/209539 (5%)]\tAll Loss: 1.9824\tTriple Loss(0): 0.1746\tClassification Loss: 1.6332\r\n",
      "Train Epoch: 21 [10240/209539 (5%)]\tAll Loss: 1.2587\tTriple Loss(1): 0.0097\tClassification Loss: 1.2392\r\n",
      "Train Epoch: 21 [10880/209539 (5%)]\tAll Loss: 1.3309\tTriple Loss(1): 0.0675\tClassification Loss: 1.1958\r\n",
      "Train Epoch: 21 [11520/209539 (5%)]\tAll Loss: 1.4059\tTriple Loss(1): 0.0217\tClassification Loss: 1.3624\r\n",
      "Train Epoch: 21 [12160/209539 (6%)]\tAll Loss: 1.1098\tTriple Loss(1): 0.0235\tClassification Loss: 1.0629\r\n",
      "Train Epoch: 21 [12800/209539 (6%)]\tAll Loss: 1.0687\tTriple Loss(1): 0.0187\tClassification Loss: 1.0312\r\n",
      "Train Epoch: 21 [13440/209539 (6%)]\tAll Loss: 1.3384\tTriple Loss(1): 0.0275\tClassification Loss: 1.2833\r\n",
      "Train Epoch: 21 [14080/209539 (7%)]\tAll Loss: 1.4055\tTriple Loss(1): 0.0546\tClassification Loss: 1.2963\r\n",
      "Train Epoch: 21 [14720/209539 (7%)]\tAll Loss: 1.5055\tTriple Loss(1): 0.0297\tClassification Loss: 1.4462\r\n",
      "Train Epoch: 21 [15360/209539 (7%)]\tAll Loss: 1.3765\tTriple Loss(1): 0.0484\tClassification Loss: 1.2797\r\n",
      "Train Epoch: 21 [16000/209539 (8%)]\tAll Loss: 1.6083\tTriple Loss(1): 0.0000\tClassification Loss: 1.6083\r\n",
      "Train Epoch: 21 [16640/209539 (8%)]\tAll Loss: 1.7361\tTriple Loss(1): 0.0433\tClassification Loss: 1.6496\r\n",
      "Train Epoch: 21 [17280/209539 (8%)]\tAll Loss: 1.6329\tTriple Loss(1): 0.0465\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 21 [17920/209539 (9%)]\tAll Loss: 1.3554\tTriple Loss(1): 0.0079\tClassification Loss: 1.3395\r\n",
      "Train Epoch: 21 [18560/209539 (9%)]\tAll Loss: 1.2507\tTriple Loss(1): 0.0295\tClassification Loss: 1.1918\r\n",
      "Train Epoch: 21 [19200/209539 (9%)]\tAll Loss: 1.4131\tTriple Loss(1): 0.0576\tClassification Loss: 1.2979\r\n",
      "Train Epoch: 21 [19840/209539 (9%)]\tAll Loss: 1.4989\tTriple Loss(0): 0.1339\tClassification Loss: 1.2310\r\n",
      "Train Epoch: 21 [20480/209539 (10%)]\tAll Loss: 1.3596\tTriple Loss(1): 0.0738\tClassification Loss: 1.2119\r\n",
      "Train Epoch: 21 [21120/209539 (10%)]\tAll Loss: 1.7339\tTriple Loss(1): 0.1399\tClassification Loss: 1.4542\r\n",
      "Train Epoch: 21 [21760/209539 (10%)]\tAll Loss: 1.3522\tTriple Loss(1): 0.0594\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 21 [22400/209539 (11%)]\tAll Loss: 1.2461\tTriple Loss(1): 0.0296\tClassification Loss: 1.1870\r\n",
      "Train Epoch: 21 [23040/209539 (11%)]\tAll Loss: 1.3601\tTriple Loss(1): 0.0000\tClassification Loss: 1.3601\r\n",
      "Train Epoch: 21 [23680/209539 (11%)]\tAll Loss: 1.2230\tTriple Loss(1): 0.0210\tClassification Loss: 1.1810\r\n",
      "Train Epoch: 21 [24320/209539 (12%)]\tAll Loss: 1.4144\tTriple Loss(1): 0.0636\tClassification Loss: 1.2873\r\n",
      "Train Epoch: 21 [24960/209539 (12%)]\tAll Loss: 1.2129\tTriple Loss(1): 0.0535\tClassification Loss: 1.1058\r\n",
      "Train Epoch: 21 [25600/209539 (12%)]\tAll Loss: 1.3504\tTriple Loss(1): 0.0308\tClassification Loss: 1.2888\r\n",
      "Train Epoch: 21 [26240/209539 (13%)]\tAll Loss: 1.5992\tTriple Loss(1): 0.0771\tClassification Loss: 1.4450\r\n",
      "Train Epoch: 21 [26880/209539 (13%)]\tAll Loss: 1.8904\tTriple Loss(0): 0.2399\tClassification Loss: 1.4105\r\n",
      "Train Epoch: 21 [27520/209539 (13%)]\tAll Loss: 1.2986\tTriple Loss(1): 0.0360\tClassification Loss: 1.2266\r\n",
      "Train Epoch: 21 [28160/209539 (13%)]\tAll Loss: 1.4793\tTriple Loss(1): 0.0000\tClassification Loss: 1.4793\r\n",
      "Train Epoch: 21 [28800/209539 (14%)]\tAll Loss: 1.5949\tTriple Loss(1): 0.0044\tClassification Loss: 1.5862\r\n",
      "Train Epoch: 21 [29440/209539 (14%)]\tAll Loss: 1.6034\tTriple Loss(1): 0.0772\tClassification Loss: 1.4490\r\n",
      "Train Epoch: 21 [30080/209539 (14%)]\tAll Loss: 1.4559\tTriple Loss(1): 0.0597\tClassification Loss: 1.3366\r\n",
      "Train Epoch: 21 [30720/209539 (15%)]\tAll Loss: 1.9273\tTriple Loss(0): 0.3872\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 21 [31360/209539 (15%)]\tAll Loss: 1.1859\tTriple Loss(1): 0.0170\tClassification Loss: 1.1519\r\n",
      "Train Epoch: 21 [32000/209539 (15%)]\tAll Loss: 1.3482\tTriple Loss(1): 0.0068\tClassification Loss: 1.3347\r\n",
      "Train Epoch: 21 [32640/209539 (16%)]\tAll Loss: 2.5135\tTriple Loss(0): 0.5654\tClassification Loss: 1.3827\r\n",
      "Train Epoch: 21 [33280/209539 (16%)]\tAll Loss: 1.7304\tTriple Loss(1): 0.0425\tClassification Loss: 1.6455\r\n",
      "Train Epoch: 21 [33920/209539 (16%)]\tAll Loss: 2.2815\tTriple Loss(0): 0.4497\tClassification Loss: 1.3821\r\n",
      "Train Epoch: 21 [34560/209539 (16%)]\tAll Loss: 1.3605\tTriple Loss(1): 0.0328\tClassification Loss: 1.2950\r\n",
      "Train Epoch: 21 [35200/209539 (17%)]\tAll Loss: 1.1370\tTriple Loss(1): 0.0205\tClassification Loss: 1.0960\r\n",
      "Train Epoch: 21 [35840/209539 (17%)]\tAll Loss: 1.1979\tTriple Loss(1): 0.0146\tClassification Loss: 1.1688\r\n",
      "Train Epoch: 21 [36480/209539 (17%)]\tAll Loss: 1.1729\tTriple Loss(1): 0.0933\tClassification Loss: 0.9863\r\n",
      "Train Epoch: 21 [37120/209539 (18%)]\tAll Loss: 1.6442\tTriple Loss(1): 0.0105\tClassification Loss: 1.6231\r\n",
      "Train Epoch: 21 [37760/209539 (18%)]\tAll Loss: 1.4281\tTriple Loss(1): 0.0166\tClassification Loss: 1.3949\r\n",
      "Train Epoch: 21 [38400/209539 (18%)]\tAll Loss: 1.8247\tTriple Loss(1): 0.1222\tClassification Loss: 1.5802\r\n",
      "Train Epoch: 21 [39040/209539 (19%)]\tAll Loss: 1.0443\tTriple Loss(1): 0.0059\tClassification Loss: 1.0325\r\n",
      "Train Epoch: 21 [39680/209539 (19%)]\tAll Loss: 1.2683\tTriple Loss(1): 0.0151\tClassification Loss: 1.2382\r\n",
      "Train Epoch: 21 [40320/209539 (19%)]\tAll Loss: 1.5150\tTriple Loss(1): 0.0041\tClassification Loss: 1.5067\r\n",
      "Train Epoch: 21 [40960/209539 (20%)]\tAll Loss: 1.0623\tTriple Loss(1): 0.0173\tClassification Loss: 1.0277\r\n",
      "Train Epoch: 21 [41600/209539 (20%)]\tAll Loss: 1.3792\tTriple Loss(1): 0.0420\tClassification Loss: 1.2952\r\n",
      "Train Epoch: 21 [42240/209539 (20%)]\tAll Loss: 1.3484\tTriple Loss(1): 0.0777\tClassification Loss: 1.1929\r\n",
      "Train Epoch: 21 [42880/209539 (20%)]\tAll Loss: 1.0254\tTriple Loss(1): 0.0198\tClassification Loss: 0.9858\r\n",
      "Train Epoch: 21 [43520/209539 (21%)]\tAll Loss: 2.0725\tTriple Loss(0): 0.3040\tClassification Loss: 1.4644\r\n",
      "Train Epoch: 21 [44160/209539 (21%)]\tAll Loss: 1.6802\tTriple Loss(1): 0.0578\tClassification Loss: 1.5646\r\n",
      "Train Epoch: 21 [44800/209539 (21%)]\tAll Loss: 1.6714\tTriple Loss(1): 0.0441\tClassification Loss: 1.5831\r\n",
      "Train Epoch: 21 [45440/209539 (22%)]\tAll Loss: 1.9618\tTriple Loss(1): 0.0000\tClassification Loss: 1.9618\r\n",
      "Train Epoch: 21 [46080/209539 (22%)]\tAll Loss: 1.2029\tTriple Loss(1): 0.0221\tClassification Loss: 1.1586\r\n",
      "Train Epoch: 21 [46720/209539 (22%)]\tAll Loss: 2.4092\tTriple Loss(0): 0.3421\tClassification Loss: 1.7249\r\n",
      "Train Epoch: 21 [47360/209539 (23%)]\tAll Loss: 0.9973\tTriple Loss(1): 0.0094\tClassification Loss: 0.9786\r\n",
      "Train Epoch: 21 [48000/209539 (23%)]\tAll Loss: 1.3493\tTriple Loss(1): 0.0063\tClassification Loss: 1.3368\r\n",
      "Train Epoch: 21 [48640/209539 (23%)]\tAll Loss: 1.6542\tTriple Loss(1): 0.0277\tClassification Loss: 1.5987\r\n",
      "Train Epoch: 21 [49280/209539 (24%)]\tAll Loss: 1.3906\tTriple Loss(1): 0.0442\tClassification Loss: 1.3022\r\n",
      "Train Epoch: 21 [49920/209539 (24%)]\tAll Loss: 1.4998\tTriple Loss(1): 0.0181\tClassification Loss: 1.4636\r\n",
      "Train Epoch: 21 [50560/209539 (24%)]\tAll Loss: 1.7352\tTriple Loss(1): 0.0974\tClassification Loss: 1.5404\r\n",
      "Train Epoch: 21 [51200/209539 (24%)]\tAll Loss: 2.6277\tTriple Loss(0): 0.5653\tClassification Loss: 1.4971\r\n",
      "Train Epoch: 21 [51840/209539 (25%)]\tAll Loss: 1.3634\tTriple Loss(1): 0.1060\tClassification Loss: 1.1513\r\n",
      "Train Epoch: 21 [52480/209539 (25%)]\tAll Loss: 1.4358\tTriple Loss(1): 0.0460\tClassification Loss: 1.3438\r\n",
      "Train Epoch: 21 [53120/209539 (25%)]\tAll Loss: 1.3910\tTriple Loss(1): 0.0390\tClassification Loss: 1.3130\r\n",
      "Train Epoch: 21 [53760/209539 (26%)]\tAll Loss: 1.5469\tTriple Loss(1): 0.0226\tClassification Loss: 1.5017\r\n",
      "Train Epoch: 21 [54400/209539 (26%)]\tAll Loss: 2.6445\tTriple Loss(0): 0.3903\tClassification Loss: 1.8639\r\n",
      "Train Epoch: 21 [55040/209539 (26%)]\tAll Loss: 1.3291\tTriple Loss(1): 0.0588\tClassification Loss: 1.2115\r\n",
      "Train Epoch: 21 [55680/209539 (27%)]\tAll Loss: 1.7275\tTriple Loss(1): 0.0748\tClassification Loss: 1.5780\r\n",
      "Train Epoch: 21 [56320/209539 (27%)]\tAll Loss: 1.0915\tTriple Loss(1): 0.0506\tClassification Loss: 0.9903\r\n",
      "Train Epoch: 21 [56960/209539 (27%)]\tAll Loss: 1.3214\tTriple Loss(1): 0.0306\tClassification Loss: 1.2602\r\n",
      "Train Epoch: 21 [57600/209539 (27%)]\tAll Loss: 1.3119\tTriple Loss(1): 0.0659\tClassification Loss: 1.1801\r\n",
      "Train Epoch: 21 [58240/209539 (28%)]\tAll Loss: 1.3029\tTriple Loss(1): 0.0780\tClassification Loss: 1.1468\r\n",
      "Train Epoch: 21 [58880/209539 (28%)]\tAll Loss: 1.4045\tTriple Loss(1): 0.0633\tClassification Loss: 1.2778\r\n",
      "Train Epoch: 21 [59520/209539 (28%)]\tAll Loss: 1.4219\tTriple Loss(1): 0.0591\tClassification Loss: 1.3038\r\n",
      "Train Epoch: 21 [60160/209539 (29%)]\tAll Loss: 1.5854\tTriple Loss(1): 0.0336\tClassification Loss: 1.5183\r\n",
      "Train Epoch: 21 [60800/209539 (29%)]\tAll Loss: 1.5371\tTriple Loss(1): 0.0294\tClassification Loss: 1.4783\r\n",
      "Train Epoch: 21 [61440/209539 (29%)]\tAll Loss: 1.3771\tTriple Loss(1): 0.0928\tClassification Loss: 1.1915\r\n",
      "Train Epoch: 21 [62080/209539 (30%)]\tAll Loss: 1.4302\tTriple Loss(1): 0.0252\tClassification Loss: 1.3798\r\n",
      "Train Epoch: 21 [62720/209539 (30%)]\tAll Loss: 1.2651\tTriple Loss(1): 0.0008\tClassification Loss: 1.2635\r\n",
      "Train Epoch: 21 [63360/209539 (30%)]\tAll Loss: 2.3678\tTriple Loss(0): 0.4397\tClassification Loss: 1.4883\r\n",
      "Train Epoch: 21 [64000/209539 (31%)]\tAll Loss: 1.6505\tTriple Loss(1): 0.0943\tClassification Loss: 1.4620\r\n",
      "Train Epoch: 21 [64640/209539 (31%)]\tAll Loss: 1.9608\tTriple Loss(1): 0.0980\tClassification Loss: 1.7648\r\n",
      "Train Epoch: 21 [65280/209539 (31%)]\tAll Loss: 1.6026\tTriple Loss(1): 0.0427\tClassification Loss: 1.5172\r\n",
      "Train Epoch: 21 [65920/209539 (31%)]\tAll Loss: 1.5634\tTriple Loss(1): 0.0269\tClassification Loss: 1.5095\r\n",
      "Train Epoch: 21 [66560/209539 (32%)]\tAll Loss: 1.4817\tTriple Loss(1): 0.0188\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 21 [67200/209539 (32%)]\tAll Loss: 1.4556\tTriple Loss(1): 0.0510\tClassification Loss: 1.3536\r\n",
      "Train Epoch: 21 [67840/209539 (32%)]\tAll Loss: 1.8075\tTriple Loss(1): 0.1484\tClassification Loss: 1.5107\r\n",
      "Train Epoch: 21 [68480/209539 (33%)]\tAll Loss: 1.5486\tTriple Loss(1): 0.0488\tClassification Loss: 1.4510\r\n",
      "Train Epoch: 21 [69120/209539 (33%)]\tAll Loss: 2.0717\tTriple Loss(0): 0.2859\tClassification Loss: 1.4999\r\n",
      "Train Epoch: 21 [69760/209539 (33%)]\tAll Loss: 1.1923\tTriple Loss(1): 0.0515\tClassification Loss: 1.0894\r\n",
      "Train Epoch: 21 [70400/209539 (34%)]\tAll Loss: 1.3594\tTriple Loss(1): 0.0264\tClassification Loss: 1.3067\r\n",
      "Train Epoch: 21 [71040/209539 (34%)]\tAll Loss: 1.7336\tTriple Loss(1): 0.0099\tClassification Loss: 1.7139\r\n",
      "Train Epoch: 21 [71680/209539 (34%)]\tAll Loss: 1.6662\tTriple Loss(1): 0.0185\tClassification Loss: 1.6291\r\n",
      "Train Epoch: 21 [72320/209539 (35%)]\tAll Loss: 1.2850\tTriple Loss(1): 0.0589\tClassification Loss: 1.1673\r\n",
      "Train Epoch: 21 [72960/209539 (35%)]\tAll Loss: 1.2080\tTriple Loss(1): 0.0000\tClassification Loss: 1.2080\r\n",
      "Train Epoch: 21 [73600/209539 (35%)]\tAll Loss: 1.8296\tTriple Loss(0): 0.1865\tClassification Loss: 1.4565\r\n",
      "Train Epoch: 21 [74240/209539 (35%)]\tAll Loss: 2.2378\tTriple Loss(0): 0.3195\tClassification Loss: 1.5988\r\n",
      "Train Epoch: 21 [74880/209539 (36%)]\tAll Loss: 1.8220\tTriple Loss(1): 0.0000\tClassification Loss: 1.8220\r\n",
      "Train Epoch: 21 [75520/209539 (36%)]\tAll Loss: 1.9521\tTriple Loss(0): 0.4214\tClassification Loss: 1.1093\r\n",
      "Train Epoch: 21 [76160/209539 (36%)]\tAll Loss: 1.2443\tTriple Loss(1): 0.0207\tClassification Loss: 1.2028\r\n",
      "Train Epoch: 21 [76800/209539 (37%)]\tAll Loss: 1.3082\tTriple Loss(1): 0.0093\tClassification Loss: 1.2896\r\n",
      "Train Epoch: 21 [77440/209539 (37%)]\tAll Loss: 1.6384\tTriple Loss(1): 0.0814\tClassification Loss: 1.4756\r\n",
      "Train Epoch: 21 [78080/209539 (37%)]\tAll Loss: 1.3842\tTriple Loss(1): 0.0086\tClassification Loss: 1.3670\r\n",
      "Train Epoch: 21 [78720/209539 (38%)]\tAll Loss: 1.6180\tTriple Loss(1): 0.0000\tClassification Loss: 1.6180\r\n",
      "Train Epoch: 21 [79360/209539 (38%)]\tAll Loss: 1.2998\tTriple Loss(1): 0.0514\tClassification Loss: 1.1969\r\n",
      "Train Epoch: 21 [80000/209539 (38%)]\tAll Loss: 1.5055\tTriple Loss(1): 0.0363\tClassification Loss: 1.4329\r\n",
      "Train Epoch: 21 [80640/209539 (38%)]\tAll Loss: 1.3254\tTriple Loss(1): 0.0240\tClassification Loss: 1.2774\r\n",
      "Train Epoch: 21 [81280/209539 (39%)]\tAll Loss: 1.5029\tTriple Loss(1): 0.0132\tClassification Loss: 1.4765\r\n",
      "Train Epoch: 21 [81920/209539 (39%)]\tAll Loss: 1.2604\tTriple Loss(1): 0.0025\tClassification Loss: 1.2555\r\n",
      "Train Epoch: 21 [82560/209539 (39%)]\tAll Loss: 2.5761\tTriple Loss(0): 0.4944\tClassification Loss: 1.5873\r\n",
      "Train Epoch: 21 [83200/209539 (40%)]\tAll Loss: 1.4012\tTriple Loss(1): 0.0165\tClassification Loss: 1.3683\r\n",
      "Train Epoch: 21 [83840/209539 (40%)]\tAll Loss: 1.5185\tTriple Loss(1): 0.0528\tClassification Loss: 1.4129\r\n",
      "Train Epoch: 21 [84480/209539 (40%)]\tAll Loss: 1.2731\tTriple Loss(1): 0.0342\tClassification Loss: 1.2047\r\n",
      "Train Epoch: 21 [85120/209539 (41%)]\tAll Loss: 1.9567\tTriple Loss(1): 0.0604\tClassification Loss: 1.8358\r\n",
      "Train Epoch: 21 [85760/209539 (41%)]\tAll Loss: 1.5436\tTriple Loss(1): 0.0468\tClassification Loss: 1.4500\r\n",
      "Train Epoch: 21 [86400/209539 (41%)]\tAll Loss: 1.4444\tTriple Loss(1): 0.0026\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 21 [87040/209539 (42%)]\tAll Loss: 1.3345\tTriple Loss(1): 0.0313\tClassification Loss: 1.2719\r\n",
      "Train Epoch: 21 [87680/209539 (42%)]\tAll Loss: 1.4452\tTriple Loss(1): 0.0506\tClassification Loss: 1.3439\r\n",
      "Train Epoch: 21 [88320/209539 (42%)]\tAll Loss: 1.6839\tTriple Loss(1): 0.1221\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 21 [88960/209539 (42%)]\tAll Loss: 1.4832\tTriple Loss(1): 0.0337\tClassification Loss: 1.4158\r\n",
      "Train Epoch: 21 [89600/209539 (43%)]\tAll Loss: 1.5519\tTriple Loss(1): 0.0000\tClassification Loss: 1.5519\r\n",
      "Train Epoch: 21 [90240/209539 (43%)]\tAll Loss: 1.3712\tTriple Loss(1): 0.0507\tClassification Loss: 1.2698\r\n",
      "Train Epoch: 21 [90880/209539 (43%)]\tAll Loss: 1.7867\tTriple Loss(1): 0.0498\tClassification Loss: 1.6870\r\n",
      "Train Epoch: 21 [91520/209539 (44%)]\tAll Loss: 2.1889\tTriple Loss(0): 0.4391\tClassification Loss: 1.3107\r\n",
      "Train Epoch: 21 [92160/209539 (44%)]\tAll Loss: 1.4162\tTriple Loss(1): 0.0117\tClassification Loss: 1.3928\r\n",
      "Train Epoch: 21 [92800/209539 (44%)]\tAll Loss: 1.3027\tTriple Loss(1): 0.0072\tClassification Loss: 1.2883\r\n",
      "Train Epoch: 21 [93440/209539 (45%)]\tAll Loss: 1.5090\tTriple Loss(1): 0.0440\tClassification Loss: 1.4211\r\n",
      "Train Epoch: 21 [94080/209539 (45%)]\tAll Loss: 1.2964\tTriple Loss(1): 0.0151\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 21 [94720/209539 (45%)]\tAll Loss: 2.0485\tTriple Loss(0): 0.3108\tClassification Loss: 1.4270\r\n",
      "Train Epoch: 21 [95360/209539 (46%)]\tAll Loss: 1.4219\tTriple Loss(1): 0.0206\tClassification Loss: 1.3808\r\n",
      "Train Epoch: 21 [96000/209539 (46%)]\tAll Loss: 1.5367\tTriple Loss(1): 0.0733\tClassification Loss: 1.3901\r\n",
      "Train Epoch: 21 [96640/209539 (46%)]\tAll Loss: 1.6848\tTriple Loss(1): 0.0444\tClassification Loss: 1.5960\r\n",
      "Train Epoch: 21 [97280/209539 (46%)]\tAll Loss: 1.5817\tTriple Loss(1): 0.1448\tClassification Loss: 1.2922\r\n",
      "Train Epoch: 21 [97920/209539 (47%)]\tAll Loss: 1.6228\tTriple Loss(1): 0.0028\tClassification Loss: 1.6173\r\n",
      "Train Epoch: 21 [98560/209539 (47%)]\tAll Loss: 1.8569\tTriple Loss(1): 0.0484\tClassification Loss: 1.7601\r\n",
      "Train Epoch: 21 [99200/209539 (47%)]\tAll Loss: 1.5622\tTriple Loss(1): 0.1362\tClassification Loss: 1.2898\r\n",
      "Train Epoch: 21 [99840/209539 (48%)]\tAll Loss: 1.6342\tTriple Loss(1): 0.0558\tClassification Loss: 1.5226\r\n",
      "Train Epoch: 21 [100480/209539 (48%)]\tAll Loss: 1.7048\tTriple Loss(0): 0.1787\tClassification Loss: 1.3473\r\n",
      "Train Epoch: 21 [101120/209539 (48%)]\tAll Loss: 1.5548\tTriple Loss(1): 0.0762\tClassification Loss: 1.4024\r\n",
      "Train Epoch: 21 [101760/209539 (49%)]\tAll Loss: 1.2908\tTriple Loss(1): 0.0402\tClassification Loss: 1.2104\r\n",
      "Train Epoch: 21 [102400/209539 (49%)]\tAll Loss: 1.1797\tTriple Loss(1): 0.0000\tClassification Loss: 1.1797\r\n",
      "Train Epoch: 21 [103040/209539 (49%)]\tAll Loss: 1.2951\tTriple Loss(1): 0.0389\tClassification Loss: 1.2173\r\n",
      "Train Epoch: 21 [103680/209539 (49%)]\tAll Loss: 1.6191\tTriple Loss(1): 0.1011\tClassification Loss: 1.4170\r\n",
      "Train Epoch: 21 [104320/209539 (50%)]\tAll Loss: 1.9862\tTriple Loss(0): 0.3471\tClassification Loss: 1.2919\r\n",
      "Train Epoch: 21 [104960/209539 (50%)]\tAll Loss: 1.3275\tTriple Loss(1): 0.0329\tClassification Loss: 1.2617\r\n",
      "Train Epoch: 21 [105600/209539 (50%)]\tAll Loss: 2.0455\tTriple Loss(0): 0.3206\tClassification Loss: 1.4042\r\n",
      "Train Epoch: 21 [106240/209539 (51%)]\tAll Loss: 1.6047\tTriple Loss(1): 0.1326\tClassification Loss: 1.3396\r\n",
      "Train Epoch: 21 [106880/209539 (51%)]\tAll Loss: 1.6988\tTriple Loss(0): 0.2235\tClassification Loss: 1.2519\r\n",
      "Train Epoch: 21 [107520/209539 (51%)]\tAll Loss: 1.3776\tTriple Loss(1): 0.0329\tClassification Loss: 1.3119\r\n",
      "Train Epoch: 21 [108160/209539 (52%)]\tAll Loss: 1.2871\tTriple Loss(1): 0.0311\tClassification Loss: 1.2249\r\n",
      "Train Epoch: 21 [108800/209539 (52%)]\tAll Loss: 2.1602\tTriple Loss(0): 0.4309\tClassification Loss: 1.2985\r\n",
      "Train Epoch: 21 [109440/209539 (52%)]\tAll Loss: 2.4184\tTriple Loss(0): 0.3797\tClassification Loss: 1.6591\r\n",
      "Train Epoch: 21 [110080/209539 (53%)]\tAll Loss: 1.5663\tTriple Loss(1): 0.0574\tClassification Loss: 1.4514\r\n",
      "Train Epoch: 21 [110720/209539 (53%)]\tAll Loss: 1.4951\tTriple Loss(1): 0.0811\tClassification Loss: 1.3329\r\n",
      "Train Epoch: 21 [111360/209539 (53%)]\tAll Loss: 1.2315\tTriple Loss(1): 0.0467\tClassification Loss: 1.1381\r\n",
      "Train Epoch: 21 [112000/209539 (53%)]\tAll Loss: 1.3964\tTriple Loss(1): 0.0598\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 21 [112640/209539 (54%)]\tAll Loss: 1.3721\tTriple Loss(1): 0.0333\tClassification Loss: 1.3054\r\n",
      "Train Epoch: 21 [113280/209539 (54%)]\tAll Loss: 1.1796\tTriple Loss(1): 0.0465\tClassification Loss: 1.0867\r\n",
      "Train Epoch: 21 [113920/209539 (54%)]\tAll Loss: 1.3397\tTriple Loss(1): 0.0000\tClassification Loss: 1.3397\r\n",
      "Train Epoch: 21 [114560/209539 (55%)]\tAll Loss: 1.4795\tTriple Loss(1): 0.0376\tClassification Loss: 1.4043\r\n",
      "Train Epoch: 21 [115200/209539 (55%)]\tAll Loss: 1.7128\tTriple Loss(1): 0.1199\tClassification Loss: 1.4730\r\n",
      "Train Epoch: 21 [115840/209539 (55%)]\tAll Loss: 2.3907\tTriple Loss(0): 0.5332\tClassification Loss: 1.3244\r\n",
      "Train Epoch: 21 [116480/209539 (56%)]\tAll Loss: 1.7359\tTriple Loss(1): 0.0751\tClassification Loss: 1.5858\r\n",
      "Train Epoch: 21 [117120/209539 (56%)]\tAll Loss: 2.3142\tTriple Loss(0): 0.3164\tClassification Loss: 1.6814\r\n",
      "Train Epoch: 21 [117760/209539 (56%)]\tAll Loss: 1.4194\tTriple Loss(1): 0.0619\tClassification Loss: 1.2957\r\n",
      "Train Epoch: 21 [118400/209539 (57%)]\tAll Loss: 1.0702\tTriple Loss(1): 0.0101\tClassification Loss: 1.0499\r\n",
      "Train Epoch: 21 [119040/209539 (57%)]\tAll Loss: 2.3841\tTriple Loss(0): 0.4174\tClassification Loss: 1.5494\r\n",
      "Train Epoch: 21 [119680/209539 (57%)]\tAll Loss: 1.9704\tTriple Loss(0): 0.3204\tClassification Loss: 1.3296\r\n",
      "Train Epoch: 21 [120320/209539 (57%)]\tAll Loss: 1.4331\tTriple Loss(1): 0.0312\tClassification Loss: 1.3707\r\n",
      "Train Epoch: 21 [120960/209539 (58%)]\tAll Loss: 1.2978\tTriple Loss(1): 0.0252\tClassification Loss: 1.2475\r\n",
      "Train Epoch: 21 [121600/209539 (58%)]\tAll Loss: 1.3006\tTriple Loss(1): 0.0018\tClassification Loss: 1.2970\r\n",
      "Train Epoch: 21 [122240/209539 (58%)]\tAll Loss: 1.3257\tTriple Loss(1): 0.1013\tClassification Loss: 1.1231\r\n",
      "Train Epoch: 21 [122880/209539 (59%)]\tAll Loss: 1.3498\tTriple Loss(1): 0.0739\tClassification Loss: 1.2021\r\n",
      "Train Epoch: 21 [123520/209539 (59%)]\tAll Loss: 2.2952\tTriple Loss(0): 0.4088\tClassification Loss: 1.4776\r\n",
      "Train Epoch: 21 [124160/209539 (59%)]\tAll Loss: 1.6520\tTriple Loss(1): 0.1199\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 21 [124800/209539 (60%)]\tAll Loss: 1.3344\tTriple Loss(1): 0.0242\tClassification Loss: 1.2861\r\n",
      "Train Epoch: 21 [125440/209539 (60%)]\tAll Loss: 1.3670\tTriple Loss(1): 0.0613\tClassification Loss: 1.2443\r\n",
      "Train Epoch: 21 [126080/209539 (60%)]\tAll Loss: 2.0522\tTriple Loss(0): 0.3324\tClassification Loss: 1.3873\r\n",
      "Train Epoch: 21 [126720/209539 (60%)]\tAll Loss: 2.4920\tTriple Loss(0): 0.4232\tClassification Loss: 1.6457\r\n",
      "Train Epoch: 21 [127360/209539 (61%)]\tAll Loss: 1.4523\tTriple Loss(1): 0.0000\tClassification Loss: 1.4523\r\n",
      "Train Epoch: 21 [128000/209539 (61%)]\tAll Loss: 1.6919\tTriple Loss(1): 0.0712\tClassification Loss: 1.5495\r\n",
      "Train Epoch: 21 [128640/209539 (61%)]\tAll Loss: 1.8584\tTriple Loss(0): 0.3909\tClassification Loss: 1.0767\r\n",
      "Train Epoch: 21 [129280/209539 (62%)]\tAll Loss: 1.6081\tTriple Loss(1): 0.0206\tClassification Loss: 1.5668\r\n",
      "Train Epoch: 21 [129920/209539 (62%)]\tAll Loss: 2.4742\tTriple Loss(0): 0.6424\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 21 [130560/209539 (62%)]\tAll Loss: 1.1810\tTriple Loss(1): 0.0408\tClassification Loss: 1.0994\r\n",
      "Train Epoch: 21 [131200/209539 (63%)]\tAll Loss: 1.3853\tTriple Loss(1): 0.0366\tClassification Loss: 1.3121\r\n",
      "Train Epoch: 21 [131840/209539 (63%)]\tAll Loss: 1.3768\tTriple Loss(1): 0.0198\tClassification Loss: 1.3373\r\n",
      "Train Epoch: 21 [132480/209539 (63%)]\tAll Loss: 1.1935\tTriple Loss(1): 0.0717\tClassification Loss: 1.0500\r\n",
      "Train Epoch: 21 [133120/209539 (64%)]\tAll Loss: 1.1407\tTriple Loss(1): 0.0000\tClassification Loss: 1.1407\r\n",
      "Train Epoch: 21 [133760/209539 (64%)]\tAll Loss: 1.9984\tTriple Loss(0): 0.3422\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 21 [134400/209539 (64%)]\tAll Loss: 1.2413\tTriple Loss(1): 0.0496\tClassification Loss: 1.1420\r\n",
      "Train Epoch: 21 [135040/209539 (64%)]\tAll Loss: 2.1827\tTriple Loss(0): 0.3151\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 21 [135680/209539 (65%)]\tAll Loss: 1.7119\tTriple Loss(1): 0.0444\tClassification Loss: 1.6232\r\n",
      "Train Epoch: 21 [136320/209539 (65%)]\tAll Loss: 1.4829\tTriple Loss(1): 0.0174\tClassification Loss: 1.4480\r\n",
      "Train Epoch: 21 [136960/209539 (65%)]\tAll Loss: 1.3380\tTriple Loss(1): 0.0385\tClassification Loss: 1.2610\r\n",
      "Train Epoch: 21 [137600/209539 (66%)]\tAll Loss: 1.4370\tTriple Loss(1): 0.0304\tClassification Loss: 1.3762\r\n",
      "Train Epoch: 21 [138240/209539 (66%)]\tAll Loss: 1.5610\tTriple Loss(1): 0.0286\tClassification Loss: 1.5038\r\n",
      "Train Epoch: 21 [138880/209539 (66%)]\tAll Loss: 1.3411\tTriple Loss(1): 0.0000\tClassification Loss: 1.3411\r\n",
      "Train Epoch: 21 [139520/209539 (67%)]\tAll Loss: 1.3684\tTriple Loss(1): 0.0432\tClassification Loss: 1.2820\r\n",
      "Train Epoch: 21 [140160/209539 (67%)]\tAll Loss: 1.3419\tTriple Loss(1): 0.0199\tClassification Loss: 1.3021\r\n",
      "Train Epoch: 21 [140800/209539 (67%)]\tAll Loss: 1.6953\tTriple Loss(1): 0.0641\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 21 [141440/209539 (68%)]\tAll Loss: 1.4456\tTriple Loss(1): 0.0244\tClassification Loss: 1.3967\r\n",
      "Train Epoch: 21 [142080/209539 (68%)]\tAll Loss: 1.3105\tTriple Loss(1): 0.0260\tClassification Loss: 1.2585\r\n",
      "Train Epoch: 21 [142720/209539 (68%)]\tAll Loss: 1.7783\tTriple Loss(1): 0.1019\tClassification Loss: 1.5745\r\n",
      "Train Epoch: 21 [143360/209539 (68%)]\tAll Loss: 1.4008\tTriple Loss(0): 0.1881\tClassification Loss: 1.0246\r\n",
      "Train Epoch: 21 [144000/209539 (69%)]\tAll Loss: 1.4034\tTriple Loss(1): 0.0660\tClassification Loss: 1.2714\r\n",
      "Train Epoch: 21 [144640/209539 (69%)]\tAll Loss: 1.2749\tTriple Loss(1): 0.0116\tClassification Loss: 1.2517\r\n",
      "Train Epoch: 21 [145280/209539 (69%)]\tAll Loss: 2.7518\tTriple Loss(0): 0.5647\tClassification Loss: 1.6223\r\n",
      "Train Epoch: 21 [145920/209539 (70%)]\tAll Loss: 1.5569\tTriple Loss(1): 0.0452\tClassification Loss: 1.4664\r\n",
      "Train Epoch: 21 [146560/209539 (70%)]\tAll Loss: 1.0585\tTriple Loss(1): 0.0334\tClassification Loss: 0.9918\r\n",
      "Train Epoch: 21 [147200/209539 (70%)]\tAll Loss: 1.5775\tTriple Loss(1): 0.0254\tClassification Loss: 1.5266\r\n",
      "Train Epoch: 21 [147840/209539 (71%)]\tAll Loss: 1.1853\tTriple Loss(1): 0.0263\tClassification Loss: 1.1328\r\n",
      "Train Epoch: 21 [148480/209539 (71%)]\tAll Loss: 1.2723\tTriple Loss(1): 0.0783\tClassification Loss: 1.1157\r\n",
      "Train Epoch: 21 [149120/209539 (71%)]\tAll Loss: 1.6018\tTriple Loss(1): 0.0134\tClassification Loss: 1.5751\r\n",
      "Train Epoch: 21 [149760/209539 (71%)]\tAll Loss: 1.1554\tTriple Loss(1): 0.0239\tClassification Loss: 1.1076\r\n",
      "Train Epoch: 21 [150400/209539 (72%)]\tAll Loss: 1.8020\tTriple Loss(1): 0.1085\tClassification Loss: 1.5849\r\n",
      "Train Epoch: 21 [151040/209539 (72%)]\tAll Loss: 1.5770\tTriple Loss(1): 0.0439\tClassification Loss: 1.4892\r\n",
      "Train Epoch: 21 [151680/209539 (72%)]\tAll Loss: 1.5838\tTriple Loss(1): 0.0657\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 21 [152320/209539 (73%)]\tAll Loss: 1.2599\tTriple Loss(1): 0.0569\tClassification Loss: 1.1462\r\n",
      "Train Epoch: 21 [152960/209539 (73%)]\tAll Loss: 1.3237\tTriple Loss(1): 0.0447\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 21 [153600/209539 (73%)]\tAll Loss: 1.4579\tTriple Loss(1): 0.0712\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 21 [154240/209539 (74%)]\tAll Loss: 1.3909\tTriple Loss(1): 0.0557\tClassification Loss: 1.2794\r\n",
      "Train Epoch: 21 [154880/209539 (74%)]\tAll Loss: 1.6562\tTriple Loss(1): 0.1094\tClassification Loss: 1.4373\r\n",
      "Train Epoch: 21 [155520/209539 (74%)]\tAll Loss: 1.4928\tTriple Loss(1): 0.0282\tClassification Loss: 1.4364\r\n",
      "Train Epoch: 21 [156160/209539 (75%)]\tAll Loss: 1.4043\tTriple Loss(1): 0.0106\tClassification Loss: 1.3832\r\n",
      "Train Epoch: 21 [156800/209539 (75%)]\tAll Loss: 2.0050\tTriple Loss(1): 0.0842\tClassification Loss: 1.8367\r\n",
      "Train Epoch: 21 [157440/209539 (75%)]\tAll Loss: 1.5863\tTriple Loss(1): 0.0385\tClassification Loss: 1.5092\r\n",
      "Train Epoch: 21 [158080/209539 (75%)]\tAll Loss: 1.7310\tTriple Loss(0): 0.2950\tClassification Loss: 1.1410\r\n",
      "Train Epoch: 21 [158720/209539 (76%)]\tAll Loss: 1.2928\tTriple Loss(1): 0.1041\tClassification Loss: 1.0845\r\n",
      "Train Epoch: 21 [159360/209539 (76%)]\tAll Loss: 1.1583\tTriple Loss(1): 0.0028\tClassification Loss: 1.1527\r\n",
      "Train Epoch: 21 [160000/209539 (76%)]\tAll Loss: 2.5069\tTriple Loss(0): 0.4791\tClassification Loss: 1.5487\r\n",
      "Train Epoch: 21 [160640/209539 (77%)]\tAll Loss: 1.2935\tTriple Loss(1): 0.0740\tClassification Loss: 1.1455\r\n",
      "Train Epoch: 21 [161280/209539 (77%)]\tAll Loss: 1.7612\tTriple Loss(0): 0.2847\tClassification Loss: 1.1917\r\n",
      "Train Epoch: 21 [161920/209539 (77%)]\tAll Loss: 1.4741\tTriple Loss(1): 0.0837\tClassification Loss: 1.3066\r\n",
      "Train Epoch: 21 [162560/209539 (78%)]\tAll Loss: 1.3210\tTriple Loss(1): 0.0275\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 21 [163200/209539 (78%)]\tAll Loss: 1.3977\tTriple Loss(1): 0.0353\tClassification Loss: 1.3271\r\n",
      "Train Epoch: 21 [163840/209539 (78%)]\tAll Loss: 1.5813\tTriple Loss(1): 0.0821\tClassification Loss: 1.4172\r\n",
      "Train Epoch: 21 [164480/209539 (78%)]\tAll Loss: 1.1382\tTriple Loss(1): 0.0145\tClassification Loss: 1.1093\r\n",
      "Train Epoch: 21 [165120/209539 (79%)]\tAll Loss: 1.2164\tTriple Loss(1): 0.0531\tClassification Loss: 1.1103\r\n",
      "Train Epoch: 21 [165760/209539 (79%)]\tAll Loss: 1.9794\tTriple Loss(0): 0.3065\tClassification Loss: 1.3664\r\n",
      "Train Epoch: 21 [166400/209539 (79%)]\tAll Loss: 1.3284\tTriple Loss(1): 0.0179\tClassification Loss: 1.2926\r\n",
      "Train Epoch: 21 [167040/209539 (80%)]\tAll Loss: 2.6055\tTriple Loss(0): 0.5217\tClassification Loss: 1.5621\r\n",
      "Train Epoch: 21 [167680/209539 (80%)]\tAll Loss: 1.5928\tTriple Loss(1): 0.0024\tClassification Loss: 1.5879\r\n",
      "Train Epoch: 21 [168320/209539 (80%)]\tAll Loss: 1.2308\tTriple Loss(1): 0.0000\tClassification Loss: 1.2307\r\n",
      "Train Epoch: 21 [168960/209539 (81%)]\tAll Loss: 1.4151\tTriple Loss(1): 0.0465\tClassification Loss: 1.3221\r\n",
      "Train Epoch: 21 [169600/209539 (81%)]\tAll Loss: 1.2682\tTriple Loss(1): 0.0246\tClassification Loss: 1.2190\r\n",
      "Train Epoch: 21 [170240/209539 (81%)]\tAll Loss: 1.3090\tTriple Loss(1): 0.0152\tClassification Loss: 1.2786\r\n",
      "Train Epoch: 21 [170880/209539 (82%)]\tAll Loss: 1.3637\tTriple Loss(1): 0.0440\tClassification Loss: 1.2758\r\n",
      "Train Epoch: 21 [171520/209539 (82%)]\tAll Loss: 1.4178\tTriple Loss(1): 0.0856\tClassification Loss: 1.2466\r\n",
      "Train Epoch: 21 [172160/209539 (82%)]\tAll Loss: 1.5317\tTriple Loss(1): 0.0407\tClassification Loss: 1.4504\r\n",
      "Train Epoch: 21 [172800/209539 (82%)]\tAll Loss: 1.3776\tTriple Loss(1): 0.0295\tClassification Loss: 1.3185\r\n",
      "Train Epoch: 21 [173440/209539 (83%)]\tAll Loss: 1.6591\tTriple Loss(1): 0.0432\tClassification Loss: 1.5727\r\n",
      "Train Epoch: 21 [174080/209539 (83%)]\tAll Loss: 1.3100\tTriple Loss(1): 0.0000\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 21 [174720/209539 (83%)]\tAll Loss: 1.6267\tTriple Loss(0): 0.1884\tClassification Loss: 1.2499\r\n",
      "Train Epoch: 21 [175360/209539 (84%)]\tAll Loss: 1.6062\tTriple Loss(1): 0.0440\tClassification Loss: 1.5182\r\n",
      "Train Epoch: 21 [176000/209539 (84%)]\tAll Loss: 1.3957\tTriple Loss(1): 0.0076\tClassification Loss: 1.3804\r\n",
      "Train Epoch: 21 [176640/209539 (84%)]\tAll Loss: 0.9698\tTriple Loss(1): 0.0423\tClassification Loss: 0.8852\r\n",
      "Train Epoch: 21 [177280/209539 (85%)]\tAll Loss: 1.7556\tTriple Loss(1): 0.0047\tClassification Loss: 1.7463\r\n",
      "Train Epoch: 21 [177920/209539 (85%)]\tAll Loss: 1.5298\tTriple Loss(1): 0.0133\tClassification Loss: 1.5032\r\n",
      "Train Epoch: 21 [178560/209539 (85%)]\tAll Loss: 1.3322\tTriple Loss(1): 0.0524\tClassification Loss: 1.2273\r\n",
      "Train Epoch: 21 [179200/209539 (86%)]\tAll Loss: 1.3835\tTriple Loss(1): 0.0104\tClassification Loss: 1.3626\r\n",
      "Train Epoch: 21 [179840/209539 (86%)]\tAll Loss: 1.5980\tTriple Loss(1): 0.1097\tClassification Loss: 1.3787\r\n",
      "Train Epoch: 21 [180480/209539 (86%)]\tAll Loss: 1.2928\tTriple Loss(1): 0.0179\tClassification Loss: 1.2569\r\n",
      "Train Epoch: 21 [181120/209539 (86%)]\tAll Loss: 1.6562\tTriple Loss(1): 0.0528\tClassification Loss: 1.5506\r\n",
      "Train Epoch: 21 [181760/209539 (87%)]\tAll Loss: 1.8332\tTriple Loss(0): 0.2920\tClassification Loss: 1.2493\r\n",
      "Train Epoch: 21 [182400/209539 (87%)]\tAll Loss: 2.1590\tTriple Loss(0): 0.2287\tClassification Loss: 1.7017\r\n",
      "Train Epoch: 21 [183040/209539 (87%)]\tAll Loss: 1.3741\tTriple Loss(1): 0.0472\tClassification Loss: 1.2798\r\n",
      "Train Epoch: 21 [183680/209539 (88%)]\tAll Loss: 1.1127\tTriple Loss(1): 0.0545\tClassification Loss: 1.0037\r\n",
      "Train Epoch: 21 [184320/209539 (88%)]\tAll Loss: 1.4337\tTriple Loss(1): 0.0824\tClassification Loss: 1.2688\r\n",
      "Train Epoch: 21 [184960/209539 (88%)]\tAll Loss: 1.1194\tTriple Loss(1): 0.0176\tClassification Loss: 1.0841\r\n",
      "Train Epoch: 21 [185600/209539 (89%)]\tAll Loss: 1.5722\tTriple Loss(1): 0.0000\tClassification Loss: 1.5722\r\n",
      "Train Epoch: 21 [186240/209539 (89%)]\tAll Loss: 1.4705\tTriple Loss(1): 0.0178\tClassification Loss: 1.4348\r\n",
      "Train Epoch: 21 [186880/209539 (89%)]\tAll Loss: 2.1238\tTriple Loss(0): 0.3161\tClassification Loss: 1.4917\r\n",
      "Train Epoch: 21 [187520/209539 (89%)]\tAll Loss: 2.1047\tTriple Loss(0): 0.2840\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 21 [188160/209539 (90%)]\tAll Loss: 1.6571\tTriple Loss(0): 0.4016\tClassification Loss: 0.8540\r\n",
      "Train Epoch: 21 [188800/209539 (90%)]\tAll Loss: 1.2489\tTriple Loss(1): 0.0580\tClassification Loss: 1.1330\r\n",
      "Train Epoch: 21 [189440/209539 (90%)]\tAll Loss: 2.5390\tTriple Loss(0): 0.5499\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 21 [190080/209539 (91%)]\tAll Loss: 1.3407\tTriple Loss(1): 0.0000\tClassification Loss: 1.3407\r\n",
      "Train Epoch: 21 [190720/209539 (91%)]\tAll Loss: 2.0073\tTriple Loss(0): 0.3055\tClassification Loss: 1.3963\r\n",
      "Train Epoch: 21 [191360/209539 (91%)]\tAll Loss: 1.3116\tTriple Loss(1): 0.0434\tClassification Loss: 1.2249\r\n",
      "Train Epoch: 21 [192000/209539 (92%)]\tAll Loss: 1.8075\tTriple Loss(1): 0.0100\tClassification Loss: 1.7874\r\n",
      "Train Epoch: 21 [192640/209539 (92%)]\tAll Loss: 2.1995\tTriple Loss(0): 0.4431\tClassification Loss: 1.3132\r\n",
      "Train Epoch: 21 [193280/209539 (92%)]\tAll Loss: 1.1863\tTriple Loss(1): 0.0000\tClassification Loss: 1.1863\r\n",
      "Train Epoch: 21 [193920/209539 (93%)]\tAll Loss: 2.2634\tTriple Loss(0): 0.4678\tClassification Loss: 1.3278\r\n",
      "Train Epoch: 21 [194560/209539 (93%)]\tAll Loss: 1.3254\tTriple Loss(1): 0.0820\tClassification Loss: 1.1613\r\n",
      "Train Epoch: 21 [195200/209539 (93%)]\tAll Loss: 1.3166\tTriple Loss(1): 0.0000\tClassification Loss: 1.3166\r\n",
      "Train Epoch: 21 [195840/209539 (93%)]\tAll Loss: 1.3981\tTriple Loss(0): 0.2587\tClassification Loss: 0.8806\r\n",
      "Train Epoch: 21 [196480/209539 (94%)]\tAll Loss: 1.6178\tTriple Loss(1): 0.0067\tClassification Loss: 1.6044\r\n",
      "Train Epoch: 21 [197120/209539 (94%)]\tAll Loss: 2.4812\tTriple Loss(0): 0.4970\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 21 [197760/209539 (94%)]\tAll Loss: 1.3010\tTriple Loss(1): 0.0000\tClassification Loss: 1.3010\r\n",
      "Train Epoch: 21 [198400/209539 (95%)]\tAll Loss: 1.9227\tTriple Loss(0): 0.2748\tClassification Loss: 1.3731\r\n",
      "Train Epoch: 21 [199040/209539 (95%)]\tAll Loss: 1.7146\tTriple Loss(1): 0.0409\tClassification Loss: 1.6327\r\n",
      "Train Epoch: 21 [199680/209539 (95%)]\tAll Loss: 1.4258\tTriple Loss(1): 0.1011\tClassification Loss: 1.2237\r\n",
      "Train Epoch: 21 [200320/209539 (96%)]\tAll Loss: 2.0887\tTriple Loss(0): 0.3239\tClassification Loss: 1.4408\r\n",
      "Train Epoch: 21 [200960/209539 (96%)]\tAll Loss: 1.3159\tTriple Loss(1): 0.0854\tClassification Loss: 1.1452\r\n",
      "Train Epoch: 21 [201600/209539 (96%)]\tAll Loss: 1.2506\tTriple Loss(1): 0.0784\tClassification Loss: 1.0938\r\n",
      "Train Epoch: 21 [202240/209539 (97%)]\tAll Loss: 1.3582\tTriple Loss(1): 0.0171\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 21 [202880/209539 (97%)]\tAll Loss: 0.9834\tTriple Loss(1): 0.0240\tClassification Loss: 0.9354\r\n",
      "Train Epoch: 21 [203520/209539 (97%)]\tAll Loss: 1.2636\tTriple Loss(1): 0.0000\tClassification Loss: 1.2636\r\n",
      "Train Epoch: 21 [204160/209539 (97%)]\tAll Loss: 1.6698\tTriple Loss(1): 0.0000\tClassification Loss: 1.6698\r\n",
      "Train Epoch: 21 [204800/209539 (98%)]\tAll Loss: 1.7348\tTriple Loss(1): 0.0282\tClassification Loss: 1.6783\r\n",
      "Train Epoch: 21 [205440/209539 (98%)]\tAll Loss: 1.2343\tTriple Loss(1): 0.1071\tClassification Loss: 1.0202\r\n",
      "Train Epoch: 21 [206080/209539 (98%)]\tAll Loss: 1.2918\tTriple Loss(1): 0.0488\tClassification Loss: 1.1941\r\n",
      "Train Epoch: 21 [206720/209539 (99%)]\tAll Loss: 1.1139\tTriple Loss(1): 0.0000\tClassification Loss: 1.1139\r\n",
      "Train Epoch: 21 [207360/209539 (99%)]\tAll Loss: 1.2351\tTriple Loss(1): 0.0354\tClassification Loss: 1.1642\r\n",
      "Train Epoch: 21 [208000/209539 (99%)]\tAll Loss: 1.1782\tTriple Loss(1): 0.0074\tClassification Loss: 1.1635\r\n",
      "Train Epoch: 21 [208640/209539 (100%)]\tAll Loss: 1.4543\tTriple Loss(1): 0.0678\tClassification Loss: 1.3187\r\n",
      "Train Epoch: 21 [209280/209539 (100%)]\tAll Loss: 1.4572\tTriple Loss(1): 0.0144\tClassification Loss: 1.4285\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/21_epochs\r\n",
      "Train Epoch: 22 [0/209539 (0%)]\tAll Loss: 2.0253\tTriple Loss(1): 0.1556\tClassification Loss: 1.7141\r\n",
      "\r\n",
      "Test set: Average loss: 1.1688\r\n",
      "Top 1 Accuracy: 52876/80128 (66%)\r\n",
      "Top 3 Accuracy: 68828/80128 (86%)\r\n",
      "Top 5 Accuracy: 73867/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 22 [640/209539 (0%)]\tAll Loss: 1.5317\tTriple Loss(1): 0.0952\tClassification Loss: 1.3413\r\n",
      "Train Epoch: 22 [1280/209539 (1%)]\tAll Loss: 1.2039\tTriple Loss(1): 0.0219\tClassification Loss: 1.1600\r\n",
      "Train Epoch: 22 [1920/209539 (1%)]\tAll Loss: 1.3447\tTriple Loss(1): 0.0244\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 22 [2560/209539 (1%)]\tAll Loss: 1.7221\tTriple Loss(1): 0.0171\tClassification Loss: 1.6879\r\n",
      "Train Epoch: 22 [3200/209539 (2%)]\tAll Loss: 1.6384\tTriple Loss(1): 0.0835\tClassification Loss: 1.4714\r\n",
      "Train Epoch: 22 [3840/209539 (2%)]\tAll Loss: 1.2642\tTriple Loss(1): 0.0545\tClassification Loss: 1.1553\r\n",
      "Train Epoch: 22 [4480/209539 (2%)]\tAll Loss: 2.1000\tTriple Loss(0): 0.3876\tClassification Loss: 1.3249\r\n",
      "Train Epoch: 22 [5120/209539 (2%)]\tAll Loss: 1.3940\tTriple Loss(1): 0.0301\tClassification Loss: 1.3338\r\n",
      "Train Epoch: 22 [5760/209539 (3%)]\tAll Loss: 1.7322\tTriple Loss(1): 0.0549\tClassification Loss: 1.6223\r\n",
      "Train Epoch: 22 [6400/209539 (3%)]\tAll Loss: 1.6723\tTriple Loss(0): 0.3252\tClassification Loss: 1.0220\r\n",
      "Train Epoch: 22 [7040/209539 (3%)]\tAll Loss: 1.2784\tTriple Loss(1): 0.0426\tClassification Loss: 1.1933\r\n",
      "Train Epoch: 22 [7680/209539 (4%)]\tAll Loss: 1.1745\tTriple Loss(1): 0.0004\tClassification Loss: 1.1738\r\n",
      "Train Epoch: 22 [8320/209539 (4%)]\tAll Loss: 1.4932\tTriple Loss(1): 0.0886\tClassification Loss: 1.3159\r\n",
      "Train Epoch: 22 [8960/209539 (4%)]\tAll Loss: 1.3072\tTriple Loss(1): 0.0558\tClassification Loss: 1.1956\r\n",
      "Train Epoch: 22 [9600/209539 (5%)]\tAll Loss: 1.6757\tTriple Loss(1): 0.0198\tClassification Loss: 1.6361\r\n",
      "Train Epoch: 22 [10240/209539 (5%)]\tAll Loss: 1.2113\tTriple Loss(1): 0.0766\tClassification Loss: 1.0582\r\n",
      "Train Epoch: 22 [10880/209539 (5%)]\tAll Loss: 1.3792\tTriple Loss(1): 0.0353\tClassification Loss: 1.3085\r\n",
      "Train Epoch: 22 [11520/209539 (5%)]\tAll Loss: 1.4076\tTriple Loss(1): 0.0399\tClassification Loss: 1.3278\r\n",
      "Train Epoch: 22 [12160/209539 (6%)]\tAll Loss: 1.1283\tTriple Loss(1): 0.0000\tClassification Loss: 1.1283\r\n",
      "Train Epoch: 22 [12800/209539 (6%)]\tAll Loss: 1.0739\tTriple Loss(1): 0.0342\tClassification Loss: 1.0055\r\n",
      "Train Epoch: 22 [13440/209539 (6%)]\tAll Loss: 1.9653\tTriple Loss(0): 0.3456\tClassification Loss: 1.2740\r\n",
      "Train Epoch: 22 [14080/209539 (7%)]\tAll Loss: 1.3343\tTriple Loss(1): 0.0000\tClassification Loss: 1.3343\r\n",
      "Train Epoch: 22 [14720/209539 (7%)]\tAll Loss: 1.5286\tTriple Loss(1): 0.0385\tClassification Loss: 1.4515\r\n",
      "Train Epoch: 22 [15360/209539 (7%)]\tAll Loss: 1.4607\tTriple Loss(1): 0.0351\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 22 [16000/209539 (8%)]\tAll Loss: 2.1188\tTriple Loss(0): 0.3183\tClassification Loss: 1.4823\r\n",
      "Train Epoch: 22 [16640/209539 (8%)]\tAll Loss: 1.7769\tTriple Loss(1): 0.0334\tClassification Loss: 1.7102\r\n",
      "Train Epoch: 22 [17280/209539 (8%)]\tAll Loss: 1.5431\tTriple Loss(1): 0.0070\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 22 [17920/209539 (9%)]\tAll Loss: 1.4175\tTriple Loss(1): 0.0459\tClassification Loss: 1.3257\r\n",
      "Train Epoch: 22 [18560/209539 (9%)]\tAll Loss: 1.3490\tTriple Loss(1): 0.0359\tClassification Loss: 1.2772\r\n",
      "Train Epoch: 22 [19200/209539 (9%)]\tAll Loss: 1.5087\tTriple Loss(1): 0.0151\tClassification Loss: 1.4786\r\n",
      "Train Epoch: 22 [19840/209539 (9%)]\tAll Loss: 2.0805\tTriple Loss(0): 0.4860\tClassification Loss: 1.1084\r\n",
      "Train Epoch: 22 [20480/209539 (10%)]\tAll Loss: 1.2693\tTriple Loss(1): 0.0316\tClassification Loss: 1.2060\r\n",
      "Train Epoch: 22 [21120/209539 (10%)]\tAll Loss: 1.9219\tTriple Loss(0): 0.2592\tClassification Loss: 1.4034\r\n",
      "Train Epoch: 22 [21760/209539 (10%)]\tAll Loss: 1.4239\tTriple Loss(1): 0.0459\tClassification Loss: 1.3322\r\n",
      "Train Epoch: 22 [22400/209539 (11%)]\tAll Loss: 1.2293\tTriple Loss(1): 0.0565\tClassification Loss: 1.1162\r\n",
      "Train Epoch: 22 [23040/209539 (11%)]\tAll Loss: 1.6154\tTriple Loss(1): 0.0865\tClassification Loss: 1.4424\r\n",
      "Train Epoch: 22 [23680/209539 (11%)]\tAll Loss: 1.2805\tTriple Loss(1): 0.0743\tClassification Loss: 1.1319\r\n",
      "Train Epoch: 22 [24320/209539 (12%)]\tAll Loss: 2.2067\tTriple Loss(0): 0.4377\tClassification Loss: 1.3314\r\n",
      "Train Epoch: 22 [24960/209539 (12%)]\tAll Loss: 2.1980\tTriple Loss(0): 0.4719\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 22 [25600/209539 (12%)]\tAll Loss: 1.1743\tTriple Loss(1): 0.0000\tClassification Loss: 1.1743\r\n",
      "Train Epoch: 22 [26240/209539 (13%)]\tAll Loss: 1.4450\tTriple Loss(0): 0.1722\tClassification Loss: 1.1006\r\n",
      "Train Epoch: 22 [26880/209539 (13%)]\tAll Loss: 1.7278\tTriple Loss(1): 0.0944\tClassification Loss: 1.5390\r\n",
      "Train Epoch: 22 [27520/209539 (13%)]\tAll Loss: 1.3541\tTriple Loss(1): 0.0200\tClassification Loss: 1.3141\r\n",
      "Train Epoch: 22 [28160/209539 (13%)]\tAll Loss: 2.6635\tTriple Loss(0): 0.5306\tClassification Loss: 1.6023\r\n",
      "Train Epoch: 22 [28800/209539 (14%)]\tAll Loss: 1.5748\tTriple Loss(1): 0.0082\tClassification Loss: 1.5584\r\n",
      "Train Epoch: 22 [29440/209539 (14%)]\tAll Loss: 2.3383\tTriple Loss(0): 0.4471\tClassification Loss: 1.4441\r\n",
      "Train Epoch: 22 [30080/209539 (14%)]\tAll Loss: 1.4314\tTriple Loss(1): 0.1165\tClassification Loss: 1.1984\r\n",
      "Train Epoch: 22 [30720/209539 (15%)]\tAll Loss: 1.6311\tTriple Loss(1): 0.1676\tClassification Loss: 1.2959\r\n",
      "Train Epoch: 22 [31360/209539 (15%)]\tAll Loss: 1.2739\tTriple Loss(1): 0.0447\tClassification Loss: 1.1845\r\n",
      "Train Epoch: 22 [32000/209539 (15%)]\tAll Loss: 1.4178\tTriple Loss(1): 0.0519\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 22 [32640/209539 (16%)]\tAll Loss: 1.3654\tTriple Loss(1): 0.0604\tClassification Loss: 1.2446\r\n",
      "Train Epoch: 22 [33280/209539 (16%)]\tAll Loss: 1.4365\tTriple Loss(1): 0.0204\tClassification Loss: 1.3958\r\n",
      "Train Epoch: 22 [33920/209539 (16%)]\tAll Loss: 1.9880\tTriple Loss(0): 0.3508\tClassification Loss: 1.2864\r\n",
      "Train Epoch: 22 [34560/209539 (16%)]\tAll Loss: 1.5643\tTriple Loss(1): 0.1978\tClassification Loss: 1.1686\r\n",
      "Train Epoch: 22 [35200/209539 (17%)]\tAll Loss: 1.2074\tTriple Loss(1): 0.0148\tClassification Loss: 1.1778\r\n",
      "Train Epoch: 22 [35840/209539 (17%)]\tAll Loss: 1.3471\tTriple Loss(1): 0.0185\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 22 [36480/209539 (17%)]\tAll Loss: 1.2777\tTriple Loss(1): 0.0033\tClassification Loss: 1.2711\r\n",
      "Train Epoch: 22 [37120/209539 (18%)]\tAll Loss: 1.7260\tTriple Loss(1): 0.0689\tClassification Loss: 1.5883\r\n",
      "Train Epoch: 22 [37760/209539 (18%)]\tAll Loss: 1.5111\tTriple Loss(1): 0.0290\tClassification Loss: 1.4531\r\n",
      "Train Epoch: 22 [38400/209539 (18%)]\tAll Loss: 2.2876\tTriple Loss(0): 0.4643\tClassification Loss: 1.3589\r\n",
      "Train Epoch: 22 [39040/209539 (19%)]\tAll Loss: 1.8877\tTriple Loss(0): 0.4214\tClassification Loss: 1.0448\r\n",
      "Train Epoch: 22 [39680/209539 (19%)]\tAll Loss: 1.2911\tTriple Loss(1): 0.0000\tClassification Loss: 1.2911\r\n",
      "Train Epoch: 22 [40320/209539 (19%)]\tAll Loss: 1.4770\tTriple Loss(1): 0.0613\tClassification Loss: 1.3544\r\n",
      "Train Epoch: 22 [40960/209539 (20%)]\tAll Loss: 1.2375\tTriple Loss(1): 0.0335\tClassification Loss: 1.1704\r\n",
      "Train Epoch: 22 [41600/209539 (20%)]\tAll Loss: 1.3882\tTriple Loss(1): 0.0698\tClassification Loss: 1.2486\r\n",
      "Train Epoch: 22 [42240/209539 (20%)]\tAll Loss: 1.2907\tTriple Loss(1): 0.0572\tClassification Loss: 1.1763\r\n",
      "Train Epoch: 22 [42880/209539 (20%)]\tAll Loss: 1.4038\tTriple Loss(1): 0.2127\tClassification Loss: 0.9785\r\n",
      "Train Epoch: 22 [43520/209539 (21%)]\tAll Loss: 1.4329\tTriple Loss(1): 0.0459\tClassification Loss: 1.3411\r\n",
      "Train Epoch: 22 [44160/209539 (21%)]\tAll Loss: 2.0370\tTriple Loss(0): 0.2830\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 22 [44800/209539 (21%)]\tAll Loss: 1.6514\tTriple Loss(1): 0.0119\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 22 [45440/209539 (22%)]\tAll Loss: 1.8080\tTriple Loss(1): 0.0388\tClassification Loss: 1.7303\r\n",
      "Train Epoch: 22 [46080/209539 (22%)]\tAll Loss: 1.3632\tTriple Loss(1): 0.1110\tClassification Loss: 1.1413\r\n",
      "Train Epoch: 22 [46720/209539 (22%)]\tAll Loss: 1.5530\tTriple Loss(1): 0.0000\tClassification Loss: 1.5530\r\n",
      "Train Epoch: 22 [47360/209539 (23%)]\tAll Loss: 1.0339\tTriple Loss(1): 0.0020\tClassification Loss: 1.0300\r\n",
      "Train Epoch: 22 [48000/209539 (23%)]\tAll Loss: 1.3828\tTriple Loss(1): 0.0810\tClassification Loss: 1.2208\r\n",
      "Train Epoch: 22 [48640/209539 (23%)]\tAll Loss: 2.4247\tTriple Loss(0): 0.3353\tClassification Loss: 1.7540\r\n",
      "Train Epoch: 22 [49280/209539 (24%)]\tAll Loss: 1.4715\tTriple Loss(1): 0.0915\tClassification Loss: 1.2885\r\n",
      "Train Epoch: 22 [49920/209539 (24%)]\tAll Loss: 2.1095\tTriple Loss(0): 0.2844\tClassification Loss: 1.5408\r\n",
      "Train Epoch: 22 [50560/209539 (24%)]\tAll Loss: 1.5169\tTriple Loss(1): 0.0300\tClassification Loss: 1.4569\r\n",
      "Train Epoch: 22 [51200/209539 (24%)]\tAll Loss: 1.6400\tTriple Loss(1): 0.0917\tClassification Loss: 1.4567\r\n",
      "Train Epoch: 22 [51840/209539 (25%)]\tAll Loss: 1.4681\tTriple Loss(1): 0.0322\tClassification Loss: 1.4038\r\n",
      "Train Epoch: 22 [52480/209539 (25%)]\tAll Loss: 1.3291\tTriple Loss(1): 0.0076\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 22 [53120/209539 (25%)]\tAll Loss: 2.1257\tTriple Loss(0): 0.4229\tClassification Loss: 1.2798\r\n",
      "Train Epoch: 22 [53760/209539 (26%)]\tAll Loss: 2.2711\tTriple Loss(0): 0.4418\tClassification Loss: 1.3874\r\n",
      "Train Epoch: 22 [54400/209539 (26%)]\tAll Loss: 1.7878\tTriple Loss(1): 0.0025\tClassification Loss: 1.7828\r\n",
      "Train Epoch: 22 [55040/209539 (26%)]\tAll Loss: 1.0454\tTriple Loss(1): 0.0242\tClassification Loss: 0.9970\r\n",
      "Train Epoch: 22 [55680/209539 (27%)]\tAll Loss: 1.7741\tTriple Loss(1): 0.0351\tClassification Loss: 1.7038\r\n",
      "Train Epoch: 22 [56320/209539 (27%)]\tAll Loss: 1.0933\tTriple Loss(1): 0.0540\tClassification Loss: 0.9853\r\n",
      "Train Epoch: 22 [56960/209539 (27%)]\tAll Loss: 1.3928\tTriple Loss(1): 0.0760\tClassification Loss: 1.2409\r\n",
      "Train Epoch: 22 [57600/209539 (27%)]\tAll Loss: 1.8007\tTriple Loss(0): 0.2779\tClassification Loss: 1.2450\r\n",
      "Train Epoch: 22 [58240/209539 (28%)]\tAll Loss: 0.9881\tTriple Loss(1): 0.0035\tClassification Loss: 0.9811\r\n",
      "Train Epoch: 22 [58880/209539 (28%)]\tAll Loss: 1.4324\tTriple Loss(1): 0.0180\tClassification Loss: 1.3963\r\n",
      "Train Epoch: 22 [59520/209539 (28%)]\tAll Loss: 1.2324\tTriple Loss(1): 0.0375\tClassification Loss: 1.1574\r\n",
      "Train Epoch: 22 [60160/209539 (29%)]\tAll Loss: 1.5920\tTriple Loss(1): 0.0307\tClassification Loss: 1.5305\r\n",
      "Train Epoch: 22 [60800/209539 (29%)]\tAll Loss: 1.3529\tTriple Loss(1): 0.0113\tClassification Loss: 1.3303\r\n",
      "Train Epoch: 22 [61440/209539 (29%)]\tAll Loss: 1.3054\tTriple Loss(1): 0.0000\tClassification Loss: 1.3054\r\n",
      "Train Epoch: 22 [62080/209539 (30%)]\tAll Loss: 1.8361\tTriple Loss(1): 0.1274\tClassification Loss: 1.5813\r\n",
      "Train Epoch: 22 [62720/209539 (30%)]\tAll Loss: 1.4960\tTriple Loss(1): 0.0546\tClassification Loss: 1.3868\r\n",
      "Train Epoch: 22 [63360/209539 (30%)]\tAll Loss: 1.4383\tTriple Loss(1): 0.0290\tClassification Loss: 1.3803\r\n",
      "Train Epoch: 22 [64000/209539 (31%)]\tAll Loss: 1.5746\tTriple Loss(1): 0.0442\tClassification Loss: 1.4862\r\n",
      "Train Epoch: 22 [64640/209539 (31%)]\tAll Loss: 1.6301\tTriple Loss(1): 0.0412\tClassification Loss: 1.5477\r\n",
      "Train Epoch: 22 [65280/209539 (31%)]\tAll Loss: 1.4390\tTriple Loss(1): 0.0550\tClassification Loss: 1.3291\r\n",
      "Train Epoch: 22 [65920/209539 (31%)]\tAll Loss: 1.5363\tTriple Loss(1): 0.0348\tClassification Loss: 1.4666\r\n",
      "Train Epoch: 22 [66560/209539 (32%)]\tAll Loss: 1.2005\tTriple Loss(1): 0.0000\tClassification Loss: 1.2005\r\n",
      "Train Epoch: 22 [67200/209539 (32%)]\tAll Loss: 2.3228\tTriple Loss(0): 0.4883\tClassification Loss: 1.3462\r\n",
      "Train Epoch: 22 [67840/209539 (32%)]\tAll Loss: 1.4376\tTriple Loss(1): 0.0362\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 22 [68480/209539 (33%)]\tAll Loss: 1.5907\tTriple Loss(1): 0.0358\tClassification Loss: 1.5191\r\n",
      "Train Epoch: 22 [69120/209539 (33%)]\tAll Loss: 1.6724\tTriple Loss(1): 0.0175\tClassification Loss: 1.6373\r\n",
      "Train Epoch: 22 [69760/209539 (33%)]\tAll Loss: 2.0317\tTriple Loss(0): 0.4521\tClassification Loss: 1.1275\r\n",
      "Train Epoch: 22 [70400/209539 (34%)]\tAll Loss: 1.2469\tTriple Loss(1): 0.0799\tClassification Loss: 1.0872\r\n",
      "Train Epoch: 22 [71040/209539 (34%)]\tAll Loss: 1.8108\tTriple Loss(1): 0.0000\tClassification Loss: 1.8108\r\n",
      "Train Epoch: 22 [71680/209539 (34%)]\tAll Loss: 1.6200\tTriple Loss(1): 0.0190\tClassification Loss: 1.5821\r\n",
      "Train Epoch: 22 [72320/209539 (35%)]\tAll Loss: 1.1912\tTriple Loss(1): 0.0000\tClassification Loss: 1.1912\r\n",
      "Train Epoch: 22 [72960/209539 (35%)]\tAll Loss: 1.3131\tTriple Loss(1): 0.0276\tClassification Loss: 1.2579\r\n",
      "Train Epoch: 22 [73600/209539 (35%)]\tAll Loss: 1.5139\tTriple Loss(1): 0.0449\tClassification Loss: 1.4240\r\n",
      "Train Epoch: 22 [74240/209539 (35%)]\tAll Loss: 1.5933\tTriple Loss(1): 0.0339\tClassification Loss: 1.5255\r\n",
      "Train Epoch: 22 [74880/209539 (36%)]\tAll Loss: 2.3788\tTriple Loss(0): 0.2531\tClassification Loss: 1.8725\r\n",
      "Train Epoch: 22 [75520/209539 (36%)]\tAll Loss: 1.9683\tTriple Loss(0): 0.4215\tClassification Loss: 1.1252\r\n",
      "Train Epoch: 22 [76160/209539 (36%)]\tAll Loss: 2.1076\tTriple Loss(0): 0.4145\tClassification Loss: 1.2786\r\n",
      "Train Epoch: 22 [76800/209539 (37%)]\tAll Loss: 1.3661\tTriple Loss(1): 0.0419\tClassification Loss: 1.2823\r\n",
      "Train Epoch: 22 [77440/209539 (37%)]\tAll Loss: 2.3630\tTriple Loss(0): 0.3957\tClassification Loss: 1.5715\r\n",
      "Train Epoch: 22 [78080/209539 (37%)]\tAll Loss: 1.5069\tTriple Loss(1): 0.0420\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 22 [78720/209539 (38%)]\tAll Loss: 1.4016\tTriple Loss(1): 0.0000\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 22 [79360/209539 (38%)]\tAll Loss: 1.3762\tTriple Loss(1): 0.0157\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 22 [80000/209539 (38%)]\tAll Loss: 1.6600\tTriple Loss(1): 0.1038\tClassification Loss: 1.4525\r\n",
      "Train Epoch: 22 [80640/209539 (38%)]\tAll Loss: 1.5413\tTriple Loss(1): 0.0581\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 22 [81280/209539 (39%)]\tAll Loss: 1.7179\tTriple Loss(1): 0.0067\tClassification Loss: 1.7045\r\n",
      "Train Epoch: 22 [81920/209539 (39%)]\tAll Loss: 1.2870\tTriple Loss(1): 0.0356\tClassification Loss: 1.2159\r\n",
      "Train Epoch: 22 [82560/209539 (39%)]\tAll Loss: 1.7329\tTriple Loss(1): 0.1544\tClassification Loss: 1.4240\r\n",
      "Train Epoch: 22 [83200/209539 (40%)]\tAll Loss: 2.1362\tTriple Loss(0): 0.3621\tClassification Loss: 1.4119\r\n",
      "Train Epoch: 22 [83840/209539 (40%)]\tAll Loss: 1.5616\tTriple Loss(1): 0.0819\tClassification Loss: 1.3978\r\n",
      "Train Epoch: 22 [84480/209539 (40%)]\tAll Loss: 1.1663\tTriple Loss(1): 0.0653\tClassification Loss: 1.0357\r\n",
      "Train Epoch: 22 [85120/209539 (41%)]\tAll Loss: 1.6925\tTriple Loss(1): 0.0089\tClassification Loss: 1.6747\r\n",
      "Train Epoch: 22 [85760/209539 (41%)]\tAll Loss: 1.2298\tTriple Loss(1): 0.0073\tClassification Loss: 1.2153\r\n",
      "Train Epoch: 22 [86400/209539 (41%)]\tAll Loss: 1.4949\tTriple Loss(1): 0.0246\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 22 [87040/209539 (42%)]\tAll Loss: 1.4028\tTriple Loss(1): 0.0132\tClassification Loss: 1.3764\r\n",
      "Train Epoch: 22 [87680/209539 (42%)]\tAll Loss: 1.1459\tTriple Loss(1): 0.0278\tClassification Loss: 1.0904\r\n",
      "Train Epoch: 22 [88320/209539 (42%)]\tAll Loss: 1.4384\tTriple Loss(1): 0.0255\tClassification Loss: 1.3874\r\n",
      "Train Epoch: 22 [88960/209539 (42%)]\tAll Loss: 1.6047\tTriple Loss(1): 0.0789\tClassification Loss: 1.4469\r\n",
      "Train Epoch: 22 [89600/209539 (43%)]\tAll Loss: 1.3839\tTriple Loss(1): 0.0184\tClassification Loss: 1.3471\r\n",
      "Train Epoch: 22 [90240/209539 (43%)]\tAll Loss: 1.3121\tTriple Loss(1): 0.0207\tClassification Loss: 1.2708\r\n",
      "Train Epoch: 22 [90880/209539 (43%)]\tAll Loss: 2.2832\tTriple Loss(0): 0.3075\tClassification Loss: 1.6682\r\n",
      "Train Epoch: 22 [91520/209539 (44%)]\tAll Loss: 2.0580\tTriple Loss(0): 0.3195\tClassification Loss: 1.4189\r\n",
      "Train Epoch: 22 [92160/209539 (44%)]\tAll Loss: 1.3023\tTriple Loss(1): 0.0491\tClassification Loss: 1.2040\r\n",
      "Train Epoch: 22 [92800/209539 (44%)]\tAll Loss: 1.2102\tTriple Loss(1): 0.0733\tClassification Loss: 1.0636\r\n",
      "Train Epoch: 22 [93440/209539 (45%)]\tAll Loss: 1.6984\tTriple Loss(1): 0.0533\tClassification Loss: 1.5918\r\n",
      "Train Epoch: 22 [94080/209539 (45%)]\tAll Loss: 1.0852\tTriple Loss(1): 0.0116\tClassification Loss: 1.0621\r\n",
      "Train Epoch: 22 [94720/209539 (45%)]\tAll Loss: 2.0164\tTriple Loss(0): 0.2341\tClassification Loss: 1.5482\r\n",
      "Train Epoch: 22 [95360/209539 (46%)]\tAll Loss: 1.6448\tTriple Loss(1): 0.0381\tClassification Loss: 1.5687\r\n",
      "Train Epoch: 22 [96000/209539 (46%)]\tAll Loss: 1.4230\tTriple Loss(1): 0.0075\tClassification Loss: 1.4080\r\n",
      "Train Epoch: 22 [96640/209539 (46%)]\tAll Loss: 1.5373\tTriple Loss(1): 0.0449\tClassification Loss: 1.4475\r\n",
      "Train Epoch: 22 [97280/209539 (46%)]\tAll Loss: 1.2806\tTriple Loss(1): 0.0264\tClassification Loss: 1.2279\r\n",
      "Train Epoch: 22 [97920/209539 (47%)]\tAll Loss: 1.6074\tTriple Loss(1): 0.1129\tClassification Loss: 1.3816\r\n",
      "Train Epoch: 22 [98560/209539 (47%)]\tAll Loss: 1.7192\tTriple Loss(1): 0.0443\tClassification Loss: 1.6306\r\n",
      "Train Epoch: 22 [99200/209539 (47%)]\tAll Loss: 1.3157\tTriple Loss(1): 0.0122\tClassification Loss: 1.2913\r\n",
      "Train Epoch: 22 [99840/209539 (48%)]\tAll Loss: 1.5538\tTriple Loss(1): 0.0340\tClassification Loss: 1.4857\r\n",
      "Train Epoch: 22 [100480/209539 (48%)]\tAll Loss: 1.3631\tTriple Loss(1): 0.0273\tClassification Loss: 1.3086\r\n",
      "Train Epoch: 22 [101120/209539 (48%)]\tAll Loss: 1.1562\tTriple Loss(1): 0.0067\tClassification Loss: 1.1427\r\n",
      "Train Epoch: 22 [101760/209539 (49%)]\tAll Loss: 1.4538\tTriple Loss(1): 0.0520\tClassification Loss: 1.3497\r\n",
      "Train Epoch: 22 [102400/209539 (49%)]\tAll Loss: 1.3863\tTriple Loss(1): 0.0458\tClassification Loss: 1.2948\r\n",
      "Train Epoch: 22 [103040/209539 (49%)]\tAll Loss: 2.1121\tTriple Loss(0): 0.4319\tClassification Loss: 1.2483\r\n",
      "Train Epoch: 22 [103680/209539 (49%)]\tAll Loss: 1.5873\tTriple Loss(1): 0.0485\tClassification Loss: 1.4903\r\n",
      "Train Epoch: 22 [104320/209539 (50%)]\tAll Loss: 1.3796\tTriple Loss(1): 0.0861\tClassification Loss: 1.2074\r\n",
      "Train Epoch: 22 [104960/209539 (50%)]\tAll Loss: 1.3795\tTriple Loss(1): 0.0591\tClassification Loss: 1.2614\r\n",
      "Train Epoch: 22 [105600/209539 (50%)]\tAll Loss: 1.3582\tTriple Loss(1): 0.0179\tClassification Loss: 1.3223\r\n",
      "Train Epoch: 22 [106240/209539 (51%)]\tAll Loss: 1.2644\tTriple Loss(1): 0.0323\tClassification Loss: 1.1998\r\n",
      "Train Epoch: 22 [106880/209539 (51%)]\tAll Loss: 1.3743\tTriple Loss(1): 0.0297\tClassification Loss: 1.3149\r\n",
      "Train Epoch: 22 [107520/209539 (51%)]\tAll Loss: 1.3340\tTriple Loss(1): 0.0201\tClassification Loss: 1.2938\r\n",
      "Train Epoch: 22 [108160/209539 (52%)]\tAll Loss: 1.3982\tTriple Loss(1): 0.0948\tClassification Loss: 1.2087\r\n",
      "Train Epoch: 22 [108800/209539 (52%)]\tAll Loss: 1.4372\tTriple Loss(1): 0.0276\tClassification Loss: 1.3820\r\n",
      "Train Epoch: 22 [109440/209539 (52%)]\tAll Loss: 2.6128\tTriple Loss(0): 0.4103\tClassification Loss: 1.7923\r\n",
      "Train Epoch: 22 [110080/209539 (53%)]\tAll Loss: 2.3345\tTriple Loss(0): 0.3746\tClassification Loss: 1.5853\r\n",
      "Train Epoch: 22 [110720/209539 (53%)]\tAll Loss: 1.3007\tTriple Loss(1): 0.0129\tClassification Loss: 1.2750\r\n",
      "Train Epoch: 22 [111360/209539 (53%)]\tAll Loss: 1.3852\tTriple Loss(1): 0.0882\tClassification Loss: 1.2088\r\n",
      "Train Epoch: 22 [112000/209539 (53%)]\tAll Loss: 1.1657\tTriple Loss(1): 0.0124\tClassification Loss: 1.1409\r\n",
      "Train Epoch: 22 [112640/209539 (54%)]\tAll Loss: 1.4130\tTriple Loss(1): 0.0476\tClassification Loss: 1.3177\r\n",
      "Train Epoch: 22 [113280/209539 (54%)]\tAll Loss: 1.2484\tTriple Loss(1): 0.0249\tClassification Loss: 1.1986\r\n",
      "Train Epoch: 22 [113920/209539 (54%)]\tAll Loss: 1.6250\tTriple Loss(1): 0.0391\tClassification Loss: 1.5467\r\n",
      "Train Epoch: 22 [114560/209539 (55%)]\tAll Loss: 1.3838\tTriple Loss(1): 0.0113\tClassification Loss: 1.3611\r\n",
      "Train Epoch: 22 [115200/209539 (55%)]\tAll Loss: 1.6510\tTriple Loss(1): 0.0441\tClassification Loss: 1.5628\r\n",
      "Train Epoch: 22 [115840/209539 (55%)]\tAll Loss: 1.9958\tTriple Loss(0): 0.2721\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 22 [116480/209539 (56%)]\tAll Loss: 1.9200\tTriple Loss(0): 0.1605\tClassification Loss: 1.5989\r\n",
      "Train Epoch: 22 [117120/209539 (56%)]\tAll Loss: 1.3746\tTriple Loss(1): 0.0157\tClassification Loss: 1.3431\r\n",
      "Train Epoch: 22 [117760/209539 (56%)]\tAll Loss: 1.3297\tTriple Loss(1): 0.0287\tClassification Loss: 1.2722\r\n",
      "Train Epoch: 22 [118400/209539 (57%)]\tAll Loss: 1.0709\tTriple Loss(1): 0.0114\tClassification Loss: 1.0481\r\n",
      "Train Epoch: 22 [119040/209539 (57%)]\tAll Loss: 1.7360\tTriple Loss(1): 0.0548\tClassification Loss: 1.6264\r\n",
      "Train Epoch: 22 [119680/209539 (57%)]\tAll Loss: 1.2791\tTriple Loss(1): 0.0206\tClassification Loss: 1.2379\r\n",
      "Train Epoch: 22 [120320/209539 (57%)]\tAll Loss: 1.3647\tTriple Loss(1): 0.0871\tClassification Loss: 1.1904\r\n",
      "Train Epoch: 22 [120960/209539 (58%)]\tAll Loss: 1.3229\tTriple Loss(1): 0.0436\tClassification Loss: 1.2358\r\n",
      "Train Epoch: 22 [121600/209539 (58%)]\tAll Loss: 1.3075\tTriple Loss(1): 0.0942\tClassification Loss: 1.1190\r\n",
      "Train Epoch: 22 [122240/209539 (58%)]\tAll Loss: 1.1953\tTriple Loss(1): 0.0653\tClassification Loss: 1.0648\r\n",
      "Train Epoch: 22 [122880/209539 (59%)]\tAll Loss: 1.0572\tTriple Loss(1): 0.0014\tClassification Loss: 1.0544\r\n",
      "Train Epoch: 22 [123520/209539 (59%)]\tAll Loss: 1.4257\tTriple Loss(1): 0.0594\tClassification Loss: 1.3068\r\n",
      "Train Epoch: 22 [124160/209539 (59%)]\tAll Loss: 1.4963\tTriple Loss(1): 0.0619\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 22 [124800/209539 (60%)]\tAll Loss: 1.3388\tTriple Loss(1): 0.0403\tClassification Loss: 1.2582\r\n",
      "Train Epoch: 22 [125440/209539 (60%)]\tAll Loss: 2.0442\tTriple Loss(0): 0.2874\tClassification Loss: 1.4694\r\n",
      "Train Epoch: 22 [126080/209539 (60%)]\tAll Loss: 1.3576\tTriple Loss(1): 0.0000\tClassification Loss: 1.3576\r\n",
      "Train Epoch: 22 [126720/209539 (60%)]\tAll Loss: 1.6780\tTriple Loss(1): 0.0620\tClassification Loss: 1.5539\r\n",
      "Train Epoch: 22 [127360/209539 (61%)]\tAll Loss: 1.3133\tTriple Loss(1): 0.0201\tClassification Loss: 1.2730\r\n",
      "Train Epoch: 22 [128000/209539 (61%)]\tAll Loss: 1.5564\tTriple Loss(1): 0.0273\tClassification Loss: 1.5018\r\n",
      "Train Epoch: 22 [128640/209539 (61%)]\tAll Loss: 1.3366\tTriple Loss(1): 0.0332\tClassification Loss: 1.2702\r\n",
      "Train Epoch: 22 [129280/209539 (62%)]\tAll Loss: 1.3540\tTriple Loss(1): 0.0000\tClassification Loss: 1.3540\r\n",
      "Train Epoch: 22 [129920/209539 (62%)]\tAll Loss: 1.1732\tTriple Loss(1): 0.0408\tClassification Loss: 1.0916\r\n",
      "Train Epoch: 22 [130560/209539 (62%)]\tAll Loss: 1.0800\tTriple Loss(1): 0.0023\tClassification Loss: 1.0754\r\n",
      "Train Epoch: 22 [131200/209539 (63%)]\tAll Loss: 1.1529\tTriple Loss(1): 0.0004\tClassification Loss: 1.1521\r\n",
      "Train Epoch: 22 [131840/209539 (63%)]\tAll Loss: 1.4213\tTriple Loss(1): 0.0153\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 22 [132480/209539 (63%)]\tAll Loss: 1.1004\tTriple Loss(1): 0.0000\tClassification Loss: 1.1004\r\n",
      "Train Epoch: 22 [133120/209539 (64%)]\tAll Loss: 1.3076\tTriple Loss(1): 0.0416\tClassification Loss: 1.2243\r\n",
      "Train Epoch: 22 [133760/209539 (64%)]\tAll Loss: 1.4430\tTriple Loss(1): 0.0729\tClassification Loss: 1.2971\r\n",
      "Train Epoch: 22 [134400/209539 (64%)]\tAll Loss: 1.2329\tTriple Loss(1): 0.0393\tClassification Loss: 1.1543\r\n",
      "Train Epoch: 22 [135040/209539 (64%)]\tAll Loss: 1.4493\tTriple Loss(1): 0.0424\tClassification Loss: 1.3645\r\n",
      "Train Epoch: 22 [135680/209539 (65%)]\tAll Loss: 1.6215\tTriple Loss(1): 0.0587\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 22 [136320/209539 (65%)]\tAll Loss: 1.9506\tTriple Loss(0): 0.2691\tClassification Loss: 1.4124\r\n",
      "Train Epoch: 22 [136960/209539 (65%)]\tAll Loss: 1.3949\tTriple Loss(1): 0.0711\tClassification Loss: 1.2526\r\n",
      "Train Epoch: 22 [137600/209539 (66%)]\tAll Loss: 1.1987\tTriple Loss(1): 0.0107\tClassification Loss: 1.1774\r\n",
      "Train Epoch: 22 [138240/209539 (66%)]\tAll Loss: 2.0644\tTriple Loss(0): 0.2709\tClassification Loss: 1.5226\r\n",
      "Train Epoch: 22 [138880/209539 (66%)]\tAll Loss: 1.3144\tTriple Loss(1): 0.0256\tClassification Loss: 1.2633\r\n",
      "Train Epoch: 22 [139520/209539 (67%)]\tAll Loss: 1.5715\tTriple Loss(1): 0.0680\tClassification Loss: 1.4355\r\n",
      "Train Epoch: 22 [140160/209539 (67%)]\tAll Loss: 1.3955\tTriple Loss(1): 0.0654\tClassification Loss: 1.2646\r\n",
      "Train Epoch: 22 [140800/209539 (67%)]\tAll Loss: 1.5700\tTriple Loss(1): 0.0336\tClassification Loss: 1.5027\r\n",
      "Train Epoch: 22 [141440/209539 (68%)]\tAll Loss: 1.5677\tTriple Loss(1): 0.0268\tClassification Loss: 1.5141\r\n",
      "Train Epoch: 22 [142080/209539 (68%)]\tAll Loss: 1.4450\tTriple Loss(1): 0.0720\tClassification Loss: 1.3009\r\n",
      "Train Epoch: 22 [142720/209539 (68%)]\tAll Loss: 1.6770\tTriple Loss(1): 0.0039\tClassification Loss: 1.6691\r\n",
      "Train Epoch: 22 [143360/209539 (68%)]\tAll Loss: 1.1200\tTriple Loss(1): 0.0449\tClassification Loss: 1.0302\r\n",
      "Train Epoch: 22 [144000/209539 (69%)]\tAll Loss: 2.1407\tTriple Loss(0): 0.3476\tClassification Loss: 1.4455\r\n",
      "Train Epoch: 22 [144640/209539 (69%)]\tAll Loss: 1.3665\tTriple Loss(1): 0.0551\tClassification Loss: 1.2562\r\n",
      "Train Epoch: 22 [145280/209539 (69%)]\tAll Loss: 2.2133\tTriple Loss(0): 0.4269\tClassification Loss: 1.3595\r\n",
      "Train Epoch: 22 [145920/209539 (70%)]\tAll Loss: 1.3057\tTriple Loss(1): 0.0292\tClassification Loss: 1.2474\r\n",
      "Train Epoch: 22 [146560/209539 (70%)]\tAll Loss: 2.0392\tTriple Loss(0): 0.4483\tClassification Loss: 1.1427\r\n",
      "Train Epoch: 22 [147200/209539 (70%)]\tAll Loss: 1.6366\tTriple Loss(0): 0.1112\tClassification Loss: 1.4141\r\n",
      "Train Epoch: 22 [147840/209539 (71%)]\tAll Loss: 1.2282\tTriple Loss(1): 0.0682\tClassification Loss: 1.0919\r\n",
      "Train Epoch: 22 [148480/209539 (71%)]\tAll Loss: 1.2844\tTriple Loss(1): 0.1012\tClassification Loss: 1.0821\r\n",
      "Train Epoch: 22 [149120/209539 (71%)]\tAll Loss: 1.8062\tTriple Loss(0): 0.2217\tClassification Loss: 1.3628\r\n",
      "Train Epoch: 22 [149760/209539 (71%)]\tAll Loss: 2.2541\tTriple Loss(0): 0.5629\tClassification Loss: 1.1283\r\n",
      "Train Epoch: 22 [150400/209539 (72%)]\tAll Loss: 1.5862\tTriple Loss(1): 0.0016\tClassification Loss: 1.5830\r\n",
      "Train Epoch: 22 [151040/209539 (72%)]\tAll Loss: 1.0437\tTriple Loss(1): 0.0430\tClassification Loss: 0.9576\r\n",
      "Train Epoch: 22 [151680/209539 (72%)]\tAll Loss: 1.5018\tTriple Loss(1): 0.0167\tClassification Loss: 1.4684\r\n",
      "Train Epoch: 22 [152320/209539 (73%)]\tAll Loss: 1.7657\tTriple Loss(0): 0.3534\tClassification Loss: 1.0589\r\n",
      "Train Epoch: 22 [152960/209539 (73%)]\tAll Loss: 1.6779\tTriple Loss(0): 0.2587\tClassification Loss: 1.1605\r\n",
      "Train Epoch: 22 [153600/209539 (73%)]\tAll Loss: 1.4865\tTriple Loss(1): 0.0128\tClassification Loss: 1.4608\r\n",
      "Train Epoch: 22 [154240/209539 (74%)]\tAll Loss: 1.3215\tTriple Loss(1): 0.0479\tClassification Loss: 1.2258\r\n",
      "Train Epoch: 22 [154880/209539 (74%)]\tAll Loss: 1.5043\tTriple Loss(1): 0.0231\tClassification Loss: 1.4581\r\n",
      "Train Epoch: 22 [155520/209539 (74%)]\tAll Loss: 1.2868\tTriple Loss(1): 0.0217\tClassification Loss: 1.2433\r\n",
      "Train Epoch: 22 [156160/209539 (75%)]\tAll Loss: 1.3760\tTriple Loss(1): 0.0000\tClassification Loss: 1.3760\r\n",
      "Train Epoch: 22 [156800/209539 (75%)]\tAll Loss: 2.2264\tTriple Loss(0): 0.3556\tClassification Loss: 1.5153\r\n",
      "Train Epoch: 22 [157440/209539 (75%)]\tAll Loss: 1.6211\tTriple Loss(1): 0.0378\tClassification Loss: 1.5455\r\n",
      "Train Epoch: 22 [158080/209539 (75%)]\tAll Loss: 1.1429\tTriple Loss(1): 0.0426\tClassification Loss: 1.0577\r\n",
      "Train Epoch: 22 [158720/209539 (76%)]\tAll Loss: 1.2096\tTriple Loss(1): 0.0002\tClassification Loss: 1.2092\r\n",
      "Train Epoch: 22 [159360/209539 (76%)]\tAll Loss: 1.2731\tTriple Loss(1): 0.1008\tClassification Loss: 1.0714\r\n",
      "Train Epoch: 22 [160000/209539 (76%)]\tAll Loss: 1.5728\tTriple Loss(1): 0.0205\tClassification Loss: 1.5319\r\n",
      "Train Epoch: 22 [160640/209539 (77%)]\tAll Loss: 1.2026\tTriple Loss(1): 0.0507\tClassification Loss: 1.1011\r\n",
      "Train Epoch: 22 [161280/209539 (77%)]\tAll Loss: 1.9253\tTriple Loss(0): 0.3538\tClassification Loss: 1.2177\r\n",
      "Train Epoch: 22 [161920/209539 (77%)]\tAll Loss: 1.3529\tTriple Loss(1): 0.0040\tClassification Loss: 1.3449\r\n",
      "Train Epoch: 22 [162560/209539 (78%)]\tAll Loss: 1.2410\tTriple Loss(1): 0.0292\tClassification Loss: 1.1825\r\n",
      "Train Epoch: 22 [163200/209539 (78%)]\tAll Loss: 1.3137\tTriple Loss(1): 0.0577\tClassification Loss: 1.1983\r\n",
      "Train Epoch: 22 [163840/209539 (78%)]\tAll Loss: 1.6483\tTriple Loss(0): 0.2984\tClassification Loss: 1.0515\r\n",
      "Train Epoch: 22 [164480/209539 (78%)]\tAll Loss: 1.1510\tTriple Loss(1): 0.0104\tClassification Loss: 1.1301\r\n",
      "Train Epoch: 22 [165120/209539 (79%)]\tAll Loss: 1.1295\tTriple Loss(1): 0.0070\tClassification Loss: 1.1154\r\n",
      "Train Epoch: 22 [165760/209539 (79%)]\tAll Loss: 1.6673\tTriple Loss(1): 0.0435\tClassification Loss: 1.5803\r\n",
      "Train Epoch: 22 [166400/209539 (79%)]\tAll Loss: 1.3967\tTriple Loss(1): 0.0562\tClassification Loss: 1.2842\r\n",
      "Train Epoch: 22 [167040/209539 (80%)]\tAll Loss: 1.4911\tTriple Loss(1): 0.0350\tClassification Loss: 1.4211\r\n",
      "Train Epoch: 22 [167680/209539 (80%)]\tAll Loss: 1.3609\tTriple Loss(1): 0.0000\tClassification Loss: 1.3609\r\n",
      "Train Epoch: 22 [168320/209539 (80%)]\tAll Loss: 1.1496\tTriple Loss(1): 0.0000\tClassification Loss: 1.1496\r\n",
      "Train Epoch: 22 [168960/209539 (81%)]\tAll Loss: 1.9248\tTriple Loss(0): 0.2912\tClassification Loss: 1.3423\r\n",
      "Train Epoch: 22 [169600/209539 (81%)]\tAll Loss: 1.4039\tTriple Loss(1): 0.0287\tClassification Loss: 1.3466\r\n",
      "Train Epoch: 22 [170240/209539 (81%)]\tAll Loss: 1.3869\tTriple Loss(1): 0.0428\tClassification Loss: 1.3013\r\n",
      "Train Epoch: 22 [170880/209539 (82%)]\tAll Loss: 1.5092\tTriple Loss(1): 0.0105\tClassification Loss: 1.4882\r\n",
      "Train Epoch: 22 [171520/209539 (82%)]\tAll Loss: 1.3531\tTriple Loss(1): 0.1064\tClassification Loss: 1.1403\r\n",
      "Train Epoch: 22 [172160/209539 (82%)]\tAll Loss: 1.5777\tTriple Loss(1): 0.0528\tClassification Loss: 1.4721\r\n",
      "Train Epoch: 22 [172800/209539 (82%)]\tAll Loss: 1.3814\tTriple Loss(1): 0.0225\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 22 [173440/209539 (83%)]\tAll Loss: 1.5789\tTriple Loss(1): 0.0319\tClassification Loss: 1.5150\r\n",
      "Train Epoch: 22 [174080/209539 (83%)]\tAll Loss: 1.3634\tTriple Loss(1): 0.0288\tClassification Loss: 1.3058\r\n",
      "Train Epoch: 22 [174720/209539 (83%)]\tAll Loss: 1.3282\tTriple Loss(1): 0.0396\tClassification Loss: 1.2491\r\n",
      "Train Epoch: 22 [175360/209539 (84%)]\tAll Loss: 1.5098\tTriple Loss(1): 0.0260\tClassification Loss: 1.4578\r\n",
      "Train Epoch: 22 [176000/209539 (84%)]\tAll Loss: 1.5594\tTriple Loss(1): 0.1370\tClassification Loss: 1.2853\r\n",
      "Train Epoch: 22 [176640/209539 (84%)]\tAll Loss: 1.0893\tTriple Loss(1): 0.0622\tClassification Loss: 0.9649\r\n",
      "Train Epoch: 22 [177280/209539 (85%)]\tAll Loss: 1.5592\tTriple Loss(1): 0.0317\tClassification Loss: 1.4958\r\n",
      "Train Epoch: 22 [177920/209539 (85%)]\tAll Loss: 1.3660\tTriple Loss(1): 0.0300\tClassification Loss: 1.3060\r\n",
      "Train Epoch: 22 [178560/209539 (85%)]\tAll Loss: 1.2792\tTriple Loss(1): 0.0227\tClassification Loss: 1.2337\r\n",
      "Train Epoch: 22 [179200/209539 (86%)]\tAll Loss: 1.5702\tTriple Loss(1): 0.0122\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 22 [179840/209539 (86%)]\tAll Loss: 1.4560\tTriple Loss(1): 0.0493\tClassification Loss: 1.3575\r\n",
      "Train Epoch: 22 [180480/209539 (86%)]\tAll Loss: 2.0359\tTriple Loss(0): 0.3006\tClassification Loss: 1.4346\r\n",
      "Train Epoch: 22 [181120/209539 (86%)]\tAll Loss: 1.4570\tTriple Loss(1): 0.0081\tClassification Loss: 1.4407\r\n",
      "Train Epoch: 22 [181760/209539 (87%)]\tAll Loss: 1.1670\tTriple Loss(1): 0.0000\tClassification Loss: 1.1670\r\n",
      "Train Epoch: 22 [182400/209539 (87%)]\tAll Loss: 1.6816\tTriple Loss(1): 0.0463\tClassification Loss: 1.5891\r\n",
      "Train Epoch: 22 [183040/209539 (87%)]\tAll Loss: 1.4288\tTriple Loss(1): 0.0001\tClassification Loss: 1.4285\r\n",
      "Train Epoch: 22 [183680/209539 (88%)]\tAll Loss: 1.8503\tTriple Loss(0): 0.4237\tClassification Loss: 1.0029\r\n",
      "Train Epoch: 22 [184320/209539 (88%)]\tAll Loss: 1.3085\tTriple Loss(1): 0.0371\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 22 [184960/209539 (88%)]\tAll Loss: 1.1509\tTriple Loss(1): 0.0129\tClassification Loss: 1.1251\r\n",
      "Train Epoch: 22 [185600/209539 (89%)]\tAll Loss: 1.3181\tTriple Loss(1): 0.0235\tClassification Loss: 1.2712\r\n",
      "Train Epoch: 22 [186240/209539 (89%)]\tAll Loss: 1.4213\tTriple Loss(1): 0.0248\tClassification Loss: 1.3717\r\n",
      "Train Epoch: 22 [186880/209539 (89%)]\tAll Loss: 1.4134\tTriple Loss(1): 0.0483\tClassification Loss: 1.3168\r\n",
      "Train Epoch: 22 [187520/209539 (89%)]\tAll Loss: 1.3763\tTriple Loss(1): 0.0000\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 22 [188160/209539 (90%)]\tAll Loss: 1.8911\tTriple Loss(0): 0.4035\tClassification Loss: 1.0842\r\n",
      "Train Epoch: 22 [188800/209539 (90%)]\tAll Loss: 1.2743\tTriple Loss(1): 0.0791\tClassification Loss: 1.1161\r\n",
      "Train Epoch: 22 [189440/209539 (90%)]\tAll Loss: 2.0589\tTriple Loss(0): 0.4399\tClassification Loss: 1.1790\r\n",
      "Train Epoch: 22 [190080/209539 (91%)]\tAll Loss: 1.2076\tTriple Loss(1): 0.0114\tClassification Loss: 1.1848\r\n",
      "Train Epoch: 22 [190720/209539 (91%)]\tAll Loss: 2.1931\tTriple Loss(0): 0.4488\tClassification Loss: 1.2956\r\n",
      "Train Epoch: 22 [191360/209539 (91%)]\tAll Loss: 1.9168\tTriple Loss(0): 0.4101\tClassification Loss: 1.0967\r\n",
      "Train Epoch: 22 [192000/209539 (92%)]\tAll Loss: 1.7916\tTriple Loss(1): 0.0478\tClassification Loss: 1.6961\r\n",
      "Train Epoch: 22 [192640/209539 (92%)]\tAll Loss: 1.2187\tTriple Loss(1): 0.0012\tClassification Loss: 1.2162\r\n",
      "Train Epoch: 22 [193280/209539 (92%)]\tAll Loss: 1.1822\tTriple Loss(1): 0.0488\tClassification Loss: 1.0845\r\n",
      "Train Epoch: 22 [193920/209539 (93%)]\tAll Loss: 1.2244\tTriple Loss(1): 0.0074\tClassification Loss: 1.2096\r\n",
      "Train Epoch: 22 [194560/209539 (93%)]\tAll Loss: 1.2297\tTriple Loss(1): 0.0293\tClassification Loss: 1.1711\r\n",
      "Train Epoch: 22 [195200/209539 (93%)]\tAll Loss: 1.6935\tTriple Loss(1): 0.0479\tClassification Loss: 1.5978\r\n",
      "Train Epoch: 22 [195840/209539 (93%)]\tAll Loss: 0.9746\tTriple Loss(1): 0.0277\tClassification Loss: 0.9192\r\n",
      "Train Epoch: 22 [196480/209539 (94%)]\tAll Loss: 1.6319\tTriple Loss(1): 0.0129\tClassification Loss: 1.6062\r\n",
      "Train Epoch: 22 [197120/209539 (94%)]\tAll Loss: 1.5425\tTriple Loss(1): 0.0186\tClassification Loss: 1.5053\r\n",
      "Train Epoch: 22 [197760/209539 (94%)]\tAll Loss: 1.2473\tTriple Loss(1): 0.0057\tClassification Loss: 1.2358\r\n",
      "Train Epoch: 22 [198400/209539 (95%)]\tAll Loss: 1.2919\tTriple Loss(1): 0.0394\tClassification Loss: 1.2131\r\n",
      "Train Epoch: 22 [199040/209539 (95%)]\tAll Loss: 1.4928\tTriple Loss(1): 0.0349\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 22 [199680/209539 (95%)]\tAll Loss: 1.1815\tTriple Loss(1): 0.0076\tClassification Loss: 1.1663\r\n",
      "Train Epoch: 22 [200320/209539 (96%)]\tAll Loss: 1.7514\tTriple Loss(0): 0.3052\tClassification Loss: 1.1409\r\n",
      "Train Epoch: 22 [200960/209539 (96%)]\tAll Loss: 1.2805\tTriple Loss(1): 0.0484\tClassification Loss: 1.1836\r\n",
      "Train Epoch: 22 [201600/209539 (96%)]\tAll Loss: 1.1057\tTriple Loss(1): 0.0176\tClassification Loss: 1.0705\r\n",
      "Train Epoch: 22 [202240/209539 (97%)]\tAll Loss: 1.2585\tTriple Loss(1): 0.0198\tClassification Loss: 1.2188\r\n",
      "Train Epoch: 22 [202880/209539 (97%)]\tAll Loss: 0.9870\tTriple Loss(1): 0.0056\tClassification Loss: 0.9757\r\n",
      "Train Epoch: 22 [203520/209539 (97%)]\tAll Loss: 1.2034\tTriple Loss(1): 0.0050\tClassification Loss: 1.1934\r\n",
      "Train Epoch: 22 [204160/209539 (97%)]\tAll Loss: 1.7695\tTriple Loss(1): 0.0107\tClassification Loss: 1.7482\r\n",
      "Train Epoch: 22 [204800/209539 (98%)]\tAll Loss: 2.1753\tTriple Loss(0): 0.3707\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 22 [205440/209539 (98%)]\tAll Loss: 1.3396\tTriple Loss(1): 0.0725\tClassification Loss: 1.1947\r\n",
      "Train Epoch: 22 [206080/209539 (98%)]\tAll Loss: 1.1334\tTriple Loss(1): 0.0128\tClassification Loss: 1.1079\r\n",
      "Train Epoch: 22 [206720/209539 (99%)]\tAll Loss: 1.3199\tTriple Loss(1): 0.0826\tClassification Loss: 1.1548\r\n",
      "Train Epoch: 22 [207360/209539 (99%)]\tAll Loss: 1.2507\tTriple Loss(1): 0.0099\tClassification Loss: 1.2309\r\n",
      "Train Epoch: 22 [208000/209539 (99%)]\tAll Loss: 1.5106\tTriple Loss(1): 0.0383\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 22 [208640/209539 (100%)]\tAll Loss: 1.3513\tTriple Loss(1): 0.0296\tClassification Loss: 1.2922\r\n",
      "Train Epoch: 22 [209280/209539 (100%)]\tAll Loss: 2.4483\tTriple Loss(0): 0.4260\tClassification Loss: 1.5962\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/22_epochs\r\n",
      "Train Epoch: 23 [0/209539 (0%)]\tAll Loss: 1.9058\tTriple Loss(1): 0.1247\tClassification Loss: 1.6563\r\n",
      "\r\n",
      "Test set: Average loss: 1.1471\r\n",
      "Top 1 Accuracy: 53420/80128 (67%)\r\n",
      "Top 3 Accuracy: 69112/80128 (86%)\r\n",
      "Top 5 Accuracy: 74063/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 23 [640/209539 (0%)]\tAll Loss: 2.3908\tTriple Loss(0): 0.5555\tClassification Loss: 1.2798\r\n",
      "Train Epoch: 23 [1280/209539 (1%)]\tAll Loss: 1.1489\tTriple Loss(1): 0.0222\tClassification Loss: 1.1045\r\n",
      "Train Epoch: 23 [1920/209539 (1%)]\tAll Loss: 1.8421\tTriple Loss(0): 0.2337\tClassification Loss: 1.3747\r\n",
      "Train Epoch: 23 [2560/209539 (1%)]\tAll Loss: 1.7473\tTriple Loss(1): 0.0599\tClassification Loss: 1.6275\r\n",
      "Train Epoch: 23 [3200/209539 (2%)]\tAll Loss: 1.4617\tTriple Loss(1): 0.0158\tClassification Loss: 1.4300\r\n",
      "Train Epoch: 23 [3840/209539 (2%)]\tAll Loss: 1.1475\tTriple Loss(1): 0.0000\tClassification Loss: 1.1475\r\n",
      "Train Epoch: 23 [4480/209539 (2%)]\tAll Loss: 1.5509\tTriple Loss(1): 0.0284\tClassification Loss: 1.4942\r\n",
      "Train Epoch: 23 [5120/209539 (2%)]\tAll Loss: 1.2428\tTriple Loss(1): 0.0008\tClassification Loss: 1.2412\r\n",
      "Train Epoch: 23 [5760/209539 (3%)]\tAll Loss: 1.5033\tTriple Loss(1): 0.0113\tClassification Loss: 1.4808\r\n",
      "Train Epoch: 23 [6400/209539 (3%)]\tAll Loss: 1.0798\tTriple Loss(1): 0.0156\tClassification Loss: 1.0487\r\n",
      "Train Epoch: 23 [7040/209539 (3%)]\tAll Loss: 1.0997\tTriple Loss(1): 0.0004\tClassification Loss: 1.0990\r\n",
      "Train Epoch: 23 [7680/209539 (4%)]\tAll Loss: 1.4133\tTriple Loss(1): 0.0572\tClassification Loss: 1.2990\r\n",
      "Train Epoch: 23 [8320/209539 (4%)]\tAll Loss: 1.2679\tTriple Loss(1): 0.0115\tClassification Loss: 1.2450\r\n",
      "Train Epoch: 23 [8960/209539 (4%)]\tAll Loss: 1.3407\tTriple Loss(1): 0.0353\tClassification Loss: 1.2701\r\n",
      "Train Epoch: 23 [9600/209539 (5%)]\tAll Loss: 1.7088\tTriple Loss(1): 0.0000\tClassification Loss: 1.7088\r\n",
      "Train Epoch: 23 [10240/209539 (5%)]\tAll Loss: 1.8789\tTriple Loss(0): 0.4208\tClassification Loss: 1.0372\r\n",
      "Train Epoch: 23 [10880/209539 (5%)]\tAll Loss: 1.2189\tTriple Loss(1): 0.0064\tClassification Loss: 1.2061\r\n",
      "Train Epoch: 23 [11520/209539 (5%)]\tAll Loss: 1.9065\tTriple Loss(0): 0.2856\tClassification Loss: 1.3353\r\n",
      "Train Epoch: 23 [12160/209539 (6%)]\tAll Loss: 1.0832\tTriple Loss(1): 0.0240\tClassification Loss: 1.0353\r\n",
      "Train Epoch: 23 [12800/209539 (6%)]\tAll Loss: 1.1419\tTriple Loss(1): 0.0654\tClassification Loss: 1.0112\r\n",
      "Train Epoch: 23 [13440/209539 (6%)]\tAll Loss: 1.1480\tTriple Loss(1): 0.0370\tClassification Loss: 1.0740\r\n",
      "Train Epoch: 23 [14080/209539 (7%)]\tAll Loss: 1.1584\tTriple Loss(1): 0.0193\tClassification Loss: 1.1197\r\n",
      "Train Epoch: 23 [14720/209539 (7%)]\tAll Loss: 1.3206\tTriple Loss(1): 0.0000\tClassification Loss: 1.3206\r\n",
      "Train Epoch: 23 [15360/209539 (7%)]\tAll Loss: 1.2082\tTriple Loss(1): 0.0501\tClassification Loss: 1.1081\r\n",
      "Train Epoch: 23 [16000/209539 (8%)]\tAll Loss: 2.3839\tTriple Loss(0): 0.3753\tClassification Loss: 1.6333\r\n",
      "Train Epoch: 23 [16640/209539 (8%)]\tAll Loss: 1.7323\tTriple Loss(1): 0.0243\tClassification Loss: 1.6838\r\n",
      "Train Epoch: 23 [17280/209539 (8%)]\tAll Loss: 1.3110\tTriple Loss(1): 0.0377\tClassification Loss: 1.2356\r\n",
      "Train Epoch: 23 [17920/209539 (9%)]\tAll Loss: 1.3048\tTriple Loss(1): 0.0162\tClassification Loss: 1.2725\r\n",
      "Train Epoch: 23 [18560/209539 (9%)]\tAll Loss: 1.5650\tTriple Loss(1): 0.0878\tClassification Loss: 1.3894\r\n",
      "Train Epoch: 23 [19200/209539 (9%)]\tAll Loss: 1.3134\tTriple Loss(1): 0.0701\tClassification Loss: 1.1733\r\n",
      "Train Epoch: 23 [19840/209539 (9%)]\tAll Loss: 1.8200\tTriple Loss(0): 0.2941\tClassification Loss: 1.2318\r\n",
      "Train Epoch: 23 [20480/209539 (10%)]\tAll Loss: 1.1469\tTriple Loss(1): 0.0307\tClassification Loss: 1.0854\r\n",
      "Train Epoch: 23 [21120/209539 (10%)]\tAll Loss: 1.5279\tTriple Loss(1): 0.0386\tClassification Loss: 1.4507\r\n",
      "Train Epoch: 23 [21760/209539 (10%)]\tAll Loss: 1.3639\tTriple Loss(1): 0.0287\tClassification Loss: 1.3066\r\n",
      "Train Epoch: 23 [22400/209539 (11%)]\tAll Loss: 1.1509\tTriple Loss(1): 0.0246\tClassification Loss: 1.1018\r\n",
      "Train Epoch: 23 [23040/209539 (11%)]\tAll Loss: 1.4750\tTriple Loss(1): 0.0038\tClassification Loss: 1.4673\r\n",
      "Train Epoch: 23 [23680/209539 (11%)]\tAll Loss: 1.0326\tTriple Loss(1): 0.0000\tClassification Loss: 1.0326\r\n",
      "Train Epoch: 23 [24320/209539 (12%)]\tAll Loss: 1.5082\tTriple Loss(1): 0.0221\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 23 [24960/209539 (12%)]\tAll Loss: 1.2195\tTriple Loss(1): 0.0056\tClassification Loss: 1.2082\r\n",
      "Train Epoch: 23 [25600/209539 (12%)]\tAll Loss: 1.3354\tTriple Loss(1): 0.0259\tClassification Loss: 1.2837\r\n",
      "Train Epoch: 23 [26240/209539 (13%)]\tAll Loss: 1.3385\tTriple Loss(1): 0.0181\tClassification Loss: 1.3024\r\n",
      "Train Epoch: 23 [26880/209539 (13%)]\tAll Loss: 1.5432\tTriple Loss(1): 0.0460\tClassification Loss: 1.4511\r\n",
      "Train Epoch: 23 [27520/209539 (13%)]\tAll Loss: 1.0909\tTriple Loss(1): 0.0275\tClassification Loss: 1.0360\r\n",
      "Train Epoch: 23 [28160/209539 (13%)]\tAll Loss: 1.5455\tTriple Loss(1): 0.0226\tClassification Loss: 1.5003\r\n",
      "Train Epoch: 23 [28800/209539 (14%)]\tAll Loss: 1.5230\tTriple Loss(1): 0.0103\tClassification Loss: 1.5024\r\n",
      "Train Epoch: 23 [29440/209539 (14%)]\tAll Loss: 1.4721\tTriple Loss(1): 0.0087\tClassification Loss: 1.4548\r\n",
      "Train Epoch: 23 [30080/209539 (14%)]\tAll Loss: 1.0830\tTriple Loss(1): 0.0000\tClassification Loss: 1.0830\r\n",
      "Train Epoch: 23 [30720/209539 (15%)]\tAll Loss: 1.2116\tTriple Loss(1): 0.0388\tClassification Loss: 1.1341\r\n",
      "Train Epoch: 23 [31360/209539 (15%)]\tAll Loss: 1.4560\tTriple Loss(1): 0.0672\tClassification Loss: 1.3217\r\n",
      "Train Epoch: 23 [32000/209539 (15%)]\tAll Loss: 1.3148\tTriple Loss(1): 0.0532\tClassification Loss: 1.2083\r\n",
      "Train Epoch: 23 [32640/209539 (16%)]\tAll Loss: 1.9395\tTriple Loss(0): 0.3545\tClassification Loss: 1.2304\r\n",
      "Train Epoch: 23 [33280/209539 (16%)]\tAll Loss: 1.5135\tTriple Loss(1): 0.0593\tClassification Loss: 1.3949\r\n",
      "Train Epoch: 23 [33920/209539 (16%)]\tAll Loss: 1.3524\tTriple Loss(1): 0.0283\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 23 [34560/209539 (16%)]\tAll Loss: 1.3641\tTriple Loss(1): 0.0030\tClassification Loss: 1.3581\r\n",
      "Train Epoch: 23 [35200/209539 (17%)]\tAll Loss: 1.1226\tTriple Loss(1): 0.0000\tClassification Loss: 1.1226\r\n",
      "Train Epoch: 23 [35840/209539 (17%)]\tAll Loss: 1.2499\tTriple Loss(1): 0.0000\tClassification Loss: 1.2499\r\n",
      "Train Epoch: 23 [36480/209539 (17%)]\tAll Loss: 1.1716\tTriple Loss(1): 0.0596\tClassification Loss: 1.0524\r\n",
      "Train Epoch: 23 [37120/209539 (18%)]\tAll Loss: 1.6229\tTriple Loss(1): 0.0000\tClassification Loss: 1.6229\r\n",
      "Train Epoch: 23 [37760/209539 (18%)]\tAll Loss: 1.5873\tTriple Loss(1): 0.1076\tClassification Loss: 1.3721\r\n",
      "Train Epoch: 23 [38400/209539 (18%)]\tAll Loss: 1.4287\tTriple Loss(1): 0.0124\tClassification Loss: 1.4038\r\n",
      "Train Epoch: 23 [39040/209539 (19%)]\tAll Loss: 1.0897\tTriple Loss(1): 0.0333\tClassification Loss: 1.0232\r\n",
      "Train Epoch: 23 [39680/209539 (19%)]\tAll Loss: 1.6348\tTriple Loss(0): 0.2520\tClassification Loss: 1.1309\r\n",
      "Train Epoch: 23 [40320/209539 (19%)]\tAll Loss: 1.4334\tTriple Loss(1): 0.0159\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 23 [40960/209539 (20%)]\tAll Loss: 1.2161\tTriple Loss(1): 0.0363\tClassification Loss: 1.1435\r\n",
      "Train Epoch: 23 [41600/209539 (20%)]\tAll Loss: 1.3688\tTriple Loss(1): 0.0145\tClassification Loss: 1.3398\r\n",
      "Train Epoch: 23 [42240/209539 (20%)]\tAll Loss: 1.3486\tTriple Loss(1): 0.0269\tClassification Loss: 1.2947\r\n",
      "Train Epoch: 23 [42880/209539 (20%)]\tAll Loss: 1.1068\tTriple Loss(1): 0.0000\tClassification Loss: 1.1068\r\n",
      "Train Epoch: 23 [43520/209539 (21%)]\tAll Loss: 1.5308\tTriple Loss(1): 0.0530\tClassification Loss: 1.4248\r\n",
      "Train Epoch: 23 [44160/209539 (21%)]\tAll Loss: 1.8084\tTriple Loss(1): 0.0448\tClassification Loss: 1.7188\r\n",
      "Train Epoch: 23 [44800/209539 (21%)]\tAll Loss: 1.6457\tTriple Loss(1): 0.0413\tClassification Loss: 1.5631\r\n",
      "Train Epoch: 23 [45440/209539 (22%)]\tAll Loss: 1.9724\tTriple Loss(1): 0.0309\tClassification Loss: 1.9105\r\n",
      "Train Epoch: 23 [46080/209539 (22%)]\tAll Loss: 1.3342\tTriple Loss(1): 0.0337\tClassification Loss: 1.2667\r\n",
      "Train Epoch: 23 [46720/209539 (22%)]\tAll Loss: 1.7258\tTriple Loss(1): 0.0295\tClassification Loss: 1.6669\r\n",
      "Train Epoch: 23 [47360/209539 (23%)]\tAll Loss: 1.0777\tTriple Loss(1): 0.0344\tClassification Loss: 1.0090\r\n",
      "Train Epoch: 23 [48000/209539 (23%)]\tAll Loss: 1.3590\tTriple Loss(1): 0.0000\tClassification Loss: 1.3590\r\n",
      "Train Epoch: 23 [48640/209539 (23%)]\tAll Loss: 2.0250\tTriple Loss(0): 0.1748\tClassification Loss: 1.6755\r\n",
      "Train Epoch: 23 [49280/209539 (24%)]\tAll Loss: 1.2847\tTriple Loss(1): 0.0229\tClassification Loss: 1.2389\r\n",
      "Train Epoch: 23 [49920/209539 (24%)]\tAll Loss: 1.8141\tTriple Loss(0): 0.1340\tClassification Loss: 1.5461\r\n",
      "Train Epoch: 23 [50560/209539 (24%)]\tAll Loss: 1.4633\tTriple Loss(1): 0.0191\tClassification Loss: 1.4250\r\n",
      "Train Epoch: 23 [51200/209539 (24%)]\tAll Loss: 1.3129\tTriple Loss(1): 0.0094\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 23 [51840/209539 (25%)]\tAll Loss: 1.1546\tTriple Loss(1): 0.0315\tClassification Loss: 1.0916\r\n",
      "Train Epoch: 23 [52480/209539 (25%)]\tAll Loss: 1.2431\tTriple Loss(1): 0.0677\tClassification Loss: 1.1077\r\n",
      "Train Epoch: 23 [53120/209539 (25%)]\tAll Loss: 1.3949\tTriple Loss(1): 0.0351\tClassification Loss: 1.3246\r\n",
      "Train Epoch: 23 [53760/209539 (26%)]\tAll Loss: 2.3499\tTriple Loss(0): 0.3611\tClassification Loss: 1.6277\r\n",
      "Train Epoch: 23 [54400/209539 (26%)]\tAll Loss: 1.8291\tTriple Loss(1): 0.0553\tClassification Loss: 1.7185\r\n",
      "Train Epoch: 23 [55040/209539 (26%)]\tAll Loss: 1.0720\tTriple Loss(1): 0.0234\tClassification Loss: 1.0253\r\n",
      "Train Epoch: 23 [55680/209539 (27%)]\tAll Loss: 1.7848\tTriple Loss(1): 0.0804\tClassification Loss: 1.6240\r\n",
      "Train Epoch: 23 [56320/209539 (27%)]\tAll Loss: 1.1514\tTriple Loss(1): 0.0380\tClassification Loss: 1.0754\r\n",
      "Train Epoch: 23 [56960/209539 (27%)]\tAll Loss: 1.2589\tTriple Loss(1): 0.0191\tClassification Loss: 1.2208\r\n",
      "Train Epoch: 23 [57600/209539 (27%)]\tAll Loss: 1.2058\tTriple Loss(1): 0.0399\tClassification Loss: 1.1259\r\n",
      "Train Epoch: 23 [58240/209539 (28%)]\tAll Loss: 1.0554\tTriple Loss(1): 0.0124\tClassification Loss: 1.0306\r\n",
      "Train Epoch: 23 [58880/209539 (28%)]\tAll Loss: 1.3480\tTriple Loss(1): 0.0281\tClassification Loss: 1.2917\r\n",
      "Train Epoch: 23 [59520/209539 (28%)]\tAll Loss: 1.3995\tTriple Loss(0): 0.1559\tClassification Loss: 1.0876\r\n",
      "Train Epoch: 23 [60160/209539 (29%)]\tAll Loss: 2.2558\tTriple Loss(0): 0.3916\tClassification Loss: 1.4727\r\n",
      "Train Epoch: 23 [60800/209539 (29%)]\tAll Loss: 1.4781\tTriple Loss(1): 0.0000\tClassification Loss: 1.4781\r\n",
      "Train Epoch: 23 [61440/209539 (29%)]\tAll Loss: 1.4344\tTriple Loss(1): 0.0000\tClassification Loss: 1.4344\r\n",
      "Train Epoch: 23 [62080/209539 (30%)]\tAll Loss: 1.4468\tTriple Loss(1): 0.0875\tClassification Loss: 1.2717\r\n",
      "Train Epoch: 23 [62720/209539 (30%)]\tAll Loss: 1.2591\tTriple Loss(1): 0.0228\tClassification Loss: 1.2135\r\n",
      "Train Epoch: 23 [63360/209539 (30%)]\tAll Loss: 2.0808\tTriple Loss(0): 0.4257\tClassification Loss: 1.2295\r\n",
      "Train Epoch: 23 [64000/209539 (31%)]\tAll Loss: 1.4844\tTriple Loss(1): 0.0789\tClassification Loss: 1.3265\r\n",
      "Train Epoch: 23 [64640/209539 (31%)]\tAll Loss: 1.7315\tTriple Loss(1): 0.0733\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 23 [65280/209539 (31%)]\tAll Loss: 1.4109\tTriple Loss(1): 0.0182\tClassification Loss: 1.3745\r\n",
      "Train Epoch: 23 [65920/209539 (31%)]\tAll Loss: 1.5716\tTriple Loss(1): 0.0247\tClassification Loss: 1.5223\r\n",
      "Train Epoch: 23 [66560/209539 (32%)]\tAll Loss: 1.3219\tTriple Loss(1): 0.0319\tClassification Loss: 1.2582\r\n",
      "Train Epoch: 23 [67200/209539 (32%)]\tAll Loss: 1.2082\tTriple Loss(1): 0.0088\tClassification Loss: 1.1907\r\n",
      "Train Epoch: 23 [67840/209539 (32%)]\tAll Loss: 1.4968\tTriple Loss(1): 0.0062\tClassification Loss: 1.4843\r\n",
      "Train Epoch: 23 [68480/209539 (33%)]\tAll Loss: 2.5712\tTriple Loss(0): 0.5669\tClassification Loss: 1.4373\r\n",
      "Train Epoch: 23 [69120/209539 (33%)]\tAll Loss: 1.4778\tTriple Loss(1): 0.0258\tClassification Loss: 1.4261\r\n",
      "Train Epoch: 23 [69760/209539 (33%)]\tAll Loss: 1.1875\tTriple Loss(1): 0.0302\tClassification Loss: 1.1271\r\n",
      "Train Epoch: 23 [70400/209539 (34%)]\tAll Loss: 1.2355\tTriple Loss(1): 0.0171\tClassification Loss: 1.2013\r\n",
      "Train Epoch: 23 [71040/209539 (34%)]\tAll Loss: 1.6761\tTriple Loss(1): 0.0378\tClassification Loss: 1.6004\r\n",
      "Train Epoch: 23 [71680/209539 (34%)]\tAll Loss: 1.6951\tTriple Loss(1): 0.0506\tClassification Loss: 1.5939\r\n",
      "Train Epoch: 23 [72320/209539 (35%)]\tAll Loss: 1.2514\tTriple Loss(1): 0.0083\tClassification Loss: 1.2348\r\n",
      "Train Epoch: 23 [72960/209539 (35%)]\tAll Loss: 1.1028\tTriple Loss(1): 0.0221\tClassification Loss: 1.0586\r\n",
      "Train Epoch: 23 [73600/209539 (35%)]\tAll Loss: 1.4301\tTriple Loss(1): 0.0615\tClassification Loss: 1.3071\r\n",
      "Train Epoch: 23 [74240/209539 (35%)]\tAll Loss: 1.5138\tTriple Loss(1): 0.0051\tClassification Loss: 1.5037\r\n",
      "Train Epoch: 23 [74880/209539 (36%)]\tAll Loss: 1.8583\tTriple Loss(1): 0.0414\tClassification Loss: 1.7754\r\n",
      "Train Epoch: 23 [75520/209539 (36%)]\tAll Loss: 1.1452\tTriple Loss(1): 0.0659\tClassification Loss: 1.0134\r\n",
      "Train Epoch: 23 [76160/209539 (36%)]\tAll Loss: 1.2626\tTriple Loss(1): 0.0717\tClassification Loss: 1.1192\r\n",
      "Train Epoch: 23 [76800/209539 (37%)]\tAll Loss: 1.1552\tTriple Loss(1): 0.0376\tClassification Loss: 1.0800\r\n",
      "Train Epoch: 23 [77440/209539 (37%)]\tAll Loss: 1.4600\tTriple Loss(1): 0.0468\tClassification Loss: 1.3665\r\n",
      "Train Epoch: 23 [78080/209539 (37%)]\tAll Loss: 1.4082\tTriple Loss(1): 0.0263\tClassification Loss: 1.3555\r\n",
      "Train Epoch: 23 [78720/209539 (38%)]\tAll Loss: 1.4187\tTriple Loss(1): 0.0026\tClassification Loss: 1.4135\r\n",
      "Train Epoch: 23 [79360/209539 (38%)]\tAll Loss: 1.1401\tTriple Loss(1): 0.0632\tClassification Loss: 1.0138\r\n",
      "Train Epoch: 23 [80000/209539 (38%)]\tAll Loss: 1.2449\tTriple Loss(1): 0.0553\tClassification Loss: 1.1342\r\n",
      "Train Epoch: 23 [80640/209539 (38%)]\tAll Loss: 1.4131\tTriple Loss(1): 0.0000\tClassification Loss: 1.4131\r\n",
      "Train Epoch: 23 [81280/209539 (39%)]\tAll Loss: 1.5952\tTriple Loss(1): 0.1299\tClassification Loss: 1.3354\r\n",
      "Train Epoch: 23 [81920/209539 (39%)]\tAll Loss: 1.2101\tTriple Loss(1): 0.0255\tClassification Loss: 1.1591\r\n",
      "Train Epoch: 23 [82560/209539 (39%)]\tAll Loss: 1.4797\tTriple Loss(1): 0.0987\tClassification Loss: 1.2823\r\n",
      "Train Epoch: 23 [83200/209539 (40%)]\tAll Loss: 1.6444\tTriple Loss(1): 0.0884\tClassification Loss: 1.4676\r\n",
      "Train Epoch: 23 [83840/209539 (40%)]\tAll Loss: 1.2094\tTriple Loss(1): 0.0015\tClassification Loss: 1.2063\r\n",
      "Train Epoch: 23 [84480/209539 (40%)]\tAll Loss: 1.2013\tTriple Loss(1): 0.0000\tClassification Loss: 1.2013\r\n",
      "Train Epoch: 23 [85120/209539 (41%)]\tAll Loss: 1.9371\tTriple Loss(1): 0.1093\tClassification Loss: 1.7186\r\n",
      "Train Epoch: 23 [85760/209539 (41%)]\tAll Loss: 1.4495\tTriple Loss(1): 0.0413\tClassification Loss: 1.3669\r\n",
      "Train Epoch: 23 [86400/209539 (41%)]\tAll Loss: 1.4181\tTriple Loss(1): 0.0086\tClassification Loss: 1.4009\r\n",
      "Train Epoch: 23 [87040/209539 (42%)]\tAll Loss: 1.1970\tTriple Loss(1): 0.0151\tClassification Loss: 1.1668\r\n",
      "Train Epoch: 23 [87680/209539 (42%)]\tAll Loss: 1.2959\tTriple Loss(1): 0.0308\tClassification Loss: 1.2343\r\n",
      "Train Epoch: 23 [88320/209539 (42%)]\tAll Loss: 1.5247\tTriple Loss(1): 0.0863\tClassification Loss: 1.3522\r\n",
      "Train Epoch: 23 [88960/209539 (42%)]\tAll Loss: 1.4853\tTriple Loss(1): 0.0559\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 23 [89600/209539 (43%)]\tAll Loss: 1.3937\tTriple Loss(1): 0.0109\tClassification Loss: 1.3719\r\n",
      "Train Epoch: 23 [90240/209539 (43%)]\tAll Loss: 1.2523\tTriple Loss(1): 0.0357\tClassification Loss: 1.1810\r\n",
      "Train Epoch: 23 [90880/209539 (43%)]\tAll Loss: 1.8073\tTriple Loss(1): 0.0662\tClassification Loss: 1.6749\r\n",
      "Train Epoch: 23 [91520/209539 (44%)]\tAll Loss: 1.3791\tTriple Loss(1): 0.0215\tClassification Loss: 1.3361\r\n",
      "Train Epoch: 23 [92160/209539 (44%)]\tAll Loss: 1.4430\tTriple Loss(1): 0.0992\tClassification Loss: 1.2447\r\n",
      "Train Epoch: 23 [92800/209539 (44%)]\tAll Loss: 1.2355\tTriple Loss(1): 0.0798\tClassification Loss: 1.0758\r\n",
      "Train Epoch: 23 [93440/209539 (45%)]\tAll Loss: 1.9736\tTriple Loss(0): 0.3247\tClassification Loss: 1.3242\r\n",
      "Train Epoch: 23 [94080/209539 (45%)]\tAll Loss: 1.4130\tTriple Loss(1): 0.0584\tClassification Loss: 1.2962\r\n",
      "Train Epoch: 23 [94720/209539 (45%)]\tAll Loss: 1.3744\tTriple Loss(1): 0.0127\tClassification Loss: 1.3490\r\n",
      "Train Epoch: 23 [95360/209539 (46%)]\tAll Loss: 1.5654\tTriple Loss(1): 0.0516\tClassification Loss: 1.4622\r\n",
      "Train Epoch: 23 [96000/209539 (46%)]\tAll Loss: 1.5295\tTriple Loss(1): 0.0398\tClassification Loss: 1.4500\r\n",
      "Train Epoch: 23 [96640/209539 (46%)]\tAll Loss: 1.5157\tTriple Loss(1): 0.0540\tClassification Loss: 1.4076\r\n",
      "Train Epoch: 23 [97280/209539 (46%)]\tAll Loss: 1.3341\tTriple Loss(1): 0.0387\tClassification Loss: 1.2567\r\n",
      "Train Epoch: 23 [97920/209539 (47%)]\tAll Loss: 1.4612\tTriple Loss(1): 0.0404\tClassification Loss: 1.3805\r\n",
      "Train Epoch: 23 [98560/209539 (47%)]\tAll Loss: 1.7293\tTriple Loss(1): 0.0608\tClassification Loss: 1.6077\r\n",
      "Train Epoch: 23 [99200/209539 (47%)]\tAll Loss: 1.2043\tTriple Loss(1): 0.0371\tClassification Loss: 1.1302\r\n",
      "Train Epoch: 23 [99840/209539 (48%)]\tAll Loss: 1.4440\tTriple Loss(1): 0.0163\tClassification Loss: 1.4113\r\n",
      "Train Epoch: 23 [100480/209539 (48%)]\tAll Loss: 1.4919\tTriple Loss(1): 0.0758\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 23 [101120/209539 (48%)]\tAll Loss: 1.8671\tTriple Loss(0): 0.3675\tClassification Loss: 1.1322\r\n",
      "Train Epoch: 23 [101760/209539 (49%)]\tAll Loss: 1.3585\tTriple Loss(1): 0.0579\tClassification Loss: 1.2426\r\n",
      "Train Epoch: 23 [102400/209539 (49%)]\tAll Loss: 1.7827\tTriple Loss(0): 0.2518\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 23 [103040/209539 (49%)]\tAll Loss: 1.3467\tTriple Loss(1): 0.0173\tClassification Loss: 1.3121\r\n",
      "Train Epoch: 23 [103680/209539 (49%)]\tAll Loss: 1.4170\tTriple Loss(1): 0.0039\tClassification Loss: 1.4092\r\n",
      "Train Epoch: 23 [104320/209539 (50%)]\tAll Loss: 1.0852\tTriple Loss(1): 0.0102\tClassification Loss: 1.0648\r\n",
      "Train Epoch: 23 [104960/209539 (50%)]\tAll Loss: 2.0802\tTriple Loss(0): 0.4499\tClassification Loss: 1.1805\r\n",
      "Train Epoch: 23 [105600/209539 (50%)]\tAll Loss: 1.4227\tTriple Loss(1): 0.0598\tClassification Loss: 1.3031\r\n",
      "Train Epoch: 23 [106240/209539 (51%)]\tAll Loss: 2.6722\tTriple Loss(0): 0.6438\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 23 [106880/209539 (51%)]\tAll Loss: 1.2845\tTriple Loss(1): 0.0357\tClassification Loss: 1.2131\r\n",
      "Train Epoch: 23 [107520/209539 (51%)]\tAll Loss: 1.9919\tTriple Loss(0): 0.3404\tClassification Loss: 1.3111\r\n",
      "Train Epoch: 23 [108160/209539 (52%)]\tAll Loss: 1.3332\tTriple Loss(1): 0.0314\tClassification Loss: 1.2704\r\n",
      "Train Epoch: 23 [108800/209539 (52%)]\tAll Loss: 1.2949\tTriple Loss(1): 0.0649\tClassification Loss: 1.1650\r\n",
      "Train Epoch: 23 [109440/209539 (52%)]\tAll Loss: 2.4491\tTriple Loss(0): 0.3705\tClassification Loss: 1.7081\r\n",
      "Train Epoch: 23 [110080/209539 (53%)]\tAll Loss: 1.5967\tTriple Loss(1): 0.0818\tClassification Loss: 1.4330\r\n",
      "Train Epoch: 23 [110720/209539 (53%)]\tAll Loss: 1.4693\tTriple Loss(1): 0.0568\tClassification Loss: 1.3557\r\n",
      "Train Epoch: 23 [111360/209539 (53%)]\tAll Loss: 1.1452\tTriple Loss(1): 0.0543\tClassification Loss: 1.0366\r\n",
      "Train Epoch: 23 [112000/209539 (53%)]\tAll Loss: 1.7939\tTriple Loss(0): 0.3530\tClassification Loss: 1.0880\r\n",
      "Train Epoch: 23 [112640/209539 (54%)]\tAll Loss: 1.3605\tTriple Loss(1): 0.0571\tClassification Loss: 1.2463\r\n",
      "Train Epoch: 23 [113280/209539 (54%)]\tAll Loss: 1.6070\tTriple Loss(0): 0.3342\tClassification Loss: 0.9386\r\n",
      "Train Epoch: 23 [113920/209539 (54%)]\tAll Loss: 2.0267\tTriple Loss(0): 0.3487\tClassification Loss: 1.3293\r\n",
      "Train Epoch: 23 [114560/209539 (55%)]\tAll Loss: 1.4004\tTriple Loss(1): 0.0360\tClassification Loss: 1.3285\r\n",
      "Train Epoch: 23 [115200/209539 (55%)]\tAll Loss: 2.2963\tTriple Loss(0): 0.3702\tClassification Loss: 1.5558\r\n",
      "Train Epoch: 23 [115840/209539 (55%)]\tAll Loss: 1.4064\tTriple Loss(1): 0.0338\tClassification Loss: 1.3388\r\n",
      "Train Epoch: 23 [116480/209539 (56%)]\tAll Loss: 1.4997\tTriple Loss(1): 0.0381\tClassification Loss: 1.4235\r\n",
      "Train Epoch: 23 [117120/209539 (56%)]\tAll Loss: 1.5410\tTriple Loss(1): 0.1230\tClassification Loss: 1.2949\r\n",
      "Train Epoch: 23 [117760/209539 (56%)]\tAll Loss: 1.2231\tTriple Loss(1): 0.0298\tClassification Loss: 1.1634\r\n",
      "Train Epoch: 23 [118400/209539 (57%)]\tAll Loss: 1.7993\tTriple Loss(0): 0.3719\tClassification Loss: 1.0555\r\n",
      "Train Epoch: 23 [119040/209539 (57%)]\tAll Loss: 1.4536\tTriple Loss(1): 0.0242\tClassification Loss: 1.4051\r\n",
      "Train Epoch: 23 [119680/209539 (57%)]\tAll Loss: 1.4384\tTriple Loss(1): 0.0750\tClassification Loss: 1.2883\r\n",
      "Train Epoch: 23 [120320/209539 (57%)]\tAll Loss: 1.8506\tTriple Loss(0): 0.2799\tClassification Loss: 1.2907\r\n",
      "Train Epoch: 23 [120960/209539 (58%)]\tAll Loss: 1.3470\tTriple Loss(1): 0.0343\tClassification Loss: 1.2784\r\n",
      "Train Epoch: 23 [121600/209539 (58%)]\tAll Loss: 1.1483\tTriple Loss(1): 0.0097\tClassification Loss: 1.1289\r\n",
      "Train Epoch: 23 [122240/209539 (58%)]\tAll Loss: 1.1910\tTriple Loss(1): 0.0576\tClassification Loss: 1.0758\r\n",
      "Train Epoch: 23 [122880/209539 (59%)]\tAll Loss: 1.1501\tTriple Loss(1): 0.0000\tClassification Loss: 1.1501\r\n",
      "Train Epoch: 23 [123520/209539 (59%)]\tAll Loss: 1.5028\tTriple Loss(1): 0.0002\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 23 [124160/209539 (59%)]\tAll Loss: 1.5425\tTriple Loss(1): 0.0855\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 23 [124800/209539 (60%)]\tAll Loss: 1.0630\tTriple Loss(1): 0.0000\tClassification Loss: 1.0630\r\n",
      "Train Epoch: 23 [125440/209539 (60%)]\tAll Loss: 2.0502\tTriple Loss(0): 0.3393\tClassification Loss: 1.3716\r\n",
      "Train Epoch: 23 [126080/209539 (60%)]\tAll Loss: 1.3953\tTriple Loss(1): 0.0085\tClassification Loss: 1.3783\r\n",
      "Train Epoch: 23 [126720/209539 (60%)]\tAll Loss: 1.3864\tTriple Loss(1): 0.0000\tClassification Loss: 1.3864\r\n",
      "Train Epoch: 23 [127360/209539 (61%)]\tAll Loss: 2.2237\tTriple Loss(0): 0.3292\tClassification Loss: 1.5653\r\n",
      "Train Epoch: 23 [128000/209539 (61%)]\tAll Loss: 2.0477\tTriple Loss(0): 0.2929\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 23 [128640/209539 (61%)]\tAll Loss: 1.2213\tTriple Loss(1): 0.0505\tClassification Loss: 1.1204\r\n",
      "Train Epoch: 23 [129280/209539 (62%)]\tAll Loss: 1.5788\tTriple Loss(1): 0.0052\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 23 [129920/209539 (62%)]\tAll Loss: 1.7798\tTriple Loss(0): 0.3351\tClassification Loss: 1.1095\r\n",
      "Train Epoch: 23 [130560/209539 (62%)]\tAll Loss: 1.3062\tTriple Loss(1): 0.0540\tClassification Loss: 1.1983\r\n",
      "Train Epoch: 23 [131200/209539 (63%)]\tAll Loss: 1.1352\tTriple Loss(1): 0.0000\tClassification Loss: 1.1352\r\n",
      "Train Epoch: 23 [131840/209539 (63%)]\tAll Loss: 1.4336\tTriple Loss(1): 0.0039\tClassification Loss: 1.4258\r\n",
      "Train Epoch: 23 [132480/209539 (63%)]\tAll Loss: 1.3024\tTriple Loss(1): 0.0578\tClassification Loss: 1.1869\r\n",
      "Train Epoch: 23 [133120/209539 (64%)]\tAll Loss: 1.2580\tTriple Loss(1): 0.0549\tClassification Loss: 1.1482\r\n",
      "Train Epoch: 23 [133760/209539 (64%)]\tAll Loss: 1.4758\tTriple Loss(1): 0.0305\tClassification Loss: 1.4147\r\n",
      "Train Epoch: 23 [134400/209539 (64%)]\tAll Loss: 1.0284\tTriple Loss(1): 0.0003\tClassification Loss: 1.0278\r\n",
      "Train Epoch: 23 [135040/209539 (64%)]\tAll Loss: 1.4038\tTriple Loss(1): 0.0787\tClassification Loss: 1.2463\r\n",
      "Train Epoch: 23 [135680/209539 (65%)]\tAll Loss: 1.5040\tTriple Loss(1): 0.0051\tClassification Loss: 1.4939\r\n",
      "Train Epoch: 23 [136320/209539 (65%)]\tAll Loss: 1.5395\tTriple Loss(1): 0.0777\tClassification Loss: 1.3840\r\n",
      "Train Epoch: 23 [136960/209539 (65%)]\tAll Loss: 1.1645\tTriple Loss(1): 0.0141\tClassification Loss: 1.1362\r\n",
      "Train Epoch: 23 [137600/209539 (66%)]\tAll Loss: 1.3751\tTriple Loss(1): 0.0326\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 23 [138240/209539 (66%)]\tAll Loss: 1.5832\tTriple Loss(1): 0.0995\tClassification Loss: 1.3842\r\n",
      "Train Epoch: 23 [138880/209539 (66%)]\tAll Loss: 1.8091\tTriple Loss(0): 0.3227\tClassification Loss: 1.1638\r\n",
      "Train Epoch: 23 [139520/209539 (67%)]\tAll Loss: 1.3389\tTriple Loss(1): 0.0032\tClassification Loss: 1.3325\r\n",
      "Train Epoch: 23 [140160/209539 (67%)]\tAll Loss: 1.2839\tTriple Loss(1): 0.0333\tClassification Loss: 1.2173\r\n",
      "Train Epoch: 23 [140800/209539 (67%)]\tAll Loss: 1.6095\tTriple Loss(1): 0.1069\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 23 [141440/209539 (68%)]\tAll Loss: 1.3587\tTriple Loss(1): 0.0134\tClassification Loss: 1.3319\r\n",
      "Train Epoch: 23 [142080/209539 (68%)]\tAll Loss: 1.7721\tTriple Loss(0): 0.3272\tClassification Loss: 1.1178\r\n",
      "Train Epoch: 23 [142720/209539 (68%)]\tAll Loss: 1.5916\tTriple Loss(1): 0.0255\tClassification Loss: 1.5407\r\n",
      "Train Epoch: 23 [143360/209539 (68%)]\tAll Loss: 1.5391\tTriple Loss(0): 0.2591\tClassification Loss: 1.0209\r\n",
      "Train Epoch: 23 [144000/209539 (69%)]\tAll Loss: 2.2282\tTriple Loss(0): 0.4806\tClassification Loss: 1.2670\r\n",
      "Train Epoch: 23 [144640/209539 (69%)]\tAll Loss: 1.2929\tTriple Loss(1): 0.0254\tClassification Loss: 1.2421\r\n",
      "Train Epoch: 23 [145280/209539 (69%)]\tAll Loss: 1.6591\tTriple Loss(1): 0.0748\tClassification Loss: 1.5096\r\n",
      "Train Epoch: 23 [145920/209539 (70%)]\tAll Loss: 1.3035\tTriple Loss(1): 0.0132\tClassification Loss: 1.2771\r\n",
      "Train Epoch: 23 [146560/209539 (70%)]\tAll Loss: 1.4108\tTriple Loss(1): 0.1354\tClassification Loss: 1.1399\r\n",
      "Train Epoch: 23 [147200/209539 (70%)]\tAll Loss: 1.4825\tTriple Loss(1): 0.0014\tClassification Loss: 1.4797\r\n",
      "Train Epoch: 23 [147840/209539 (71%)]\tAll Loss: 1.0956\tTriple Loss(1): 0.0000\tClassification Loss: 1.0956\r\n",
      "Train Epoch: 23 [148480/209539 (71%)]\tAll Loss: 1.1566\tTriple Loss(1): 0.0693\tClassification Loss: 1.0179\r\n",
      "Train Epoch: 23 [149120/209539 (71%)]\tAll Loss: 1.2911\tTriple Loss(1): 0.0103\tClassification Loss: 1.2706\r\n",
      "Train Epoch: 23 [149760/209539 (71%)]\tAll Loss: 1.1178\tTriple Loss(1): 0.0895\tClassification Loss: 0.9388\r\n",
      "Train Epoch: 23 [150400/209539 (72%)]\tAll Loss: 1.4839\tTriple Loss(1): 0.0090\tClassification Loss: 1.4658\r\n",
      "Train Epoch: 23 [151040/209539 (72%)]\tAll Loss: 1.3172\tTriple Loss(1): 0.0565\tClassification Loss: 1.2041\r\n",
      "Train Epoch: 23 [151680/209539 (72%)]\tAll Loss: 1.4712\tTriple Loss(1): 0.0045\tClassification Loss: 1.4623\r\n",
      "Train Epoch: 23 [152320/209539 (73%)]\tAll Loss: 1.1404\tTriple Loss(1): 0.0529\tClassification Loss: 1.0347\r\n",
      "Train Epoch: 23 [152960/209539 (73%)]\tAll Loss: 2.0125\tTriple Loss(0): 0.4095\tClassification Loss: 1.1936\r\n",
      "Train Epoch: 23 [153600/209539 (73%)]\tAll Loss: 1.4742\tTriple Loss(1): 0.0677\tClassification Loss: 1.3387\r\n",
      "Train Epoch: 23 [154240/209539 (74%)]\tAll Loss: 1.4698\tTriple Loss(1): 0.0856\tClassification Loss: 1.2985\r\n",
      "Train Epoch: 23 [154880/209539 (74%)]\tAll Loss: 1.5243\tTriple Loss(1): 0.0029\tClassification Loss: 1.5185\r\n",
      "Train Epoch: 23 [155520/209539 (74%)]\tAll Loss: 2.1451\tTriple Loss(0): 0.2780\tClassification Loss: 1.5891\r\n",
      "Train Epoch: 23 [156160/209539 (75%)]\tAll Loss: 2.0196\tTriple Loss(0): 0.2854\tClassification Loss: 1.4489\r\n",
      "Train Epoch: 23 [156800/209539 (75%)]\tAll Loss: 2.3640\tTriple Loss(0): 0.3301\tClassification Loss: 1.7039\r\n",
      "Train Epoch: 23 [157440/209539 (75%)]\tAll Loss: 1.6782\tTriple Loss(1): 0.0506\tClassification Loss: 1.5770\r\n",
      "Train Epoch: 23 [158080/209539 (75%)]\tAll Loss: 1.2756\tTriple Loss(1): 0.0290\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 23 [158720/209539 (76%)]\tAll Loss: 2.1269\tTriple Loss(0): 0.5396\tClassification Loss: 1.0476\r\n",
      "Train Epoch: 23 [159360/209539 (76%)]\tAll Loss: 1.0421\tTriple Loss(1): 0.0274\tClassification Loss: 0.9874\r\n",
      "Train Epoch: 23 [160000/209539 (76%)]\tAll Loss: 1.7447\tTriple Loss(1): 0.0617\tClassification Loss: 1.6212\r\n",
      "Train Epoch: 23 [160640/209539 (77%)]\tAll Loss: 1.3752\tTriple Loss(1): 0.0728\tClassification Loss: 1.2296\r\n",
      "Train Epoch: 23 [161280/209539 (77%)]\tAll Loss: 1.3223\tTriple Loss(1): 0.0171\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 23 [161920/209539 (77%)]\tAll Loss: 2.1143\tTriple Loss(0): 0.3960\tClassification Loss: 1.3223\r\n",
      "Train Epoch: 23 [162560/209539 (78%)]\tAll Loss: 1.4113\tTriple Loss(1): 0.0329\tClassification Loss: 1.3455\r\n",
      "Train Epoch: 23 [163200/209539 (78%)]\tAll Loss: 1.1319\tTriple Loss(1): 0.0000\tClassification Loss: 1.1319\r\n",
      "Train Epoch: 23 [163840/209539 (78%)]\tAll Loss: 1.3881\tTriple Loss(1): 0.0657\tClassification Loss: 1.2566\r\n",
      "Train Epoch: 23 [164480/209539 (78%)]\tAll Loss: 2.1920\tTriple Loss(0): 0.4545\tClassification Loss: 1.2831\r\n",
      "Train Epoch: 23 [165120/209539 (79%)]\tAll Loss: 1.7895\tTriple Loss(0): 0.3331\tClassification Loss: 1.1233\r\n",
      "Train Epoch: 23 [165760/209539 (79%)]\tAll Loss: 1.2649\tTriple Loss(1): 0.0250\tClassification Loss: 1.2149\r\n",
      "Train Epoch: 23 [166400/209539 (79%)]\tAll Loss: 2.1061\tTriple Loss(0): 0.3517\tClassification Loss: 1.4026\r\n",
      "Train Epoch: 23 [167040/209539 (80%)]\tAll Loss: 2.0021\tTriple Loss(0): 0.2514\tClassification Loss: 1.4993\r\n",
      "Train Epoch: 23 [167680/209539 (80%)]\tAll Loss: 1.3452\tTriple Loss(1): 0.0427\tClassification Loss: 1.2598\r\n",
      "Train Epoch: 23 [168320/209539 (80%)]\tAll Loss: 1.3670\tTriple Loss(1): 0.1238\tClassification Loss: 1.1194\r\n",
      "Train Epoch: 23 [168960/209539 (81%)]\tAll Loss: 1.8131\tTriple Loss(0): 0.3791\tClassification Loss: 1.0550\r\n",
      "Train Epoch: 23 [169600/209539 (81%)]\tAll Loss: 2.4365\tTriple Loss(0): 0.5357\tClassification Loss: 1.3650\r\n",
      "Train Epoch: 23 [170240/209539 (81%)]\tAll Loss: 1.4449\tTriple Loss(1): 0.0237\tClassification Loss: 1.3974\r\n",
      "Train Epoch: 23 [170880/209539 (82%)]\tAll Loss: 1.3048\tTriple Loss(1): 0.0075\tClassification Loss: 1.2898\r\n",
      "Train Epoch: 23 [171520/209539 (82%)]\tAll Loss: 1.2276\tTriple Loss(0): 0.1161\tClassification Loss: 0.9953\r\n",
      "Train Epoch: 23 [172160/209539 (82%)]\tAll Loss: 1.4570\tTriple Loss(1): 0.0333\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 23 [172800/209539 (82%)]\tAll Loss: 1.3623\tTriple Loss(1): 0.0453\tClassification Loss: 1.2717\r\n",
      "Train Epoch: 23 [173440/209539 (83%)]\tAll Loss: 2.1714\tTriple Loss(0): 0.2718\tClassification Loss: 1.6278\r\n",
      "Train Epoch: 23 [174080/209539 (83%)]\tAll Loss: 1.9103\tTriple Loss(0): 0.2864\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 23 [174720/209539 (83%)]\tAll Loss: 2.4707\tTriple Loss(0): 0.4794\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 23 [175360/209539 (84%)]\tAll Loss: 1.6014\tTriple Loss(1): 0.0797\tClassification Loss: 1.4421\r\n",
      "Train Epoch: 23 [176000/209539 (84%)]\tAll Loss: 1.2687\tTriple Loss(1): 0.0239\tClassification Loss: 1.2210\r\n",
      "Train Epoch: 23 [176640/209539 (84%)]\tAll Loss: 0.9717\tTriple Loss(1): 0.0467\tClassification Loss: 0.8784\r\n",
      "Train Epoch: 23 [177280/209539 (85%)]\tAll Loss: 1.4303\tTriple Loss(1): 0.0000\tClassification Loss: 1.4303\r\n",
      "Train Epoch: 23 [177920/209539 (85%)]\tAll Loss: 1.5475\tTriple Loss(1): 0.0217\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 23 [178560/209539 (85%)]\tAll Loss: 1.1394\tTriple Loss(1): 0.0181\tClassification Loss: 1.1032\r\n",
      "Train Epoch: 23 [179200/209539 (86%)]\tAll Loss: 2.1508\tTriple Loss(0): 0.4031\tClassification Loss: 1.3445\r\n",
      "Train Epoch: 23 [179840/209539 (86%)]\tAll Loss: 1.3622\tTriple Loss(1): 0.0155\tClassification Loss: 1.3311\r\n",
      "Train Epoch: 23 [180480/209539 (86%)]\tAll Loss: 1.5622\tTriple Loss(1): 0.1159\tClassification Loss: 1.3304\r\n",
      "Train Epoch: 23 [181120/209539 (86%)]\tAll Loss: 1.5690\tTriple Loss(1): 0.0179\tClassification Loss: 1.5333\r\n",
      "Train Epoch: 23 [181760/209539 (87%)]\tAll Loss: 1.4192\tTriple Loss(1): 0.0892\tClassification Loss: 1.2407\r\n",
      "Train Epoch: 23 [182400/209539 (87%)]\tAll Loss: 2.0714\tTriple Loss(0): 0.2866\tClassification Loss: 1.4982\r\n",
      "Train Epoch: 23 [183040/209539 (87%)]\tAll Loss: 2.2379\tTriple Loss(0): 0.4433\tClassification Loss: 1.3512\r\n",
      "Train Epoch: 23 [183680/209539 (88%)]\tAll Loss: 1.9333\tTriple Loss(0): 0.4197\tClassification Loss: 1.0939\r\n",
      "Train Epoch: 23 [184320/209539 (88%)]\tAll Loss: 1.4505\tTriple Loss(1): 0.0548\tClassification Loss: 1.3410\r\n",
      "Train Epoch: 23 [184960/209539 (88%)]\tAll Loss: 1.2404\tTriple Loss(1): 0.0406\tClassification Loss: 1.1592\r\n",
      "Train Epoch: 23 [185600/209539 (89%)]\tAll Loss: 1.3874\tTriple Loss(1): 0.0281\tClassification Loss: 1.3312\r\n",
      "Train Epoch: 23 [186240/209539 (89%)]\tAll Loss: 1.3702\tTriple Loss(1): 0.0000\tClassification Loss: 1.3702\r\n",
      "Train Epoch: 23 [186880/209539 (89%)]\tAll Loss: 1.4171\tTriple Loss(1): 0.0404\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 23 [187520/209539 (89%)]\tAll Loss: 1.4799\tTriple Loss(1): 0.0014\tClassification Loss: 1.4771\r\n",
      "Train Epoch: 23 [188160/209539 (90%)]\tAll Loss: 1.0568\tTriple Loss(1): 0.0359\tClassification Loss: 0.9851\r\n",
      "Train Epoch: 23 [188800/209539 (90%)]\tAll Loss: 1.2723\tTriple Loss(1): 0.0098\tClassification Loss: 1.2526\r\n",
      "Train Epoch: 23 [189440/209539 (90%)]\tAll Loss: 1.1946\tTriple Loss(1): 0.0114\tClassification Loss: 1.1717\r\n",
      "Train Epoch: 23 [190080/209539 (91%)]\tAll Loss: 1.2748\tTriple Loss(1): 0.0220\tClassification Loss: 1.2308\r\n",
      "Train Epoch: 23 [190720/209539 (91%)]\tAll Loss: 1.2785\tTriple Loss(1): 0.0263\tClassification Loss: 1.2260\r\n",
      "Train Epoch: 23 [191360/209539 (91%)]\tAll Loss: 1.0835\tTriple Loss(1): 0.0000\tClassification Loss: 1.0835\r\n",
      "Train Epoch: 23 [192000/209539 (92%)]\tAll Loss: 1.5306\tTriple Loss(1): 0.0023\tClassification Loss: 1.5261\r\n",
      "Train Epoch: 23 [192640/209539 (92%)]\tAll Loss: 2.1093\tTriple Loss(0): 0.3748\tClassification Loss: 1.3597\r\n",
      "Train Epoch: 23 [193280/209539 (92%)]\tAll Loss: 1.4451\tTriple Loss(1): 0.1366\tClassification Loss: 1.1718\r\n",
      "Train Epoch: 23 [193920/209539 (93%)]\tAll Loss: 1.9623\tTriple Loss(0): 0.3740\tClassification Loss: 1.2143\r\n",
      "Train Epoch: 23 [194560/209539 (93%)]\tAll Loss: 1.9019\tTriple Loss(0): 0.3061\tClassification Loss: 1.2897\r\n",
      "Train Epoch: 23 [195200/209539 (93%)]\tAll Loss: 1.3795\tTriple Loss(1): 0.0000\tClassification Loss: 1.3795\r\n",
      "Train Epoch: 23 [195840/209539 (93%)]\tAll Loss: 1.0027\tTriple Loss(1): 0.0161\tClassification Loss: 0.9706\r\n",
      "Train Epoch: 23 [196480/209539 (94%)]\tAll Loss: 1.6417\tTriple Loss(1): 0.0514\tClassification Loss: 1.5390\r\n",
      "Train Epoch: 23 [197120/209539 (94%)]\tAll Loss: 1.2883\tTriple Loss(1): 0.0161\tClassification Loss: 1.2561\r\n",
      "Train Epoch: 23 [197760/209539 (94%)]\tAll Loss: 1.3800\tTriple Loss(1): 0.0203\tClassification Loss: 1.3394\r\n",
      "Train Epoch: 23 [198400/209539 (95%)]\tAll Loss: 1.5255\tTriple Loss(1): 0.0311\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 23 [199040/209539 (95%)]\tAll Loss: 1.6384\tTriple Loss(1): 0.0588\tClassification Loss: 1.5208\r\n",
      "Train Epoch: 23 [199680/209539 (95%)]\tAll Loss: 1.3315\tTriple Loss(1): 0.0410\tClassification Loss: 1.2495\r\n",
      "Train Epoch: 23 [200320/209539 (96%)]\tAll Loss: 1.9304\tTriple Loss(0): 0.2653\tClassification Loss: 1.3998\r\n",
      "Train Epoch: 23 [200960/209539 (96%)]\tAll Loss: 1.1370\tTriple Loss(1): 0.0211\tClassification Loss: 1.0949\r\n",
      "Train Epoch: 23 [201600/209539 (96%)]\tAll Loss: 1.2287\tTriple Loss(1): 0.0214\tClassification Loss: 1.1860\r\n",
      "Train Epoch: 23 [202240/209539 (97%)]\tAll Loss: 1.3790\tTriple Loss(1): 0.0259\tClassification Loss: 1.3273\r\n",
      "Train Epoch: 23 [202880/209539 (97%)]\tAll Loss: 1.7486\tTriple Loss(0): 0.3852\tClassification Loss: 0.9782\r\n",
      "Train Epoch: 23 [203520/209539 (97%)]\tAll Loss: 1.5785\tTriple Loss(1): 0.0047\tClassification Loss: 1.5692\r\n",
      "Train Epoch: 23 [204160/209539 (97%)]\tAll Loss: 1.6999\tTriple Loss(1): 0.0039\tClassification Loss: 1.6921\r\n",
      "Train Epoch: 23 [204800/209539 (98%)]\tAll Loss: 1.5330\tTriple Loss(1): 0.0139\tClassification Loss: 1.5051\r\n",
      "Train Epoch: 23 [205440/209539 (98%)]\tAll Loss: 1.9424\tTriple Loss(0): 0.4541\tClassification Loss: 1.0342\r\n",
      "Train Epoch: 23 [206080/209539 (98%)]\tAll Loss: 1.8490\tTriple Loss(0): 0.2254\tClassification Loss: 1.3981\r\n",
      "Train Epoch: 23 [206720/209539 (99%)]\tAll Loss: 1.1683\tTriple Loss(1): 0.0000\tClassification Loss: 1.1683\r\n",
      "Train Epoch: 23 [207360/209539 (99%)]\tAll Loss: 1.2646\tTriple Loss(1): 0.0413\tClassification Loss: 1.1820\r\n",
      "Train Epoch: 23 [208000/209539 (99%)]\tAll Loss: 1.1593\tTriple Loss(1): 0.0070\tClassification Loss: 1.1453\r\n",
      "Train Epoch: 23 [208640/209539 (100%)]\tAll Loss: 1.3873\tTriple Loss(1): 0.0141\tClassification Loss: 1.3591\r\n",
      "Train Epoch: 23 [209280/209539 (100%)]\tAll Loss: 1.4147\tTriple Loss(1): 0.0358\tClassification Loss: 1.3430\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/23_epochs\r\n",
      "Train Epoch: 24 [0/209539 (0%)]\tAll Loss: 1.9630\tTriple Loss(1): 0.1923\tClassification Loss: 1.5784\r\n",
      "\r\n",
      "Test set: Average loss: 1.1441\r\n",
      "Top 1 Accuracy: 53543/80128 (67%)\r\n",
      "Top 3 Accuracy: 69228/80128 (86%)\r\n",
      "Top 5 Accuracy: 74108/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 24 [640/209539 (0%)]\tAll Loss: 1.4828\tTriple Loss(1): 0.0793\tClassification Loss: 1.3242\r\n",
      "Train Epoch: 24 [1280/209539 (1%)]\tAll Loss: 1.3655\tTriple Loss(1): 0.0147\tClassification Loss: 1.3362\r\n",
      "Train Epoch: 24 [1920/209539 (1%)]\tAll Loss: 1.3407\tTriple Loss(1): 0.0000\tClassification Loss: 1.3407\r\n",
      "Train Epoch: 24 [2560/209539 (1%)]\tAll Loss: 1.5522\tTriple Loss(1): 0.0233\tClassification Loss: 1.5057\r\n",
      "Train Epoch: 24 [3200/209539 (2%)]\tAll Loss: 1.4977\tTriple Loss(1): 0.0212\tClassification Loss: 1.4552\r\n",
      "Train Epoch: 24 [3840/209539 (2%)]\tAll Loss: 1.9255\tTriple Loss(0): 0.2826\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 24 [4480/209539 (2%)]\tAll Loss: 1.5904\tTriple Loss(1): 0.0507\tClassification Loss: 1.4890\r\n",
      "Train Epoch: 24 [5120/209539 (2%)]\tAll Loss: 1.7690\tTriple Loss(0): 0.2254\tClassification Loss: 1.3182\r\n",
      "Train Epoch: 24 [5760/209539 (3%)]\tAll Loss: 1.4477\tTriple Loss(1): 0.0295\tClassification Loss: 1.3887\r\n",
      "Train Epoch: 24 [6400/209539 (3%)]\tAll Loss: 1.8298\tTriple Loss(0): 0.3200\tClassification Loss: 1.1898\r\n",
      "Train Epoch: 24 [7040/209539 (3%)]\tAll Loss: 1.1406\tTriple Loss(1): 0.0000\tClassification Loss: 1.1406\r\n",
      "Train Epoch: 24 [7680/209539 (4%)]\tAll Loss: 1.1530\tTriple Loss(1): 0.0299\tClassification Loss: 1.0933\r\n",
      "Train Epoch: 24 [8320/209539 (4%)]\tAll Loss: 1.2548\tTriple Loss(1): 0.0213\tClassification Loss: 1.2123\r\n",
      "Train Epoch: 24 [8960/209539 (4%)]\tAll Loss: 1.8387\tTriple Loss(0): 0.2681\tClassification Loss: 1.3024\r\n",
      "Train Epoch: 24 [9600/209539 (5%)]\tAll Loss: 2.4191\tTriple Loss(0): 0.4254\tClassification Loss: 1.5684\r\n",
      "Train Epoch: 24 [10240/209539 (5%)]\tAll Loss: 1.7059\tTriple Loss(0): 0.3052\tClassification Loss: 1.0955\r\n",
      "Train Epoch: 24 [10880/209539 (5%)]\tAll Loss: 1.8793\tTriple Loss(0): 0.2963\tClassification Loss: 1.2868\r\n",
      "Train Epoch: 24 [11520/209539 (5%)]\tAll Loss: 1.2531\tTriple Loss(1): 0.0346\tClassification Loss: 1.1838\r\n",
      "Train Epoch: 24 [12160/209539 (6%)]\tAll Loss: 1.2371\tTriple Loss(1): 0.0639\tClassification Loss: 1.1092\r\n",
      "Train Epoch: 24 [12800/209539 (6%)]\tAll Loss: 1.0401\tTriple Loss(1): 0.0282\tClassification Loss: 0.9836\r\n",
      "Train Epoch: 24 [13440/209539 (6%)]\tAll Loss: 1.2051\tTriple Loss(1): 0.0689\tClassification Loss: 1.0672\r\n",
      "Train Epoch: 24 [14080/209539 (7%)]\tAll Loss: 2.0393\tTriple Loss(0): 0.3021\tClassification Loss: 1.4351\r\n",
      "Train Epoch: 24 [14720/209539 (7%)]\tAll Loss: 1.4167\tTriple Loss(1): 0.0058\tClassification Loss: 1.4050\r\n",
      "Train Epoch: 24 [15360/209539 (7%)]\tAll Loss: 1.3350\tTriple Loss(1): 0.0509\tClassification Loss: 1.2332\r\n",
      "Train Epoch: 24 [16000/209539 (8%)]\tAll Loss: 1.7140\tTriple Loss(1): 0.0284\tClassification Loss: 1.6572\r\n",
      "Train Epoch: 24 [16640/209539 (8%)]\tAll Loss: 1.5889\tTriple Loss(1): 0.0241\tClassification Loss: 1.5408\r\n",
      "Train Epoch: 24 [17280/209539 (8%)]\tAll Loss: 1.3838\tTriple Loss(1): 0.0371\tClassification Loss: 1.3096\r\n",
      "Train Epoch: 24 [17920/209539 (9%)]\tAll Loss: 1.4834\tTriple Loss(1): 0.0258\tClassification Loss: 1.4319\r\n",
      "Train Epoch: 24 [18560/209539 (9%)]\tAll Loss: 1.1738\tTriple Loss(1): 0.0167\tClassification Loss: 1.1404\r\n",
      "Train Epoch: 24 [19200/209539 (9%)]\tAll Loss: 1.5030\tTriple Loss(1): 0.1153\tClassification Loss: 1.2724\r\n",
      "Train Epoch: 24 [19840/209539 (9%)]\tAll Loss: 2.0302\tTriple Loss(0): 0.4099\tClassification Loss: 1.2103\r\n",
      "Train Epoch: 24 [20480/209539 (10%)]\tAll Loss: 1.1071\tTriple Loss(1): 0.0030\tClassification Loss: 1.1010\r\n",
      "Train Epoch: 24 [21120/209539 (10%)]\tAll Loss: 1.4531\tTriple Loss(1): 0.0110\tClassification Loss: 1.4311\r\n",
      "Train Epoch: 24 [21760/209539 (10%)]\tAll Loss: 1.1954\tTriple Loss(1): 0.0165\tClassification Loss: 1.1624\r\n",
      "Train Epoch: 24 [22400/209539 (11%)]\tAll Loss: 1.8601\tTriple Loss(0): 0.4094\tClassification Loss: 1.0413\r\n",
      "Train Epoch: 24 [23040/209539 (11%)]\tAll Loss: 1.4036\tTriple Loss(1): 0.0357\tClassification Loss: 1.3323\r\n",
      "Train Epoch: 24 [23680/209539 (11%)]\tAll Loss: 0.9569\tTriple Loss(1): 0.0291\tClassification Loss: 0.8987\r\n",
      "Train Epoch: 24 [24320/209539 (12%)]\tAll Loss: 1.4073\tTriple Loss(1): 0.0070\tClassification Loss: 1.3934\r\n",
      "Train Epoch: 24 [24960/209539 (12%)]\tAll Loss: 1.4703\tTriple Loss(1): 0.0595\tClassification Loss: 1.3514\r\n",
      "Train Epoch: 24 [25600/209539 (12%)]\tAll Loss: 1.9785\tTriple Loss(0): 0.3397\tClassification Loss: 1.2991\r\n",
      "Train Epoch: 24 [26240/209539 (13%)]\tAll Loss: 1.2373\tTriple Loss(1): 0.0367\tClassification Loss: 1.1639\r\n",
      "Train Epoch: 24 [26880/209539 (13%)]\tAll Loss: 1.4507\tTriple Loss(1): 0.0412\tClassification Loss: 1.3683\r\n",
      "Train Epoch: 24 [27520/209539 (13%)]\tAll Loss: 1.2151\tTriple Loss(1): 0.0000\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 24 [28160/209539 (13%)]\tAll Loss: 1.5011\tTriple Loss(1): 0.0587\tClassification Loss: 1.3836\r\n",
      "Train Epoch: 24 [28800/209539 (14%)]\tAll Loss: 1.9090\tTriple Loss(0): 0.2101\tClassification Loss: 1.4887\r\n",
      "Train Epoch: 24 [29440/209539 (14%)]\tAll Loss: 1.2965\tTriple Loss(1): 0.0250\tClassification Loss: 1.2465\r\n",
      "Train Epoch: 24 [30080/209539 (14%)]\tAll Loss: 1.2031\tTriple Loss(1): 0.0056\tClassification Loss: 1.1919\r\n",
      "Train Epoch: 24 [30720/209539 (15%)]\tAll Loss: 1.6260\tTriple Loss(0): 0.1572\tClassification Loss: 1.3116\r\n",
      "Train Epoch: 24 [31360/209539 (15%)]\tAll Loss: 1.2762\tTriple Loss(1): 0.0207\tClassification Loss: 1.2349\r\n",
      "Train Epoch: 24 [32000/209539 (15%)]\tAll Loss: 1.6152\tTriple Loss(1): 0.0703\tClassification Loss: 1.4746\r\n",
      "Train Epoch: 24 [32640/209539 (16%)]\tAll Loss: 1.2532\tTriple Loss(1): 0.0126\tClassification Loss: 1.2280\r\n",
      "Train Epoch: 24 [33280/209539 (16%)]\tAll Loss: 1.4050\tTriple Loss(1): 0.0692\tClassification Loss: 1.2667\r\n",
      "Train Epoch: 24 [33920/209539 (16%)]\tAll Loss: 2.0170\tTriple Loss(0): 0.3453\tClassification Loss: 1.3264\r\n",
      "Train Epoch: 24 [34560/209539 (16%)]\tAll Loss: 1.3668\tTriple Loss(1): 0.1043\tClassification Loss: 1.1581\r\n",
      "Train Epoch: 24 [35200/209539 (17%)]\tAll Loss: 2.0894\tTriple Loss(0): 0.4150\tClassification Loss: 1.2593\r\n",
      "Train Epoch: 24 [35840/209539 (17%)]\tAll Loss: 1.0437\tTriple Loss(1): 0.0340\tClassification Loss: 0.9758\r\n",
      "Train Epoch: 24 [36480/209539 (17%)]\tAll Loss: 1.2421\tTriple Loss(1): 0.0292\tClassification Loss: 1.1837\r\n",
      "Train Epoch: 24 [37120/209539 (18%)]\tAll Loss: 2.2908\tTriple Loss(0): 0.2409\tClassification Loss: 1.8091\r\n",
      "Train Epoch: 24 [37760/209539 (18%)]\tAll Loss: 1.5071\tTriple Loss(1): 0.0927\tClassification Loss: 1.3216\r\n",
      "Train Epoch: 24 [38400/209539 (18%)]\tAll Loss: 1.4607\tTriple Loss(1): 0.0860\tClassification Loss: 1.2886\r\n",
      "Train Epoch: 24 [39040/209539 (19%)]\tAll Loss: 1.0926\tTriple Loss(1): 0.0646\tClassification Loss: 0.9633\r\n",
      "Train Epoch: 24 [39680/209539 (19%)]\tAll Loss: 1.4230\tTriple Loss(1): 0.0220\tClassification Loss: 1.3790\r\n",
      "Train Epoch: 24 [40320/209539 (19%)]\tAll Loss: 1.8989\tTriple Loss(0): 0.3305\tClassification Loss: 1.2379\r\n",
      "Train Epoch: 24 [40960/209539 (20%)]\tAll Loss: 2.0035\tTriple Loss(0): 0.4399\tClassification Loss: 1.1236\r\n",
      "Train Epoch: 24 [41600/209539 (20%)]\tAll Loss: 1.5741\tTriple Loss(0): 0.2110\tClassification Loss: 1.1520\r\n",
      "Train Epoch: 24 [42240/209539 (20%)]\tAll Loss: 1.2981\tTriple Loss(1): 0.0186\tClassification Loss: 1.2610\r\n",
      "Train Epoch: 24 [42880/209539 (20%)]\tAll Loss: 0.9765\tTriple Loss(1): 0.0000\tClassification Loss: 0.9765\r\n",
      "Train Epoch: 24 [43520/209539 (21%)]\tAll Loss: 1.5646\tTriple Loss(1): 0.0787\tClassification Loss: 1.4071\r\n",
      "Train Epoch: 24 [44160/209539 (21%)]\tAll Loss: 1.6257\tTriple Loss(1): 0.0487\tClassification Loss: 1.5284\r\n",
      "Train Epoch: 24 [44800/209539 (21%)]\tAll Loss: 1.6007\tTriple Loss(1): 0.0530\tClassification Loss: 1.4948\r\n",
      "Train Epoch: 24 [45440/209539 (22%)]\tAll Loss: 1.7198\tTriple Loss(1): 0.0000\tClassification Loss: 1.7198\r\n",
      "Train Epoch: 24 [46080/209539 (22%)]\tAll Loss: 1.3465\tTriple Loss(1): 0.0865\tClassification Loss: 1.1736\r\n",
      "Train Epoch: 24 [46720/209539 (22%)]\tAll Loss: 1.7549\tTriple Loss(1): 0.0000\tClassification Loss: 1.7549\r\n",
      "Train Epoch: 24 [47360/209539 (23%)]\tAll Loss: 1.0661\tTriple Loss(1): 0.0085\tClassification Loss: 1.0492\r\n",
      "Train Epoch: 24 [48000/209539 (23%)]\tAll Loss: 1.2681\tTriple Loss(1): 0.0000\tClassification Loss: 1.2681\r\n",
      "Train Epoch: 24 [48640/209539 (23%)]\tAll Loss: 1.9205\tTriple Loss(1): 0.0304\tClassification Loss: 1.8597\r\n",
      "Train Epoch: 24 [49280/209539 (24%)]\tAll Loss: 1.2466\tTriple Loss(1): 0.0292\tClassification Loss: 1.1882\r\n",
      "Train Epoch: 24 [49920/209539 (24%)]\tAll Loss: 1.3689\tTriple Loss(1): 0.0250\tClassification Loss: 1.3190\r\n",
      "Train Epoch: 24 [50560/209539 (24%)]\tAll Loss: 2.2269\tTriple Loss(0): 0.3788\tClassification Loss: 1.4692\r\n",
      "Train Epoch: 24 [51200/209539 (24%)]\tAll Loss: 1.4215\tTriple Loss(1): 0.0047\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 24 [51840/209539 (25%)]\tAll Loss: 1.3524\tTriple Loss(1): 0.0032\tClassification Loss: 1.3460\r\n",
      "Train Epoch: 24 [52480/209539 (25%)]\tAll Loss: 1.4192\tTriple Loss(0): 0.1811\tClassification Loss: 1.0571\r\n",
      "Train Epoch: 24 [53120/209539 (25%)]\tAll Loss: 1.2178\tTriple Loss(1): 0.0401\tClassification Loss: 1.1376\r\n",
      "Train Epoch: 24 [53760/209539 (26%)]\tAll Loss: 1.6428\tTriple Loss(1): 0.0295\tClassification Loss: 1.5837\r\n",
      "Train Epoch: 24 [54400/209539 (26%)]\tAll Loss: 2.4699\tTriple Loss(0): 0.3680\tClassification Loss: 1.7339\r\n",
      "Train Epoch: 24 [55040/209539 (26%)]\tAll Loss: 0.9990\tTriple Loss(1): 0.0373\tClassification Loss: 0.9245\r\n",
      "Train Epoch: 24 [55680/209539 (27%)]\tAll Loss: 1.6187\tTriple Loss(1): 0.0049\tClassification Loss: 1.6090\r\n",
      "Train Epoch: 24 [56320/209539 (27%)]\tAll Loss: 1.1256\tTriple Loss(1): 0.0661\tClassification Loss: 0.9933\r\n",
      "Train Epoch: 24 [56960/209539 (27%)]\tAll Loss: 1.4007\tTriple Loss(1): 0.0132\tClassification Loss: 1.3743\r\n",
      "Train Epoch: 24 [57600/209539 (27%)]\tAll Loss: 1.3615\tTriple Loss(1): 0.1033\tClassification Loss: 1.1550\r\n",
      "Train Epoch: 24 [58240/209539 (28%)]\tAll Loss: 1.1622\tTriple Loss(1): 0.0233\tClassification Loss: 1.1156\r\n",
      "Train Epoch: 24 [58880/209539 (28%)]\tAll Loss: 1.4056\tTriple Loss(1): 0.0499\tClassification Loss: 1.3058\r\n",
      "Train Epoch: 24 [59520/209539 (28%)]\tAll Loss: 1.2841\tTriple Loss(1): 0.0149\tClassification Loss: 1.2544\r\n",
      "Train Epoch: 24 [60160/209539 (29%)]\tAll Loss: 1.7751\tTriple Loss(1): 0.1244\tClassification Loss: 1.5263\r\n",
      "Train Epoch: 24 [60800/209539 (29%)]\tAll Loss: 2.2199\tTriple Loss(0): 0.4183\tClassification Loss: 1.3833\r\n",
      "Train Epoch: 24 [61440/209539 (29%)]\tAll Loss: 1.2561\tTriple Loss(1): 0.0039\tClassification Loss: 1.2483\r\n",
      "Train Epoch: 24 [62080/209539 (30%)]\tAll Loss: 1.5750\tTriple Loss(1): 0.0737\tClassification Loss: 1.4276\r\n",
      "Train Epoch: 24 [62720/209539 (30%)]\tAll Loss: 1.4910\tTriple Loss(1): 0.0497\tClassification Loss: 1.3915\r\n",
      "Train Epoch: 24 [63360/209539 (30%)]\tAll Loss: 1.2960\tTriple Loss(1): 0.0128\tClassification Loss: 1.2704\r\n",
      "Train Epoch: 24 [64000/209539 (31%)]\tAll Loss: 1.5943\tTriple Loss(1): 0.0160\tClassification Loss: 1.5623\r\n",
      "Train Epoch: 24 [64640/209539 (31%)]\tAll Loss: 1.3574\tTriple Loss(1): 0.0106\tClassification Loss: 1.3362\r\n",
      "Train Epoch: 24 [65280/209539 (31%)]\tAll Loss: 1.7499\tTriple Loss(1): 0.1894\tClassification Loss: 1.3710\r\n",
      "Train Epoch: 24 [65920/209539 (31%)]\tAll Loss: 1.4526\tTriple Loss(1): 0.0022\tClassification Loss: 1.4482\r\n",
      "Train Epoch: 24 [66560/209539 (32%)]\tAll Loss: 1.3150\tTriple Loss(1): 0.0103\tClassification Loss: 1.2943\r\n",
      "Train Epoch: 24 [67200/209539 (32%)]\tAll Loss: 1.3226\tTriple Loss(1): 0.0453\tClassification Loss: 1.2320\r\n",
      "Train Epoch: 24 [67840/209539 (32%)]\tAll Loss: 1.2101\tTriple Loss(1): 0.0166\tClassification Loss: 1.1769\r\n",
      "Train Epoch: 24 [68480/209539 (33%)]\tAll Loss: 1.5172\tTriple Loss(1): 0.0744\tClassification Loss: 1.3684\r\n",
      "Train Epoch: 24 [69120/209539 (33%)]\tAll Loss: 1.6360\tTriple Loss(1): 0.0619\tClassification Loss: 1.5122\r\n",
      "Train Epoch: 24 [69760/209539 (33%)]\tAll Loss: 1.2115\tTriple Loss(1): 0.0098\tClassification Loss: 1.1918\r\n",
      "Train Epoch: 24 [70400/209539 (34%)]\tAll Loss: 1.1418\tTriple Loss(1): 0.0000\tClassification Loss: 1.1418\r\n",
      "Train Epoch: 24 [71040/209539 (34%)]\tAll Loss: 1.5844\tTriple Loss(1): 0.0121\tClassification Loss: 1.5602\r\n",
      "Train Epoch: 24 [71680/209539 (34%)]\tAll Loss: 2.2316\tTriple Loss(0): 0.3782\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 24 [72320/209539 (35%)]\tAll Loss: 1.2210\tTriple Loss(1): 0.0078\tClassification Loss: 1.2054\r\n",
      "Train Epoch: 24 [72960/209539 (35%)]\tAll Loss: 1.3113\tTriple Loss(1): 0.0268\tClassification Loss: 1.2577\r\n",
      "Train Epoch: 24 [73600/209539 (35%)]\tAll Loss: 1.4964\tTriple Loss(1): 0.0271\tClassification Loss: 1.4422\r\n",
      "Train Epoch: 24 [74240/209539 (35%)]\tAll Loss: 1.6748\tTriple Loss(1): 0.0664\tClassification Loss: 1.5419\r\n",
      "Train Epoch: 24 [74880/209539 (36%)]\tAll Loss: 1.9007\tTriple Loss(1): 0.0244\tClassification Loss: 1.8520\r\n",
      "Train Epoch: 24 [75520/209539 (36%)]\tAll Loss: 2.2059\tTriple Loss(0): 0.5948\tClassification Loss: 1.0163\r\n",
      "Train Epoch: 24 [76160/209539 (36%)]\tAll Loss: 1.1429\tTriple Loss(1): 0.0136\tClassification Loss: 1.1157\r\n",
      "Train Epoch: 24 [76800/209539 (37%)]\tAll Loss: 1.2470\tTriple Loss(1): 0.0219\tClassification Loss: 1.2032\r\n",
      "Train Epoch: 24 [77440/209539 (37%)]\tAll Loss: 1.5281\tTriple Loss(1): 0.0546\tClassification Loss: 1.4190\r\n",
      "Train Epoch: 24 [78080/209539 (37%)]\tAll Loss: 1.4844\tTriple Loss(1): 0.0868\tClassification Loss: 1.3108\r\n",
      "Train Epoch: 24 [78720/209539 (38%)]\tAll Loss: 1.5909\tTriple Loss(1): 0.0556\tClassification Loss: 1.4796\r\n",
      "Train Epoch: 24 [79360/209539 (38%)]\tAll Loss: 1.4255\tTriple Loss(1): 0.0174\tClassification Loss: 1.3906\r\n",
      "Train Epoch: 24 [80000/209539 (38%)]\tAll Loss: 1.8780\tTriple Loss(0): 0.2499\tClassification Loss: 1.3782\r\n",
      "Train Epoch: 24 [80640/209539 (38%)]\tAll Loss: 1.4169\tTriple Loss(1): 0.0000\tClassification Loss: 1.4169\r\n",
      "Train Epoch: 24 [81280/209539 (39%)]\tAll Loss: 1.4783\tTriple Loss(1): 0.0681\tClassification Loss: 1.3421\r\n",
      "Train Epoch: 24 [81920/209539 (39%)]\tAll Loss: 1.0523\tTriple Loss(1): 0.0000\tClassification Loss: 1.0523\r\n",
      "Train Epoch: 24 [82560/209539 (39%)]\tAll Loss: 1.2585\tTriple Loss(1): 0.0112\tClassification Loss: 1.2361\r\n",
      "Train Epoch: 24 [83200/209539 (40%)]\tAll Loss: 1.2406\tTriple Loss(1): 0.0000\tClassification Loss: 1.2406\r\n",
      "Train Epoch: 24 [83840/209539 (40%)]\tAll Loss: 1.2591\tTriple Loss(1): 0.0140\tClassification Loss: 1.2311\r\n",
      "Train Epoch: 24 [84480/209539 (40%)]\tAll Loss: 1.3161\tTriple Loss(1): 0.1055\tClassification Loss: 1.1051\r\n",
      "Train Epoch: 24 [85120/209539 (41%)]\tAll Loss: 1.9185\tTriple Loss(1): 0.0459\tClassification Loss: 1.8266\r\n",
      "Train Epoch: 24 [85760/209539 (41%)]\tAll Loss: 1.3263\tTriple Loss(1): 0.0000\tClassification Loss: 1.3263\r\n",
      "Train Epoch: 24 [86400/209539 (41%)]\tAll Loss: 1.9804\tTriple Loss(0): 0.3593\tClassification Loss: 1.2618\r\n",
      "Train Epoch: 24 [87040/209539 (42%)]\tAll Loss: 1.5020\tTriple Loss(1): 0.0662\tClassification Loss: 1.3695\r\n",
      "Train Epoch: 24 [87680/209539 (42%)]\tAll Loss: 1.3236\tTriple Loss(1): 0.0290\tClassification Loss: 1.2655\r\n",
      "Train Epoch: 24 [88320/209539 (42%)]\tAll Loss: 1.4482\tTriple Loss(1): 0.0457\tClassification Loss: 1.3568\r\n",
      "Train Epoch: 24 [88960/209539 (42%)]\tAll Loss: 1.4708\tTriple Loss(1): 0.0502\tClassification Loss: 1.3704\r\n",
      "Train Epoch: 24 [89600/209539 (43%)]\tAll Loss: 2.2273\tTriple Loss(0): 0.3218\tClassification Loss: 1.5837\r\n",
      "Train Epoch: 24 [90240/209539 (43%)]\tAll Loss: 1.2674\tTriple Loss(1): 0.0285\tClassification Loss: 1.2104\r\n",
      "Train Epoch: 24 [90880/209539 (43%)]\tAll Loss: 1.6047\tTriple Loss(1): 0.0391\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 24 [91520/209539 (44%)]\tAll Loss: 1.4865\tTriple Loss(1): 0.0410\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 24 [92160/209539 (44%)]\tAll Loss: 1.8926\tTriple Loss(0): 0.3899\tClassification Loss: 1.1127\r\n",
      "Train Epoch: 24 [92800/209539 (44%)]\tAll Loss: 1.2182\tTriple Loss(1): 0.0787\tClassification Loss: 1.0608\r\n",
      "Train Epoch: 24 [93440/209539 (45%)]\tAll Loss: 1.6663\tTriple Loss(1): 0.0873\tClassification Loss: 1.4917\r\n",
      "Train Epoch: 24 [94080/209539 (45%)]\tAll Loss: 1.6253\tTriple Loss(0): 0.3103\tClassification Loss: 1.0048\r\n",
      "Train Epoch: 24 [94720/209539 (45%)]\tAll Loss: 1.3180\tTriple Loss(1): 0.0230\tClassification Loss: 1.2720\r\n",
      "Train Epoch: 24 [95360/209539 (46%)]\tAll Loss: 1.3666\tTriple Loss(1): 0.0121\tClassification Loss: 1.3424\r\n",
      "Train Epoch: 24 [96000/209539 (46%)]\tAll Loss: 1.4173\tTriple Loss(1): 0.0224\tClassification Loss: 1.3725\r\n",
      "Train Epoch: 24 [96640/209539 (46%)]\tAll Loss: 1.7321\tTriple Loss(1): 0.0416\tClassification Loss: 1.6490\r\n",
      "Train Epoch: 24 [97280/209539 (46%)]\tAll Loss: 1.3108\tTriple Loss(1): 0.0251\tClassification Loss: 1.2606\r\n",
      "Train Epoch: 24 [97920/209539 (47%)]\tAll Loss: 2.0864\tTriple Loss(0): 0.3454\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 24 [98560/209539 (47%)]\tAll Loss: 1.8000\tTriple Loss(1): 0.0827\tClassification Loss: 1.6346\r\n",
      "Train Epoch: 24 [99200/209539 (47%)]\tAll Loss: 1.7653\tTriple Loss(0): 0.2503\tClassification Loss: 1.2646\r\n",
      "Train Epoch: 24 [99840/209539 (48%)]\tAll Loss: 1.3418\tTriple Loss(1): 0.0262\tClassification Loss: 1.2894\r\n",
      "Train Epoch: 24 [100480/209539 (48%)]\tAll Loss: 1.5807\tTriple Loss(1): 0.0969\tClassification Loss: 1.3870\r\n",
      "Train Epoch: 24 [101120/209539 (48%)]\tAll Loss: 1.3201\tTriple Loss(1): 0.0276\tClassification Loss: 1.2650\r\n",
      "Train Epoch: 24 [101760/209539 (49%)]\tAll Loss: 1.2799\tTriple Loss(1): 0.0448\tClassification Loss: 1.1903\r\n",
      "Train Epoch: 24 [102400/209539 (49%)]\tAll Loss: 1.3126\tTriple Loss(1): 0.0561\tClassification Loss: 1.2003\r\n",
      "Train Epoch: 24 [103040/209539 (49%)]\tAll Loss: 1.2284\tTriple Loss(1): 0.0038\tClassification Loss: 1.2208\r\n",
      "Train Epoch: 24 [103680/209539 (49%)]\tAll Loss: 1.8769\tTriple Loss(0): 0.1830\tClassification Loss: 1.5109\r\n",
      "Train Epoch: 24 [104320/209539 (50%)]\tAll Loss: 1.6829\tTriple Loss(0): 0.3136\tClassification Loss: 1.0558\r\n",
      "Train Epoch: 24 [104960/209539 (50%)]\tAll Loss: 1.0880\tTriple Loss(1): 0.0028\tClassification Loss: 1.0824\r\n",
      "Train Epoch: 24 [105600/209539 (50%)]\tAll Loss: 1.4025\tTriple Loss(1): 0.0334\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 24 [106240/209539 (51%)]\tAll Loss: 1.2850\tTriple Loss(1): 0.0445\tClassification Loss: 1.1961\r\n",
      "Train Epoch: 24 [106880/209539 (51%)]\tAll Loss: 1.1837\tTriple Loss(1): 0.0041\tClassification Loss: 1.1755\r\n",
      "Train Epoch: 24 [107520/209539 (51%)]\tAll Loss: 1.2397\tTriple Loss(1): 0.0062\tClassification Loss: 1.2274\r\n",
      "Train Epoch: 24 [108160/209539 (52%)]\tAll Loss: 1.2270\tTriple Loss(1): 0.0049\tClassification Loss: 1.2171\r\n",
      "Train Epoch: 24 [108800/209539 (52%)]\tAll Loss: 1.2336\tTriple Loss(1): 0.0813\tClassification Loss: 1.0711\r\n",
      "Train Epoch: 24 [109440/209539 (52%)]\tAll Loss: 2.0918\tTriple Loss(0): 0.2953\tClassification Loss: 1.5011\r\n",
      "Train Epoch: 24 [110080/209539 (53%)]\tAll Loss: 1.4597\tTriple Loss(1): 0.0079\tClassification Loss: 1.4440\r\n",
      "Train Epoch: 24 [110720/209539 (53%)]\tAll Loss: 1.7789\tTriple Loss(0): 0.2318\tClassification Loss: 1.3154\r\n",
      "Train Epoch: 24 [111360/209539 (53%)]\tAll Loss: 0.9780\tTriple Loss(1): 0.0368\tClassification Loss: 0.9044\r\n",
      "Train Epoch: 24 [112000/209539 (53%)]\tAll Loss: 1.2517\tTriple Loss(1): 0.0064\tClassification Loss: 1.2388\r\n",
      "Train Epoch: 24 [112640/209539 (54%)]\tAll Loss: 1.4712\tTriple Loss(1): 0.0319\tClassification Loss: 1.4074\r\n",
      "Train Epoch: 24 [113280/209539 (54%)]\tAll Loss: 1.0306\tTriple Loss(1): 0.0080\tClassification Loss: 1.0146\r\n",
      "Train Epoch: 24 [113920/209539 (54%)]\tAll Loss: 1.5844\tTriple Loss(1): 0.0921\tClassification Loss: 1.4003\r\n",
      "Train Epoch: 24 [114560/209539 (55%)]\tAll Loss: 2.3897\tTriple Loss(0): 0.5243\tClassification Loss: 1.3411\r\n",
      "Train Epoch: 24 [115200/209539 (55%)]\tAll Loss: 1.6403\tTriple Loss(1): 0.0127\tClassification Loss: 1.6150\r\n",
      "Train Epoch: 24 [115840/209539 (55%)]\tAll Loss: 1.2131\tTriple Loss(1): 0.0039\tClassification Loss: 1.2052\r\n",
      "Train Epoch: 24 [116480/209539 (56%)]\tAll Loss: 2.4719\tTriple Loss(0): 0.4879\tClassification Loss: 1.4960\r\n",
      "Train Epoch: 24 [117120/209539 (56%)]\tAll Loss: 1.2236\tTriple Loss(1): 0.0134\tClassification Loss: 1.1969\r\n",
      "Train Epoch: 24 [117760/209539 (56%)]\tAll Loss: 1.6229\tTriple Loss(0): 0.1831\tClassification Loss: 1.2567\r\n",
      "Train Epoch: 24 [118400/209539 (57%)]\tAll Loss: 1.0197\tTriple Loss(1): 0.0433\tClassification Loss: 0.9331\r\n",
      "Train Epoch: 24 [119040/209539 (57%)]\tAll Loss: 1.3530\tTriple Loss(1): 0.0105\tClassification Loss: 1.3321\r\n",
      "Train Epoch: 24 [119680/209539 (57%)]\tAll Loss: 2.0160\tTriple Loss(0): 0.4373\tClassification Loss: 1.1414\r\n",
      "Train Epoch: 24 [120320/209539 (57%)]\tAll Loss: 1.3083\tTriple Loss(1): 0.0693\tClassification Loss: 1.1697\r\n",
      "Train Epoch: 24 [120960/209539 (58%)]\tAll Loss: 1.2228\tTriple Loss(1): 0.0124\tClassification Loss: 1.1980\r\n",
      "Train Epoch: 24 [121600/209539 (58%)]\tAll Loss: 1.2762\tTriple Loss(1): 0.0715\tClassification Loss: 1.1331\r\n",
      "Train Epoch: 24 [122240/209539 (58%)]\tAll Loss: 1.3231\tTriple Loss(1): 0.0700\tClassification Loss: 1.1831\r\n",
      "Train Epoch: 24 [122880/209539 (59%)]\tAll Loss: 1.2490\tTriple Loss(1): 0.0168\tClassification Loss: 1.2155\r\n",
      "Train Epoch: 24 [123520/209539 (59%)]\tAll Loss: 1.6521\tTriple Loss(1): 0.0926\tClassification Loss: 1.4669\r\n",
      "Train Epoch: 24 [124160/209539 (59%)]\tAll Loss: 1.3064\tTriple Loss(1): 0.0424\tClassification Loss: 1.2216\r\n",
      "Train Epoch: 24 [124800/209539 (60%)]\tAll Loss: 1.0869\tTriple Loss(1): 0.0143\tClassification Loss: 1.0584\r\n",
      "Train Epoch: 24 [125440/209539 (60%)]\tAll Loss: 1.4772\tTriple Loss(1): 0.0531\tClassification Loss: 1.3709\r\n",
      "Train Epoch: 24 [126080/209539 (60%)]\tAll Loss: 1.5094\tTriple Loss(1): 0.0662\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 24 [126720/209539 (60%)]\tAll Loss: 1.2337\tTriple Loss(1): 0.0026\tClassification Loss: 1.2285\r\n",
      "Train Epoch: 24 [127360/209539 (61%)]\tAll Loss: 1.4820\tTriple Loss(1): 0.0595\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 24 [128000/209539 (61%)]\tAll Loss: 1.7030\tTriple Loss(1): 0.0671\tClassification Loss: 1.5688\r\n",
      "Train Epoch: 24 [128640/209539 (61%)]\tAll Loss: 1.0850\tTriple Loss(1): 0.0376\tClassification Loss: 1.0098\r\n",
      "Train Epoch: 24 [129280/209539 (62%)]\tAll Loss: 2.2741\tTriple Loss(0): 0.3803\tClassification Loss: 1.5135\r\n",
      "Train Epoch: 24 [129920/209539 (62%)]\tAll Loss: 1.1316\tTriple Loss(1): 0.0076\tClassification Loss: 1.1164\r\n",
      "Train Epoch: 24 [130560/209539 (62%)]\tAll Loss: 1.2904\tTriple Loss(1): 0.0263\tClassification Loss: 1.2379\r\n",
      "Train Epoch: 24 [131200/209539 (63%)]\tAll Loss: 1.1241\tTriple Loss(1): 0.0461\tClassification Loss: 1.0319\r\n",
      "Train Epoch: 24 [131840/209539 (63%)]\tAll Loss: 1.4363\tTriple Loss(1): 0.0520\tClassification Loss: 1.3323\r\n",
      "Train Epoch: 24 [132480/209539 (63%)]\tAll Loss: 1.2615\tTriple Loss(1): 0.0608\tClassification Loss: 1.1399\r\n",
      "Train Epoch: 24 [133120/209539 (64%)]\tAll Loss: 1.3503\tTriple Loss(1): 0.0465\tClassification Loss: 1.2573\r\n",
      "Train Epoch: 24 [133760/209539 (64%)]\tAll Loss: 1.3854\tTriple Loss(1): 0.0238\tClassification Loss: 1.3378\r\n",
      "Train Epoch: 24 [134400/209539 (64%)]\tAll Loss: 1.9918\tTriple Loss(0): 0.5102\tClassification Loss: 0.9715\r\n",
      "Train Epoch: 24 [135040/209539 (64%)]\tAll Loss: 1.5030\tTriple Loss(1): 0.0576\tClassification Loss: 1.3878\r\n",
      "Train Epoch: 24 [135680/209539 (65%)]\tAll Loss: 1.6056\tTriple Loss(1): 0.0015\tClassification Loss: 1.6027\r\n",
      "Train Epoch: 24 [136320/209539 (65%)]\tAll Loss: 1.8528\tTriple Loss(0): 0.2038\tClassification Loss: 1.4451\r\n",
      "Train Epoch: 24 [136960/209539 (65%)]\tAll Loss: 1.2984\tTriple Loss(1): 0.0392\tClassification Loss: 1.2200\r\n",
      "Train Epoch: 24 [137600/209539 (66%)]\tAll Loss: 1.9925\tTriple Loss(0): 0.3562\tClassification Loss: 1.2802\r\n",
      "Train Epoch: 24 [138240/209539 (66%)]\tAll Loss: 1.6187\tTriple Loss(1): 0.0178\tClassification Loss: 1.5831\r\n",
      "Train Epoch: 24 [138880/209539 (66%)]\tAll Loss: 1.2207\tTriple Loss(1): 0.0000\tClassification Loss: 1.2207\r\n",
      "Train Epoch: 24 [139520/209539 (67%)]\tAll Loss: 1.3736\tTriple Loss(1): 0.0262\tClassification Loss: 1.3211\r\n",
      "Train Epoch: 24 [140160/209539 (67%)]\tAll Loss: 1.4047\tTriple Loss(1): 0.0293\tClassification Loss: 1.3462\r\n",
      "Train Epoch: 24 [140800/209539 (67%)]\tAll Loss: 1.5767\tTriple Loss(1): 0.0000\tClassification Loss: 1.5767\r\n",
      "Train Epoch: 24 [141440/209539 (68%)]\tAll Loss: 1.4680\tTriple Loss(1): 0.0499\tClassification Loss: 1.3682\r\n",
      "Train Epoch: 24 [142080/209539 (68%)]\tAll Loss: 1.1883\tTriple Loss(1): 0.0550\tClassification Loss: 1.0783\r\n",
      "Train Epoch: 24 [142720/209539 (68%)]\tAll Loss: 1.6569\tTriple Loss(1): 0.0038\tClassification Loss: 1.6493\r\n",
      "Train Epoch: 24 [143360/209539 (68%)]\tAll Loss: 1.2420\tTriple Loss(1): 0.0416\tClassification Loss: 1.1588\r\n",
      "Train Epoch: 24 [144000/209539 (69%)]\tAll Loss: 1.5213\tTriple Loss(1): 0.0329\tClassification Loss: 1.4556\r\n",
      "Train Epoch: 24 [144640/209539 (69%)]\tAll Loss: 2.1025\tTriple Loss(0): 0.5275\tClassification Loss: 1.0476\r\n",
      "Train Epoch: 24 [145280/209539 (69%)]\tAll Loss: 1.7759\tTriple Loss(0): 0.2560\tClassification Loss: 1.2638\r\n",
      "Train Epoch: 24 [145920/209539 (70%)]\tAll Loss: 1.5247\tTriple Loss(1): 0.0614\tClassification Loss: 1.4019\r\n",
      "Train Epoch: 24 [146560/209539 (70%)]\tAll Loss: 1.0865\tTriple Loss(1): 0.0334\tClassification Loss: 1.0197\r\n",
      "Train Epoch: 24 [147200/209539 (70%)]\tAll Loss: 1.4052\tTriple Loss(1): 0.0265\tClassification Loss: 1.3523\r\n",
      "Train Epoch: 24 [147840/209539 (71%)]\tAll Loss: 1.4414\tTriple Loss(0): 0.2331\tClassification Loss: 0.9751\r\n",
      "Train Epoch: 24 [148480/209539 (71%)]\tAll Loss: 1.2996\tTriple Loss(1): 0.0502\tClassification Loss: 1.1993\r\n",
      "Train Epoch: 24 [149120/209539 (71%)]\tAll Loss: 1.4019\tTriple Loss(1): 0.0000\tClassification Loss: 1.4019\r\n",
      "Train Epoch: 24 [149760/209539 (71%)]\tAll Loss: 1.1492\tTriple Loss(1): 0.0699\tClassification Loss: 1.0093\r\n",
      "Train Epoch: 24 [150400/209539 (72%)]\tAll Loss: 1.3538\tTriple Loss(1): 0.0148\tClassification Loss: 1.3242\r\n",
      "Train Epoch: 24 [151040/209539 (72%)]\tAll Loss: 2.0465\tTriple Loss(0): 0.4102\tClassification Loss: 1.2261\r\n",
      "Train Epoch: 24 [151680/209539 (72%)]\tAll Loss: 1.4164\tTriple Loss(1): 0.0165\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 24 [152320/209539 (73%)]\tAll Loss: 1.2247\tTriple Loss(1): 0.0671\tClassification Loss: 1.0906\r\n",
      "Train Epoch: 24 [152960/209539 (73%)]\tAll Loss: 1.1143\tTriple Loss(1): 0.0363\tClassification Loss: 1.0416\r\n",
      "Train Epoch: 24 [153600/209539 (73%)]\tAll Loss: 1.7560\tTriple Loss(0): 0.2398\tClassification Loss: 1.2764\r\n",
      "Train Epoch: 24 [154240/209539 (74%)]\tAll Loss: 1.2308\tTriple Loss(1): 0.0160\tClassification Loss: 1.1987\r\n",
      "Train Epoch: 24 [154880/209539 (74%)]\tAll Loss: 1.5448\tTriple Loss(1): 0.0755\tClassification Loss: 1.3937\r\n",
      "Train Epoch: 24 [155520/209539 (74%)]\tAll Loss: 1.3176\tTriple Loss(1): 0.0454\tClassification Loss: 1.2269\r\n",
      "Train Epoch: 24 [156160/209539 (75%)]\tAll Loss: 2.1839\tTriple Loss(0): 0.4014\tClassification Loss: 1.3810\r\n",
      "Train Epoch: 24 [156800/209539 (75%)]\tAll Loss: 1.7888\tTriple Loss(1): 0.0301\tClassification Loss: 1.7285\r\n",
      "Train Epoch: 24 [157440/209539 (75%)]\tAll Loss: 2.5663\tTriple Loss(0): 0.5358\tClassification Loss: 1.4946\r\n",
      "Train Epoch: 24 [158080/209539 (75%)]\tAll Loss: 1.3669\tTriple Loss(1): 0.0025\tClassification Loss: 1.3618\r\n",
      "Train Epoch: 24 [158720/209539 (76%)]\tAll Loss: 1.3414\tTriple Loss(1): 0.0149\tClassification Loss: 1.3116\r\n",
      "Train Epoch: 24 [159360/209539 (76%)]\tAll Loss: 1.0733\tTriple Loss(1): 0.0178\tClassification Loss: 1.0377\r\n",
      "Train Epoch: 24 [160000/209539 (76%)]\tAll Loss: 1.4258\tTriple Loss(1): 0.0069\tClassification Loss: 1.4120\r\n",
      "Train Epoch: 24 [160640/209539 (77%)]\tAll Loss: 1.2151\tTriple Loss(1): 0.0584\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 24 [161280/209539 (77%)]\tAll Loss: 1.6492\tTriple Loss(0): 0.2469\tClassification Loss: 1.1553\r\n",
      "Train Epoch: 24 [161920/209539 (77%)]\tAll Loss: 1.1544\tTriple Loss(1): 0.0060\tClassification Loss: 1.1423\r\n",
      "Train Epoch: 24 [162560/209539 (78%)]\tAll Loss: 1.3086\tTriple Loss(1): 0.0674\tClassification Loss: 1.1738\r\n",
      "Train Epoch: 24 [163200/209539 (78%)]\tAll Loss: 1.2559\tTriple Loss(1): 0.0654\tClassification Loss: 1.1251\r\n",
      "Train Epoch: 24 [163840/209539 (78%)]\tAll Loss: 1.3826\tTriple Loss(1): 0.0000\tClassification Loss: 1.3826\r\n",
      "Train Epoch: 24 [164480/209539 (78%)]\tAll Loss: 1.2046\tTriple Loss(1): 0.0271\tClassification Loss: 1.1504\r\n",
      "Train Epoch: 24 [165120/209539 (79%)]\tAll Loss: 1.0840\tTriple Loss(1): 0.0166\tClassification Loss: 1.0508\r\n",
      "Train Epoch: 24 [165760/209539 (79%)]\tAll Loss: 2.3009\tTriple Loss(0): 0.4462\tClassification Loss: 1.4085\r\n",
      "Train Epoch: 24 [166400/209539 (79%)]\tAll Loss: 2.1460\tTriple Loss(0): 0.4292\tClassification Loss: 1.2876\r\n",
      "Train Epoch: 24 [167040/209539 (80%)]\tAll Loss: 1.5613\tTriple Loss(1): 0.0206\tClassification Loss: 1.5201\r\n",
      "Train Epoch: 24 [167680/209539 (80%)]\tAll Loss: 1.3093\tTriple Loss(1): 0.0087\tClassification Loss: 1.2920\r\n",
      "Train Epoch: 24 [168320/209539 (80%)]\tAll Loss: 1.4186\tTriple Loss(1): 0.0764\tClassification Loss: 1.2658\r\n",
      "Train Epoch: 24 [168960/209539 (81%)]\tAll Loss: 1.0445\tTriple Loss(1): 0.0088\tClassification Loss: 1.0270\r\n",
      "Train Epoch: 24 [169600/209539 (81%)]\tAll Loss: 2.1152\tTriple Loss(0): 0.3775\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 24 [170240/209539 (81%)]\tAll Loss: 1.2213\tTriple Loss(1): 0.0213\tClassification Loss: 1.1787\r\n",
      "Train Epoch: 24 [170880/209539 (82%)]\tAll Loss: 1.3498\tTriple Loss(1): 0.0193\tClassification Loss: 1.3112\r\n",
      "Train Epoch: 24 [171520/209539 (82%)]\tAll Loss: 1.1850\tTriple Loss(1): 0.0445\tClassification Loss: 1.0960\r\n",
      "Train Epoch: 24 [172160/209539 (82%)]\tAll Loss: 2.0608\tTriple Loss(0): 0.2941\tClassification Loss: 1.4726\r\n",
      "Train Epoch: 24 [172800/209539 (82%)]\tAll Loss: 1.3340\tTriple Loss(1): 0.0136\tClassification Loss: 1.3068\r\n",
      "Train Epoch: 24 [173440/209539 (83%)]\tAll Loss: 1.4420\tTriple Loss(1): 0.0079\tClassification Loss: 1.4262\r\n",
      "Train Epoch: 24 [174080/209539 (83%)]\tAll Loss: 1.8058\tTriple Loss(0): 0.3325\tClassification Loss: 1.1408\r\n",
      "Train Epoch: 24 [174720/209539 (83%)]\tAll Loss: 1.3936\tTriple Loss(1): 0.0000\tClassification Loss: 1.3935\r\n",
      "Train Epoch: 24 [175360/209539 (84%)]\tAll Loss: 1.3789\tTriple Loss(1): 0.0149\tClassification Loss: 1.3490\r\n",
      "Train Epoch: 24 [176000/209539 (84%)]\tAll Loss: 1.4508\tTriple Loss(1): 0.0000\tClassification Loss: 1.4508\r\n",
      "Train Epoch: 24 [176640/209539 (84%)]\tAll Loss: 0.9377\tTriple Loss(1): 0.0042\tClassification Loss: 0.9292\r\n",
      "Train Epoch: 24 [177280/209539 (85%)]\tAll Loss: 1.6961\tTriple Loss(1): 0.0032\tClassification Loss: 1.6898\r\n",
      "Train Epoch: 24 [177920/209539 (85%)]\tAll Loss: 1.5838\tTriple Loss(1): 0.0450\tClassification Loss: 1.4939\r\n",
      "Train Epoch: 24 [178560/209539 (85%)]\tAll Loss: 1.1624\tTriple Loss(1): 0.0145\tClassification Loss: 1.1334\r\n",
      "Train Epoch: 24 [179200/209539 (86%)]\tAll Loss: 1.4659\tTriple Loss(1): 0.0139\tClassification Loss: 1.4382\r\n",
      "Train Epoch: 24 [179840/209539 (86%)]\tAll Loss: 1.3604\tTriple Loss(1): 0.0200\tClassification Loss: 1.3204\r\n",
      "Train Epoch: 24 [180480/209539 (86%)]\tAll Loss: 1.4181\tTriple Loss(1): 0.0636\tClassification Loss: 1.2909\r\n",
      "Train Epoch: 24 [181120/209539 (86%)]\tAll Loss: 1.5245\tTriple Loss(1): 0.0259\tClassification Loss: 1.4728\r\n",
      "Train Epoch: 24 [181760/209539 (87%)]\tAll Loss: 1.1531\tTriple Loss(1): 0.0418\tClassification Loss: 1.0696\r\n",
      "Train Epoch: 24 [182400/209539 (87%)]\tAll Loss: 1.5248\tTriple Loss(1): 0.0411\tClassification Loss: 1.4427\r\n",
      "Train Epoch: 24 [183040/209539 (87%)]\tAll Loss: 1.3321\tTriple Loss(1): 0.0038\tClassification Loss: 1.3245\r\n",
      "Train Epoch: 24 [183680/209539 (88%)]\tAll Loss: 1.2549\tTriple Loss(1): 0.0281\tClassification Loss: 1.1986\r\n",
      "Train Epoch: 24 [184320/209539 (88%)]\tAll Loss: 1.3670\tTriple Loss(1): 0.0073\tClassification Loss: 1.3525\r\n",
      "Train Epoch: 24 [184960/209539 (88%)]\tAll Loss: 1.1244\tTriple Loss(1): 0.0053\tClassification Loss: 1.1138\r\n",
      "Train Epoch: 24 [185600/209539 (89%)]\tAll Loss: 1.4540\tTriple Loss(1): 0.0204\tClassification Loss: 1.4132\r\n",
      "Train Epoch: 24 [186240/209539 (89%)]\tAll Loss: 1.3446\tTriple Loss(1): 0.0217\tClassification Loss: 1.3012\r\n",
      "Train Epoch: 24 [186880/209539 (89%)]\tAll Loss: 1.2369\tTriple Loss(1): 0.0000\tClassification Loss: 1.2369\r\n",
      "Train Epoch: 24 [187520/209539 (89%)]\tAll Loss: 1.9197\tTriple Loss(0): 0.2653\tClassification Loss: 1.3891\r\n",
      "Train Epoch: 24 [188160/209539 (90%)]\tAll Loss: 1.0577\tTriple Loss(1): 0.0159\tClassification Loss: 1.0260\r\n",
      "Train Epoch: 24 [188800/209539 (90%)]\tAll Loss: 1.2764\tTriple Loss(1): 0.0230\tClassification Loss: 1.2305\r\n",
      "Train Epoch: 24 [189440/209539 (90%)]\tAll Loss: 1.6579\tTriple Loss(0): 0.2546\tClassification Loss: 1.1487\r\n",
      "Train Epoch: 24 [190080/209539 (91%)]\tAll Loss: 1.2896\tTriple Loss(1): 0.0234\tClassification Loss: 1.2427\r\n",
      "Train Epoch: 24 [190720/209539 (91%)]\tAll Loss: 1.3173\tTriple Loss(1): 0.0276\tClassification Loss: 1.2621\r\n",
      "Train Epoch: 24 [191360/209539 (91%)]\tAll Loss: 1.9093\tTriple Loss(0): 0.4411\tClassification Loss: 1.0272\r\n",
      "Train Epoch: 24 [192000/209539 (92%)]\tAll Loss: 1.5923\tTriple Loss(1): 0.0742\tClassification Loss: 1.4439\r\n",
      "Train Epoch: 24 [192640/209539 (92%)]\tAll Loss: 1.2803\tTriple Loss(1): 0.0611\tClassification Loss: 1.1582\r\n",
      "Train Epoch: 24 [193280/209539 (92%)]\tAll Loss: 1.1512\tTriple Loss(1): 0.0196\tClassification Loss: 1.1120\r\n",
      "Train Epoch: 24 [193920/209539 (93%)]\tAll Loss: 1.2065\tTriple Loss(1): 0.0284\tClassification Loss: 1.1497\r\n",
      "Train Epoch: 24 [194560/209539 (93%)]\tAll Loss: 1.2267\tTriple Loss(1): 0.0312\tClassification Loss: 1.1643\r\n",
      "Train Epoch: 24 [195200/209539 (93%)]\tAll Loss: 1.3509\tTriple Loss(1): 0.0230\tClassification Loss: 1.3049\r\n",
      "Train Epoch: 24 [195840/209539 (93%)]\tAll Loss: 1.0688\tTriple Loss(1): 0.0298\tClassification Loss: 1.0092\r\n",
      "Train Epoch: 24 [196480/209539 (94%)]\tAll Loss: 1.4587\tTriple Loss(1): 0.0000\tClassification Loss: 1.4587\r\n",
      "Train Epoch: 24 [197120/209539 (94%)]\tAll Loss: 2.4806\tTriple Loss(0): 0.4609\tClassification Loss: 1.5588\r\n",
      "Train Epoch: 24 [197760/209539 (94%)]\tAll Loss: 1.4063\tTriple Loss(1): 0.0266\tClassification Loss: 1.3530\r\n",
      "Train Epoch: 24 [198400/209539 (95%)]\tAll Loss: 1.1903\tTriple Loss(1): 0.0000\tClassification Loss: 1.1903\r\n",
      "Train Epoch: 24 [199040/209539 (95%)]\tAll Loss: 1.5687\tTriple Loss(1): 0.0412\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 24 [199680/209539 (95%)]\tAll Loss: 1.2603\tTriple Loss(1): 0.0404\tClassification Loss: 1.1794\r\n",
      "Train Epoch: 24 [200320/209539 (96%)]\tAll Loss: 1.2545\tTriple Loss(1): 0.0140\tClassification Loss: 1.2265\r\n",
      "Train Epoch: 24 [200960/209539 (96%)]\tAll Loss: 0.9985\tTriple Loss(1): 0.0212\tClassification Loss: 0.9562\r\n",
      "Train Epoch: 24 [201600/209539 (96%)]\tAll Loss: 1.2434\tTriple Loss(1): 0.0642\tClassification Loss: 1.1150\r\n",
      "Train Epoch: 24 [202240/209539 (97%)]\tAll Loss: 1.4041\tTriple Loss(1): 0.0379\tClassification Loss: 1.3282\r\n",
      "Train Epoch: 24 [202880/209539 (97%)]\tAll Loss: 1.0804\tTriple Loss(1): 0.0649\tClassification Loss: 0.9505\r\n",
      "Train Epoch: 24 [203520/209539 (97%)]\tAll Loss: 1.3028\tTriple Loss(1): 0.0566\tClassification Loss: 1.1895\r\n",
      "Train Epoch: 24 [204160/209539 (97%)]\tAll Loss: 1.7615\tTriple Loss(1): 0.0095\tClassification Loss: 1.7424\r\n",
      "Train Epoch: 24 [204800/209539 (98%)]\tAll Loss: 1.5827\tTriple Loss(1): 0.0597\tClassification Loss: 1.4633\r\n",
      "Train Epoch: 24 [205440/209539 (98%)]\tAll Loss: 1.0107\tTriple Loss(1): 0.0015\tClassification Loss: 1.0076\r\n",
      "Train Epoch: 24 [206080/209539 (98%)]\tAll Loss: 1.2710\tTriple Loss(1): 0.1064\tClassification Loss: 1.0583\r\n",
      "Train Epoch: 24 [206720/209539 (99%)]\tAll Loss: 1.3014\tTriple Loss(1): 0.0445\tClassification Loss: 1.2124\r\n",
      "Train Epoch: 24 [207360/209539 (99%)]\tAll Loss: 1.1533\tTriple Loss(1): 0.0177\tClassification Loss: 1.1178\r\n",
      "Train Epoch: 24 [208000/209539 (99%)]\tAll Loss: 1.0198\tTriple Loss(1): 0.0190\tClassification Loss: 0.9817\r\n",
      "Train Epoch: 24 [208640/209539 (100%)]\tAll Loss: 1.4045\tTriple Loss(1): 0.0063\tClassification Loss: 1.3919\r\n",
      "Train Epoch: 24 [209280/209539 (100%)]\tAll Loss: 1.4973\tTriple Loss(1): 0.0000\tClassification Loss: 1.4973\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/24_epochs\r\n",
      "Train Epoch: 25 [0/209539 (0%)]\tAll Loss: 1.7638\tTriple Loss(1): 0.0795\tClassification Loss: 1.6047\r\n",
      "\r\n",
      "Test set: Average loss: 1.1320\r\n",
      "Top 1 Accuracy: 53817/80128 (67%)\r\n",
      "Top 3 Accuracy: 69392/80128 (87%)\r\n",
      "Top 5 Accuracy: 74215/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 25 [640/209539 (0%)]\tAll Loss: 1.6098\tTriple Loss(1): 0.0460\tClassification Loss: 1.5178\r\n",
      "Train Epoch: 25 [1280/209539 (1%)]\tAll Loss: 1.2101\tTriple Loss(1): 0.0418\tClassification Loss: 1.1264\r\n",
      "Train Epoch: 25 [1920/209539 (1%)]\tAll Loss: 1.2596\tTriple Loss(1): 0.0349\tClassification Loss: 1.1898\r\n",
      "Train Epoch: 25 [2560/209539 (1%)]\tAll Loss: 1.5622\tTriple Loss(1): 0.0384\tClassification Loss: 1.4855\r\n",
      "Train Epoch: 25 [3200/209539 (2%)]\tAll Loss: 1.5211\tTriple Loss(1): 0.0000\tClassification Loss: 1.5211\r\n",
      "Train Epoch: 25 [3840/209539 (2%)]\tAll Loss: 1.1156\tTriple Loss(1): 0.0608\tClassification Loss: 0.9939\r\n",
      "Train Epoch: 25 [4480/209539 (2%)]\tAll Loss: 1.3724\tTriple Loss(1): 0.0149\tClassification Loss: 1.3426\r\n",
      "Train Epoch: 25 [5120/209539 (2%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0000\tClassification Loss: 1.2107\r\n",
      "Train Epoch: 25 [5760/209539 (3%)]\tAll Loss: 1.7478\tTriple Loss(0): 0.2451\tClassification Loss: 1.2576\r\n",
      "Train Epoch: 25 [6400/209539 (3%)]\tAll Loss: 1.1077\tTriple Loss(1): 0.0054\tClassification Loss: 1.0969\r\n",
      "Train Epoch: 25 [7040/209539 (3%)]\tAll Loss: 1.1787\tTriple Loss(1): 0.0182\tClassification Loss: 1.1424\r\n",
      "Train Epoch: 25 [7680/209539 (4%)]\tAll Loss: 1.0654\tTriple Loss(1): 0.0577\tClassification Loss: 0.9501\r\n",
      "Train Epoch: 25 [8320/209539 (4%)]\tAll Loss: 1.4375\tTriple Loss(1): 0.0516\tClassification Loss: 1.3344\r\n",
      "Train Epoch: 25 [8960/209539 (4%)]\tAll Loss: 1.4717\tTriple Loss(1): 0.0253\tClassification Loss: 1.4210\r\n",
      "Train Epoch: 25 [9600/209539 (5%)]\tAll Loss: 1.4194\tTriple Loss(1): 0.0208\tClassification Loss: 1.3779\r\n",
      "Train Epoch: 25 [10240/209539 (5%)]\tAll Loss: 1.2495\tTriple Loss(1): 0.0152\tClassification Loss: 1.2191\r\n",
      "Train Epoch: 25 [10880/209539 (5%)]\tAll Loss: 1.1946\tTriple Loss(1): 0.0000\tClassification Loss: 1.1946\r\n",
      "Train Epoch: 25 [11520/209539 (5%)]\tAll Loss: 1.4632\tTriple Loss(1): 0.0301\tClassification Loss: 1.4031\r\n",
      "Train Epoch: 25 [12160/209539 (6%)]\tAll Loss: 1.1744\tTriple Loss(1): 0.0673\tClassification Loss: 1.0398\r\n",
      "Train Epoch: 25 [12800/209539 (6%)]\tAll Loss: 1.6176\tTriple Loss(0): 0.3645\tClassification Loss: 0.8887\r\n",
      "Train Epoch: 25 [13440/209539 (6%)]\tAll Loss: 1.1995\tTriple Loss(1): 0.0020\tClassification Loss: 1.1955\r\n",
      "Train Epoch: 25 [14080/209539 (7%)]\tAll Loss: 1.3291\tTriple Loss(1): 0.0324\tClassification Loss: 1.2642\r\n",
      "Train Epoch: 25 [14720/209539 (7%)]\tAll Loss: 1.9168\tTriple Loss(0): 0.1633\tClassification Loss: 1.5902\r\n",
      "Train Epoch: 25 [15360/209539 (7%)]\tAll Loss: 1.0913\tTriple Loss(1): 0.0000\tClassification Loss: 1.0913\r\n",
      "Train Epoch: 25 [16000/209539 (8%)]\tAll Loss: 1.6233\tTriple Loss(1): 0.0058\tClassification Loss: 1.6117\r\n",
      "Train Epoch: 25 [16640/209539 (8%)]\tAll Loss: 1.6958\tTriple Loss(1): 0.0168\tClassification Loss: 1.6622\r\n",
      "Train Epoch: 25 [17280/209539 (8%)]\tAll Loss: 1.7740\tTriple Loss(0): 0.2923\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 25 [17920/209539 (9%)]\tAll Loss: 1.2686\tTriple Loss(1): 0.0310\tClassification Loss: 1.2066\r\n",
      "Train Epoch: 25 [18560/209539 (9%)]\tAll Loss: 1.4132\tTriple Loss(1): 0.0273\tClassification Loss: 1.3585\r\n",
      "Train Epoch: 25 [19200/209539 (9%)]\tAll Loss: 1.2686\tTriple Loss(1): 0.0190\tClassification Loss: 1.2306\r\n",
      "Train Epoch: 25 [19840/209539 (9%)]\tAll Loss: 1.2427\tTriple Loss(1): 0.0393\tClassification Loss: 1.1641\r\n",
      "Train Epoch: 25 [20480/209539 (10%)]\tAll Loss: 1.3472\tTriple Loss(1): 0.0055\tClassification Loss: 1.3362\r\n",
      "Train Epoch: 25 [21120/209539 (10%)]\tAll Loss: 1.4109\tTriple Loss(1): 0.0068\tClassification Loss: 1.3972\r\n",
      "Train Epoch: 25 [21760/209539 (10%)]\tAll Loss: 1.2228\tTriple Loss(1): 0.0353\tClassification Loss: 1.1522\r\n",
      "Train Epoch: 25 [22400/209539 (11%)]\tAll Loss: 1.2460\tTriple Loss(1): 0.0896\tClassification Loss: 1.0668\r\n",
      "Train Epoch: 25 [23040/209539 (11%)]\tAll Loss: 1.2917\tTriple Loss(1): 0.0400\tClassification Loss: 1.2118\r\n",
      "Train Epoch: 25 [23680/209539 (11%)]\tAll Loss: 1.6081\tTriple Loss(0): 0.3094\tClassification Loss: 0.9892\r\n",
      "Train Epoch: 25 [24320/209539 (12%)]\tAll Loss: 2.0123\tTriple Loss(0): 0.3651\tClassification Loss: 1.2821\r\n",
      "Train Epoch: 25 [24960/209539 (12%)]\tAll Loss: 1.3829\tTriple Loss(1): 0.1277\tClassification Loss: 1.1274\r\n",
      "Train Epoch: 25 [25600/209539 (12%)]\tAll Loss: 1.3747\tTriple Loss(1): 0.0109\tClassification Loss: 1.3530\r\n",
      "Train Epoch: 25 [26240/209539 (13%)]\tAll Loss: 1.2636\tTriple Loss(1): 0.0491\tClassification Loss: 1.1654\r\n",
      "Train Epoch: 25 [26880/209539 (13%)]\tAll Loss: 1.4096\tTriple Loss(1): 0.0637\tClassification Loss: 1.2822\r\n",
      "Train Epoch: 25 [27520/209539 (13%)]\tAll Loss: 1.2565\tTriple Loss(1): 0.0319\tClassification Loss: 1.1926\r\n",
      "Train Epoch: 25 [28160/209539 (13%)]\tAll Loss: 1.5793\tTriple Loss(1): 0.1051\tClassification Loss: 1.3691\r\n",
      "Train Epoch: 25 [28800/209539 (14%)]\tAll Loss: 1.6054\tTriple Loss(1): 0.0469\tClassification Loss: 1.5115\r\n",
      "Train Epoch: 25 [29440/209539 (14%)]\tAll Loss: 1.6265\tTriple Loss(1): 0.0400\tClassification Loss: 1.5466\r\n",
      "Train Epoch: 25 [30080/209539 (14%)]\tAll Loss: 2.2444\tTriple Loss(0): 0.5075\tClassification Loss: 1.2295\r\n",
      "Train Epoch: 25 [30720/209539 (15%)]\tAll Loss: 1.3674\tTriple Loss(1): 0.0678\tClassification Loss: 1.2319\r\n",
      "Train Epoch: 25 [31360/209539 (15%)]\tAll Loss: 1.2006\tTriple Loss(1): 0.0317\tClassification Loss: 1.1371\r\n",
      "Train Epoch: 25 [32000/209539 (15%)]\tAll Loss: 1.3287\tTriple Loss(1): 0.0380\tClassification Loss: 1.2526\r\n",
      "Train Epoch: 25 [32640/209539 (16%)]\tAll Loss: 1.2639\tTriple Loss(1): 0.0279\tClassification Loss: 1.2081\r\n",
      "Train Epoch: 25 [33280/209539 (16%)]\tAll Loss: 1.3059\tTriple Loss(1): 0.0318\tClassification Loss: 1.2423\r\n",
      "Train Epoch: 25 [33920/209539 (16%)]\tAll Loss: 1.3892\tTriple Loss(1): 0.0315\tClassification Loss: 1.3262\r\n",
      "Train Epoch: 25 [34560/209539 (16%)]\tAll Loss: 1.1810\tTriple Loss(1): 0.0106\tClassification Loss: 1.1598\r\n",
      "Train Epoch: 25 [35200/209539 (17%)]\tAll Loss: 1.2691\tTriple Loss(1): 0.0458\tClassification Loss: 1.1775\r\n",
      "Train Epoch: 25 [35840/209539 (17%)]\tAll Loss: 1.5622\tTriple Loss(0): 0.2160\tClassification Loss: 1.1303\r\n",
      "Train Epoch: 25 [36480/209539 (17%)]\tAll Loss: 1.1253\tTriple Loss(1): 0.0879\tClassification Loss: 0.9495\r\n",
      "Train Epoch: 25 [37120/209539 (18%)]\tAll Loss: 1.6155\tTriple Loss(1): 0.1070\tClassification Loss: 1.4014\r\n",
      "Train Epoch: 25 [37760/209539 (18%)]\tAll Loss: 1.3760\tTriple Loss(1): 0.0522\tClassification Loss: 1.2716\r\n",
      "Train Epoch: 25 [38400/209539 (18%)]\tAll Loss: 1.4141\tTriple Loss(1): 0.0262\tClassification Loss: 1.3616\r\n",
      "Train Epoch: 25 [39040/209539 (19%)]\tAll Loss: 0.9109\tTriple Loss(1): 0.0151\tClassification Loss: 0.8807\r\n",
      "Train Epoch: 25 [39680/209539 (19%)]\tAll Loss: 1.8255\tTriple Loss(0): 0.3445\tClassification Loss: 1.1365\r\n",
      "Train Epoch: 25 [40320/209539 (19%)]\tAll Loss: 1.2966\tTriple Loss(1): 0.0000\tClassification Loss: 1.2966\r\n",
      "Train Epoch: 25 [40960/209539 (20%)]\tAll Loss: 1.2044\tTriple Loss(1): 0.0188\tClassification Loss: 1.1668\r\n",
      "Train Epoch: 25 [41600/209539 (20%)]\tAll Loss: 1.4638\tTriple Loss(1): 0.0591\tClassification Loss: 1.3456\r\n",
      "Train Epoch: 25 [42240/209539 (20%)]\tAll Loss: 1.3823\tTriple Loss(1): 0.0185\tClassification Loss: 1.3454\r\n",
      "Train Epoch: 25 [42880/209539 (20%)]\tAll Loss: 1.1192\tTriple Loss(1): 0.0136\tClassification Loss: 1.0920\r\n",
      "Train Epoch: 25 [43520/209539 (21%)]\tAll Loss: 1.5590\tTriple Loss(1): 0.0784\tClassification Loss: 1.4021\r\n",
      "Train Epoch: 25 [44160/209539 (21%)]\tAll Loss: 1.6507\tTriple Loss(1): 0.0446\tClassification Loss: 1.5615\r\n",
      "Train Epoch: 25 [44800/209539 (21%)]\tAll Loss: 1.7478\tTriple Loss(1): 0.0151\tClassification Loss: 1.7176\r\n",
      "Train Epoch: 25 [45440/209539 (22%)]\tAll Loss: 2.1918\tTriple Loss(0): 0.2693\tClassification Loss: 1.6531\r\n",
      "Train Epoch: 25 [46080/209539 (22%)]\tAll Loss: 1.1469\tTriple Loss(1): 0.0149\tClassification Loss: 1.1171\r\n",
      "Train Epoch: 25 [46720/209539 (22%)]\tAll Loss: 2.4778\tTriple Loss(0): 0.4433\tClassification Loss: 1.5912\r\n",
      "Train Epoch: 25 [47360/209539 (23%)]\tAll Loss: 0.9523\tTriple Loss(1): 0.0287\tClassification Loss: 0.8950\r\n",
      "Train Epoch: 25 [48000/209539 (23%)]\tAll Loss: 1.0949\tTriple Loss(1): 0.0098\tClassification Loss: 1.0753\r\n",
      "Train Epoch: 25 [48640/209539 (23%)]\tAll Loss: 1.6851\tTriple Loss(1): 0.0272\tClassification Loss: 1.6308\r\n",
      "Train Epoch: 25 [49280/209539 (24%)]\tAll Loss: 1.1859\tTriple Loss(1): 0.0000\tClassification Loss: 1.1859\r\n",
      "Train Epoch: 25 [49920/209539 (24%)]\tAll Loss: 1.8093\tTriple Loss(0): 0.2592\tClassification Loss: 1.2909\r\n",
      "Train Epoch: 25 [50560/209539 (24%)]\tAll Loss: 1.5300\tTriple Loss(1): 0.0220\tClassification Loss: 1.4859\r\n",
      "Train Epoch: 25 [51200/209539 (24%)]\tAll Loss: 1.3702\tTriple Loss(1): 0.0146\tClassification Loss: 1.3410\r\n",
      "Train Epoch: 25 [51840/209539 (25%)]\tAll Loss: 1.7154\tTriple Loss(0): 0.2781\tClassification Loss: 1.1592\r\n",
      "Train Epoch: 25 [52480/209539 (25%)]\tAll Loss: 1.1944\tTriple Loss(1): 0.0418\tClassification Loss: 1.1109\r\n",
      "Train Epoch: 25 [53120/209539 (25%)]\tAll Loss: 1.2881\tTriple Loss(1): 0.0000\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 25 [53760/209539 (26%)]\tAll Loss: 1.6735\tTriple Loss(1): 0.0377\tClassification Loss: 1.5981\r\n",
      "Train Epoch: 25 [54400/209539 (26%)]\tAll Loss: 1.6016\tTriple Loss(1): 0.0316\tClassification Loss: 1.5383\r\n",
      "Train Epoch: 25 [55040/209539 (26%)]\tAll Loss: 1.2098\tTriple Loss(1): 0.0973\tClassification Loss: 1.0151\r\n",
      "Train Epoch: 25 [55680/209539 (27%)]\tAll Loss: 1.6378\tTriple Loss(1): 0.0730\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 25 [56320/209539 (27%)]\tAll Loss: 1.0606\tTriple Loss(1): 0.0353\tClassification Loss: 0.9900\r\n",
      "Train Epoch: 25 [56960/209539 (27%)]\tAll Loss: 1.4426\tTriple Loss(1): 0.1320\tClassification Loss: 1.1786\r\n",
      "Train Epoch: 25 [57600/209539 (27%)]\tAll Loss: 1.1458\tTriple Loss(1): 0.0163\tClassification Loss: 1.1133\r\n",
      "Train Epoch: 25 [58240/209539 (28%)]\tAll Loss: 1.7197\tTriple Loss(0): 0.3815\tClassification Loss: 0.9567\r\n",
      "Train Epoch: 25 [58880/209539 (28%)]\tAll Loss: 1.5397\tTriple Loss(1): 0.0561\tClassification Loss: 1.4274\r\n",
      "Train Epoch: 25 [59520/209539 (28%)]\tAll Loss: 1.4773\tTriple Loss(1): 0.1579\tClassification Loss: 1.1614\r\n",
      "Train Epoch: 25 [60160/209539 (29%)]\tAll Loss: 1.3471\tTriple Loss(1): 0.0037\tClassification Loss: 1.3397\r\n",
      "Train Epoch: 25 [60800/209539 (29%)]\tAll Loss: 1.3728\tTriple Loss(1): 0.0143\tClassification Loss: 1.3441\r\n",
      "Train Epoch: 25 [61440/209539 (29%)]\tAll Loss: 1.5013\tTriple Loss(1): 0.0827\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 25 [62080/209539 (30%)]\tAll Loss: 1.4166\tTriple Loss(1): 0.0598\tClassification Loss: 1.2971\r\n",
      "Train Epoch: 25 [62720/209539 (30%)]\tAll Loss: 1.2418\tTriple Loss(1): 0.0269\tClassification Loss: 1.1879\r\n",
      "Train Epoch: 25 [63360/209539 (30%)]\tAll Loss: 1.3489\tTriple Loss(1): 0.0448\tClassification Loss: 1.2594\r\n",
      "Train Epoch: 25 [64000/209539 (31%)]\tAll Loss: 2.1580\tTriple Loss(0): 0.4281\tClassification Loss: 1.3018\r\n",
      "Train Epoch: 25 [64640/209539 (31%)]\tAll Loss: 1.8080\tTriple Loss(1): 0.0310\tClassification Loss: 1.7459\r\n",
      "Train Epoch: 25 [65280/209539 (31%)]\tAll Loss: 1.8416\tTriple Loss(0): 0.3297\tClassification Loss: 1.1821\r\n",
      "Train Epoch: 25 [65920/209539 (31%)]\tAll Loss: 1.9805\tTriple Loss(0): 0.2769\tClassification Loss: 1.4267\r\n",
      "Train Epoch: 25 [66560/209539 (32%)]\tAll Loss: 1.2590\tTriple Loss(1): 0.0565\tClassification Loss: 1.1461\r\n",
      "Train Epoch: 25 [67200/209539 (32%)]\tAll Loss: 1.4426\tTriple Loss(1): 0.0141\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 25 [67840/209539 (32%)]\tAll Loss: 1.2862\tTriple Loss(1): 0.1050\tClassification Loss: 1.0761\r\n",
      "Train Epoch: 25 [68480/209539 (33%)]\tAll Loss: 1.3894\tTriple Loss(1): 0.0331\tClassification Loss: 1.3233\r\n",
      "Train Epoch: 25 [69120/209539 (33%)]\tAll Loss: 1.3972\tTriple Loss(1): 0.0586\tClassification Loss: 1.2800\r\n",
      "Train Epoch: 25 [69760/209539 (33%)]\tAll Loss: 1.0905\tTriple Loss(1): 0.0309\tClassification Loss: 1.0286\r\n",
      "Train Epoch: 25 [70400/209539 (34%)]\tAll Loss: 1.2000\tTriple Loss(1): 0.0705\tClassification Loss: 1.0589\r\n",
      "Train Epoch: 25 [71040/209539 (34%)]\tAll Loss: 1.5370\tTriple Loss(1): 0.0269\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 25 [71680/209539 (34%)]\tAll Loss: 2.1832\tTriple Loss(0): 0.2858\tClassification Loss: 1.6116\r\n",
      "Train Epoch: 25 [72320/209539 (35%)]\tAll Loss: 1.3411\tTriple Loss(1): 0.1054\tClassification Loss: 1.1302\r\n",
      "Train Epoch: 25 [72960/209539 (35%)]\tAll Loss: 1.4340\tTriple Loss(1): 0.1051\tClassification Loss: 1.2238\r\n",
      "Train Epoch: 25 [73600/209539 (35%)]\tAll Loss: 1.3551\tTriple Loss(1): 0.0307\tClassification Loss: 1.2936\r\n",
      "Train Epoch: 25 [74240/209539 (35%)]\tAll Loss: 2.0695\tTriple Loss(0): 0.3072\tClassification Loss: 1.4551\r\n",
      "Train Epoch: 25 [74880/209539 (36%)]\tAll Loss: 1.7844\tTriple Loss(1): 0.0087\tClassification Loss: 1.7670\r\n",
      "Train Epoch: 25 [75520/209539 (36%)]\tAll Loss: 1.6838\tTriple Loss(0): 0.3416\tClassification Loss: 1.0005\r\n",
      "Train Epoch: 25 [76160/209539 (36%)]\tAll Loss: 1.2608\tTriple Loss(1): 0.0241\tClassification Loss: 1.2126\r\n",
      "Train Epoch: 25 [76800/209539 (37%)]\tAll Loss: 1.1876\tTriple Loss(1): 0.0078\tClassification Loss: 1.1720\r\n",
      "Train Epoch: 25 [77440/209539 (37%)]\tAll Loss: 1.2904\tTriple Loss(1): 0.0052\tClassification Loss: 1.2800\r\n",
      "Train Epoch: 25 [78080/209539 (37%)]\tAll Loss: 2.2819\tTriple Loss(0): 0.4637\tClassification Loss: 1.3546\r\n",
      "Train Epoch: 25 [78720/209539 (38%)]\tAll Loss: 1.5081\tTriple Loss(1): 0.0099\tClassification Loss: 1.4884\r\n",
      "Train Epoch: 25 [79360/209539 (38%)]\tAll Loss: 1.7386\tTriple Loss(0): 0.2946\tClassification Loss: 1.1495\r\n",
      "Train Epoch: 25 [80000/209539 (38%)]\tAll Loss: 1.3188\tTriple Loss(1): 0.0001\tClassification Loss: 1.3187\r\n",
      "Train Epoch: 25 [80640/209539 (38%)]\tAll Loss: 1.6158\tTriple Loss(1): 0.0808\tClassification Loss: 1.4541\r\n",
      "Train Epoch: 25 [81280/209539 (39%)]\tAll Loss: 1.4581\tTriple Loss(1): 0.0004\tClassification Loss: 1.4572\r\n",
      "Train Epoch: 25 [81920/209539 (39%)]\tAll Loss: 1.1289\tTriple Loss(1): 0.0195\tClassification Loss: 1.0900\r\n",
      "Train Epoch: 25 [82560/209539 (39%)]\tAll Loss: 1.6955\tTriple Loss(1): 0.1078\tClassification Loss: 1.4799\r\n",
      "Train Epoch: 25 [83200/209539 (40%)]\tAll Loss: 1.5752\tTriple Loss(1): 0.0368\tClassification Loss: 1.5015\r\n",
      "Train Epoch: 25 [83840/209539 (40%)]\tAll Loss: 2.0645\tTriple Loss(0): 0.3799\tClassification Loss: 1.3047\r\n",
      "Train Epoch: 25 [84480/209539 (40%)]\tAll Loss: 1.3336\tTriple Loss(1): 0.0727\tClassification Loss: 1.1882\r\n",
      "Train Epoch: 25 [85120/209539 (41%)]\tAll Loss: 1.8689\tTriple Loss(1): 0.0367\tClassification Loss: 1.7955\r\n",
      "Train Epoch: 25 [85760/209539 (41%)]\tAll Loss: 2.0342\tTriple Loss(0): 0.3690\tClassification Loss: 1.2961\r\n",
      "Train Epoch: 25 [86400/209539 (41%)]\tAll Loss: 1.4659\tTriple Loss(1): 0.0011\tClassification Loss: 1.4638\r\n",
      "Train Epoch: 25 [87040/209539 (42%)]\tAll Loss: 1.3890\tTriple Loss(1): 0.0810\tClassification Loss: 1.2270\r\n",
      "Train Epoch: 25 [87680/209539 (42%)]\tAll Loss: 1.3277\tTriple Loss(1): 0.0526\tClassification Loss: 1.2226\r\n",
      "Train Epoch: 25 [88320/209539 (42%)]\tAll Loss: 1.4736\tTriple Loss(1): 0.0510\tClassification Loss: 1.3716\r\n",
      "Train Epoch: 25 [88960/209539 (42%)]\tAll Loss: 1.5370\tTriple Loss(1): 0.0814\tClassification Loss: 1.3742\r\n",
      "Train Epoch: 25 [89600/209539 (43%)]\tAll Loss: 1.6679\tTriple Loss(1): 0.0836\tClassification Loss: 1.5008\r\n",
      "Train Epoch: 25 [90240/209539 (43%)]\tAll Loss: 1.7560\tTriple Loss(0): 0.2450\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 25 [90880/209539 (43%)]\tAll Loss: 1.6935\tTriple Loss(1): 0.0681\tClassification Loss: 1.5574\r\n",
      "Train Epoch: 25 [91520/209539 (44%)]\tAll Loss: 1.5492\tTriple Loss(1): 0.1075\tClassification Loss: 1.3343\r\n",
      "Train Epoch: 25 [92160/209539 (44%)]\tAll Loss: 1.0815\tTriple Loss(1): 0.0342\tClassification Loss: 1.0130\r\n",
      "Train Epoch: 25 [92800/209539 (44%)]\tAll Loss: 1.2128\tTriple Loss(1): 0.0321\tClassification Loss: 1.1486\r\n",
      "Train Epoch: 25 [93440/209539 (45%)]\tAll Loss: 2.0996\tTriple Loss(0): 0.3090\tClassification Loss: 1.4816\r\n",
      "Train Epoch: 25 [94080/209539 (45%)]\tAll Loss: 1.4495\tTriple Loss(1): 0.0197\tClassification Loss: 1.4100\r\n",
      "Train Epoch: 25 [94720/209539 (45%)]\tAll Loss: 1.3861\tTriple Loss(1): 0.0000\tClassification Loss: 1.3861\r\n",
      "Train Epoch: 25 [95360/209539 (46%)]\tAll Loss: 1.1450\tTriple Loss(1): 0.0000\tClassification Loss: 1.1450\r\n",
      "Train Epoch: 25 [96000/209539 (46%)]\tAll Loss: 1.4666\tTriple Loss(1): 0.0297\tClassification Loss: 1.4072\r\n",
      "Train Epoch: 25 [96640/209539 (46%)]\tAll Loss: 1.7453\tTriple Loss(1): 0.0543\tClassification Loss: 1.6367\r\n",
      "Train Epoch: 25 [97280/209539 (46%)]\tAll Loss: 1.3910\tTriple Loss(1): 0.0662\tClassification Loss: 1.2587\r\n",
      "Train Epoch: 25 [97920/209539 (47%)]\tAll Loss: 1.3143\tTriple Loss(1): 0.0238\tClassification Loss: 1.2666\r\n",
      "Train Epoch: 25 [98560/209539 (47%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.0642\tClassification Loss: 1.6323\r\n",
      "Train Epoch: 25 [99200/209539 (47%)]\tAll Loss: 1.2109\tTriple Loss(1): 0.0290\tClassification Loss: 1.1530\r\n",
      "Train Epoch: 25 [99840/209539 (48%)]\tAll Loss: 1.2201\tTriple Loss(1): 0.0081\tClassification Loss: 1.2038\r\n",
      "Train Epoch: 25 [100480/209539 (48%)]\tAll Loss: 2.5727\tTriple Loss(0): 0.6048\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 25 [101120/209539 (48%)]\tAll Loss: 1.4114\tTriple Loss(1): 0.0024\tClassification Loss: 1.4066\r\n",
      "Train Epoch: 25 [101760/209539 (49%)]\tAll Loss: 2.0938\tTriple Loss(0): 0.4380\tClassification Loss: 1.2178\r\n",
      "Train Epoch: 25 [102400/209539 (49%)]\tAll Loss: 1.7182\tTriple Loss(0): 0.3181\tClassification Loss: 1.0820\r\n",
      "Train Epoch: 25 [103040/209539 (49%)]\tAll Loss: 1.2322\tTriple Loss(1): 0.0647\tClassification Loss: 1.1028\r\n",
      "Train Epoch: 25 [103680/209539 (49%)]\tAll Loss: 1.4968\tTriple Loss(1): 0.0389\tClassification Loss: 1.4190\r\n",
      "Train Epoch: 25 [104320/209539 (50%)]\tAll Loss: 1.2578\tTriple Loss(1): 0.0425\tClassification Loss: 1.1729\r\n",
      "Train Epoch: 25 [104960/209539 (50%)]\tAll Loss: 1.4572\tTriple Loss(1): 0.0367\tClassification Loss: 1.3838\r\n",
      "Train Epoch: 25 [105600/209539 (50%)]\tAll Loss: 2.1763\tTriple Loss(0): 0.3612\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 25 [106240/209539 (51%)]\tAll Loss: 1.4670\tTriple Loss(1): 0.0028\tClassification Loss: 1.4615\r\n",
      "Train Epoch: 25 [106880/209539 (51%)]\tAll Loss: 1.2392\tTriple Loss(1): 0.0243\tClassification Loss: 1.1905\r\n",
      "Train Epoch: 25 [107520/209539 (51%)]\tAll Loss: 1.7651\tTriple Loss(0): 0.3004\tClassification Loss: 1.1644\r\n",
      "Train Epoch: 25 [108160/209539 (52%)]\tAll Loss: 1.4642\tTriple Loss(1): 0.0000\tClassification Loss: 1.4642\r\n",
      "Train Epoch: 25 [108800/209539 (52%)]\tAll Loss: 1.1426\tTriple Loss(1): 0.0000\tClassification Loss: 1.1426\r\n",
      "Train Epoch: 25 [109440/209539 (52%)]\tAll Loss: 2.3875\tTriple Loss(0): 0.4554\tClassification Loss: 1.4768\r\n",
      "Train Epoch: 25 [110080/209539 (53%)]\tAll Loss: 1.5610\tTriple Loss(1): 0.0564\tClassification Loss: 1.4482\r\n",
      "Train Epoch: 25 [110720/209539 (53%)]\tAll Loss: 1.3770\tTriple Loss(1): 0.0321\tClassification Loss: 1.3128\r\n",
      "Train Epoch: 25 [111360/209539 (53%)]\tAll Loss: 2.0029\tTriple Loss(0): 0.4442\tClassification Loss: 1.1146\r\n",
      "Train Epoch: 25 [112000/209539 (53%)]\tAll Loss: 1.3470\tTriple Loss(1): 0.1178\tClassification Loss: 1.1113\r\n",
      "Train Epoch: 25 [112640/209539 (54%)]\tAll Loss: 1.1571\tTriple Loss(1): 0.0000\tClassification Loss: 1.1571\r\n",
      "Train Epoch: 25 [113280/209539 (54%)]\tAll Loss: 1.5152\tTriple Loss(0): 0.2577\tClassification Loss: 0.9999\r\n",
      "Train Epoch: 25 [113920/209539 (54%)]\tAll Loss: 1.4096\tTriple Loss(1): 0.0448\tClassification Loss: 1.3200\r\n",
      "Train Epoch: 25 [114560/209539 (55%)]\tAll Loss: 1.4337\tTriple Loss(1): 0.0274\tClassification Loss: 1.3790\r\n",
      "Train Epoch: 25 [115200/209539 (55%)]\tAll Loss: 1.5361\tTriple Loss(1): 0.0131\tClassification Loss: 1.5099\r\n",
      "Train Epoch: 25 [115840/209539 (55%)]\tAll Loss: 1.3041\tTriple Loss(1): 0.0307\tClassification Loss: 1.2426\r\n",
      "Train Epoch: 25 [116480/209539 (56%)]\tAll Loss: 1.3041\tTriple Loss(1): 0.0383\tClassification Loss: 1.2274\r\n",
      "Train Epoch: 25 [117120/209539 (56%)]\tAll Loss: 1.3223\tTriple Loss(1): 0.0460\tClassification Loss: 1.2303\r\n",
      "Train Epoch: 25 [117760/209539 (56%)]\tAll Loss: 1.3171\tTriple Loss(1): 0.0397\tClassification Loss: 1.2378\r\n",
      "Train Epoch: 25 [118400/209539 (57%)]\tAll Loss: 1.1082\tTriple Loss(1): 0.0531\tClassification Loss: 1.0020\r\n",
      "Train Epoch: 25 [119040/209539 (57%)]\tAll Loss: 2.3991\tTriple Loss(0): 0.4310\tClassification Loss: 1.5371\r\n",
      "Train Epoch: 25 [119680/209539 (57%)]\tAll Loss: 1.2346\tTriple Loss(1): 0.0000\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 25 [120320/209539 (57%)]\tAll Loss: 1.2930\tTriple Loss(1): 0.0265\tClassification Loss: 1.2400\r\n",
      "Train Epoch: 25 [120960/209539 (58%)]\tAll Loss: 1.0433\tTriple Loss(1): 0.0060\tClassification Loss: 1.0313\r\n",
      "Train Epoch: 25 [121600/209539 (58%)]\tAll Loss: 1.2736\tTriple Loss(1): 0.0226\tClassification Loss: 1.2283\r\n",
      "Train Epoch: 25 [122240/209539 (58%)]\tAll Loss: 0.9011\tTriple Loss(1): 0.0000\tClassification Loss: 0.9011\r\n",
      "Train Epoch: 25 [122880/209539 (59%)]\tAll Loss: 1.2600\tTriple Loss(1): 0.0535\tClassification Loss: 1.1531\r\n",
      "Train Epoch: 25 [123520/209539 (59%)]\tAll Loss: 1.3144\tTriple Loss(1): 0.0226\tClassification Loss: 1.2692\r\n",
      "Train Epoch: 25 [124160/209539 (59%)]\tAll Loss: 1.9870\tTriple Loss(0): 0.3439\tClassification Loss: 1.2992\r\n",
      "Train Epoch: 25 [124800/209539 (60%)]\tAll Loss: 1.1881\tTriple Loss(1): 0.0541\tClassification Loss: 1.0799\r\n",
      "Train Epoch: 25 [125440/209539 (60%)]\tAll Loss: 2.1678\tTriple Loss(0): 0.3499\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 25 [126080/209539 (60%)]\tAll Loss: 1.8676\tTriple Loss(0): 0.2099\tClassification Loss: 1.4478\r\n",
      "Train Epoch: 25 [126720/209539 (60%)]\tAll Loss: 1.5302\tTriple Loss(1): 0.0296\tClassification Loss: 1.4710\r\n",
      "Train Epoch: 25 [127360/209539 (61%)]\tAll Loss: 1.4691\tTriple Loss(1): 0.0664\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 25 [128000/209539 (61%)]\tAll Loss: 2.4422\tTriple Loss(0): 0.4688\tClassification Loss: 1.5046\r\n",
      "Train Epoch: 25 [128640/209539 (61%)]\tAll Loss: 1.1708\tTriple Loss(1): 0.0696\tClassification Loss: 1.0316\r\n",
      "Train Epoch: 25 [129280/209539 (62%)]\tAll Loss: 1.5660\tTriple Loss(1): 0.0538\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 25 [129920/209539 (62%)]\tAll Loss: 1.1960\tTriple Loss(1): 0.0326\tClassification Loss: 1.1307\r\n",
      "Train Epoch: 25 [130560/209539 (62%)]\tAll Loss: 1.1557\tTriple Loss(1): 0.0410\tClassification Loss: 1.0738\r\n",
      "Train Epoch: 25 [131200/209539 (63%)]\tAll Loss: 1.5096\tTriple Loss(0): 0.1926\tClassification Loss: 1.1245\r\n",
      "Train Epoch: 25 [131840/209539 (63%)]\tAll Loss: 2.1583\tTriple Loss(0): 0.3700\tClassification Loss: 1.4183\r\n",
      "Train Epoch: 25 [132480/209539 (63%)]\tAll Loss: 1.3140\tTriple Loss(1): 0.0453\tClassification Loss: 1.2235\r\n",
      "Train Epoch: 25 [133120/209539 (64%)]\tAll Loss: 2.1556\tTriple Loss(0): 0.5154\tClassification Loss: 1.1249\r\n",
      "Train Epoch: 25 [133760/209539 (64%)]\tAll Loss: 1.3223\tTriple Loss(1): 0.0785\tClassification Loss: 1.1653\r\n",
      "Train Epoch: 25 [134400/209539 (64%)]\tAll Loss: 1.2700\tTriple Loss(1): 0.0825\tClassification Loss: 1.1049\r\n",
      "Train Epoch: 25 [135040/209539 (64%)]\tAll Loss: 2.3467\tTriple Loss(0): 0.4548\tClassification Loss: 1.4371\r\n",
      "Train Epoch: 25 [135680/209539 (65%)]\tAll Loss: 1.5823\tTriple Loss(1): 0.0479\tClassification Loss: 1.4866\r\n",
      "Train Epoch: 25 [136320/209539 (65%)]\tAll Loss: 1.4970\tTriple Loss(1): 0.0008\tClassification Loss: 1.4953\r\n",
      "Train Epoch: 25 [136960/209539 (65%)]\tAll Loss: 1.3304\tTriple Loss(1): 0.0210\tClassification Loss: 1.2884\r\n",
      "Train Epoch: 25 [137600/209539 (66%)]\tAll Loss: 1.3456\tTriple Loss(1): 0.0370\tClassification Loss: 1.2716\r\n",
      "Train Epoch: 25 [138240/209539 (66%)]\tAll Loss: 1.6022\tTriple Loss(1): 0.0605\tClassification Loss: 1.4812\r\n",
      "Train Epoch: 25 [138880/209539 (66%)]\tAll Loss: 1.4545\tTriple Loss(1): 0.0019\tClassification Loss: 1.4507\r\n",
      "Train Epoch: 25 [139520/209539 (67%)]\tAll Loss: 1.2626\tTriple Loss(1): 0.0654\tClassification Loss: 1.1319\r\n",
      "Train Epoch: 25 [140160/209539 (67%)]\tAll Loss: 1.8587\tTriple Loss(0): 0.3470\tClassification Loss: 1.1647\r\n",
      "Train Epoch: 25 [140800/209539 (67%)]\tAll Loss: 1.5540\tTriple Loss(1): 0.0211\tClassification Loss: 1.5119\r\n",
      "Train Epoch: 25 [141440/209539 (68%)]\tAll Loss: 1.9128\tTriple Loss(0): 0.3374\tClassification Loss: 1.2381\r\n",
      "Train Epoch: 25 [142080/209539 (68%)]\tAll Loss: 1.4227\tTriple Loss(1): 0.0935\tClassification Loss: 1.2356\r\n",
      "Train Epoch: 25 [142720/209539 (68%)]\tAll Loss: 1.6280\tTriple Loss(1): 0.0164\tClassification Loss: 1.5952\r\n",
      "Train Epoch: 25 [143360/209539 (68%)]\tAll Loss: 1.3411\tTriple Loss(1): 0.0289\tClassification Loss: 1.2833\r\n",
      "Train Epoch: 25 [144000/209539 (69%)]\tAll Loss: 1.4857\tTriple Loss(1): 0.0303\tClassification Loss: 1.4251\r\n",
      "Train Epoch: 25 [144640/209539 (69%)]\tAll Loss: 1.3050\tTriple Loss(1): 0.0470\tClassification Loss: 1.2110\r\n",
      "Train Epoch: 25 [145280/209539 (69%)]\tAll Loss: 1.5607\tTriple Loss(1): 0.0000\tClassification Loss: 1.5607\r\n",
      "Train Epoch: 25 [145920/209539 (70%)]\tAll Loss: 2.0000\tTriple Loss(0): 0.2869\tClassification Loss: 1.4262\r\n",
      "Train Epoch: 25 [146560/209539 (70%)]\tAll Loss: 1.0861\tTriple Loss(1): 0.0463\tClassification Loss: 0.9936\r\n",
      "Train Epoch: 25 [147200/209539 (70%)]\tAll Loss: 1.3987\tTriple Loss(1): 0.0000\tClassification Loss: 1.3987\r\n",
      "Train Epoch: 25 [147840/209539 (71%)]\tAll Loss: 1.9057\tTriple Loss(0): 0.3864\tClassification Loss: 1.1329\r\n",
      "Train Epoch: 25 [148480/209539 (71%)]\tAll Loss: 1.1209\tTriple Loss(1): 0.0513\tClassification Loss: 1.0183\r\n",
      "Train Epoch: 25 [149120/209539 (71%)]\tAll Loss: 1.4169\tTriple Loss(1): 0.0314\tClassification Loss: 1.3542\r\n",
      "Train Epoch: 25 [149760/209539 (71%)]\tAll Loss: 1.0803\tTriple Loss(1): 0.0172\tClassification Loss: 1.0460\r\n",
      "Train Epoch: 25 [150400/209539 (72%)]\tAll Loss: 2.4597\tTriple Loss(0): 0.5155\tClassification Loss: 1.4287\r\n",
      "Train Epoch: 25 [151040/209539 (72%)]\tAll Loss: 1.2968\tTriple Loss(1): 0.0398\tClassification Loss: 1.2172\r\n",
      "Train Epoch: 25 [151680/209539 (72%)]\tAll Loss: 1.3395\tTriple Loss(1): 0.0383\tClassification Loss: 1.2629\r\n",
      "Train Epoch: 25 [152320/209539 (73%)]\tAll Loss: 1.7229\tTriple Loss(0): 0.3153\tClassification Loss: 1.0923\r\n",
      "Train Epoch: 25 [152960/209539 (73%)]\tAll Loss: 1.2277\tTriple Loss(1): 0.0000\tClassification Loss: 1.2277\r\n",
      "Train Epoch: 25 [153600/209539 (73%)]\tAll Loss: 1.6707\tTriple Loss(1): 0.0181\tClassification Loss: 1.6344\r\n",
      "Train Epoch: 25 [154240/209539 (74%)]\tAll Loss: 1.3340\tTriple Loss(1): 0.0103\tClassification Loss: 1.3134\r\n",
      "Train Epoch: 25 [154880/209539 (74%)]\tAll Loss: 1.3160\tTriple Loss(1): 0.0017\tClassification Loss: 1.3125\r\n",
      "Train Epoch: 25 [155520/209539 (74%)]\tAll Loss: 1.2606\tTriple Loss(1): 0.0329\tClassification Loss: 1.1949\r\n",
      "Train Epoch: 25 [156160/209539 (75%)]\tAll Loss: 1.4147\tTriple Loss(1): 0.0198\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 25 [156800/209539 (75%)]\tAll Loss: 1.6657\tTriple Loss(1): 0.0540\tClassification Loss: 1.5577\r\n",
      "Train Epoch: 25 [157440/209539 (75%)]\tAll Loss: 1.4023\tTriple Loss(1): 0.0043\tClassification Loss: 1.3937\r\n",
      "Train Epoch: 25 [158080/209539 (75%)]\tAll Loss: 1.4971\tTriple Loss(1): 0.0755\tClassification Loss: 1.3461\r\n",
      "Train Epoch: 25 [158720/209539 (76%)]\tAll Loss: 1.7316\tTriple Loss(0): 0.3186\tClassification Loss: 1.0944\r\n",
      "Train Epoch: 25 [159360/209539 (76%)]\tAll Loss: 0.8987\tTriple Loss(1): 0.0000\tClassification Loss: 0.8987\r\n",
      "Train Epoch: 25 [160000/209539 (76%)]\tAll Loss: 2.1449\tTriple Loss(0): 0.2801\tClassification Loss: 1.5848\r\n",
      "Train Epoch: 25 [160640/209539 (77%)]\tAll Loss: 0.9688\tTriple Loss(1): 0.0006\tClassification Loss: 0.9675\r\n",
      "Train Epoch: 25 [161280/209539 (77%)]\tAll Loss: 1.0905\tTriple Loss(1): 0.0134\tClassification Loss: 1.0637\r\n",
      "Train Epoch: 25 [161920/209539 (77%)]\tAll Loss: 1.8825\tTriple Loss(0): 0.3057\tClassification Loss: 1.2710\r\n",
      "Train Epoch: 25 [162560/209539 (78%)]\tAll Loss: 1.3147\tTriple Loss(1): 0.0307\tClassification Loss: 1.2534\r\n",
      "Train Epoch: 25 [163200/209539 (78%)]\tAll Loss: 1.6332\tTriple Loss(0): 0.2479\tClassification Loss: 1.1374\r\n",
      "Train Epoch: 25 [163840/209539 (78%)]\tAll Loss: 1.1521\tTriple Loss(1): 0.0079\tClassification Loss: 1.1363\r\n",
      "Train Epoch: 25 [164480/209539 (78%)]\tAll Loss: 1.2944\tTriple Loss(1): 0.0128\tClassification Loss: 1.2689\r\n",
      "Train Epoch: 25 [165120/209539 (79%)]\tAll Loss: 1.1511\tTriple Loss(1): 0.1315\tClassification Loss: 0.8882\r\n",
      "Train Epoch: 25 [165760/209539 (79%)]\tAll Loss: 1.1735\tTriple Loss(1): 0.0161\tClassification Loss: 1.1413\r\n",
      "Train Epoch: 25 [166400/209539 (79%)]\tAll Loss: 1.4500\tTriple Loss(1): 0.0428\tClassification Loss: 1.3644\r\n",
      "Train Epoch: 25 [167040/209539 (80%)]\tAll Loss: 2.0513\tTriple Loss(0): 0.2823\tClassification Loss: 1.4868\r\n",
      "Train Epoch: 25 [167680/209539 (80%)]\tAll Loss: 1.4095\tTriple Loss(1): 0.0096\tClassification Loss: 1.3902\r\n",
      "Train Epoch: 25 [168320/209539 (80%)]\tAll Loss: 1.2294\tTriple Loss(1): 0.1000\tClassification Loss: 1.0295\r\n",
      "Train Epoch: 25 [168960/209539 (81%)]\tAll Loss: 1.0891\tTriple Loss(1): 0.0249\tClassification Loss: 1.0393\r\n",
      "Train Epoch: 25 [169600/209539 (81%)]\tAll Loss: 1.1546\tTriple Loss(1): 0.0031\tClassification Loss: 1.1485\r\n",
      "Train Epoch: 25 [170240/209539 (81%)]\tAll Loss: 1.3743\tTriple Loss(1): 0.0388\tClassification Loss: 1.2967\r\n",
      "Train Epoch: 25 [170880/209539 (82%)]\tAll Loss: 1.9420\tTriple Loss(0): 0.3390\tClassification Loss: 1.2639\r\n",
      "Train Epoch: 25 [171520/209539 (82%)]\tAll Loss: 1.1548\tTriple Loss(1): 0.0264\tClassification Loss: 1.1021\r\n",
      "Train Epoch: 25 [172160/209539 (82%)]\tAll Loss: 1.2977\tTriple Loss(1): 0.0240\tClassification Loss: 1.2497\r\n",
      "Train Epoch: 25 [172800/209539 (82%)]\tAll Loss: 1.4598\tTriple Loss(1): 0.0411\tClassification Loss: 1.3775\r\n",
      "Train Epoch: 25 [173440/209539 (83%)]\tAll Loss: 1.6186\tTriple Loss(1): 0.0249\tClassification Loss: 1.5689\r\n",
      "Train Epoch: 25 [174080/209539 (83%)]\tAll Loss: 2.0380\tTriple Loss(0): 0.4327\tClassification Loss: 1.1727\r\n",
      "Train Epoch: 25 [174720/209539 (83%)]\tAll Loss: 1.4275\tTriple Loss(1): 0.0172\tClassification Loss: 1.3931\r\n",
      "Train Epoch: 25 [175360/209539 (84%)]\tAll Loss: 1.5384\tTriple Loss(1): 0.0642\tClassification Loss: 1.4101\r\n",
      "Train Epoch: 25 [176000/209539 (84%)]\tAll Loss: 1.4159\tTriple Loss(1): 0.0446\tClassification Loss: 1.3267\r\n",
      "Train Epoch: 25 [176640/209539 (84%)]\tAll Loss: 0.8451\tTriple Loss(1): 0.0229\tClassification Loss: 0.7992\r\n",
      "Train Epoch: 25 [177280/209539 (85%)]\tAll Loss: 1.4874\tTriple Loss(1): 0.0041\tClassification Loss: 1.4792\r\n",
      "Train Epoch: 25 [177920/209539 (85%)]\tAll Loss: 1.4287\tTriple Loss(1): 0.0078\tClassification Loss: 1.4130\r\n",
      "Train Epoch: 25 [178560/209539 (85%)]\tAll Loss: 1.2153\tTriple Loss(1): 0.0026\tClassification Loss: 1.2101\r\n",
      "Train Epoch: 25 [179200/209539 (86%)]\tAll Loss: 1.4319\tTriple Loss(1): 0.0250\tClassification Loss: 1.3820\r\n",
      "Train Epoch: 25 [179840/209539 (86%)]\tAll Loss: 1.3938\tTriple Loss(1): 0.0136\tClassification Loss: 1.3666\r\n",
      "Train Epoch: 25 [180480/209539 (86%)]\tAll Loss: 1.2733\tTriple Loss(1): 0.0003\tClassification Loss: 1.2728\r\n",
      "Train Epoch: 25 [181120/209539 (86%)]\tAll Loss: 1.9306\tTriple Loss(0): 0.1839\tClassification Loss: 1.5627\r\n",
      "Train Epoch: 25 [181760/209539 (87%)]\tAll Loss: 1.2022\tTriple Loss(1): 0.0000\tClassification Loss: 1.2022\r\n",
      "Train Epoch: 25 [182400/209539 (87%)]\tAll Loss: 2.1125\tTriple Loss(0): 0.2838\tClassification Loss: 1.5449\r\n",
      "Train Epoch: 25 [183040/209539 (87%)]\tAll Loss: 1.6267\tTriple Loss(1): 0.1011\tClassification Loss: 1.4244\r\n",
      "Train Epoch: 25 [183680/209539 (88%)]\tAll Loss: 1.0803\tTriple Loss(1): 0.0053\tClassification Loss: 1.0696\r\n",
      "Train Epoch: 25 [184320/209539 (88%)]\tAll Loss: 1.3499\tTriple Loss(1): 0.0381\tClassification Loss: 1.2737\r\n",
      "Train Epoch: 25 [184960/209539 (88%)]\tAll Loss: 1.2311\tTriple Loss(1): 0.0059\tClassification Loss: 1.2194\r\n",
      "Train Epoch: 25 [185600/209539 (89%)]\tAll Loss: 2.1708\tTriple Loss(0): 0.4549\tClassification Loss: 1.2610\r\n",
      "Train Epoch: 25 [186240/209539 (89%)]\tAll Loss: 1.6086\tTriple Loss(1): 0.0433\tClassification Loss: 1.5219\r\n",
      "Train Epoch: 25 [186880/209539 (89%)]\tAll Loss: 1.3757\tTriple Loss(1): 0.0442\tClassification Loss: 1.2873\r\n",
      "Train Epoch: 25 [187520/209539 (89%)]\tAll Loss: 1.6378\tTriple Loss(1): 0.0986\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 25 [188160/209539 (90%)]\tAll Loss: 1.0221\tTriple Loss(1): 0.0000\tClassification Loss: 1.0221\r\n",
      "Train Epoch: 25 [188800/209539 (90%)]\tAll Loss: 1.1650\tTriple Loss(1): 0.0221\tClassification Loss: 1.1208\r\n",
      "Train Epoch: 25 [189440/209539 (90%)]\tAll Loss: 1.1902\tTriple Loss(1): 0.0012\tClassification Loss: 1.1878\r\n",
      "Train Epoch: 25 [190080/209539 (91%)]\tAll Loss: 1.2491\tTriple Loss(1): 0.0734\tClassification Loss: 1.1023\r\n",
      "Train Epoch: 25 [190720/209539 (91%)]\tAll Loss: 1.4703\tTriple Loss(1): 0.0489\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 25 [191360/209539 (91%)]\tAll Loss: 2.0738\tTriple Loss(0): 0.6036\tClassification Loss: 0.8667\r\n",
      "Train Epoch: 25 [192000/209539 (92%)]\tAll Loss: 1.5211\tTriple Loss(1): 0.0064\tClassification Loss: 1.5083\r\n",
      "Train Epoch: 25 [192640/209539 (92%)]\tAll Loss: 1.5871\tTriple Loss(0): 0.2131\tClassification Loss: 1.1610\r\n",
      "Train Epoch: 25 [193280/209539 (92%)]\tAll Loss: 1.4821\tTriple Loss(1): 0.0931\tClassification Loss: 1.2959\r\n",
      "Train Epoch: 25 [193920/209539 (93%)]\tAll Loss: 2.0491\tTriple Loss(0): 0.3328\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 25 [194560/209539 (93%)]\tAll Loss: 1.4092\tTriple Loss(1): 0.0761\tClassification Loss: 1.2569\r\n",
      "Train Epoch: 25 [195200/209539 (93%)]\tAll Loss: 1.3574\tTriple Loss(1): 0.0460\tClassification Loss: 1.2653\r\n",
      "Train Epoch: 25 [195840/209539 (93%)]\tAll Loss: 1.0124\tTriple Loss(1): 0.0487\tClassification Loss: 0.9150\r\n",
      "Train Epoch: 25 [196480/209539 (94%)]\tAll Loss: 1.6287\tTriple Loss(1): 0.0000\tClassification Loss: 1.6287\r\n",
      "Train Epoch: 25 [197120/209539 (94%)]\tAll Loss: 1.4369\tTriple Loss(1): 0.0598\tClassification Loss: 1.3174\r\n",
      "Train Epoch: 25 [197760/209539 (94%)]\tAll Loss: 1.4539\tTriple Loss(1): 0.0588\tClassification Loss: 1.3363\r\n",
      "Train Epoch: 25 [198400/209539 (95%)]\tAll Loss: 1.1601\tTriple Loss(1): 0.0154\tClassification Loss: 1.1292\r\n",
      "Train Epoch: 25 [199040/209539 (95%)]\tAll Loss: 1.5188\tTriple Loss(1): 0.0272\tClassification Loss: 1.4643\r\n",
      "Train Epoch: 25 [199680/209539 (95%)]\tAll Loss: 1.1509\tTriple Loss(1): 0.0189\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 25 [200320/209539 (96%)]\tAll Loss: 1.4177\tTriple Loss(1): 0.0222\tClassification Loss: 1.3732\r\n",
      "Train Epoch: 25 [200960/209539 (96%)]\tAll Loss: 1.0542\tTriple Loss(1): 0.0343\tClassification Loss: 0.9857\r\n",
      "Train Epoch: 25 [201600/209539 (96%)]\tAll Loss: 1.2312\tTriple Loss(1): 0.0000\tClassification Loss: 1.2312\r\n",
      "Train Epoch: 25 [202240/209539 (97%)]\tAll Loss: 1.2367\tTriple Loss(1): 0.0126\tClassification Loss: 1.2115\r\n",
      "Train Epoch: 25 [202880/209539 (97%)]\tAll Loss: 1.0379\tTriple Loss(1): 0.0274\tClassification Loss: 0.9830\r\n",
      "Train Epoch: 25 [203520/209539 (97%)]\tAll Loss: 1.2855\tTriple Loss(1): 0.0385\tClassification Loss: 1.2084\r\n",
      "Train Epoch: 25 [204160/209539 (97%)]\tAll Loss: 1.8957\tTriple Loss(1): 0.0901\tClassification Loss: 1.7156\r\n",
      "Train Epoch: 25 [204800/209539 (98%)]\tAll Loss: 1.5353\tTriple Loss(1): 0.0244\tClassification Loss: 1.4865\r\n",
      "Train Epoch: 25 [205440/209539 (98%)]\tAll Loss: 1.1006\tTriple Loss(1): 0.0284\tClassification Loss: 1.0437\r\n",
      "Train Epoch: 25 [206080/209539 (98%)]\tAll Loss: 1.5084\tTriple Loss(1): 0.0945\tClassification Loss: 1.3193\r\n",
      "Train Epoch: 25 [206720/209539 (99%)]\tAll Loss: 1.2023\tTriple Loss(1): 0.0226\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 25 [207360/209539 (99%)]\tAll Loss: 1.0141\tTriple Loss(1): 0.0157\tClassification Loss: 0.9827\r\n",
      "Train Epoch: 25 [208000/209539 (99%)]\tAll Loss: 1.0298\tTriple Loss(1): 0.0000\tClassification Loss: 1.0298\r\n",
      "Train Epoch: 25 [208640/209539 (100%)]\tAll Loss: 1.4771\tTriple Loss(1): 0.0246\tClassification Loss: 1.4280\r\n",
      "Train Epoch: 25 [209280/209539 (100%)]\tAll Loss: 1.5938\tTriple Loss(1): 0.0203\tClassification Loss: 1.5533\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/25_epochs\r\n",
      "Train Epoch: 26 [0/209539 (0%)]\tAll Loss: 1.7598\tTriple Loss(1): 0.0703\tClassification Loss: 1.6191\r\n",
      "\r\n",
      "Test set: Average loss: 1.1271\r\n",
      "Top 1 Accuracy: 53921/80128 (67%)\r\n",
      "Top 3 Accuracy: 69486/80128 (87%)\r\n",
      "Top 5 Accuracy: 74258/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 26 [640/209539 (0%)]\tAll Loss: 1.4330\tTriple Loss(1): 0.0253\tClassification Loss: 1.3825\r\n",
      "Train Epoch: 26 [1280/209539 (1%)]\tAll Loss: 1.2484\tTriple Loss(1): 0.0496\tClassification Loss: 1.1493\r\n",
      "Train Epoch: 26 [1920/209539 (1%)]\tAll Loss: 1.3749\tTriple Loss(1): 0.0389\tClassification Loss: 1.2971\r\n",
      "Train Epoch: 26 [2560/209539 (1%)]\tAll Loss: 2.1731\tTriple Loss(0): 0.3355\tClassification Loss: 1.5022\r\n",
      "Train Epoch: 26 [3200/209539 (2%)]\tAll Loss: 2.3975\tTriple Loss(0): 0.5025\tClassification Loss: 1.3926\r\n",
      "Train Epoch: 26 [3840/209539 (2%)]\tAll Loss: 1.2695\tTriple Loss(1): 0.0476\tClassification Loss: 1.1742\r\n",
      "Train Epoch: 26 [4480/209539 (2%)]\tAll Loss: 1.5040\tTriple Loss(1): 0.0841\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 26 [5120/209539 (2%)]\tAll Loss: 1.3780\tTriple Loss(1): 0.0294\tClassification Loss: 1.3192\r\n",
      "Train Epoch: 26 [5760/209539 (3%)]\tAll Loss: 2.4623\tTriple Loss(0): 0.5580\tClassification Loss: 1.3464\r\n",
      "Train Epoch: 26 [6400/209539 (3%)]\tAll Loss: 1.0353\tTriple Loss(1): 0.0444\tClassification Loss: 0.9464\r\n",
      "Train Epoch: 26 [7040/209539 (3%)]\tAll Loss: 1.0247\tTriple Loss(1): 0.0030\tClassification Loss: 1.0187\r\n",
      "Train Epoch: 26 [7680/209539 (4%)]\tAll Loss: 1.0567\tTriple Loss(1): 0.0391\tClassification Loss: 0.9785\r\n",
      "Train Epoch: 26 [8320/209539 (4%)]\tAll Loss: 1.2768\tTriple Loss(1): 0.0142\tClassification Loss: 1.2485\r\n",
      "Train Epoch: 26 [8960/209539 (4%)]\tAll Loss: 1.2208\tTriple Loss(1): 0.0026\tClassification Loss: 1.2157\r\n",
      "Train Epoch: 26 [9600/209539 (5%)]\tAll Loss: 1.6417\tTriple Loss(1): 0.0451\tClassification Loss: 1.5514\r\n",
      "Train Epoch: 26 [10240/209539 (5%)]\tAll Loss: 1.0691\tTriple Loss(1): 0.0000\tClassification Loss: 1.0691\r\n",
      "Train Epoch: 26 [10880/209539 (5%)]\tAll Loss: 1.3452\tTriple Loss(1): 0.0558\tClassification Loss: 1.2336\r\n",
      "Train Epoch: 26 [11520/209539 (5%)]\tAll Loss: 1.4977\tTriple Loss(1): 0.0512\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 26 [12160/209539 (6%)]\tAll Loss: 1.4485\tTriple Loss(1): 0.1215\tClassification Loss: 1.2055\r\n",
      "Train Epoch: 26 [12800/209539 (6%)]\tAll Loss: 0.9955\tTriple Loss(1): 0.0711\tClassification Loss: 0.8533\r\n",
      "Train Epoch: 26 [13440/209539 (6%)]\tAll Loss: 1.5991\tTriple Loss(0): 0.1734\tClassification Loss: 1.2523\r\n",
      "Train Epoch: 26 [14080/209539 (7%)]\tAll Loss: 2.0743\tTriple Loss(0): 0.3558\tClassification Loss: 1.3627\r\n",
      "Train Epoch: 26 [14720/209539 (7%)]\tAll Loss: 1.5291\tTriple Loss(1): 0.0216\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 26 [15360/209539 (7%)]\tAll Loss: 1.1697\tTriple Loss(1): 0.0052\tClassification Loss: 1.1594\r\n",
      "Train Epoch: 26 [16000/209539 (8%)]\tAll Loss: 1.6222\tTriple Loss(1): 0.0030\tClassification Loss: 1.6161\r\n",
      "Train Epoch: 26 [16640/209539 (8%)]\tAll Loss: 1.5902\tTriple Loss(1): 0.0096\tClassification Loss: 1.5710\r\n",
      "Train Epoch: 26 [17280/209539 (8%)]\tAll Loss: 1.3126\tTriple Loss(1): 0.0200\tClassification Loss: 1.2725\r\n",
      "Train Epoch: 26 [17920/209539 (9%)]\tAll Loss: 1.1959\tTriple Loss(1): 0.0175\tClassification Loss: 1.1610\r\n",
      "Train Epoch: 26 [18560/209539 (9%)]\tAll Loss: 1.3333\tTriple Loss(1): 0.0134\tClassification Loss: 1.3064\r\n",
      "Train Epoch: 26 [19200/209539 (9%)]\tAll Loss: 1.1188\tTriple Loss(1): 0.0104\tClassification Loss: 1.0979\r\n",
      "Train Epoch: 26 [19840/209539 (9%)]\tAll Loss: 1.1123\tTriple Loss(1): 0.0000\tClassification Loss: 1.1123\r\n",
      "Train Epoch: 26 [20480/209539 (10%)]\tAll Loss: 1.2373\tTriple Loss(1): 0.0531\tClassification Loss: 1.1310\r\n",
      "Train Epoch: 26 [21120/209539 (10%)]\tAll Loss: 1.4967\tTriple Loss(1): 0.0199\tClassification Loss: 1.4570\r\n",
      "Train Epoch: 26 [21760/209539 (10%)]\tAll Loss: 1.2220\tTriple Loss(1): 0.0261\tClassification Loss: 1.1698\r\n",
      "Train Epoch: 26 [22400/209539 (11%)]\tAll Loss: 1.0400\tTriple Loss(1): 0.0235\tClassification Loss: 0.9930\r\n",
      "Train Epoch: 26 [23040/209539 (11%)]\tAll Loss: 1.3183\tTriple Loss(1): 0.0372\tClassification Loss: 1.2438\r\n",
      "Train Epoch: 26 [23680/209539 (11%)]\tAll Loss: 1.1067\tTriple Loss(1): 0.0479\tClassification Loss: 1.0108\r\n",
      "Train Epoch: 26 [24320/209539 (12%)]\tAll Loss: 1.9537\tTriple Loss(0): 0.3328\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 26 [24960/209539 (12%)]\tAll Loss: 1.1183\tTriple Loss(1): 0.0092\tClassification Loss: 1.0998\r\n",
      "Train Epoch: 26 [25600/209539 (12%)]\tAll Loss: 1.8889\tTriple Loss(0): 0.3128\tClassification Loss: 1.2633\r\n",
      "Train Epoch: 26 [26240/209539 (13%)]\tAll Loss: 1.0329\tTriple Loss(1): 0.0038\tClassification Loss: 1.0253\r\n",
      "Train Epoch: 26 [26880/209539 (13%)]\tAll Loss: 1.4293\tTriple Loss(1): 0.0000\tClassification Loss: 1.4293\r\n",
      "Train Epoch: 26 [27520/209539 (13%)]\tAll Loss: 1.1204\tTriple Loss(1): 0.0129\tClassification Loss: 1.0946\r\n",
      "Train Epoch: 26 [28160/209539 (13%)]\tAll Loss: 2.1192\tTriple Loss(0): 0.4233\tClassification Loss: 1.2726\r\n",
      "Train Epoch: 26 [28800/209539 (14%)]\tAll Loss: 1.7836\tTriple Loss(1): 0.1577\tClassification Loss: 1.4682\r\n",
      "Train Epoch: 26 [29440/209539 (14%)]\tAll Loss: 1.4583\tTriple Loss(1): 0.0198\tClassification Loss: 1.4187\r\n",
      "Train Epoch: 26 [30080/209539 (14%)]\tAll Loss: 1.3061\tTriple Loss(1): 0.0129\tClassification Loss: 1.2803\r\n",
      "Train Epoch: 26 [30720/209539 (15%)]\tAll Loss: 1.1654\tTriple Loss(1): 0.0000\tClassification Loss: 1.1654\r\n",
      "Train Epoch: 26 [31360/209539 (15%)]\tAll Loss: 1.6814\tTriple Loss(0): 0.2030\tClassification Loss: 1.2754\r\n",
      "Train Epoch: 26 [32000/209539 (15%)]\tAll Loss: 1.5407\tTriple Loss(1): 0.0985\tClassification Loss: 1.3437\r\n",
      "Train Epoch: 26 [32640/209539 (16%)]\tAll Loss: 1.2053\tTriple Loss(1): 0.0024\tClassification Loss: 1.2005\r\n",
      "Train Epoch: 26 [33280/209539 (16%)]\tAll Loss: 1.2253\tTriple Loss(1): 0.0592\tClassification Loss: 1.1069\r\n",
      "Train Epoch: 26 [33920/209539 (16%)]\tAll Loss: 1.3466\tTriple Loss(1): 0.0104\tClassification Loss: 1.3258\r\n",
      "Train Epoch: 26 [34560/209539 (16%)]\tAll Loss: 1.1856\tTriple Loss(1): 0.0518\tClassification Loss: 1.0820\r\n",
      "Train Epoch: 26 [35200/209539 (17%)]\tAll Loss: 1.3836\tTriple Loss(1): 0.0493\tClassification Loss: 1.2851\r\n",
      "Train Epoch: 26 [35840/209539 (17%)]\tAll Loss: 1.1141\tTriple Loss(1): 0.0000\tClassification Loss: 1.1141\r\n",
      "Train Epoch: 26 [36480/209539 (17%)]\tAll Loss: 1.0906\tTriple Loss(1): 0.0296\tClassification Loss: 1.0313\r\n",
      "Train Epoch: 26 [37120/209539 (18%)]\tAll Loss: 1.3580\tTriple Loss(1): 0.0000\tClassification Loss: 1.3580\r\n",
      "Train Epoch: 26 [37760/209539 (18%)]\tAll Loss: 2.0201\tTriple Loss(0): 0.2742\tClassification Loss: 1.4716\r\n",
      "Train Epoch: 26 [38400/209539 (18%)]\tAll Loss: 1.2214\tTriple Loss(1): 0.0223\tClassification Loss: 1.1768\r\n",
      "Train Epoch: 26 [39040/209539 (19%)]\tAll Loss: 1.1302\tTriple Loss(1): 0.0628\tClassification Loss: 1.0046\r\n",
      "Train Epoch: 26 [39680/209539 (19%)]\tAll Loss: 1.8137\tTriple Loss(0): 0.3070\tClassification Loss: 1.1996\r\n",
      "Train Epoch: 26 [40320/209539 (19%)]\tAll Loss: 1.3615\tTriple Loss(1): 0.0501\tClassification Loss: 1.2612\r\n",
      "Train Epoch: 26 [40960/209539 (20%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0252\tClassification Loss: 1.1604\r\n",
      "Train Epoch: 26 [41600/209539 (20%)]\tAll Loss: 1.2614\tTriple Loss(1): 0.0190\tClassification Loss: 1.2235\r\n",
      "Train Epoch: 26 [42240/209539 (20%)]\tAll Loss: 1.2019\tTriple Loss(1): 0.0504\tClassification Loss: 1.1011\r\n",
      "Train Epoch: 26 [42880/209539 (20%)]\tAll Loss: 1.2997\tTriple Loss(1): 0.0179\tClassification Loss: 1.2638\r\n",
      "Train Epoch: 26 [43520/209539 (21%)]\tAll Loss: 1.4222\tTriple Loss(1): 0.0185\tClassification Loss: 1.3853\r\n",
      "Train Epoch: 26 [44160/209539 (21%)]\tAll Loss: 2.1834\tTriple Loss(0): 0.3135\tClassification Loss: 1.5565\r\n",
      "Train Epoch: 26 [44800/209539 (21%)]\tAll Loss: 1.5001\tTriple Loss(1): 0.0084\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 26 [45440/209539 (22%)]\tAll Loss: 1.7503\tTriple Loss(1): 0.0379\tClassification Loss: 1.6745\r\n",
      "Train Epoch: 26 [46080/209539 (22%)]\tAll Loss: 1.8617\tTriple Loss(0): 0.3041\tClassification Loss: 1.2535\r\n",
      "Train Epoch: 26 [46720/209539 (22%)]\tAll Loss: 1.6399\tTriple Loss(1): 0.0654\tClassification Loss: 1.5092\r\n",
      "Train Epoch: 26 [47360/209539 (23%)]\tAll Loss: 0.9034\tTriple Loss(1): 0.0000\tClassification Loss: 0.9034\r\n",
      "Train Epoch: 26 [48000/209539 (23%)]\tAll Loss: 1.2314\tTriple Loss(1): 0.0121\tClassification Loss: 1.2072\r\n",
      "Train Epoch: 26 [48640/209539 (23%)]\tAll Loss: 1.5747\tTriple Loss(1): 0.0085\tClassification Loss: 1.5577\r\n",
      "Train Epoch: 26 [49280/209539 (24%)]\tAll Loss: 1.2673\tTriple Loss(1): 0.0228\tClassification Loss: 1.2216\r\n",
      "Train Epoch: 26 [49920/209539 (24%)]\tAll Loss: 1.3732\tTriple Loss(1): 0.0510\tClassification Loss: 1.2713\r\n",
      "Train Epoch: 26 [50560/209539 (24%)]\tAll Loss: 2.0169\tTriple Loss(0): 0.2545\tClassification Loss: 1.5078\r\n",
      "Train Epoch: 26 [51200/209539 (24%)]\tAll Loss: 1.4836\tTriple Loss(1): 0.0189\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 26 [51840/209539 (25%)]\tAll Loss: 1.2135\tTriple Loss(1): 0.0038\tClassification Loss: 1.2059\r\n",
      "Train Epoch: 26 [52480/209539 (25%)]\tAll Loss: 1.2923\tTriple Loss(1): 0.0735\tClassification Loss: 1.1454\r\n",
      "Train Epoch: 26 [53120/209539 (25%)]\tAll Loss: 1.9659\tTriple Loss(0): 0.3754\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 26 [53760/209539 (26%)]\tAll Loss: 1.4330\tTriple Loss(1): 0.0000\tClassification Loss: 1.4330\r\n",
      "Train Epoch: 26 [54400/209539 (26%)]\tAll Loss: 1.6310\tTriple Loss(1): 0.0007\tClassification Loss: 1.6296\r\n",
      "Train Epoch: 26 [55040/209539 (26%)]\tAll Loss: 1.1039\tTriple Loss(1): 0.0222\tClassification Loss: 1.0595\r\n",
      "Train Epoch: 26 [55680/209539 (27%)]\tAll Loss: 1.5620\tTriple Loss(1): 0.0186\tClassification Loss: 1.5248\r\n",
      "Train Epoch: 26 [56320/209539 (27%)]\tAll Loss: 1.0178\tTriple Loss(1): 0.0403\tClassification Loss: 0.9372\r\n",
      "Train Epoch: 26 [56960/209539 (27%)]\tAll Loss: 1.3763\tTriple Loss(1): 0.0698\tClassification Loss: 1.2367\r\n",
      "Train Epoch: 26 [57600/209539 (27%)]\tAll Loss: 1.2886\tTriple Loss(1): 0.0679\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 26 [58240/209539 (28%)]\tAll Loss: 1.5219\tTriple Loss(0): 0.2578\tClassification Loss: 1.0064\r\n",
      "Train Epoch: 26 [58880/209539 (28%)]\tAll Loss: 1.3641\tTriple Loss(1): 0.0097\tClassification Loss: 1.3447\r\n",
      "Train Epoch: 26 [59520/209539 (28%)]\tAll Loss: 2.0418\tTriple Loss(0): 0.4281\tClassification Loss: 1.1857\r\n",
      "Train Epoch: 26 [60160/209539 (29%)]\tAll Loss: 1.3882\tTriple Loss(1): 0.0088\tClassification Loss: 1.3707\r\n",
      "Train Epoch: 26 [60800/209539 (29%)]\tAll Loss: 1.5686\tTriple Loss(1): 0.0155\tClassification Loss: 1.5375\r\n",
      "Train Epoch: 26 [61440/209539 (29%)]\tAll Loss: 1.4795\tTriple Loss(1): 0.0822\tClassification Loss: 1.3151\r\n",
      "Train Epoch: 26 [62080/209539 (30%)]\tAll Loss: 1.3441\tTriple Loss(1): 0.0512\tClassification Loss: 1.2417\r\n",
      "Train Epoch: 26 [62720/209539 (30%)]\tAll Loss: 1.2110\tTriple Loss(1): 0.0206\tClassification Loss: 1.1698\r\n",
      "Train Epoch: 26 [63360/209539 (30%)]\tAll Loss: 1.4937\tTriple Loss(1): 0.0517\tClassification Loss: 1.3903\r\n",
      "Train Epoch: 26 [64000/209539 (31%)]\tAll Loss: 1.9998\tTriple Loss(0): 0.2300\tClassification Loss: 1.5397\r\n",
      "Train Epoch: 26 [64640/209539 (31%)]\tAll Loss: 2.2379\tTriple Loss(0): 0.3427\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 26 [65280/209539 (31%)]\tAll Loss: 1.4147\tTriple Loss(1): 0.1009\tClassification Loss: 1.2129\r\n",
      "Train Epoch: 26 [65920/209539 (31%)]\tAll Loss: 1.3720\tTriple Loss(1): 0.0112\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 26 [66560/209539 (32%)]\tAll Loss: 1.1855\tTriple Loss(1): 0.0000\tClassification Loss: 1.1855\r\n",
      "Train Epoch: 26 [67200/209539 (32%)]\tAll Loss: 1.2614\tTriple Loss(1): 0.0556\tClassification Loss: 1.1502\r\n",
      "Train Epoch: 26 [67840/209539 (32%)]\tAll Loss: 1.4446\tTriple Loss(1): 0.0714\tClassification Loss: 1.3017\r\n",
      "Train Epoch: 26 [68480/209539 (33%)]\tAll Loss: 1.7312\tTriple Loss(0): 0.1646\tClassification Loss: 1.4020\r\n",
      "Train Epoch: 26 [69120/209539 (33%)]\tAll Loss: 1.5510\tTriple Loss(1): 0.0536\tClassification Loss: 1.4437\r\n",
      "Train Epoch: 26 [69760/209539 (33%)]\tAll Loss: 1.1641\tTriple Loss(1): 0.0632\tClassification Loss: 1.0377\r\n",
      "Train Epoch: 26 [70400/209539 (34%)]\tAll Loss: 1.2461\tTriple Loss(1): 0.0646\tClassification Loss: 1.1168\r\n",
      "Train Epoch: 26 [71040/209539 (34%)]\tAll Loss: 1.5446\tTriple Loss(1): 0.0223\tClassification Loss: 1.5001\r\n",
      "Train Epoch: 26 [71680/209539 (34%)]\tAll Loss: 1.5746\tTriple Loss(1): 0.0366\tClassification Loss: 1.5014\r\n",
      "Train Epoch: 26 [72320/209539 (35%)]\tAll Loss: 1.3046\tTriple Loss(1): 0.0380\tClassification Loss: 1.2287\r\n",
      "Train Epoch: 26 [72960/209539 (35%)]\tAll Loss: 1.9213\tTriple Loss(0): 0.3715\tClassification Loss: 1.1783\r\n",
      "Train Epoch: 26 [73600/209539 (35%)]\tAll Loss: 2.1704\tTriple Loss(0): 0.4286\tClassification Loss: 1.3132\r\n",
      "Train Epoch: 26 [74240/209539 (35%)]\tAll Loss: 1.6378\tTriple Loss(1): 0.0271\tClassification Loss: 1.5835\r\n",
      "Train Epoch: 26 [74880/209539 (36%)]\tAll Loss: 1.9272\tTriple Loss(1): 0.0928\tClassification Loss: 1.7415\r\n",
      "Train Epoch: 26 [75520/209539 (36%)]\tAll Loss: 0.9972\tTriple Loss(1): 0.0028\tClassification Loss: 0.9917\r\n",
      "Train Epoch: 26 [76160/209539 (36%)]\tAll Loss: 1.2266\tTriple Loss(1): 0.0314\tClassification Loss: 1.1638\r\n",
      "Train Epoch: 26 [76800/209539 (37%)]\tAll Loss: 1.1428\tTriple Loss(1): 0.0042\tClassification Loss: 1.1344\r\n",
      "Train Epoch: 26 [77440/209539 (37%)]\tAll Loss: 1.2746\tTriple Loss(1): 0.0225\tClassification Loss: 1.2296\r\n",
      "Train Epoch: 26 [78080/209539 (37%)]\tAll Loss: 1.3715\tTriple Loss(1): 0.0000\tClassification Loss: 1.3715\r\n",
      "Train Epoch: 26 [78720/209539 (38%)]\tAll Loss: 1.3977\tTriple Loss(1): 0.0000\tClassification Loss: 1.3977\r\n",
      "Train Epoch: 26 [79360/209539 (38%)]\tAll Loss: 1.3029\tTriple Loss(1): 0.0000\tClassification Loss: 1.3029\r\n",
      "Train Epoch: 26 [80000/209539 (38%)]\tAll Loss: 2.0717\tTriple Loss(0): 0.4316\tClassification Loss: 1.2085\r\n",
      "Train Epoch: 26 [80640/209539 (38%)]\tAll Loss: 1.5038\tTriple Loss(1): 0.0284\tClassification Loss: 1.4470\r\n",
      "Train Epoch: 26 [81280/209539 (39%)]\tAll Loss: 1.3451\tTriple Loss(1): 0.0068\tClassification Loss: 1.3314\r\n",
      "Train Epoch: 26 [81920/209539 (39%)]\tAll Loss: 1.1120\tTriple Loss(1): 0.0000\tClassification Loss: 1.1120\r\n",
      "Train Epoch: 26 [82560/209539 (39%)]\tAll Loss: 1.4328\tTriple Loss(1): 0.0173\tClassification Loss: 1.3981\r\n",
      "Train Epoch: 26 [83200/209539 (40%)]\tAll Loss: 1.4787\tTriple Loss(1): 0.0771\tClassification Loss: 1.3244\r\n",
      "Train Epoch: 26 [83840/209539 (40%)]\tAll Loss: 1.2668\tTriple Loss(1): 0.0249\tClassification Loss: 1.2171\r\n",
      "Train Epoch: 26 [84480/209539 (40%)]\tAll Loss: 1.1360\tTriple Loss(1): 0.0080\tClassification Loss: 1.1199\r\n",
      "Train Epoch: 26 [85120/209539 (41%)]\tAll Loss: 1.8258\tTriple Loss(1): 0.0893\tClassification Loss: 1.6472\r\n",
      "Train Epoch: 26 [85760/209539 (41%)]\tAll Loss: 1.1462\tTriple Loss(1): 0.0204\tClassification Loss: 1.1053\r\n",
      "Train Epoch: 26 [86400/209539 (41%)]\tAll Loss: 1.9400\tTriple Loss(0): 0.3766\tClassification Loss: 1.1867\r\n",
      "Train Epoch: 26 [87040/209539 (42%)]\tAll Loss: 1.3598\tTriple Loss(1): 0.0064\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 26 [87680/209539 (42%)]\tAll Loss: 1.4197\tTriple Loss(1): 0.0079\tClassification Loss: 1.4040\r\n",
      "Train Epoch: 26 [88320/209539 (42%)]\tAll Loss: 1.2772\tTriple Loss(1): 0.0209\tClassification Loss: 1.2354\r\n",
      "Train Epoch: 26 [88960/209539 (42%)]\tAll Loss: 1.3374\tTriple Loss(1): 0.0226\tClassification Loss: 1.2921\r\n",
      "Train Epoch: 26 [89600/209539 (43%)]\tAll Loss: 1.5024\tTriple Loss(1): 0.0222\tClassification Loss: 1.4579\r\n",
      "Train Epoch: 26 [90240/209539 (43%)]\tAll Loss: 1.2352\tTriple Loss(1): 0.0370\tClassification Loss: 1.1611\r\n",
      "Train Epoch: 26 [90880/209539 (43%)]\tAll Loss: 1.8030\tTriple Loss(1): 0.0051\tClassification Loss: 1.7928\r\n",
      "Train Epoch: 26 [91520/209539 (44%)]\tAll Loss: 1.2774\tTriple Loss(1): 0.0000\tClassification Loss: 1.2774\r\n",
      "Train Epoch: 26 [92160/209539 (44%)]\tAll Loss: 1.0614\tTriple Loss(1): 0.0257\tClassification Loss: 1.0099\r\n",
      "Train Epoch: 26 [92800/209539 (44%)]\tAll Loss: 1.2286\tTriple Loss(1): 0.0457\tClassification Loss: 1.1371\r\n",
      "Train Epoch: 26 [93440/209539 (45%)]\tAll Loss: 1.6283\tTriple Loss(1): 0.0414\tClassification Loss: 1.5456\r\n",
      "Train Epoch: 26 [94080/209539 (45%)]\tAll Loss: 1.2038\tTriple Loss(1): 0.0256\tClassification Loss: 1.1526\r\n",
      "Train Epoch: 26 [94720/209539 (45%)]\tAll Loss: 1.5039\tTriple Loss(1): 0.0413\tClassification Loss: 1.4214\r\n",
      "Train Epoch: 26 [95360/209539 (46%)]\tAll Loss: 1.4968\tTriple Loss(1): 0.0368\tClassification Loss: 1.4232\r\n",
      "Train Epoch: 26 [96000/209539 (46%)]\tAll Loss: 1.5250\tTriple Loss(1): 0.0578\tClassification Loss: 1.4094\r\n",
      "Train Epoch: 26 [96640/209539 (46%)]\tAll Loss: 1.3605\tTriple Loss(1): 0.0340\tClassification Loss: 1.2925\r\n",
      "Train Epoch: 26 [97280/209539 (46%)]\tAll Loss: 1.1850\tTriple Loss(1): 0.0000\tClassification Loss: 1.1850\r\n",
      "Train Epoch: 26 [97920/209539 (47%)]\tAll Loss: 1.5915\tTriple Loss(1): 0.0499\tClassification Loss: 1.4918\r\n",
      "Train Epoch: 26 [98560/209539 (47%)]\tAll Loss: 1.7868\tTriple Loss(1): 0.0433\tClassification Loss: 1.7002\r\n",
      "Train Epoch: 26 [99200/209539 (47%)]\tAll Loss: 1.3210\tTriple Loss(1): 0.0478\tClassification Loss: 1.2255\r\n",
      "Train Epoch: 26 [99840/209539 (48%)]\tAll Loss: 1.2897\tTriple Loss(1): 0.0109\tClassification Loss: 1.2678\r\n",
      "Train Epoch: 26 [100480/209539 (48%)]\tAll Loss: 1.4117\tTriple Loss(1): 0.0638\tClassification Loss: 1.2841\r\n",
      "Train Epoch: 26 [101120/209539 (48%)]\tAll Loss: 1.2244\tTriple Loss(1): 0.0000\tClassification Loss: 1.2244\r\n",
      "Train Epoch: 26 [101760/209539 (49%)]\tAll Loss: 1.2176\tTriple Loss(1): 0.0000\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 26 [102400/209539 (49%)]\tAll Loss: 2.1509\tTriple Loss(0): 0.4348\tClassification Loss: 1.2812\r\n",
      "Train Epoch: 26 [103040/209539 (49%)]\tAll Loss: 1.2558\tTriple Loss(1): 0.0141\tClassification Loss: 1.2277\r\n",
      "Train Epoch: 26 [103680/209539 (49%)]\tAll Loss: 1.2768\tTriple Loss(1): 0.0000\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 26 [104320/209539 (50%)]\tAll Loss: 1.1024\tTriple Loss(1): 0.0000\tClassification Loss: 1.1024\r\n",
      "Train Epoch: 26 [104960/209539 (50%)]\tAll Loss: 1.7597\tTriple Loss(0): 0.2313\tClassification Loss: 1.2970\r\n",
      "Train Epoch: 26 [105600/209539 (50%)]\tAll Loss: 1.1421\tTriple Loss(1): 0.0000\tClassification Loss: 1.1421\r\n",
      "Train Epoch: 26 [106240/209539 (51%)]\tAll Loss: 1.1357\tTriple Loss(1): 0.0000\tClassification Loss: 1.1357\r\n",
      "Train Epoch: 26 [106880/209539 (51%)]\tAll Loss: 1.8813\tTriple Loss(0): 0.3150\tClassification Loss: 1.2513\r\n",
      "Train Epoch: 26 [107520/209539 (51%)]\tAll Loss: 1.2871\tTriple Loss(1): 0.0061\tClassification Loss: 1.2750\r\n",
      "Train Epoch: 26 [108160/209539 (52%)]\tAll Loss: 1.5351\tTriple Loss(0): 0.1421\tClassification Loss: 1.2510\r\n",
      "Train Epoch: 26 [108800/209539 (52%)]\tAll Loss: 1.6970\tTriple Loss(0): 0.2520\tClassification Loss: 1.1930\r\n",
      "Train Epoch: 26 [109440/209539 (52%)]\tAll Loss: 1.6537\tTriple Loss(1): 0.0511\tClassification Loss: 1.5515\r\n",
      "Train Epoch: 26 [110080/209539 (53%)]\tAll Loss: 1.7910\tTriple Loss(1): 0.0329\tClassification Loss: 1.7251\r\n",
      "Train Epoch: 26 [110720/209539 (53%)]\tAll Loss: 1.3689\tTriple Loss(1): 0.0204\tClassification Loss: 1.3280\r\n",
      "Train Epoch: 26 [111360/209539 (53%)]\tAll Loss: 1.1070\tTriple Loss(1): 0.0050\tClassification Loss: 1.0969\r\n",
      "Train Epoch: 26 [112000/209539 (53%)]\tAll Loss: 1.7203\tTriple Loss(0): 0.2969\tClassification Loss: 1.1265\r\n",
      "Train Epoch: 26 [112640/209539 (54%)]\tAll Loss: 1.2997\tTriple Loss(1): 0.0222\tClassification Loss: 1.2553\r\n",
      "Train Epoch: 26 [113280/209539 (54%)]\tAll Loss: 1.0376\tTriple Loss(1): 0.0266\tClassification Loss: 0.9843\r\n",
      "Train Epoch: 26 [113920/209539 (54%)]\tAll Loss: 1.4797\tTriple Loss(1): 0.1000\tClassification Loss: 1.2797\r\n",
      "Train Epoch: 26 [114560/209539 (55%)]\tAll Loss: 1.2672\tTriple Loss(1): 0.0010\tClassification Loss: 1.2651\r\n",
      "Train Epoch: 26 [115200/209539 (55%)]\tAll Loss: 1.4926\tTriple Loss(1): 0.0554\tClassification Loss: 1.3818\r\n",
      "Train Epoch: 26 [115840/209539 (55%)]\tAll Loss: 1.7494\tTriple Loss(0): 0.1738\tClassification Loss: 1.4018\r\n",
      "Train Epoch: 26 [116480/209539 (56%)]\tAll Loss: 1.4516\tTriple Loss(1): 0.0560\tClassification Loss: 1.3396\r\n",
      "Train Epoch: 26 [117120/209539 (56%)]\tAll Loss: 1.3092\tTriple Loss(1): 0.0015\tClassification Loss: 1.3062\r\n",
      "Train Epoch: 26 [117760/209539 (56%)]\tAll Loss: 1.2224\tTriple Loss(1): 0.0008\tClassification Loss: 1.2207\r\n",
      "Train Epoch: 26 [118400/209539 (57%)]\tAll Loss: 0.9363\tTriple Loss(1): 0.0145\tClassification Loss: 0.9072\r\n",
      "Train Epoch: 26 [119040/209539 (57%)]\tAll Loss: 1.4410\tTriple Loss(1): 0.0000\tClassification Loss: 1.4410\r\n",
      "Train Epoch: 26 [119680/209539 (57%)]\tAll Loss: 1.4112\tTriple Loss(1): 0.1099\tClassification Loss: 1.1914\r\n",
      "Train Epoch: 26 [120320/209539 (57%)]\tAll Loss: 1.4196\tTriple Loss(1): 0.0604\tClassification Loss: 1.2987\r\n",
      "Train Epoch: 26 [120960/209539 (58%)]\tAll Loss: 1.1997\tTriple Loss(1): 0.0298\tClassification Loss: 1.1402\r\n",
      "Train Epoch: 26 [121600/209539 (58%)]\tAll Loss: 1.0708\tTriple Loss(1): 0.0410\tClassification Loss: 0.9889\r\n",
      "Train Epoch: 26 [122240/209539 (58%)]\tAll Loss: 0.9570\tTriple Loss(1): 0.0051\tClassification Loss: 0.9468\r\n",
      "Train Epoch: 26 [122880/209539 (59%)]\tAll Loss: 1.6565\tTriple Loss(0): 0.2820\tClassification Loss: 1.0925\r\n",
      "Train Epoch: 26 [123520/209539 (59%)]\tAll Loss: 2.0141\tTriple Loss(0): 0.3359\tClassification Loss: 1.3423\r\n",
      "Train Epoch: 26 [124160/209539 (59%)]\tAll Loss: 2.0699\tTriple Loss(0): 0.4674\tClassification Loss: 1.1350\r\n",
      "Train Epoch: 26 [124800/209539 (60%)]\tAll Loss: 1.2656\tTriple Loss(1): 0.0611\tClassification Loss: 1.1435\r\n",
      "Train Epoch: 26 [125440/209539 (60%)]\tAll Loss: 1.5037\tTriple Loss(1): 0.0601\tClassification Loss: 1.3836\r\n",
      "Train Epoch: 26 [126080/209539 (60%)]\tAll Loss: 1.8144\tTriple Loss(0): 0.2037\tClassification Loss: 1.4069\r\n",
      "Train Epoch: 26 [126720/209539 (60%)]\tAll Loss: 1.3763\tTriple Loss(1): 0.0191\tClassification Loss: 1.3381\r\n",
      "Train Epoch: 26 [127360/209539 (61%)]\tAll Loss: 1.4460\tTriple Loss(1): 0.0433\tClassification Loss: 1.3594\r\n",
      "Train Epoch: 26 [128000/209539 (61%)]\tAll Loss: 1.3118\tTriple Loss(1): 0.0130\tClassification Loss: 1.2857\r\n",
      "Train Epoch: 26 [128640/209539 (61%)]\tAll Loss: 1.1479\tTriple Loss(1): 0.0089\tClassification Loss: 1.1301\r\n",
      "Train Epoch: 26 [129280/209539 (62%)]\tAll Loss: 2.2073\tTriple Loss(0): 0.4099\tClassification Loss: 1.3875\r\n",
      "Train Epoch: 26 [129920/209539 (62%)]\tAll Loss: 1.3947\tTriple Loss(1): 0.0579\tClassification Loss: 1.2789\r\n",
      "Train Epoch: 26 [130560/209539 (62%)]\tAll Loss: 1.7646\tTriple Loss(0): 0.3119\tClassification Loss: 1.1408\r\n",
      "Train Epoch: 26 [131200/209539 (63%)]\tAll Loss: 1.2078\tTriple Loss(1): 0.0255\tClassification Loss: 1.1569\r\n",
      "Train Epoch: 26 [131840/209539 (63%)]\tAll Loss: 1.3349\tTriple Loss(1): 0.0207\tClassification Loss: 1.2936\r\n",
      "Train Epoch: 26 [132480/209539 (63%)]\tAll Loss: 1.0604\tTriple Loss(1): 0.0690\tClassification Loss: 0.9224\r\n",
      "Train Epoch: 26 [133120/209539 (64%)]\tAll Loss: 1.6852\tTriple Loss(0): 0.2516\tClassification Loss: 1.1820\r\n",
      "Train Epoch: 26 [133760/209539 (64%)]\tAll Loss: 1.4118\tTriple Loss(1): 0.0077\tClassification Loss: 1.3964\r\n",
      "Train Epoch: 26 [134400/209539 (64%)]\tAll Loss: 1.1498\tTriple Loss(1): 0.0681\tClassification Loss: 1.0136\r\n",
      "Train Epoch: 26 [135040/209539 (64%)]\tAll Loss: 1.1980\tTriple Loss(1): 0.0258\tClassification Loss: 1.1465\r\n",
      "Train Epoch: 26 [135680/209539 (65%)]\tAll Loss: 1.6281\tTriple Loss(1): 0.0649\tClassification Loss: 1.4983\r\n",
      "Train Epoch: 26 [136320/209539 (65%)]\tAll Loss: 1.3734\tTriple Loss(1): 0.0000\tClassification Loss: 1.3734\r\n",
      "Train Epoch: 26 [136960/209539 (65%)]\tAll Loss: 1.2554\tTriple Loss(1): 0.0390\tClassification Loss: 1.1773\r\n",
      "Train Epoch: 26 [137600/209539 (66%)]\tAll Loss: 1.2766\tTriple Loss(1): 0.0224\tClassification Loss: 1.2319\r\n",
      "Train Epoch: 26 [138240/209539 (66%)]\tAll Loss: 1.6687\tTriple Loss(1): 0.0498\tClassification Loss: 1.5691\r\n",
      "Train Epoch: 26 [138880/209539 (66%)]\tAll Loss: 1.5145\tTriple Loss(0): 0.1622\tClassification Loss: 1.1900\r\n",
      "Train Epoch: 26 [139520/209539 (67%)]\tAll Loss: 1.0753\tTriple Loss(1): 0.0027\tClassification Loss: 1.0699\r\n",
      "Train Epoch: 26 [140160/209539 (67%)]\tAll Loss: 1.2249\tTriple Loss(1): 0.0000\tClassification Loss: 1.2249\r\n",
      "Train Epoch: 26 [140800/209539 (67%)]\tAll Loss: 1.4417\tTriple Loss(1): 0.0010\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 26 [141440/209539 (68%)]\tAll Loss: 1.9495\tTriple Loss(0): 0.2467\tClassification Loss: 1.4561\r\n",
      "Train Epoch: 26 [142080/209539 (68%)]\tAll Loss: 1.1298\tTriple Loss(1): 0.0311\tClassification Loss: 1.0676\r\n",
      "Train Epoch: 26 [142720/209539 (68%)]\tAll Loss: 2.0354\tTriple Loss(0): 0.2683\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 26 [143360/209539 (68%)]\tAll Loss: 1.2494\tTriple Loss(1): 0.0334\tClassification Loss: 1.1825\r\n",
      "Train Epoch: 26 [144000/209539 (69%)]\tAll Loss: 1.2883\tTriple Loss(1): 0.0326\tClassification Loss: 1.2231\r\n",
      "Train Epoch: 26 [144640/209539 (69%)]\tAll Loss: 1.7506\tTriple Loss(0): 0.3274\tClassification Loss: 1.0958\r\n",
      "Train Epoch: 26 [145280/209539 (69%)]\tAll Loss: 1.2684\tTriple Loss(1): 0.0000\tClassification Loss: 1.2684\r\n",
      "Train Epoch: 26 [145920/209539 (70%)]\tAll Loss: 1.2752\tTriple Loss(1): 0.0227\tClassification Loss: 1.2297\r\n",
      "Train Epoch: 26 [146560/209539 (70%)]\tAll Loss: 1.2538\tTriple Loss(1): 0.0616\tClassification Loss: 1.1306\r\n",
      "Train Epoch: 26 [147200/209539 (70%)]\tAll Loss: 1.6212\tTriple Loss(1): 0.0453\tClassification Loss: 1.5306\r\n",
      "Train Epoch: 26 [147840/209539 (71%)]\tAll Loss: 1.0105\tTriple Loss(1): 0.0321\tClassification Loss: 0.9464\r\n",
      "Train Epoch: 26 [148480/209539 (71%)]\tAll Loss: 1.7354\tTriple Loss(0): 0.3486\tClassification Loss: 1.0382\r\n",
      "Train Epoch: 26 [149120/209539 (71%)]\tAll Loss: 1.5875\tTriple Loss(1): 0.0742\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 26 [149760/209539 (71%)]\tAll Loss: 1.1320\tTriple Loss(1): 0.0234\tClassification Loss: 1.0851\r\n",
      "Train Epoch: 26 [150400/209539 (72%)]\tAll Loss: 1.5156\tTriple Loss(1): 0.0191\tClassification Loss: 1.4774\r\n",
      "Train Epoch: 26 [151040/209539 (72%)]\tAll Loss: 1.1594\tTriple Loss(1): 0.0007\tClassification Loss: 1.1580\r\n",
      "Train Epoch: 26 [151680/209539 (72%)]\tAll Loss: 1.4592\tTriple Loss(1): 0.0273\tClassification Loss: 1.4047\r\n",
      "Train Epoch: 26 [152320/209539 (73%)]\tAll Loss: 1.3440\tTriple Loss(1): 0.0400\tClassification Loss: 1.2640\r\n",
      "Train Epoch: 26 [152960/209539 (73%)]\tAll Loss: 2.1335\tTriple Loss(0): 0.4461\tClassification Loss: 1.2413\r\n",
      "Train Epoch: 26 [153600/209539 (73%)]\tAll Loss: 1.6722\tTriple Loss(0): 0.1629\tClassification Loss: 1.3463\r\n",
      "Train Epoch: 26 [154240/209539 (74%)]\tAll Loss: 1.2021\tTriple Loss(1): 0.0352\tClassification Loss: 1.1316\r\n",
      "Train Epoch: 26 [154880/209539 (74%)]\tAll Loss: 1.9607\tTriple Loss(0): 0.2784\tClassification Loss: 1.4038\r\n",
      "Train Epoch: 26 [155520/209539 (74%)]\tAll Loss: 1.3417\tTriple Loss(1): 0.0429\tClassification Loss: 1.2559\r\n",
      "Train Epoch: 26 [156160/209539 (75%)]\tAll Loss: 1.3016\tTriple Loss(1): 0.0037\tClassification Loss: 1.2942\r\n",
      "Train Epoch: 26 [156800/209539 (75%)]\tAll Loss: 1.7528\tTriple Loss(1): 0.0032\tClassification Loss: 1.7463\r\n",
      "Train Epoch: 26 [157440/209539 (75%)]\tAll Loss: 2.3413\tTriple Loss(0): 0.4467\tClassification Loss: 1.4478\r\n",
      "Train Epoch: 26 [158080/209539 (75%)]\tAll Loss: 1.1998\tTriple Loss(1): 0.0000\tClassification Loss: 1.1998\r\n",
      "Train Epoch: 26 [158720/209539 (76%)]\tAll Loss: 1.1985\tTriple Loss(1): 0.0065\tClassification Loss: 1.1855\r\n",
      "Train Epoch: 26 [159360/209539 (76%)]\tAll Loss: 1.0508\tTriple Loss(1): 0.0393\tClassification Loss: 0.9722\r\n",
      "Train Epoch: 26 [160000/209539 (76%)]\tAll Loss: 1.8233\tTriple Loss(0): 0.1625\tClassification Loss: 1.4983\r\n",
      "Train Epoch: 26 [160640/209539 (77%)]\tAll Loss: 1.1995\tTriple Loss(1): 0.0203\tClassification Loss: 1.1588\r\n",
      "Train Epoch: 26 [161280/209539 (77%)]\tAll Loss: 1.4049\tTriple Loss(1): 0.0000\tClassification Loss: 1.4049\r\n",
      "Train Epoch: 26 [161920/209539 (77%)]\tAll Loss: 1.3822\tTriple Loss(1): 0.0373\tClassification Loss: 1.3076\r\n",
      "Train Epoch: 26 [162560/209539 (78%)]\tAll Loss: 1.2054\tTriple Loss(1): 0.0101\tClassification Loss: 1.1853\r\n",
      "Train Epoch: 26 [163200/209539 (78%)]\tAll Loss: 1.0883\tTriple Loss(1): 0.0000\tClassification Loss: 1.0883\r\n",
      "Train Epoch: 26 [163840/209539 (78%)]\tAll Loss: 1.2378\tTriple Loss(1): 0.0428\tClassification Loss: 1.1523\r\n",
      "Train Epoch: 26 [164480/209539 (78%)]\tAll Loss: 1.2095\tTriple Loss(1): 0.0698\tClassification Loss: 1.0698\r\n",
      "Train Epoch: 26 [165120/209539 (79%)]\tAll Loss: 1.0751\tTriple Loss(1): 0.0000\tClassification Loss: 1.0751\r\n",
      "Train Epoch: 26 [165760/209539 (79%)]\tAll Loss: 1.3098\tTriple Loss(1): 0.0421\tClassification Loss: 1.2256\r\n",
      "Train Epoch: 26 [166400/209539 (79%)]\tAll Loss: 2.0734\tTriple Loss(0): 0.4107\tClassification Loss: 1.2520\r\n",
      "Train Epoch: 26 [167040/209539 (80%)]\tAll Loss: 1.5080\tTriple Loss(1): 0.0505\tClassification Loss: 1.4070\r\n",
      "Train Epoch: 26 [167680/209539 (80%)]\tAll Loss: 1.2644\tTriple Loss(1): 0.0205\tClassification Loss: 1.2233\r\n",
      "Train Epoch: 26 [168320/209539 (80%)]\tAll Loss: 1.0873\tTriple Loss(1): 0.0066\tClassification Loss: 1.0741\r\n",
      "Train Epoch: 26 [168960/209539 (81%)]\tAll Loss: 1.1232\tTriple Loss(1): 0.0190\tClassification Loss: 1.0853\r\n",
      "Train Epoch: 26 [169600/209539 (81%)]\tAll Loss: 1.3534\tTriple Loss(1): 0.0499\tClassification Loss: 1.2537\r\n",
      "Train Epoch: 26 [170240/209539 (81%)]\tAll Loss: 1.2205\tTriple Loss(1): 0.0613\tClassification Loss: 1.0979\r\n",
      "Train Epoch: 26 [170880/209539 (82%)]\tAll Loss: 1.3538\tTriple Loss(1): 0.0125\tClassification Loss: 1.3288\r\n",
      "Train Epoch: 26 [171520/209539 (82%)]\tAll Loss: 1.1337\tTriple Loss(1): 0.0326\tClassification Loss: 1.0685\r\n",
      "Train Epoch: 26 [172160/209539 (82%)]\tAll Loss: 1.4657\tTriple Loss(1): 0.0169\tClassification Loss: 1.4319\r\n",
      "Train Epoch: 26 [172800/209539 (82%)]\tAll Loss: 1.2435\tTriple Loss(1): 0.0000\tClassification Loss: 1.2435\r\n",
      "Train Epoch: 26 [173440/209539 (83%)]\tAll Loss: 1.9676\tTriple Loss(0): 0.2096\tClassification Loss: 1.5484\r\n",
      "Train Epoch: 26 [174080/209539 (83%)]\tAll Loss: 1.9351\tTriple Loss(0): 0.3955\tClassification Loss: 1.1442\r\n",
      "Train Epoch: 26 [174720/209539 (83%)]\tAll Loss: 1.3424\tTriple Loss(1): 0.0083\tClassification Loss: 1.3258\r\n",
      "Train Epoch: 26 [175360/209539 (84%)]\tAll Loss: 2.0514\tTriple Loss(0): 0.2868\tClassification Loss: 1.4778\r\n",
      "Train Epoch: 26 [176000/209539 (84%)]\tAll Loss: 2.2103\tTriple Loss(0): 0.4963\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 26 [176640/209539 (84%)]\tAll Loss: 1.5932\tTriple Loss(0): 0.3701\tClassification Loss: 0.8529\r\n",
      "Train Epoch: 26 [177280/209539 (85%)]\tAll Loss: 1.4903\tTriple Loss(1): 0.0123\tClassification Loss: 1.4658\r\n",
      "Train Epoch: 26 [177920/209539 (85%)]\tAll Loss: 1.5249\tTriple Loss(1): 0.0845\tClassification Loss: 1.3559\r\n",
      "Train Epoch: 26 [178560/209539 (85%)]\tAll Loss: 1.0899\tTriple Loss(1): 0.0000\tClassification Loss: 1.0899\r\n",
      "Train Epoch: 26 [179200/209539 (86%)]\tAll Loss: 2.1444\tTriple Loss(0): 0.3967\tClassification Loss: 1.3510\r\n",
      "Train Epoch: 26 [179840/209539 (86%)]\tAll Loss: 1.6099\tTriple Loss(0): 0.1816\tClassification Loss: 1.2466\r\n",
      "Train Epoch: 26 [180480/209539 (86%)]\tAll Loss: 1.3933\tTriple Loss(1): 0.0413\tClassification Loss: 1.3107\r\n",
      "Train Epoch: 26 [181120/209539 (86%)]\tAll Loss: 1.3118\tTriple Loss(1): 0.0016\tClassification Loss: 1.3085\r\n",
      "Train Epoch: 26 [181760/209539 (87%)]\tAll Loss: 1.1051\tTriple Loss(1): 0.0056\tClassification Loss: 1.0940\r\n",
      "Train Epoch: 26 [182400/209539 (87%)]\tAll Loss: 1.4400\tTriple Loss(1): 0.0177\tClassification Loss: 1.4045\r\n",
      "Train Epoch: 26 [183040/209539 (87%)]\tAll Loss: 1.4496\tTriple Loss(1): 0.0400\tClassification Loss: 1.3697\r\n",
      "Train Epoch: 26 [183680/209539 (88%)]\tAll Loss: 1.0656\tTriple Loss(1): 0.0323\tClassification Loss: 1.0010\r\n",
      "Train Epoch: 26 [184320/209539 (88%)]\tAll Loss: 1.2239\tTriple Loss(1): 0.0000\tClassification Loss: 1.2239\r\n",
      "Train Epoch: 26 [184960/209539 (88%)]\tAll Loss: 0.9644\tTriple Loss(1): 0.0182\tClassification Loss: 0.9280\r\n",
      "Train Epoch: 26 [185600/209539 (89%)]\tAll Loss: 1.3491\tTriple Loss(1): 0.0045\tClassification Loss: 1.3401\r\n",
      "Train Epoch: 26 [186240/209539 (89%)]\tAll Loss: 2.1159\tTriple Loss(0): 0.3250\tClassification Loss: 1.4659\r\n",
      "Train Epoch: 26 [186880/209539 (89%)]\tAll Loss: 1.3737\tTriple Loss(1): 0.0219\tClassification Loss: 1.3299\r\n",
      "Train Epoch: 26 [187520/209539 (89%)]\tAll Loss: 1.4163\tTriple Loss(1): 0.0241\tClassification Loss: 1.3680\r\n",
      "Train Epoch: 26 [188160/209539 (90%)]\tAll Loss: 0.9460\tTriple Loss(1): 0.0079\tClassification Loss: 0.9303\r\n",
      "Train Epoch: 26 [188800/209539 (90%)]\tAll Loss: 1.4105\tTriple Loss(1): 0.0583\tClassification Loss: 1.2939\r\n",
      "Train Epoch: 26 [189440/209539 (90%)]\tAll Loss: 1.3979\tTriple Loss(1): 0.0601\tClassification Loss: 1.2777\r\n",
      "Train Epoch: 26 [190080/209539 (91%)]\tAll Loss: 1.8219\tTriple Loss(0): 0.2694\tClassification Loss: 1.2832\r\n",
      "Train Epoch: 26 [190720/209539 (91%)]\tAll Loss: 1.6896\tTriple Loss(1): 0.1086\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 26 [191360/209539 (91%)]\tAll Loss: 1.4507\tTriple Loss(0): 0.2526\tClassification Loss: 0.9456\r\n",
      "Train Epoch: 26 [192000/209539 (92%)]\tAll Loss: 1.6845\tTriple Loss(1): 0.0368\tClassification Loss: 1.6109\r\n",
      "Train Epoch: 26 [192640/209539 (92%)]\tAll Loss: 1.8080\tTriple Loss(0): 0.2362\tClassification Loss: 1.3356\r\n",
      "Train Epoch: 26 [193280/209539 (92%)]\tAll Loss: 1.2486\tTriple Loss(1): 0.0366\tClassification Loss: 1.1754\r\n",
      "Train Epoch: 26 [193920/209539 (93%)]\tAll Loss: 1.3430\tTriple Loss(1): 0.0110\tClassification Loss: 1.3211\r\n",
      "Train Epoch: 26 [194560/209539 (93%)]\tAll Loss: 1.2047\tTriple Loss(1): 0.0263\tClassification Loss: 1.1521\r\n",
      "Train Epoch: 26 [195200/209539 (93%)]\tAll Loss: 1.8802\tTriple Loss(0): 0.2646\tClassification Loss: 1.3510\r\n",
      "Train Epoch: 26 [195840/209539 (93%)]\tAll Loss: 0.8692\tTriple Loss(1): 0.0063\tClassification Loss: 0.8565\r\n",
      "Train Epoch: 26 [196480/209539 (94%)]\tAll Loss: 1.4921\tTriple Loss(1): 0.0627\tClassification Loss: 1.3667\r\n",
      "Train Epoch: 26 [197120/209539 (94%)]\tAll Loss: 1.2497\tTriple Loss(1): 0.0064\tClassification Loss: 1.2368\r\n",
      "Train Epoch: 26 [197760/209539 (94%)]\tAll Loss: 2.0892\tTriple Loss(0): 0.3504\tClassification Loss: 1.3884\r\n",
      "Train Epoch: 26 [198400/209539 (95%)]\tAll Loss: 1.3159\tTriple Loss(1): 0.0227\tClassification Loss: 1.2704\r\n",
      "Train Epoch: 26 [199040/209539 (95%)]\tAll Loss: 1.4777\tTriple Loss(1): 0.0120\tClassification Loss: 1.4538\r\n",
      "Train Epoch: 26 [199680/209539 (95%)]\tAll Loss: 1.2021\tTriple Loss(1): 0.0481\tClassification Loss: 1.1059\r\n",
      "Train Epoch: 26 [200320/209539 (96%)]\tAll Loss: 1.3198\tTriple Loss(1): 0.0524\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 26 [200960/209539 (96%)]\tAll Loss: 1.2738\tTriple Loss(1): 0.0564\tClassification Loss: 1.1610\r\n",
      "Train Epoch: 26 [201600/209539 (96%)]\tAll Loss: 1.2835\tTriple Loss(1): 0.0325\tClassification Loss: 1.2185\r\n",
      "Train Epoch: 26 [202240/209539 (97%)]\tAll Loss: 1.8163\tTriple Loss(0): 0.3278\tClassification Loss: 1.1608\r\n",
      "Train Epoch: 26 [202880/209539 (97%)]\tAll Loss: 1.1367\tTriple Loss(1): 0.0459\tClassification Loss: 1.0449\r\n",
      "Train Epoch: 26 [203520/209539 (97%)]\tAll Loss: 2.0379\tTriple Loss(0): 0.3656\tClassification Loss: 1.3066\r\n",
      "Train Epoch: 26 [204160/209539 (97%)]\tAll Loss: 1.7083\tTriple Loss(1): 0.0443\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 26 [204800/209539 (98%)]\tAll Loss: 1.4371\tTriple Loss(1): 0.0186\tClassification Loss: 1.3999\r\n",
      "Train Epoch: 26 [205440/209539 (98%)]\tAll Loss: 0.9609\tTriple Loss(1): 0.0123\tClassification Loss: 0.9363\r\n",
      "Train Epoch: 26 [206080/209539 (98%)]\tAll Loss: 1.4146\tTriple Loss(1): 0.0802\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 26 [206720/209539 (99%)]\tAll Loss: 1.3755\tTriple Loss(1): 0.0737\tClassification Loss: 1.2280\r\n",
      "Train Epoch: 26 [207360/209539 (99%)]\tAll Loss: 0.9678\tTriple Loss(1): 0.0112\tClassification Loss: 0.9454\r\n",
      "Train Epoch: 26 [208000/209539 (99%)]\tAll Loss: 1.1096\tTriple Loss(1): 0.0163\tClassification Loss: 1.0769\r\n",
      "Train Epoch: 26 [208640/209539 (100%)]\tAll Loss: 1.3366\tTriple Loss(1): 0.0463\tClassification Loss: 1.2439\r\n",
      "Train Epoch: 26 [209280/209539 (100%)]\tAll Loss: 1.3386\tTriple Loss(1): 0.0316\tClassification Loss: 1.2753\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/26_epochs\r\n",
      "Train Epoch: 27 [0/209539 (0%)]\tAll Loss: 2.9244\tTriple Loss(0): 0.7083\tClassification Loss: 1.5078\r\n",
      "\r\n",
      "Test set: Average loss: 1.1426\r\n",
      "Top 1 Accuracy: 53534/80128 (67%)\r\n",
      "Top 3 Accuracy: 69266/80128 (86%)\r\n",
      "Top 5 Accuracy: 74091/80128 (92%)\r\n",
      " \r\n",
      "Train Epoch: 27 [640/209539 (0%)]\tAll Loss: 1.4837\tTriple Loss(1): 0.0933\tClassification Loss: 1.2972\r\n",
      "Train Epoch: 27 [1280/209539 (1%)]\tAll Loss: 1.2739\tTriple Loss(1): 0.0390\tClassification Loss: 1.1959\r\n",
      "Train Epoch: 27 [1920/209539 (1%)]\tAll Loss: 1.3958\tTriple Loss(1): 0.0359\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 27 [2560/209539 (1%)]\tAll Loss: 1.4746\tTriple Loss(1): 0.0118\tClassification Loss: 1.4510\r\n",
      "Train Epoch: 27 [3200/209539 (2%)]\tAll Loss: 1.3813\tTriple Loss(1): 0.0387\tClassification Loss: 1.3039\r\n",
      "Train Epoch: 27 [3840/209539 (2%)]\tAll Loss: 1.3566\tTriple Loss(1): 0.0568\tClassification Loss: 1.2430\r\n",
      "Train Epoch: 27 [4480/209539 (2%)]\tAll Loss: 1.2090\tTriple Loss(1): 0.0187\tClassification Loss: 1.1716\r\n",
      "Train Epoch: 27 [5120/209539 (2%)]\tAll Loss: 1.3859\tTriple Loss(1): 0.0472\tClassification Loss: 1.2914\r\n",
      "Train Epoch: 27 [5760/209539 (3%)]\tAll Loss: 1.3457\tTriple Loss(1): 0.0467\tClassification Loss: 1.2523\r\n",
      "Train Epoch: 27 [6400/209539 (3%)]\tAll Loss: 0.9812\tTriple Loss(1): 0.0023\tClassification Loss: 0.9766\r\n",
      "Train Epoch: 27 [7040/209539 (3%)]\tAll Loss: 1.0978\tTriple Loss(1): 0.0443\tClassification Loss: 1.0091\r\n",
      "Train Epoch: 27 [7680/209539 (4%)]\tAll Loss: 1.0956\tTriple Loss(1): 0.0391\tClassification Loss: 1.0174\r\n",
      "Train Epoch: 27 [8320/209539 (4%)]\tAll Loss: 1.3264\tTriple Loss(1): 0.0549\tClassification Loss: 1.2166\r\n",
      "Train Epoch: 27 [8960/209539 (4%)]\tAll Loss: 1.3553\tTriple Loss(1): 0.0000\tClassification Loss: 1.3553\r\n",
      "Train Epoch: 27 [9600/209539 (5%)]\tAll Loss: 1.5874\tTriple Loss(1): 0.0053\tClassification Loss: 1.5768\r\n",
      "Train Epoch: 27 [10240/209539 (5%)]\tAll Loss: 1.5679\tTriple Loss(0): 0.3012\tClassification Loss: 0.9655\r\n",
      "Train Epoch: 27 [10880/209539 (5%)]\tAll Loss: 1.1397\tTriple Loss(1): 0.0079\tClassification Loss: 1.1240\r\n",
      "Train Epoch: 27 [11520/209539 (5%)]\tAll Loss: 1.3442\tTriple Loss(1): 0.0122\tClassification Loss: 1.3197\r\n",
      "Train Epoch: 27 [12160/209539 (6%)]\tAll Loss: 1.1803\tTriple Loss(1): 0.0341\tClassification Loss: 1.1121\r\n",
      "Train Epoch: 27 [12800/209539 (6%)]\tAll Loss: 0.9356\tTriple Loss(1): 0.0000\tClassification Loss: 0.9356\r\n",
      "Train Epoch: 27 [13440/209539 (6%)]\tAll Loss: 1.2677\tTriple Loss(1): 0.0408\tClassification Loss: 1.1860\r\n",
      "Train Epoch: 27 [14080/209539 (7%)]\tAll Loss: 1.2690\tTriple Loss(1): 0.0783\tClassification Loss: 1.1125\r\n",
      "Train Epoch: 27 [14720/209539 (7%)]\tAll Loss: 1.4290\tTriple Loss(1): 0.0371\tClassification Loss: 1.3549\r\n",
      "Train Epoch: 27 [15360/209539 (7%)]\tAll Loss: 1.2140\tTriple Loss(1): 0.0372\tClassification Loss: 1.1395\r\n",
      "Train Epoch: 27 [16000/209539 (8%)]\tAll Loss: 1.5796\tTriple Loss(1): 0.0253\tClassification Loss: 1.5289\r\n",
      "Train Epoch: 27 [16640/209539 (8%)]\tAll Loss: 2.1959\tTriple Loss(0): 0.2751\tClassification Loss: 1.6456\r\n",
      "Train Epoch: 27 [17280/209539 (8%)]\tAll Loss: 1.3641\tTriple Loss(1): 0.0584\tClassification Loss: 1.2472\r\n",
      "Train Epoch: 27 [17920/209539 (9%)]\tAll Loss: 1.9495\tTriple Loss(0): 0.2943\tClassification Loss: 1.3609\r\n",
      "Train Epoch: 27 [18560/209539 (9%)]\tAll Loss: 1.4771\tTriple Loss(1): 0.1017\tClassification Loss: 1.2736\r\n",
      "Train Epoch: 27 [19200/209539 (9%)]\tAll Loss: 1.4730\tTriple Loss(1): 0.1171\tClassification Loss: 1.2389\r\n",
      "Train Epoch: 27 [19840/209539 (9%)]\tAll Loss: 1.2443\tTriple Loss(1): 0.0546\tClassification Loss: 1.1351\r\n",
      "Train Epoch: 27 [20480/209539 (10%)]\tAll Loss: 1.1055\tTriple Loss(1): 0.0000\tClassification Loss: 1.1055\r\n",
      "Train Epoch: 27 [21120/209539 (10%)]\tAll Loss: 1.5796\tTriple Loss(1): 0.0060\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 27 [21760/209539 (10%)]\tAll Loss: 1.9090\tTriple Loss(0): 0.3188\tClassification Loss: 1.2714\r\n",
      "Train Epoch: 27 [22400/209539 (11%)]\tAll Loss: 1.0478\tTriple Loss(1): 0.0000\tClassification Loss: 1.0478\r\n",
      "Train Epoch: 27 [23040/209539 (11%)]\tAll Loss: 1.3575\tTriple Loss(1): 0.0023\tClassification Loss: 1.3530\r\n",
      "Train Epoch: 27 [23680/209539 (11%)]\tAll Loss: 1.6961\tTriple Loss(0): 0.3693\tClassification Loss: 0.9574\r\n",
      "Train Epoch: 27 [24320/209539 (12%)]\tAll Loss: 1.2840\tTriple Loss(1): 0.0456\tClassification Loss: 1.1929\r\n",
      "Train Epoch: 27 [24960/209539 (12%)]\tAll Loss: 1.1003\tTriple Loss(1): 0.0041\tClassification Loss: 1.0921\r\n",
      "Train Epoch: 27 [25600/209539 (12%)]\tAll Loss: 1.1705\tTriple Loss(1): 0.0112\tClassification Loss: 1.1482\r\n",
      "Train Epoch: 27 [26240/209539 (13%)]\tAll Loss: 1.1575\tTriple Loss(1): 0.0570\tClassification Loss: 1.0435\r\n",
      "Train Epoch: 27 [26880/209539 (13%)]\tAll Loss: 1.8413\tTriple Loss(1): 0.0539\tClassification Loss: 1.7335\r\n",
      "Train Epoch: 27 [27520/209539 (13%)]\tAll Loss: 1.3608\tTriple Loss(0): 0.1029\tClassification Loss: 1.1550\r\n",
      "Train Epoch: 27 [28160/209539 (13%)]\tAll Loss: 1.2430\tTriple Loss(1): 0.0194\tClassification Loss: 1.2042\r\n",
      "Train Epoch: 27 [28800/209539 (14%)]\tAll Loss: 1.3667\tTriple Loss(1): 0.0000\tClassification Loss: 1.3667\r\n",
      "Train Epoch: 27 [29440/209539 (14%)]\tAll Loss: 1.3565\tTriple Loss(1): 0.0000\tClassification Loss: 1.3565\r\n",
      "Train Epoch: 27 [30080/209539 (14%)]\tAll Loss: 1.1193\tTriple Loss(1): 0.0111\tClassification Loss: 1.0972\r\n",
      "Train Epoch: 27 [30720/209539 (15%)]\tAll Loss: 1.6797\tTriple Loss(0): 0.2635\tClassification Loss: 1.1526\r\n",
      "Train Epoch: 27 [31360/209539 (15%)]\tAll Loss: 2.1529\tTriple Loss(0): 0.5347\tClassification Loss: 1.0835\r\n",
      "Train Epoch: 27 [32000/209539 (15%)]\tAll Loss: 2.0589\tTriple Loss(0): 0.3331\tClassification Loss: 1.3927\r\n",
      "Train Epoch: 27 [32640/209539 (16%)]\tAll Loss: 1.2581\tTriple Loss(1): 0.0000\tClassification Loss: 1.2581\r\n",
      "Train Epoch: 27 [33280/209539 (16%)]\tAll Loss: 1.3672\tTriple Loss(1): 0.0078\tClassification Loss: 1.3517\r\n",
      "Train Epoch: 27 [33920/209539 (16%)]\tAll Loss: 1.6314\tTriple Loss(0): 0.1791\tClassification Loss: 1.2731\r\n",
      "Train Epoch: 27 [34560/209539 (16%)]\tAll Loss: 1.3878\tTriple Loss(1): 0.0694\tClassification Loss: 1.2490\r\n",
      "Train Epoch: 27 [35200/209539 (17%)]\tAll Loss: 1.1914\tTriple Loss(1): 0.0215\tClassification Loss: 1.1484\r\n",
      "Train Epoch: 27 [35840/209539 (17%)]\tAll Loss: 1.2662\tTriple Loss(1): 0.0439\tClassification Loss: 1.1783\r\n",
      "Train Epoch: 27 [36480/209539 (17%)]\tAll Loss: 1.1826\tTriple Loss(1): 0.0648\tClassification Loss: 1.0530\r\n",
      "Train Epoch: 27 [37120/209539 (18%)]\tAll Loss: 1.4752\tTriple Loss(1): 0.0141\tClassification Loss: 1.4471\r\n",
      "Train Epoch: 27 [37760/209539 (18%)]\tAll Loss: 1.3071\tTriple Loss(1): 0.0588\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 27 [38400/209539 (18%)]\tAll Loss: 2.3341\tTriple Loss(0): 0.4724\tClassification Loss: 1.3893\r\n",
      "Train Epoch: 27 [39040/209539 (19%)]\tAll Loss: 1.2503\tTriple Loss(1): 0.1151\tClassification Loss: 1.0202\r\n",
      "Train Epoch: 27 [39680/209539 (19%)]\tAll Loss: 1.1168\tTriple Loss(1): 0.0000\tClassification Loss: 1.1168\r\n",
      "Train Epoch: 27 [40320/209539 (19%)]\tAll Loss: 1.3898\tTriple Loss(1): 0.0394\tClassification Loss: 1.3110\r\n",
      "Train Epoch: 27 [40960/209539 (20%)]\tAll Loss: 1.1847\tTriple Loss(1): 0.0631\tClassification Loss: 1.0584\r\n",
      "Train Epoch: 27 [41600/209539 (20%)]\tAll Loss: 1.7238\tTriple Loss(0): 0.3176\tClassification Loss: 1.0886\r\n",
      "Train Epoch: 27 [42240/209539 (20%)]\tAll Loss: 1.5879\tTriple Loss(0): 0.2282\tClassification Loss: 1.1315\r\n",
      "Train Epoch: 27 [42880/209539 (20%)]\tAll Loss: 1.0090\tTriple Loss(1): 0.0000\tClassification Loss: 1.0090\r\n",
      "Train Epoch: 27 [43520/209539 (21%)]\tAll Loss: 1.5754\tTriple Loss(0): 0.1339\tClassification Loss: 1.3077\r\n",
      "Train Epoch: 27 [44160/209539 (21%)]\tAll Loss: 1.6488\tTriple Loss(1): 0.0440\tClassification Loss: 1.5607\r\n",
      "Train Epoch: 27 [44800/209539 (21%)]\tAll Loss: 1.4763\tTriple Loss(1): 0.0095\tClassification Loss: 1.4573\r\n",
      "Train Epoch: 27 [45440/209539 (22%)]\tAll Loss: 1.7390\tTriple Loss(1): 0.0399\tClassification Loss: 1.6591\r\n",
      "Train Epoch: 27 [46080/209539 (22%)]\tAll Loss: 1.3498\tTriple Loss(1): 0.0559\tClassification Loss: 1.2379\r\n",
      "Train Epoch: 27 [46720/209539 (22%)]\tAll Loss: 1.7613\tTriple Loss(1): 0.0433\tClassification Loss: 1.6747\r\n",
      "Train Epoch: 27 [47360/209539 (23%)]\tAll Loss: 0.9398\tTriple Loss(1): 0.0189\tClassification Loss: 0.9020\r\n",
      "Train Epoch: 27 [48000/209539 (23%)]\tAll Loss: 1.1644\tTriple Loss(1): 0.0710\tClassification Loss: 1.0224\r\n",
      "Train Epoch: 27 [48640/209539 (23%)]\tAll Loss: 1.6876\tTriple Loss(1): 0.0178\tClassification Loss: 1.6521\r\n",
      "Train Epoch: 27 [49280/209539 (24%)]\tAll Loss: 1.3066\tTriple Loss(1): 0.0435\tClassification Loss: 1.2195\r\n",
      "Train Epoch: 27 [49920/209539 (24%)]\tAll Loss: 1.3697\tTriple Loss(1): 0.0166\tClassification Loss: 1.3365\r\n",
      "Train Epoch: 27 [50560/209539 (24%)]\tAll Loss: 1.5127\tTriple Loss(1): 0.0186\tClassification Loss: 1.4755\r\n",
      "Train Epoch: 27 [51200/209539 (24%)]\tAll Loss: 1.4511\tTriple Loss(1): 0.0220\tClassification Loss: 1.4070\r\n",
      "Train Epoch: 27 [51840/209539 (25%)]\tAll Loss: 1.3256\tTriple Loss(1): 0.0311\tClassification Loss: 1.2635\r\n",
      "Train Epoch: 27 [52480/209539 (25%)]\tAll Loss: 1.1787\tTriple Loss(1): 0.0142\tClassification Loss: 1.1503\r\n",
      "Train Epoch: 27 [53120/209539 (25%)]\tAll Loss: 1.9156\tTriple Loss(0): 0.3449\tClassification Loss: 1.2258\r\n",
      "Train Epoch: 27 [53760/209539 (26%)]\tAll Loss: 1.5524\tTriple Loss(1): 0.0075\tClassification Loss: 1.5373\r\n",
      "Train Epoch: 27 [54400/209539 (26%)]\tAll Loss: 1.9192\tTriple Loss(1): 0.0407\tClassification Loss: 1.8379\r\n",
      "Train Epoch: 27 [55040/209539 (26%)]\tAll Loss: 0.9343\tTriple Loss(1): 0.0247\tClassification Loss: 0.8848\r\n",
      "Train Epoch: 27 [55680/209539 (27%)]\tAll Loss: 1.6824\tTriple Loss(1): 0.0767\tClassification Loss: 1.5291\r\n",
      "Train Epoch: 27 [56320/209539 (27%)]\tAll Loss: 1.1513\tTriple Loss(1): 0.1015\tClassification Loss: 0.9482\r\n",
      "Train Epoch: 27 [56960/209539 (27%)]\tAll Loss: 1.8085\tTriple Loss(0): 0.3217\tClassification Loss: 1.1651\r\n",
      "Train Epoch: 27 [57600/209539 (27%)]\tAll Loss: 1.1021\tTriple Loss(1): 0.0019\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 27 [58240/209539 (28%)]\tAll Loss: 1.1096\tTriple Loss(1): 0.0612\tClassification Loss: 0.9872\r\n",
      "Train Epoch: 27 [58880/209539 (28%)]\tAll Loss: 2.0802\tTriple Loss(0): 0.3520\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 27 [59520/209539 (28%)]\tAll Loss: 1.1043\tTriple Loss(1): 0.0410\tClassification Loss: 1.0222\r\n",
      "Train Epoch: 27 [60160/209539 (29%)]\tAll Loss: 1.4141\tTriple Loss(1): 0.0469\tClassification Loss: 1.3204\r\n",
      "Train Epoch: 27 [60800/209539 (29%)]\tAll Loss: 1.6686\tTriple Loss(1): 0.0812\tClassification Loss: 1.5062\r\n",
      "Train Epoch: 27 [61440/209539 (29%)]\tAll Loss: 1.3038\tTriple Loss(1): 0.0000\tClassification Loss: 1.3038\r\n",
      "Train Epoch: 27 [62080/209539 (30%)]\tAll Loss: 1.6726\tTriple Loss(1): 0.0630\tClassification Loss: 1.5466\r\n",
      "Train Epoch: 27 [62720/209539 (30%)]\tAll Loss: 1.1041\tTriple Loss(1): 0.0000\tClassification Loss: 1.1041\r\n",
      "Train Epoch: 27 [63360/209539 (30%)]\tAll Loss: 2.0126\tTriple Loss(0): 0.3106\tClassification Loss: 1.3913\r\n",
      "Train Epoch: 27 [64000/209539 (31%)]\tAll Loss: 1.4255\tTriple Loss(1): 0.0036\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 27 [64640/209539 (31%)]\tAll Loss: 1.6613\tTriple Loss(1): 0.0542\tClassification Loss: 1.5530\r\n",
      "Train Epoch: 27 [65280/209539 (31%)]\tAll Loss: 1.3141\tTriple Loss(1): 0.0175\tClassification Loss: 1.2791\r\n",
      "Train Epoch: 27 [65920/209539 (31%)]\tAll Loss: 1.4354\tTriple Loss(1): 0.0223\tClassification Loss: 1.3907\r\n",
      "Train Epoch: 27 [66560/209539 (32%)]\tAll Loss: 1.4678\tTriple Loss(0): 0.1715\tClassification Loss: 1.1247\r\n",
      "Train Epoch: 27 [67200/209539 (32%)]\tAll Loss: 1.2243\tTriple Loss(1): 0.0021\tClassification Loss: 1.2201\r\n",
      "Train Epoch: 27 [67840/209539 (32%)]\tAll Loss: 1.2799\tTriple Loss(1): 0.0031\tClassification Loss: 1.2737\r\n",
      "Train Epoch: 27 [68480/209539 (33%)]\tAll Loss: 1.3000\tTriple Loss(1): 0.0000\tClassification Loss: 1.3000\r\n",
      "Train Epoch: 27 [69120/209539 (33%)]\tAll Loss: 1.5361\tTriple Loss(1): 0.0068\tClassification Loss: 1.5225\r\n",
      "Train Epoch: 27 [69760/209539 (33%)]\tAll Loss: 1.7548\tTriple Loss(0): 0.3366\tClassification Loss: 1.0817\r\n",
      "Train Epoch: 27 [70400/209539 (34%)]\tAll Loss: 1.1872\tTriple Loss(1): 0.0121\tClassification Loss: 1.1629\r\n",
      "Train Epoch: 27 [71040/209539 (34%)]\tAll Loss: 1.5623\tTriple Loss(1): 0.0000\tClassification Loss: 1.5623\r\n",
      "Train Epoch: 27 [71680/209539 (34%)]\tAll Loss: 1.5093\tTriple Loss(1): 0.0294\tClassification Loss: 1.4506\r\n",
      "Train Epoch: 27 [72320/209539 (35%)]\tAll Loss: 1.2137\tTriple Loss(1): 0.0304\tClassification Loss: 1.1530\r\n",
      "Train Epoch: 27 [72960/209539 (35%)]\tAll Loss: 1.2575\tTriple Loss(1): 0.0482\tClassification Loss: 1.1611\r\n",
      "Train Epoch: 27 [73600/209539 (35%)]\tAll Loss: 2.0201\tTriple Loss(0): 0.3629\tClassification Loss: 1.2942\r\n",
      "Train Epoch: 27 [74240/209539 (35%)]\tAll Loss: 1.7192\tTriple Loss(1): 0.0817\tClassification Loss: 1.5559\r\n",
      "Train Epoch: 27 [74880/209539 (36%)]\tAll Loss: 1.9391\tTriple Loss(1): 0.0025\tClassification Loss: 1.9341\r\n",
      "Train Epoch: 27 [75520/209539 (36%)]\tAll Loss: 1.2755\tTriple Loss(1): 0.0479\tClassification Loss: 1.1797\r\n",
      "Train Epoch: 27 [76160/209539 (36%)]\tAll Loss: 1.1648\tTriple Loss(1): 0.0169\tClassification Loss: 1.1309\r\n",
      "Train Epoch: 27 [76800/209539 (37%)]\tAll Loss: 1.1965\tTriple Loss(1): 0.0266\tClassification Loss: 1.1433\r\n",
      "Train Epoch: 27 [77440/209539 (37%)]\tAll Loss: 1.3322\tTriple Loss(1): 0.0250\tClassification Loss: 1.2821\r\n",
      "Train Epoch: 27 [78080/209539 (37%)]\tAll Loss: 1.8104\tTriple Loss(0): 0.3074\tClassification Loss: 1.1957\r\n",
      "Train Epoch: 27 [78720/209539 (38%)]\tAll Loss: 1.4394\tTriple Loss(1): 0.0674\tClassification Loss: 1.3046\r\n",
      "Train Epoch: 27 [79360/209539 (38%)]\tAll Loss: 1.7528\tTriple Loss(0): 0.2875\tClassification Loss: 1.1778\r\n",
      "Train Epoch: 27 [80000/209539 (38%)]\tAll Loss: 1.4041\tTriple Loss(1): 0.0023\tClassification Loss: 1.3995\r\n",
      "Train Epoch: 27 [80640/209539 (38%)]\tAll Loss: 1.2949\tTriple Loss(1): 0.0276\tClassification Loss: 1.2396\r\n",
      "Train Epoch: 27 [81280/209539 (39%)]\tAll Loss: 1.3877\tTriple Loss(1): 0.0173\tClassification Loss: 1.3531\r\n",
      "Train Epoch: 27 [81920/209539 (39%)]\tAll Loss: 1.1259\tTriple Loss(1): 0.0293\tClassification Loss: 1.0674\r\n",
      "Train Epoch: 27 [82560/209539 (39%)]\tAll Loss: 1.6742\tTriple Loss(1): 0.1449\tClassification Loss: 1.3844\r\n",
      "Train Epoch: 27 [83200/209539 (40%)]\tAll Loss: 2.1468\tTriple Loss(0): 0.4266\tClassification Loss: 1.2936\r\n",
      "Train Epoch: 27 [83840/209539 (40%)]\tAll Loss: 1.4162\tTriple Loss(1): 0.0000\tClassification Loss: 1.4162\r\n",
      "Train Epoch: 27 [84480/209539 (40%)]\tAll Loss: 1.9752\tTriple Loss(0): 0.3726\tClassification Loss: 1.2301\r\n",
      "Train Epoch: 27 [85120/209539 (41%)]\tAll Loss: 1.8382\tTriple Loss(1): 0.0211\tClassification Loss: 1.7959\r\n",
      "Train Epoch: 27 [85760/209539 (41%)]\tAll Loss: 1.5689\tTriple Loss(1): 0.0535\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 27 [86400/209539 (41%)]\tAll Loss: 1.2376\tTriple Loss(1): 0.0030\tClassification Loss: 1.2317\r\n",
      "Train Epoch: 27 [87040/209539 (42%)]\tAll Loss: 1.1370\tTriple Loss(1): 0.0183\tClassification Loss: 1.1003\r\n",
      "Train Epoch: 27 [87680/209539 (42%)]\tAll Loss: 1.1459\tTriple Loss(1): 0.0246\tClassification Loss: 1.0966\r\n",
      "Train Epoch: 27 [88320/209539 (42%)]\tAll Loss: 1.5415\tTriple Loss(1): 0.0326\tClassification Loss: 1.4763\r\n",
      "Train Epoch: 27 [88960/209539 (42%)]\tAll Loss: 1.3280\tTriple Loss(1): 0.0416\tClassification Loss: 1.2448\r\n",
      "Train Epoch: 27 [89600/209539 (43%)]\tAll Loss: 1.3549\tTriple Loss(1): 0.0276\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 27 [90240/209539 (43%)]\tAll Loss: 1.5107\tTriple Loss(0): 0.2050\tClassification Loss: 1.1007\r\n",
      "Train Epoch: 27 [90880/209539 (43%)]\tAll Loss: 1.5991\tTriple Loss(1): 0.0199\tClassification Loss: 1.5593\r\n",
      "Train Epoch: 27 [91520/209539 (44%)]\tAll Loss: 1.5379\tTriple Loss(1): 0.0966\tClassification Loss: 1.3446\r\n",
      "Train Epoch: 27 [92160/209539 (44%)]\tAll Loss: 1.2496\tTriple Loss(1): 0.0002\tClassification Loss: 1.2491\r\n",
      "Train Epoch: 27 [92800/209539 (44%)]\tAll Loss: 1.0985\tTriple Loss(1): 0.0156\tClassification Loss: 1.0672\r\n",
      "Train Epoch: 27 [93440/209539 (45%)]\tAll Loss: 1.4419\tTriple Loss(1): 0.0677\tClassification Loss: 1.3065\r\n",
      "Train Epoch: 27 [94080/209539 (45%)]\tAll Loss: 1.1402\tTriple Loss(1): 0.0087\tClassification Loss: 1.1229\r\n",
      "Train Epoch: 27 [94720/209539 (45%)]\tAll Loss: 1.6524\tTriple Loss(1): 0.0696\tClassification Loss: 1.5132\r\n",
      "Train Epoch: 27 [95360/209539 (46%)]\tAll Loss: 1.4631\tTriple Loss(1): 0.0456\tClassification Loss: 1.3720\r\n",
      "Train Epoch: 27 [96000/209539 (46%)]\tAll Loss: 1.5709\tTriple Loss(1): 0.0246\tClassification Loss: 1.5216\r\n",
      "Train Epoch: 27 [96640/209539 (46%)]\tAll Loss: 1.4664\tTriple Loss(1): 0.0406\tClassification Loss: 1.3852\r\n",
      "Train Epoch: 27 [97280/209539 (46%)]\tAll Loss: 1.2393\tTriple Loss(1): 0.0127\tClassification Loss: 1.2139\r\n",
      "Train Epoch: 27 [97920/209539 (47%)]\tAll Loss: 1.9009\tTriple Loss(0): 0.2648\tClassification Loss: 1.3713\r\n",
      "Train Epoch: 27 [98560/209539 (47%)]\tAll Loss: 1.6728\tTriple Loss(1): 0.0096\tClassification Loss: 1.6535\r\n",
      "Train Epoch: 27 [99200/209539 (47%)]\tAll Loss: 1.3934\tTriple Loss(1): 0.0414\tClassification Loss: 1.3107\r\n",
      "Train Epoch: 27 [99840/209539 (48%)]\tAll Loss: 1.3570\tTriple Loss(1): 0.0130\tClassification Loss: 1.3310\r\n",
      "Train Epoch: 27 [100480/209539 (48%)]\tAll Loss: 1.4337\tTriple Loss(1): 0.0042\tClassification Loss: 1.4254\r\n",
      "Train Epoch: 27 [101120/209539 (48%)]\tAll Loss: 1.5082\tTriple Loss(1): 0.0757\tClassification Loss: 1.3569\r\n",
      "Train Epoch: 27 [101760/209539 (49%)]\tAll Loss: 1.6928\tTriple Loss(0): 0.2380\tClassification Loss: 1.2167\r\n",
      "Train Epoch: 27 [102400/209539 (49%)]\tAll Loss: 1.0249\tTriple Loss(1): 0.0267\tClassification Loss: 0.9715\r\n",
      "Train Epoch: 27 [103040/209539 (49%)]\tAll Loss: 1.3073\tTriple Loss(1): 0.0180\tClassification Loss: 1.2713\r\n",
      "Train Epoch: 27 [103680/209539 (49%)]\tAll Loss: 2.0654\tTriple Loss(0): 0.3362\tClassification Loss: 1.3929\r\n",
      "Train Epoch: 27 [104320/209539 (50%)]\tAll Loss: 1.1499\tTriple Loss(1): 0.0188\tClassification Loss: 1.1124\r\n",
      "Train Epoch: 27 [104960/209539 (50%)]\tAll Loss: 1.1955\tTriple Loss(1): 0.0139\tClassification Loss: 1.1678\r\n",
      "Train Epoch: 27 [105600/209539 (50%)]\tAll Loss: 1.3685\tTriple Loss(1): 0.0000\tClassification Loss: 1.3685\r\n",
      "Train Epoch: 27 [106240/209539 (51%)]\tAll Loss: 1.0697\tTriple Loss(1): 0.0074\tClassification Loss: 1.0549\r\n",
      "Train Epoch: 27 [106880/209539 (51%)]\tAll Loss: 1.3786\tTriple Loss(1): 0.0471\tClassification Loss: 1.2843\r\n",
      "Train Epoch: 27 [107520/209539 (51%)]\tAll Loss: 1.3769\tTriple Loss(1): 0.0143\tClassification Loss: 1.3482\r\n",
      "Train Epoch: 27 [108160/209539 (52%)]\tAll Loss: 1.3840\tTriple Loss(1): 0.0867\tClassification Loss: 1.2106\r\n",
      "Train Epoch: 27 [108800/209539 (52%)]\tAll Loss: 1.7929\tTriple Loss(0): 0.3263\tClassification Loss: 1.1404\r\n",
      "Train Epoch: 27 [109440/209539 (52%)]\tAll Loss: 1.6470\tTriple Loss(1): 0.0284\tClassification Loss: 1.5902\r\n",
      "Train Epoch: 27 [110080/209539 (53%)]\tAll Loss: 1.6394\tTriple Loss(1): 0.0501\tClassification Loss: 1.5391\r\n",
      "Train Epoch: 27 [110720/209539 (53%)]\tAll Loss: 2.0962\tTriple Loss(0): 0.3105\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 27 [111360/209539 (53%)]\tAll Loss: 1.1170\tTriple Loss(1): 0.0230\tClassification Loss: 1.0710\r\n",
      "Train Epoch: 27 [112000/209539 (53%)]\tAll Loss: 1.4232\tTriple Loss(1): 0.0509\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 27 [112640/209539 (54%)]\tAll Loss: 1.5920\tTriple Loss(0): 0.2306\tClassification Loss: 1.1309\r\n",
      "Train Epoch: 27 [113280/209539 (54%)]\tAll Loss: 1.0633\tTriple Loss(1): 0.0699\tClassification Loss: 0.9235\r\n",
      "Train Epoch: 27 [113920/209539 (54%)]\tAll Loss: 1.3782\tTriple Loss(1): 0.0295\tClassification Loss: 1.3191\r\n",
      "Train Epoch: 27 [114560/209539 (55%)]\tAll Loss: 1.2570\tTriple Loss(1): 0.0286\tClassification Loss: 1.1998\r\n",
      "Train Epoch: 27 [115200/209539 (55%)]\tAll Loss: 1.5454\tTriple Loss(1): 0.0295\tClassification Loss: 1.4864\r\n",
      "Train Epoch: 27 [115840/209539 (55%)]\tAll Loss: 1.5611\tTriple Loss(1): 0.0625\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 27 [116480/209539 (56%)]\tAll Loss: 1.4987\tTriple Loss(1): 0.0633\tClassification Loss: 1.3720\r\n",
      "Train Epoch: 27 [117120/209539 (56%)]\tAll Loss: 1.2643\tTriple Loss(1): 0.0390\tClassification Loss: 1.1862\r\n",
      "Train Epoch: 27 [117760/209539 (56%)]\tAll Loss: 1.6453\tTriple Loss(0): 0.1799\tClassification Loss: 1.2855\r\n",
      "Train Epoch: 27 [118400/209539 (57%)]\tAll Loss: 1.6057\tTriple Loss(0): 0.2966\tClassification Loss: 1.0126\r\n",
      "Train Epoch: 27 [119040/209539 (57%)]\tAll Loss: 1.5986\tTriple Loss(1): 0.0481\tClassification Loss: 1.5023\r\n",
      "Train Epoch: 27 [119680/209539 (57%)]\tAll Loss: 1.4947\tTriple Loss(1): 0.0651\tClassification Loss: 1.3644\r\n",
      "Train Epoch: 27 [120320/209539 (57%)]\tAll Loss: 2.0018\tTriple Loss(0): 0.4201\tClassification Loss: 1.1616\r\n",
      "Train Epoch: 27 [120960/209539 (58%)]\tAll Loss: 1.2554\tTriple Loss(1): 0.0156\tClassification Loss: 1.2242\r\n",
      "Train Epoch: 27 [121600/209539 (58%)]\tAll Loss: 1.1075\tTriple Loss(1): 0.0000\tClassification Loss: 1.1075\r\n",
      "Train Epoch: 27 [122240/209539 (58%)]\tAll Loss: 1.2159\tTriple Loss(1): 0.0912\tClassification Loss: 1.0335\r\n",
      "Train Epoch: 27 [122880/209539 (59%)]\tAll Loss: 1.1683\tTriple Loss(1): 0.0112\tClassification Loss: 1.1459\r\n",
      "Train Epoch: 27 [123520/209539 (59%)]\tAll Loss: 1.8641\tTriple Loss(0): 0.3550\tClassification Loss: 1.1541\r\n",
      "Train Epoch: 27 [124160/209539 (59%)]\tAll Loss: 1.2825\tTriple Loss(1): 0.0108\tClassification Loss: 1.2609\r\n",
      "Train Epoch: 27 [124800/209539 (60%)]\tAll Loss: 1.0746\tTriple Loss(1): 0.0404\tClassification Loss: 0.9938\r\n",
      "Train Epoch: 27 [125440/209539 (60%)]\tAll Loss: 1.3542\tTriple Loss(1): 0.0386\tClassification Loss: 1.2769\r\n",
      "Train Epoch: 27 [126080/209539 (60%)]\tAll Loss: 1.8235\tTriple Loss(0): 0.2309\tClassification Loss: 1.3617\r\n",
      "Train Epoch: 27 [126720/209539 (60%)]\tAll Loss: 1.3882\tTriple Loss(1): 0.0083\tClassification Loss: 1.3716\r\n",
      "Train Epoch: 27 [127360/209539 (61%)]\tAll Loss: 1.3959\tTriple Loss(1): 0.0310\tClassification Loss: 1.3339\r\n",
      "Train Epoch: 27 [128000/209539 (61%)]\tAll Loss: 1.4804\tTriple Loss(1): 0.0127\tClassification Loss: 1.4550\r\n",
      "Train Epoch: 27 [128640/209539 (61%)]\tAll Loss: 1.0130\tTriple Loss(1): 0.0258\tClassification Loss: 0.9614\r\n",
      "Train Epoch: 27 [129280/209539 (62%)]\tAll Loss: 1.7578\tTriple Loss(0): 0.2294\tClassification Loss: 1.2989\r\n",
      "Train Epoch: 27 [129920/209539 (62%)]\tAll Loss: 1.1265\tTriple Loss(1): 0.0239\tClassification Loss: 1.0788\r\n",
      "Train Epoch: 27 [130560/209539 (62%)]\tAll Loss: 1.2345\tTriple Loss(1): 0.0208\tClassification Loss: 1.1929\r\n",
      "Train Epoch: 27 [131200/209539 (63%)]\tAll Loss: 1.7200\tTriple Loss(0): 0.3373\tClassification Loss: 1.0454\r\n",
      "Train Epoch: 27 [131840/209539 (63%)]\tAll Loss: 1.3509\tTriple Loss(1): 0.0543\tClassification Loss: 1.2422\r\n",
      "Train Epoch: 27 [132480/209539 (63%)]\tAll Loss: 1.0306\tTriple Loss(1): 0.0331\tClassification Loss: 0.9645\r\n",
      "Train Epoch: 27 [133120/209539 (64%)]\tAll Loss: 1.2240\tTriple Loss(1): 0.0149\tClassification Loss: 1.1942\r\n",
      "Train Epoch: 27 [133760/209539 (64%)]\tAll Loss: 1.2312\tTriple Loss(1): 0.0139\tClassification Loss: 1.2035\r\n",
      "Train Epoch: 27 [134400/209539 (64%)]\tAll Loss: 1.1309\tTriple Loss(1): 0.0332\tClassification Loss: 1.0646\r\n",
      "Train Epoch: 27 [135040/209539 (64%)]\tAll Loss: 1.3167\tTriple Loss(1): 0.0735\tClassification Loss: 1.1698\r\n",
      "Train Epoch: 27 [135680/209539 (65%)]\tAll Loss: 2.0475\tTriple Loss(0): 0.3035\tClassification Loss: 1.4405\r\n",
      "Train Epoch: 27 [136320/209539 (65%)]\tAll Loss: 1.2594\tTriple Loss(1): 0.0000\tClassification Loss: 1.2594\r\n",
      "Train Epoch: 27 [136960/209539 (65%)]\tAll Loss: 1.2548\tTriple Loss(1): 0.0023\tClassification Loss: 1.2502\r\n",
      "Train Epoch: 27 [137600/209539 (66%)]\tAll Loss: 1.2455\tTriple Loss(1): 0.0000\tClassification Loss: 1.2455\r\n",
      "Train Epoch: 27 [138240/209539 (66%)]\tAll Loss: 1.9932\tTriple Loss(0): 0.2764\tClassification Loss: 1.4403\r\n",
      "Train Epoch: 27 [138880/209539 (66%)]\tAll Loss: 1.1958\tTriple Loss(1): 0.0393\tClassification Loss: 1.1172\r\n",
      "Train Epoch: 27 [139520/209539 (67%)]\tAll Loss: 1.0359\tTriple Loss(1): 0.0000\tClassification Loss: 1.0359\r\n",
      "Train Epoch: 27 [140160/209539 (67%)]\tAll Loss: 1.1368\tTriple Loss(1): 0.0000\tClassification Loss: 1.1368\r\n",
      "Train Epoch: 27 [140800/209539 (67%)]\tAll Loss: 1.4686\tTriple Loss(1): 0.0455\tClassification Loss: 1.3777\r\n",
      "Train Epoch: 27 [141440/209539 (68%)]\tAll Loss: 1.9473\tTriple Loss(0): 0.3074\tClassification Loss: 1.3324\r\n",
      "Train Epoch: 27 [142080/209539 (68%)]\tAll Loss: 1.2752\tTriple Loss(1): 0.0379\tClassification Loss: 1.1993\r\n",
      "Train Epoch: 27 [142720/209539 (68%)]\tAll Loss: 1.8726\tTriple Loss(1): 0.0640\tClassification Loss: 1.7447\r\n",
      "Train Epoch: 27 [143360/209539 (68%)]\tAll Loss: 0.9911\tTriple Loss(1): 0.0455\tClassification Loss: 0.9001\r\n",
      "Train Epoch: 27 [144000/209539 (69%)]\tAll Loss: 1.4628\tTriple Loss(1): 0.0144\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 27 [144640/209539 (69%)]\tAll Loss: 1.2358\tTriple Loss(1): 0.0998\tClassification Loss: 1.0363\r\n",
      "Train Epoch: 27 [145280/209539 (69%)]\tAll Loss: 1.2816\tTriple Loss(1): 0.0291\tClassification Loss: 1.2235\r\n",
      "Train Epoch: 27 [145920/209539 (70%)]\tAll Loss: 1.2554\tTriple Loss(1): 0.0186\tClassification Loss: 1.2183\r\n",
      "Train Epoch: 27 [146560/209539 (70%)]\tAll Loss: 1.0266\tTriple Loss(1): 0.0000\tClassification Loss: 1.0266\r\n",
      "Train Epoch: 27 [147200/209539 (70%)]\tAll Loss: 1.3664\tTriple Loss(1): 0.0473\tClassification Loss: 1.2717\r\n",
      "Train Epoch: 27 [147840/209539 (71%)]\tAll Loss: 1.1178\tTriple Loss(1): 0.0406\tClassification Loss: 1.0365\r\n",
      "Train Epoch: 27 [148480/209539 (71%)]\tAll Loss: 1.2266\tTriple Loss(1): 0.0528\tClassification Loss: 1.1209\r\n",
      "Train Epoch: 27 [149120/209539 (71%)]\tAll Loss: 1.3725\tTriple Loss(1): 0.0155\tClassification Loss: 1.3416\r\n",
      "Train Epoch: 27 [149760/209539 (71%)]\tAll Loss: 0.9368\tTriple Loss(1): 0.0293\tClassification Loss: 0.8782\r\n",
      "Train Epoch: 27 [150400/209539 (72%)]\tAll Loss: 1.5668\tTriple Loss(1): 0.0538\tClassification Loss: 1.4591\r\n",
      "Train Epoch: 27 [151040/209539 (72%)]\tAll Loss: 1.1681\tTriple Loss(1): 0.0114\tClassification Loss: 1.1452\r\n",
      "Train Epoch: 27 [151680/209539 (72%)]\tAll Loss: 1.6295\tTriple Loss(1): 0.0310\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 27 [152320/209539 (73%)]\tAll Loss: 1.2038\tTriple Loss(1): 0.0107\tClassification Loss: 1.1824\r\n",
      "Train Epoch: 27 [152960/209539 (73%)]\tAll Loss: 1.2100\tTriple Loss(1): 0.0726\tClassification Loss: 1.0649\r\n",
      "Train Epoch: 27 [153600/209539 (73%)]\tAll Loss: 1.2801\tTriple Loss(1): 0.0193\tClassification Loss: 1.2414\r\n",
      "Train Epoch: 27 [154240/209539 (74%)]\tAll Loss: 1.1372\tTriple Loss(1): 0.0625\tClassification Loss: 1.0123\r\n",
      "Train Epoch: 27 [154880/209539 (74%)]\tAll Loss: 1.4283\tTriple Loss(1): 0.0247\tClassification Loss: 1.3789\r\n",
      "Train Epoch: 27 [155520/209539 (74%)]\tAll Loss: 1.1546\tTriple Loss(1): 0.0092\tClassification Loss: 1.1363\r\n",
      "Train Epoch: 27 [156160/209539 (75%)]\tAll Loss: 2.0562\tTriple Loss(0): 0.2855\tClassification Loss: 1.4851\r\n",
      "Train Epoch: 27 [156800/209539 (75%)]\tAll Loss: 2.2022\tTriple Loss(0): 0.2913\tClassification Loss: 1.6196\r\n",
      "Train Epoch: 27 [157440/209539 (75%)]\tAll Loss: 1.3327\tTriple Loss(1): 0.0000\tClassification Loss: 1.3327\r\n",
      "Train Epoch: 27 [158080/209539 (75%)]\tAll Loss: 1.8523\tTriple Loss(0): 0.3399\tClassification Loss: 1.1724\r\n",
      "Train Epoch: 27 [158720/209539 (76%)]\tAll Loss: 1.1301\tTriple Loss(1): 0.0010\tClassification Loss: 1.1282\r\n",
      "Train Epoch: 27 [159360/209539 (76%)]\tAll Loss: 2.0684\tTriple Loss(0): 0.5060\tClassification Loss: 1.0563\r\n",
      "Train Epoch: 27 [160000/209539 (76%)]\tAll Loss: 1.4119\tTriple Loss(1): 0.0000\tClassification Loss: 1.4119\r\n",
      "Train Epoch: 27 [160640/209539 (77%)]\tAll Loss: 1.6604\tTriple Loss(0): 0.3121\tClassification Loss: 1.0361\r\n",
      "Train Epoch: 27 [161280/209539 (77%)]\tAll Loss: 1.1923\tTriple Loss(1): 0.0354\tClassification Loss: 1.1214\r\n",
      "Train Epoch: 27 [161920/209539 (77%)]\tAll Loss: 1.1824\tTriple Loss(1): 0.0000\tClassification Loss: 1.1824\r\n",
      "Train Epoch: 27 [162560/209539 (78%)]\tAll Loss: 1.3658\tTriple Loss(1): 0.0475\tClassification Loss: 1.2709\r\n",
      "Train Epoch: 27 [163200/209539 (78%)]\tAll Loss: 1.2888\tTriple Loss(1): 0.0494\tClassification Loss: 1.1900\r\n",
      "Train Epoch: 27 [163840/209539 (78%)]\tAll Loss: 1.2869\tTriple Loss(1): 0.0587\tClassification Loss: 1.1695\r\n",
      "Train Epoch: 27 [164480/209539 (78%)]\tAll Loss: 1.3317\tTriple Loss(1): 0.0466\tClassification Loss: 1.2385\r\n",
      "Train Epoch: 27 [165120/209539 (79%)]\tAll Loss: 0.9361\tTriple Loss(1): 0.0000\tClassification Loss: 0.9361\r\n",
      "Train Epoch: 27 [165760/209539 (79%)]\tAll Loss: 1.9081\tTriple Loss(0): 0.3782\tClassification Loss: 1.1517\r\n",
      "Train Epoch: 27 [166400/209539 (79%)]\tAll Loss: 2.2122\tTriple Loss(0): 0.4565\tClassification Loss: 1.2992\r\n",
      "Train Epoch: 27 [167040/209539 (80%)]\tAll Loss: 1.6648\tTriple Loss(1): 0.0606\tClassification Loss: 1.5436\r\n",
      "Train Epoch: 27 [167680/209539 (80%)]\tAll Loss: 1.3696\tTriple Loss(1): 0.0374\tClassification Loss: 1.2949\r\n",
      "Train Epoch: 27 [168320/209539 (80%)]\tAll Loss: 1.1666\tTriple Loss(1): 0.0256\tClassification Loss: 1.1155\r\n",
      "Train Epoch: 27 [168960/209539 (81%)]\tAll Loss: 1.8133\tTriple Loss(0): 0.2673\tClassification Loss: 1.2788\r\n",
      "Train Epoch: 27 [169600/209539 (81%)]\tAll Loss: 1.5400\tTriple Loss(0): 0.1601\tClassification Loss: 1.2198\r\n",
      "Train Epoch: 27 [170240/209539 (81%)]\tAll Loss: 1.3072\tTriple Loss(1): 0.0000\tClassification Loss: 1.3072\r\n",
      "Train Epoch: 27 [170880/209539 (82%)]\tAll Loss: 1.4181\tTriple Loss(1): 0.0370\tClassification Loss: 1.3440\r\n",
      "Train Epoch: 27 [171520/209539 (82%)]\tAll Loss: 1.0927\tTriple Loss(1): 0.0096\tClassification Loss: 1.0735\r\n",
      "Train Epoch: 27 [172160/209539 (82%)]\tAll Loss: 1.1423\tTriple Loss(1): 0.0009\tClassification Loss: 1.1405\r\n",
      "Train Epoch: 27 [172800/209539 (82%)]\tAll Loss: 1.9614\tTriple Loss(0): 0.3432\tClassification Loss: 1.2751\r\n",
      "Train Epoch: 27 [173440/209539 (83%)]\tAll Loss: 1.4646\tTriple Loss(1): 0.0136\tClassification Loss: 1.4375\r\n",
      "Train Epoch: 27 [174080/209539 (83%)]\tAll Loss: 1.7379\tTriple Loss(0): 0.3764\tClassification Loss: 0.9852\r\n",
      "Train Epoch: 27 [174720/209539 (83%)]\tAll Loss: 2.1317\tTriple Loss(0): 0.4685\tClassification Loss: 1.1947\r\n",
      "Train Epoch: 27 [175360/209539 (84%)]\tAll Loss: 1.3260\tTriple Loss(1): 0.0000\tClassification Loss: 1.3260\r\n",
      "Train Epoch: 27 [176000/209539 (84%)]\tAll Loss: 1.2528\tTriple Loss(1): 0.0000\tClassification Loss: 1.2528\r\n",
      "Train Epoch: 27 [176640/209539 (84%)]\tAll Loss: 0.9121\tTriple Loss(1): 0.0226\tClassification Loss: 0.8669\r\n",
      "Train Epoch: 27 [177280/209539 (85%)]\tAll Loss: 1.2926\tTriple Loss(1): 0.0004\tClassification Loss: 1.2918\r\n",
      "Train Epoch: 27 [177920/209539 (85%)]\tAll Loss: 1.5123\tTriple Loss(1): 0.0230\tClassification Loss: 1.4663\r\n",
      "Train Epoch: 27 [178560/209539 (85%)]\tAll Loss: 1.3763\tTriple Loss(1): 0.0199\tClassification Loss: 1.3364\r\n",
      "Train Epoch: 27 [179200/209539 (86%)]\tAll Loss: 1.5342\tTriple Loss(1): 0.1010\tClassification Loss: 1.3323\r\n",
      "Train Epoch: 27 [179840/209539 (86%)]\tAll Loss: 1.4508\tTriple Loss(1): 0.0798\tClassification Loss: 1.2912\r\n",
      "Train Epoch: 27 [180480/209539 (86%)]\tAll Loss: 1.9035\tTriple Loss(0): 0.2900\tClassification Loss: 1.3235\r\n",
      "Train Epoch: 27 [181120/209539 (86%)]\tAll Loss: 1.4817\tTriple Loss(1): 0.0096\tClassification Loss: 1.4625\r\n",
      "Train Epoch: 27 [181760/209539 (87%)]\tAll Loss: 1.2908\tTriple Loss(1): 0.0925\tClassification Loss: 1.1058\r\n",
      "Train Epoch: 27 [182400/209539 (87%)]\tAll Loss: 1.6590\tTriple Loss(1): 0.0000\tClassification Loss: 1.6590\r\n",
      "Train Epoch: 27 [183040/209539 (87%)]\tAll Loss: 1.5987\tTriple Loss(1): 0.0503\tClassification Loss: 1.4980\r\n",
      "Train Epoch: 27 [183680/209539 (88%)]\tAll Loss: 1.2864\tTriple Loss(1): 0.0620\tClassification Loss: 1.1624\r\n",
      "Train Epoch: 27 [184320/209539 (88%)]\tAll Loss: 1.2979\tTriple Loss(1): 0.0020\tClassification Loss: 1.2939\r\n",
      "Train Epoch: 27 [184960/209539 (88%)]\tAll Loss: 1.0159\tTriple Loss(1): 0.0000\tClassification Loss: 1.0159\r\n",
      "Train Epoch: 27 [185600/209539 (89%)]\tAll Loss: 1.3060\tTriple Loss(1): 0.0311\tClassification Loss: 1.2438\r\n",
      "Train Epoch: 27 [186240/209539 (89%)]\tAll Loss: 2.2938\tTriple Loss(0): 0.5104\tClassification Loss: 1.2729\r\n",
      "Train Epoch: 27 [186880/209539 (89%)]\tAll Loss: 1.3459\tTriple Loss(1): 0.0191\tClassification Loss: 1.3077\r\n",
      "Train Epoch: 27 [187520/209539 (89%)]\tAll Loss: 1.5384\tTriple Loss(1): 0.1544\tClassification Loss: 1.2297\r\n",
      "Train Epoch: 27 [188160/209539 (90%)]\tAll Loss: 1.1050\tTriple Loss(1): 0.0466\tClassification Loss: 1.0117\r\n",
      "Train Epoch: 27 [188800/209539 (90%)]\tAll Loss: 1.2274\tTriple Loss(1): 0.0406\tClassification Loss: 1.1461\r\n",
      "Train Epoch: 27 [189440/209539 (90%)]\tAll Loss: 1.0648\tTriple Loss(1): 0.0307\tClassification Loss: 1.0033\r\n",
      "Train Epoch: 27 [190080/209539 (91%)]\tAll Loss: 1.8143\tTriple Loss(0): 0.2167\tClassification Loss: 1.3810\r\n",
      "Train Epoch: 27 [190720/209539 (91%)]\tAll Loss: 1.1587\tTriple Loss(1): 0.0000\tClassification Loss: 1.1587\r\n",
      "Train Epoch: 27 [191360/209539 (91%)]\tAll Loss: 1.8312\tTriple Loss(0): 0.3621\tClassification Loss: 1.1069\r\n",
      "Train Epoch: 27 [192000/209539 (92%)]\tAll Loss: 1.5628\tTriple Loss(1): 0.0253\tClassification Loss: 1.5123\r\n",
      "Train Epoch: 27 [192640/209539 (92%)]\tAll Loss: 1.4386\tTriple Loss(0): 0.1611\tClassification Loss: 1.1164\r\n",
      "Train Epoch: 27 [193280/209539 (92%)]\tAll Loss: 1.1844\tTriple Loss(1): 0.0159\tClassification Loss: 1.1527\r\n",
      "Train Epoch: 27 [193920/209539 (93%)]\tAll Loss: 1.3292\tTriple Loss(1): 0.0001\tClassification Loss: 1.3289\r\n",
      "Train Epoch: 27 [194560/209539 (93%)]\tAll Loss: 1.1772\tTriple Loss(1): 0.0210\tClassification Loss: 1.1352\r\n",
      "Train Epoch: 27 [195200/209539 (93%)]\tAll Loss: 1.4701\tTriple Loss(1): 0.0239\tClassification Loss: 1.4223\r\n",
      "Train Epoch: 27 [195840/209539 (93%)]\tAll Loss: 1.7069\tTriple Loss(0): 0.4557\tClassification Loss: 0.7956\r\n",
      "Train Epoch: 27 [196480/209539 (94%)]\tAll Loss: 1.5768\tTriple Loss(1): 0.0099\tClassification Loss: 1.5569\r\n",
      "Train Epoch: 27 [197120/209539 (94%)]\tAll Loss: 1.3727\tTriple Loss(1): 0.0678\tClassification Loss: 1.2372\r\n",
      "Train Epoch: 27 [197760/209539 (94%)]\tAll Loss: 1.4412\tTriple Loss(1): 0.0506\tClassification Loss: 1.3401\r\n",
      "Train Epoch: 27 [198400/209539 (95%)]\tAll Loss: 1.8855\tTriple Loss(0): 0.3231\tClassification Loss: 1.2394\r\n",
      "Train Epoch: 27 [199040/209539 (95%)]\tAll Loss: 1.6157\tTriple Loss(1): 0.0252\tClassification Loss: 1.5652\r\n",
      "Train Epoch: 27 [199680/209539 (95%)]\tAll Loss: 1.2158\tTriple Loss(1): 0.0866\tClassification Loss: 1.0426\r\n",
      "Train Epoch: 27 [200320/209539 (96%)]\tAll Loss: 1.2593\tTriple Loss(1): 0.0000\tClassification Loss: 1.2593\r\n",
      "Train Epoch: 27 [200960/209539 (96%)]\tAll Loss: 1.0938\tTriple Loss(1): 0.0071\tClassification Loss: 1.0795\r\n",
      "Train Epoch: 27 [201600/209539 (96%)]\tAll Loss: 1.2367\tTriple Loss(1): 0.0033\tClassification Loss: 1.2300\r\n",
      "Train Epoch: 27 [202240/209539 (97%)]\tAll Loss: 2.0784\tTriple Loss(0): 0.4665\tClassification Loss: 1.1454\r\n",
      "Train Epoch: 27 [202880/209539 (97%)]\tAll Loss: 2.0142\tTriple Loss(0): 0.5293\tClassification Loss: 0.9555\r\n",
      "Train Epoch: 27 [203520/209539 (97%)]\tAll Loss: 1.4191\tTriple Loss(1): 0.0207\tClassification Loss: 1.3776\r\n",
      "Train Epoch: 27 [204160/209539 (97%)]\tAll Loss: 2.0568\tTriple Loss(0): 0.2196\tClassification Loss: 1.6176\r\n",
      "Train Epoch: 27 [204800/209539 (98%)]\tAll Loss: 1.5454\tTriple Loss(1): 0.0043\tClassification Loss: 1.5368\r\n",
      "Train Epoch: 27 [205440/209539 (98%)]\tAll Loss: 1.0259\tTriple Loss(1): 0.0348\tClassification Loss: 0.9563\r\n",
      "Train Epoch: 27 [206080/209539 (98%)]\tAll Loss: 1.8060\tTriple Loss(0): 0.2806\tClassification Loss: 1.2449\r\n",
      "Train Epoch: 27 [206720/209539 (99%)]\tAll Loss: 1.2263\tTriple Loss(1): 0.0223\tClassification Loss: 1.1817\r\n",
      "Train Epoch: 27 [207360/209539 (99%)]\tAll Loss: 1.0028\tTriple Loss(1): 0.0235\tClassification Loss: 0.9558\r\n",
      "Train Epoch: 27 [208000/209539 (99%)]\tAll Loss: 1.0936\tTriple Loss(1): 0.0025\tClassification Loss: 1.0887\r\n",
      "Train Epoch: 27 [208640/209539 (100%)]\tAll Loss: 1.3715\tTriple Loss(1): 0.0354\tClassification Loss: 1.3006\r\n",
      "Train Epoch: 27 [209280/209539 (100%)]\tAll Loss: 1.3489\tTriple Loss(1): 0.0187\tClassification Loss: 1.3115\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/27_epochs\r\n",
      "Train Epoch: 28 [0/209539 (0%)]\tAll Loss: 1.8386\tTriple Loss(1): 0.1823\tClassification Loss: 1.4741\r\n",
      "\r\n",
      "Test set: Average loss: 1.1099\r\n",
      "Top 1 Accuracy: 54186/80128 (68%)\r\n",
      "Top 3 Accuracy: 69738/80128 (87%)\r\n",
      "Top 5 Accuracy: 74423/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 28 [640/209539 (0%)]\tAll Loss: 1.9222\tTriple Loss(0): 0.2635\tClassification Loss: 1.3952\r\n",
      "Train Epoch: 28 [1280/209539 (1%)]\tAll Loss: 1.6503\tTriple Loss(0): 0.3127\tClassification Loss: 1.0249\r\n",
      "Train Epoch: 28 [1920/209539 (1%)]\tAll Loss: 1.3523\tTriple Loss(1): 0.0465\tClassification Loss: 1.2593\r\n",
      "Train Epoch: 28 [2560/209539 (1%)]\tAll Loss: 1.5311\tTriple Loss(1): 0.0000\tClassification Loss: 1.5311\r\n",
      "Train Epoch: 28 [3200/209539 (2%)]\tAll Loss: 2.2047\tTriple Loss(0): 0.4077\tClassification Loss: 1.3892\r\n",
      "Train Epoch: 28 [3840/209539 (2%)]\tAll Loss: 1.2603\tTriple Loss(1): 0.0502\tClassification Loss: 1.1598\r\n",
      "Train Epoch: 28 [4480/209539 (2%)]\tAll Loss: 1.3067\tTriple Loss(1): 0.0000\tClassification Loss: 1.3067\r\n",
      "Train Epoch: 28 [5120/209539 (2%)]\tAll Loss: 1.2804\tTriple Loss(1): 0.0028\tClassification Loss: 1.2747\r\n",
      "Train Epoch: 28 [5760/209539 (3%)]\tAll Loss: 1.4482\tTriple Loss(1): 0.0451\tClassification Loss: 1.3581\r\n",
      "Train Epoch: 28 [6400/209539 (3%)]\tAll Loss: 1.5980\tTriple Loss(0): 0.3259\tClassification Loss: 0.9463\r\n",
      "Train Epoch: 28 [7040/209539 (3%)]\tAll Loss: 1.0088\tTriple Loss(1): 0.0273\tClassification Loss: 0.9541\r\n",
      "Train Epoch: 28 [7680/209539 (4%)]\tAll Loss: 1.0307\tTriple Loss(1): 0.0259\tClassification Loss: 0.9788\r\n",
      "Train Epoch: 28 [8320/209539 (4%)]\tAll Loss: 1.2707\tTriple Loss(1): 0.0085\tClassification Loss: 1.2537\r\n",
      "Train Epoch: 28 [8960/209539 (4%)]\tAll Loss: 2.0788\tTriple Loss(0): 0.4621\tClassification Loss: 1.1545\r\n",
      "Train Epoch: 28 [9600/209539 (5%)]\tAll Loss: 1.4289\tTriple Loss(1): 0.0000\tClassification Loss: 1.4289\r\n",
      "Train Epoch: 28 [10240/209539 (5%)]\tAll Loss: 1.1513\tTriple Loss(1): 0.0211\tClassification Loss: 1.1091\r\n",
      "Train Epoch: 28 [10880/209539 (5%)]\tAll Loss: 1.1969\tTriple Loss(1): 0.0342\tClassification Loss: 1.1285\r\n",
      "Train Epoch: 28 [11520/209539 (5%)]\tAll Loss: 1.2565\tTriple Loss(1): 0.0059\tClassification Loss: 1.2447\r\n",
      "Train Epoch: 28 [12160/209539 (6%)]\tAll Loss: 1.1045\tTriple Loss(1): 0.0092\tClassification Loss: 1.0860\r\n",
      "Train Epoch: 28 [12800/209539 (6%)]\tAll Loss: 0.9947\tTriple Loss(1): 0.0148\tClassification Loss: 0.9650\r\n",
      "Train Epoch: 28 [13440/209539 (6%)]\tAll Loss: 1.1517\tTriple Loss(1): 0.0255\tClassification Loss: 1.1007\r\n",
      "Train Epoch: 28 [14080/209539 (7%)]\tAll Loss: 1.2875\tTriple Loss(1): 0.0073\tClassification Loss: 1.2728\r\n",
      "Train Epoch: 28 [14720/209539 (7%)]\tAll Loss: 1.1587\tTriple Loss(1): 0.0133\tClassification Loss: 1.1322\r\n",
      "Train Epoch: 28 [15360/209539 (7%)]\tAll Loss: 1.2662\tTriple Loss(1): 0.0361\tClassification Loss: 1.1940\r\n",
      "Train Epoch: 28 [16000/209539 (8%)]\tAll Loss: 1.8253\tTriple Loss(0): 0.1606\tClassification Loss: 1.5042\r\n",
      "Train Epoch: 28 [16640/209539 (8%)]\tAll Loss: 1.6117\tTriple Loss(1): 0.0151\tClassification Loss: 1.5816\r\n",
      "Train Epoch: 28 [17280/209539 (8%)]\tAll Loss: 1.4209\tTriple Loss(1): 0.0676\tClassification Loss: 1.2857\r\n",
      "Train Epoch: 28 [17920/209539 (9%)]\tAll Loss: 1.7038\tTriple Loss(0): 0.2799\tClassification Loss: 1.1440\r\n",
      "Train Epoch: 28 [18560/209539 (9%)]\tAll Loss: 1.2104\tTriple Loss(1): 0.0591\tClassification Loss: 1.0922\r\n",
      "Train Epoch: 28 [19200/209539 (9%)]\tAll Loss: 1.2412\tTriple Loss(1): 0.0257\tClassification Loss: 1.1899\r\n",
      "Train Epoch: 28 [19840/209539 (9%)]\tAll Loss: 1.1905\tTriple Loss(1): 0.0104\tClassification Loss: 1.1698\r\n",
      "Train Epoch: 28 [20480/209539 (10%)]\tAll Loss: 1.1848\tTriple Loss(1): 0.0348\tClassification Loss: 1.1152\r\n",
      "Train Epoch: 28 [21120/209539 (10%)]\tAll Loss: 1.3685\tTriple Loss(1): 0.0530\tClassification Loss: 1.2626\r\n",
      "Train Epoch: 28 [21760/209539 (10%)]\tAll Loss: 1.2278\tTriple Loss(1): 0.0309\tClassification Loss: 1.1659\r\n",
      "Train Epoch: 28 [22400/209539 (11%)]\tAll Loss: 1.6793\tTriple Loss(0): 0.2902\tClassification Loss: 1.0989\r\n",
      "Train Epoch: 28 [23040/209539 (11%)]\tAll Loss: 1.1839\tTriple Loss(1): 0.0000\tClassification Loss: 1.1839\r\n",
      "Train Epoch: 28 [23680/209539 (11%)]\tAll Loss: 1.0445\tTriple Loss(1): 0.0000\tClassification Loss: 1.0445\r\n",
      "Train Epoch: 28 [24320/209539 (12%)]\tAll Loss: 1.3406\tTriple Loss(1): 0.0070\tClassification Loss: 1.3266\r\n",
      "Train Epoch: 28 [24960/209539 (12%)]\tAll Loss: 1.2151\tTriple Loss(1): 0.0000\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 28 [25600/209539 (12%)]\tAll Loss: 1.2720\tTriple Loss(1): 0.0379\tClassification Loss: 1.1963\r\n",
      "Train Epoch: 28 [26240/209539 (13%)]\tAll Loss: 1.0523\tTriple Loss(1): 0.0000\tClassification Loss: 1.0523\r\n",
      "Train Epoch: 28 [26880/209539 (13%)]\tAll Loss: 1.3375\tTriple Loss(1): 0.0148\tClassification Loss: 1.3078\r\n",
      "Train Epoch: 28 [27520/209539 (13%)]\tAll Loss: 1.3179\tTriple Loss(1): 0.0236\tClassification Loss: 1.2706\r\n",
      "Train Epoch: 28 [28160/209539 (13%)]\tAll Loss: 1.4098\tTriple Loss(1): 0.0183\tClassification Loss: 1.3733\r\n",
      "Train Epoch: 28 [28800/209539 (14%)]\tAll Loss: 1.8634\tTriple Loss(0): 0.2574\tClassification Loss: 1.3486\r\n",
      "Train Epoch: 28 [29440/209539 (14%)]\tAll Loss: 1.9746\tTriple Loss(0): 0.3628\tClassification Loss: 1.2491\r\n",
      "Train Epoch: 28 [30080/209539 (14%)]\tAll Loss: 1.1052\tTriple Loss(1): 0.0572\tClassification Loss: 0.9907\r\n",
      "Train Epoch: 28 [30720/209539 (15%)]\tAll Loss: 1.1046\tTriple Loss(1): 0.0000\tClassification Loss: 1.1046\r\n",
      "Train Epoch: 28 [31360/209539 (15%)]\tAll Loss: 1.3816\tTriple Loss(1): 0.0812\tClassification Loss: 1.2193\r\n",
      "Train Epoch: 28 [32000/209539 (15%)]\tAll Loss: 1.2507\tTriple Loss(1): 0.0036\tClassification Loss: 1.2434\r\n",
      "Train Epoch: 28 [32640/209539 (16%)]\tAll Loss: 1.3618\tTriple Loss(1): 0.0174\tClassification Loss: 1.3270\r\n",
      "Train Epoch: 28 [33280/209539 (16%)]\tAll Loss: 1.3100\tTriple Loss(1): 0.0231\tClassification Loss: 1.2637\r\n",
      "Train Epoch: 28 [33920/209539 (16%)]\tAll Loss: 1.4013\tTriple Loss(1): 0.0639\tClassification Loss: 1.2735\r\n",
      "Train Epoch: 28 [34560/209539 (16%)]\tAll Loss: 1.0740\tTriple Loss(1): 0.0000\tClassification Loss: 1.0740\r\n",
      "Train Epoch: 28 [35200/209539 (17%)]\tAll Loss: 1.1750\tTriple Loss(1): 0.0040\tClassification Loss: 1.1671\r\n",
      "Train Epoch: 28 [35840/209539 (17%)]\tAll Loss: 1.2580\tTriple Loss(1): 0.0238\tClassification Loss: 1.2104\r\n",
      "Train Epoch: 28 [36480/209539 (17%)]\tAll Loss: 0.9694\tTriple Loss(1): 0.0082\tClassification Loss: 0.9530\r\n",
      "Train Epoch: 28 [37120/209539 (18%)]\tAll Loss: 1.5470\tTriple Loss(1): 0.0571\tClassification Loss: 1.4328\r\n",
      "Train Epoch: 28 [37760/209539 (18%)]\tAll Loss: 1.8115\tTriple Loss(0): 0.2500\tClassification Loss: 1.3114\r\n",
      "Train Epoch: 28 [38400/209539 (18%)]\tAll Loss: 1.3986\tTriple Loss(1): 0.0078\tClassification Loss: 1.3830\r\n",
      "Train Epoch: 28 [39040/209539 (19%)]\tAll Loss: 0.9626\tTriple Loss(1): 0.0666\tClassification Loss: 0.8295\r\n",
      "Train Epoch: 28 [39680/209539 (19%)]\tAll Loss: 1.1306\tTriple Loss(1): 0.0026\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 28 [40320/209539 (19%)]\tAll Loss: 1.2602\tTriple Loss(1): 0.0275\tClassification Loss: 1.2053\r\n",
      "Train Epoch: 28 [40960/209539 (20%)]\tAll Loss: 1.8627\tTriple Loss(0): 0.3668\tClassification Loss: 1.1291\r\n",
      "Train Epoch: 28 [41600/209539 (20%)]\tAll Loss: 1.1193\tTriple Loss(1): 0.0015\tClassification Loss: 1.1162\r\n",
      "Train Epoch: 28 [42240/209539 (20%)]\tAll Loss: 1.3008\tTriple Loss(1): 0.0298\tClassification Loss: 1.2412\r\n",
      "Train Epoch: 28 [42880/209539 (20%)]\tAll Loss: 1.1391\tTriple Loss(1): 0.0537\tClassification Loss: 1.0318\r\n",
      "Train Epoch: 28 [43520/209539 (21%)]\tAll Loss: 1.6245\tTriple Loss(1): 0.0905\tClassification Loss: 1.4436\r\n",
      "Train Epoch: 28 [44160/209539 (21%)]\tAll Loss: 1.6881\tTriple Loss(1): 0.0182\tClassification Loss: 1.6517\r\n",
      "Train Epoch: 28 [44800/209539 (21%)]\tAll Loss: 1.4192\tTriple Loss(1): 0.0433\tClassification Loss: 1.3326\r\n",
      "Train Epoch: 28 [45440/209539 (22%)]\tAll Loss: 1.6706\tTriple Loss(1): 0.0295\tClassification Loss: 1.6116\r\n",
      "Train Epoch: 28 [46080/209539 (22%)]\tAll Loss: 1.5813\tTriple Loss(0): 0.1963\tClassification Loss: 1.1886\r\n",
      "Train Epoch: 28 [46720/209539 (22%)]\tAll Loss: 2.0402\tTriple Loss(0): 0.2380\tClassification Loss: 1.5642\r\n",
      "Train Epoch: 28 [47360/209539 (23%)]\tAll Loss: 0.9235\tTriple Loss(1): 0.0558\tClassification Loss: 0.8119\r\n",
      "Train Epoch: 28 [48000/209539 (23%)]\tAll Loss: 1.5898\tTriple Loss(0): 0.2079\tClassification Loss: 1.1740\r\n",
      "Train Epoch: 28 [48640/209539 (23%)]\tAll Loss: 1.5914\tTriple Loss(1): 0.0000\tClassification Loss: 1.5914\r\n",
      "Train Epoch: 28 [49280/209539 (24%)]\tAll Loss: 1.7749\tTriple Loss(0): 0.3089\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 28 [49920/209539 (24%)]\tAll Loss: 1.3997\tTriple Loss(1): 0.0000\tClassification Loss: 1.3997\r\n",
      "Train Epoch: 28 [50560/209539 (24%)]\tAll Loss: 1.4388\tTriple Loss(1): 0.0454\tClassification Loss: 1.3479\r\n",
      "Train Epoch: 28 [51200/209539 (24%)]\tAll Loss: 1.4215\tTriple Loss(1): 0.0000\tClassification Loss: 1.4215\r\n",
      "Train Epoch: 28 [51840/209539 (25%)]\tAll Loss: 1.3276\tTriple Loss(1): 0.0370\tClassification Loss: 1.2536\r\n",
      "Train Epoch: 28 [52480/209539 (25%)]\tAll Loss: 1.4702\tTriple Loss(1): 0.0950\tClassification Loss: 1.2802\r\n",
      "Train Epoch: 28 [53120/209539 (25%)]\tAll Loss: 1.3864\tTriple Loss(1): 0.0702\tClassification Loss: 1.2461\r\n",
      "Train Epoch: 28 [53760/209539 (26%)]\tAll Loss: 1.5571\tTriple Loss(1): 0.0178\tClassification Loss: 1.5214\r\n",
      "Train Epoch: 28 [54400/209539 (26%)]\tAll Loss: 1.8852\tTriple Loss(1): 0.0522\tClassification Loss: 1.7809\r\n",
      "Train Epoch: 28 [55040/209539 (26%)]\tAll Loss: 1.1866\tTriple Loss(1): 0.0245\tClassification Loss: 1.1375\r\n",
      "Train Epoch: 28 [55680/209539 (27%)]\tAll Loss: 1.3842\tTriple Loss(1): 0.0368\tClassification Loss: 1.3107\r\n",
      "Train Epoch: 28 [56320/209539 (27%)]\tAll Loss: 1.5448\tTriple Loss(0): 0.2797\tClassification Loss: 0.9855\r\n",
      "Train Epoch: 28 [56960/209539 (27%)]\tAll Loss: 1.2307\tTriple Loss(1): 0.0101\tClassification Loss: 1.2105\r\n",
      "Train Epoch: 28 [57600/209539 (27%)]\tAll Loss: 1.8961\tTriple Loss(0): 0.4234\tClassification Loss: 1.0493\r\n",
      "Train Epoch: 28 [58240/209539 (28%)]\tAll Loss: 1.1259\tTriple Loss(1): 0.0000\tClassification Loss: 1.1259\r\n",
      "Train Epoch: 28 [58880/209539 (28%)]\tAll Loss: 1.3291\tTriple Loss(1): 0.0137\tClassification Loss: 1.3018\r\n",
      "Train Epoch: 28 [59520/209539 (28%)]\tAll Loss: 1.3083\tTriple Loss(1): 0.0892\tClassification Loss: 1.1298\r\n",
      "Train Epoch: 28 [60160/209539 (29%)]\tAll Loss: 1.7854\tTriple Loss(0): 0.2196\tClassification Loss: 1.3462\r\n",
      "Train Epoch: 28 [60800/209539 (29%)]\tAll Loss: 1.3834\tTriple Loss(1): 0.0083\tClassification Loss: 1.3669\r\n",
      "Train Epoch: 28 [61440/209539 (29%)]\tAll Loss: 1.2011\tTriple Loss(1): 0.0000\tClassification Loss: 1.2011\r\n",
      "Train Epoch: 28 [62080/209539 (30%)]\tAll Loss: 1.3665\tTriple Loss(1): 0.0046\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 28 [62720/209539 (30%)]\tAll Loss: 1.3022\tTriple Loss(0): 0.1093\tClassification Loss: 1.0837\r\n",
      "Train Epoch: 28 [63360/209539 (30%)]\tAll Loss: 1.4081\tTriple Loss(1): 0.0591\tClassification Loss: 1.2900\r\n",
      "Train Epoch: 28 [64000/209539 (31%)]\tAll Loss: 1.4354\tTriple Loss(1): 0.0311\tClassification Loss: 1.3733\r\n",
      "Train Epoch: 28 [64640/209539 (31%)]\tAll Loss: 1.6512\tTriple Loss(1): 0.0000\tClassification Loss: 1.6512\r\n",
      "Train Epoch: 28 [65280/209539 (31%)]\tAll Loss: 1.9767\tTriple Loss(0): 0.3184\tClassification Loss: 1.3399\r\n",
      "Train Epoch: 28 [65920/209539 (31%)]\tAll Loss: 1.7157\tTriple Loss(1): 0.0495\tClassification Loss: 1.6167\r\n",
      "Train Epoch: 28 [66560/209539 (32%)]\tAll Loss: 1.2887\tTriple Loss(1): 0.0204\tClassification Loss: 1.2479\r\n",
      "Train Epoch: 28 [67200/209539 (32%)]\tAll Loss: 1.2251\tTriple Loss(1): 0.0116\tClassification Loss: 1.2019\r\n",
      "Train Epoch: 28 [67840/209539 (32%)]\tAll Loss: 1.0964\tTriple Loss(1): 0.0059\tClassification Loss: 1.0846\r\n",
      "Train Epoch: 28 [68480/209539 (33%)]\tAll Loss: 2.1903\tTriple Loss(0): 0.3983\tClassification Loss: 1.3936\r\n",
      "Train Epoch: 28 [69120/209539 (33%)]\tAll Loss: 1.4434\tTriple Loss(1): 0.0012\tClassification Loss: 1.4411\r\n",
      "Train Epoch: 28 [69760/209539 (33%)]\tAll Loss: 1.0234\tTriple Loss(1): 0.0230\tClassification Loss: 0.9774\r\n",
      "Train Epoch: 28 [70400/209539 (34%)]\tAll Loss: 1.1697\tTriple Loss(1): 0.0000\tClassification Loss: 1.1697\r\n",
      "Train Epoch: 28 [71040/209539 (34%)]\tAll Loss: 1.6639\tTriple Loss(0): 0.1755\tClassification Loss: 1.3129\r\n",
      "Train Epoch: 28 [71680/209539 (34%)]\tAll Loss: 1.5095\tTriple Loss(1): 0.0208\tClassification Loss: 1.4679\r\n",
      "Train Epoch: 28 [72320/209539 (35%)]\tAll Loss: 1.2601\tTriple Loss(1): 0.0419\tClassification Loss: 1.1763\r\n",
      "Train Epoch: 28 [72960/209539 (35%)]\tAll Loss: 1.7754\tTriple Loss(0): 0.2651\tClassification Loss: 1.2451\r\n",
      "Train Epoch: 28 [73600/209539 (35%)]\tAll Loss: 1.2516\tTriple Loss(1): 0.0217\tClassification Loss: 1.2083\r\n",
      "Train Epoch: 28 [74240/209539 (35%)]\tAll Loss: 1.5020\tTriple Loss(1): 0.0189\tClassification Loss: 1.4641\r\n",
      "Train Epoch: 28 [74880/209539 (36%)]\tAll Loss: 1.7440\tTriple Loss(1): 0.0158\tClassification Loss: 1.7124\r\n",
      "Train Epoch: 28 [75520/209539 (36%)]\tAll Loss: 0.9970\tTriple Loss(1): 0.0442\tClassification Loss: 0.9086\r\n",
      "Train Epoch: 28 [76160/209539 (36%)]\tAll Loss: 1.0641\tTriple Loss(1): 0.0000\tClassification Loss: 1.0641\r\n",
      "Train Epoch: 28 [76800/209539 (37%)]\tAll Loss: 1.1801\tTriple Loss(1): 0.0132\tClassification Loss: 1.1537\r\n",
      "Train Epoch: 28 [77440/209539 (37%)]\tAll Loss: 1.4292\tTriple Loss(1): 0.1073\tClassification Loss: 1.2147\r\n",
      "Train Epoch: 28 [78080/209539 (37%)]\tAll Loss: 1.4265\tTriple Loss(1): 0.0185\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 28 [78720/209539 (38%)]\tAll Loss: 1.5715\tTriple Loss(1): 0.0441\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 28 [79360/209539 (38%)]\tAll Loss: 1.2521\tTriple Loss(1): 0.0055\tClassification Loss: 1.2411\r\n",
      "Train Epoch: 28 [80000/209539 (38%)]\tAll Loss: 1.4745\tTriple Loss(1): 0.0789\tClassification Loss: 1.3167\r\n",
      "Train Epoch: 28 [80640/209539 (38%)]\tAll Loss: 1.3551\tTriple Loss(1): 0.0380\tClassification Loss: 1.2790\r\n",
      "Train Epoch: 28 [81280/209539 (39%)]\tAll Loss: 1.3660\tTriple Loss(1): 0.0196\tClassification Loss: 1.3268\r\n",
      "Train Epoch: 28 [81920/209539 (39%)]\tAll Loss: 1.2795\tTriple Loss(1): 0.0913\tClassification Loss: 1.0970\r\n",
      "Train Epoch: 28 [82560/209539 (39%)]\tAll Loss: 1.3530\tTriple Loss(1): 0.0414\tClassification Loss: 1.2703\r\n",
      "Train Epoch: 28 [83200/209539 (40%)]\tAll Loss: 1.4818\tTriple Loss(1): 0.0280\tClassification Loss: 1.4257\r\n",
      "Train Epoch: 28 [83840/209539 (40%)]\tAll Loss: 1.8018\tTriple Loss(0): 0.2476\tClassification Loss: 1.3067\r\n",
      "Train Epoch: 28 [84480/209539 (40%)]\tAll Loss: 1.1991\tTriple Loss(1): 0.0236\tClassification Loss: 1.1519\r\n",
      "Train Epoch: 28 [85120/209539 (41%)]\tAll Loss: 1.7442\tTriple Loss(1): 0.0216\tClassification Loss: 1.7009\r\n",
      "Train Epoch: 28 [85760/209539 (41%)]\tAll Loss: 1.3857\tTriple Loss(1): 0.0390\tClassification Loss: 1.3077\r\n",
      "Train Epoch: 28 [86400/209539 (41%)]\tAll Loss: 2.1001\tTriple Loss(0): 0.3423\tClassification Loss: 1.4154\r\n",
      "Train Epoch: 28 [87040/209539 (42%)]\tAll Loss: 1.2582\tTriple Loss(1): 0.0308\tClassification Loss: 1.1967\r\n",
      "Train Epoch: 28 [87680/209539 (42%)]\tAll Loss: 1.2368\tTriple Loss(1): 0.0301\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 28 [88320/209539 (42%)]\tAll Loss: 1.1453\tTriple Loss(1): 0.0284\tClassification Loss: 1.0885\r\n",
      "Train Epoch: 28 [88960/209539 (42%)]\tAll Loss: 1.3331\tTriple Loss(1): 0.0442\tClassification Loss: 1.2447\r\n",
      "Train Epoch: 28 [89600/209539 (43%)]\tAll Loss: 1.3867\tTriple Loss(1): 0.0249\tClassification Loss: 1.3369\r\n",
      "Train Epoch: 28 [90240/209539 (43%)]\tAll Loss: 1.1388\tTriple Loss(1): 0.0353\tClassification Loss: 1.0683\r\n",
      "Train Epoch: 28 [90880/209539 (43%)]\tAll Loss: 1.7436\tTriple Loss(1): 0.0349\tClassification Loss: 1.6737\r\n",
      "Train Epoch: 28 [91520/209539 (44%)]\tAll Loss: 1.2854\tTriple Loss(1): 0.0145\tClassification Loss: 1.2564\r\n",
      "Train Epoch: 28 [92160/209539 (44%)]\tAll Loss: 1.8381\tTriple Loss(0): 0.3033\tClassification Loss: 1.2314\r\n",
      "Train Epoch: 28 [92800/209539 (44%)]\tAll Loss: 1.1794\tTriple Loss(1): 0.0131\tClassification Loss: 1.1532\r\n",
      "Train Epoch: 28 [93440/209539 (45%)]\tAll Loss: 1.2580\tTriple Loss(1): 0.0021\tClassification Loss: 1.2538\r\n",
      "Train Epoch: 28 [94080/209539 (45%)]\tAll Loss: 1.2812\tTriple Loss(1): 0.0042\tClassification Loss: 1.2729\r\n",
      "Train Epoch: 28 [94720/209539 (45%)]\tAll Loss: 1.4452\tTriple Loss(1): 0.0155\tClassification Loss: 1.4143\r\n",
      "Train Epoch: 28 [95360/209539 (46%)]\tAll Loss: 1.7349\tTriple Loss(0): 0.2048\tClassification Loss: 1.3254\r\n",
      "Train Epoch: 28 [96000/209539 (46%)]\tAll Loss: 2.0713\tTriple Loss(0): 0.3655\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 28 [96640/209539 (46%)]\tAll Loss: 1.5312\tTriple Loss(1): 0.0159\tClassification Loss: 1.4993\r\n",
      "Train Epoch: 28 [97280/209539 (46%)]\tAll Loss: 1.1157\tTriple Loss(1): 0.0005\tClassification Loss: 1.1146\r\n",
      "Train Epoch: 28 [97920/209539 (47%)]\tAll Loss: 1.3392\tTriple Loss(1): 0.0301\tClassification Loss: 1.2791\r\n",
      "Train Epoch: 28 [98560/209539 (47%)]\tAll Loss: 1.7776\tTriple Loss(1): 0.0463\tClassification Loss: 1.6849\r\n",
      "Train Epoch: 28 [99200/209539 (47%)]\tAll Loss: 1.1462\tTriple Loss(1): 0.0265\tClassification Loss: 1.0932\r\n",
      "Train Epoch: 28 [99840/209539 (48%)]\tAll Loss: 1.3515\tTriple Loss(1): 0.0263\tClassification Loss: 1.2989\r\n",
      "Train Epoch: 28 [100480/209539 (48%)]\tAll Loss: 1.3784\tTriple Loss(1): 0.0325\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 28 [101120/209539 (48%)]\tAll Loss: 1.6320\tTriple Loss(0): 0.2173\tClassification Loss: 1.1973\r\n",
      "Train Epoch: 28 [101760/209539 (49%)]\tAll Loss: 1.3174\tTriple Loss(1): 0.0000\tClassification Loss: 1.3174\r\n",
      "Train Epoch: 28 [102400/209539 (49%)]\tAll Loss: 1.0444\tTriple Loss(1): 0.0139\tClassification Loss: 1.0166\r\n",
      "Train Epoch: 28 [103040/209539 (49%)]\tAll Loss: 1.3912\tTriple Loss(1): 0.0627\tClassification Loss: 1.2657\r\n",
      "Train Epoch: 28 [103680/209539 (49%)]\tAll Loss: 1.2155\tTriple Loss(1): 0.0120\tClassification Loss: 1.1915\r\n",
      "Train Epoch: 28 [104320/209539 (50%)]\tAll Loss: 1.1308\tTriple Loss(1): 0.0341\tClassification Loss: 1.0626\r\n",
      "Train Epoch: 28 [104960/209539 (50%)]\tAll Loss: 1.3827\tTriple Loss(1): 0.0660\tClassification Loss: 1.2506\r\n",
      "Train Epoch: 28 [105600/209539 (50%)]\tAll Loss: 1.2826\tTriple Loss(1): 0.0142\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 28 [106240/209539 (51%)]\tAll Loss: 1.2450\tTriple Loss(1): 0.0232\tClassification Loss: 1.1985\r\n",
      "Train Epoch: 28 [106880/209539 (51%)]\tAll Loss: 1.2340\tTriple Loss(1): 0.0259\tClassification Loss: 1.1821\r\n",
      "Train Epoch: 28 [107520/209539 (51%)]\tAll Loss: 1.5514\tTriple Loss(0): 0.2168\tClassification Loss: 1.1177\r\n",
      "Train Epoch: 28 [108160/209539 (52%)]\tAll Loss: 1.2697\tTriple Loss(1): 0.0284\tClassification Loss: 1.2128\r\n",
      "Train Epoch: 28 [108800/209539 (52%)]\tAll Loss: 1.4104\tTriple Loss(1): 0.0474\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 28 [109440/209539 (52%)]\tAll Loss: 2.1739\tTriple Loss(0): 0.3388\tClassification Loss: 1.4962\r\n",
      "Train Epoch: 28 [110080/209539 (53%)]\tAll Loss: 1.5252\tTriple Loss(1): 0.0182\tClassification Loss: 1.4887\r\n",
      "Train Epoch: 28 [110720/209539 (53%)]\tAll Loss: 1.5333\tTriple Loss(1): 0.0058\tClassification Loss: 1.5216\r\n",
      "Train Epoch: 28 [111360/209539 (53%)]\tAll Loss: 1.1944\tTriple Loss(1): 0.0784\tClassification Loss: 1.0377\r\n",
      "Train Epoch: 28 [112000/209539 (53%)]\tAll Loss: 1.1814\tTriple Loss(1): 0.0132\tClassification Loss: 1.1551\r\n",
      "Train Epoch: 28 [112640/209539 (54%)]\tAll Loss: 1.2604\tTriple Loss(1): 0.0045\tClassification Loss: 1.2514\r\n",
      "Train Epoch: 28 [113280/209539 (54%)]\tAll Loss: 0.9096\tTriple Loss(1): 0.0161\tClassification Loss: 0.8775\r\n",
      "Train Epoch: 28 [113920/209539 (54%)]\tAll Loss: 1.1884\tTriple Loss(1): 0.0058\tClassification Loss: 1.1767\r\n",
      "Train Epoch: 28 [114560/209539 (55%)]\tAll Loss: 1.2315\tTriple Loss(1): 0.0000\tClassification Loss: 1.2315\r\n",
      "Train Epoch: 28 [115200/209539 (55%)]\tAll Loss: 1.6099\tTriple Loss(1): 0.0614\tClassification Loss: 1.4871\r\n",
      "Train Epoch: 28 [115840/209539 (55%)]\tAll Loss: 1.4351\tTriple Loss(1): 0.0624\tClassification Loss: 1.3104\r\n",
      "Train Epoch: 28 [116480/209539 (56%)]\tAll Loss: 1.8171\tTriple Loss(0): 0.2797\tClassification Loss: 1.2577\r\n",
      "Train Epoch: 28 [117120/209539 (56%)]\tAll Loss: 1.2188\tTriple Loss(1): 0.0637\tClassification Loss: 1.0914\r\n",
      "Train Epoch: 28 [117760/209539 (56%)]\tAll Loss: 2.0034\tTriple Loss(0): 0.3763\tClassification Loss: 1.2508\r\n",
      "Train Epoch: 28 [118400/209539 (57%)]\tAll Loss: 1.1836\tTriple Loss(1): 0.0243\tClassification Loss: 1.1351\r\n",
      "Train Epoch: 28 [119040/209539 (57%)]\tAll Loss: 2.1638\tTriple Loss(0): 0.3727\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 28 [119680/209539 (57%)]\tAll Loss: 1.2604\tTriple Loss(1): 0.0380\tClassification Loss: 1.1843\r\n",
      "Train Epoch: 28 [120320/209539 (57%)]\tAll Loss: 1.2079\tTriple Loss(1): 0.0086\tClassification Loss: 1.1908\r\n",
      "Train Epoch: 28 [120960/209539 (58%)]\tAll Loss: 2.1951\tTriple Loss(0): 0.4189\tClassification Loss: 1.3573\r\n",
      "Train Epoch: 28 [121600/209539 (58%)]\tAll Loss: 1.7799\tTriple Loss(0): 0.2927\tClassification Loss: 1.1945\r\n",
      "Train Epoch: 28 [122240/209539 (58%)]\tAll Loss: 1.1959\tTriple Loss(1): 0.0227\tClassification Loss: 1.1506\r\n",
      "Train Epoch: 28 [122880/209539 (59%)]\tAll Loss: 1.2008\tTriple Loss(1): 0.0362\tClassification Loss: 1.1284\r\n",
      "Train Epoch: 28 [123520/209539 (59%)]\tAll Loss: 1.2938\tTriple Loss(1): 0.0616\tClassification Loss: 1.1707\r\n",
      "Train Epoch: 28 [124160/209539 (59%)]\tAll Loss: 1.7691\tTriple Loss(0): 0.2606\tClassification Loss: 1.2479\r\n",
      "Train Epoch: 28 [124800/209539 (60%)]\tAll Loss: 1.6507\tTriple Loss(0): 0.3024\tClassification Loss: 1.0460\r\n",
      "Train Epoch: 28 [125440/209539 (60%)]\tAll Loss: 1.1674\tTriple Loss(1): 0.0128\tClassification Loss: 1.1418\r\n",
      "Train Epoch: 28 [126080/209539 (60%)]\tAll Loss: 1.2859\tTriple Loss(1): 0.0371\tClassification Loss: 1.2116\r\n",
      "Train Epoch: 28 [126720/209539 (60%)]\tAll Loss: 1.9758\tTriple Loss(0): 0.2228\tClassification Loss: 1.5302\r\n",
      "Train Epoch: 28 [127360/209539 (61%)]\tAll Loss: 1.3471\tTriple Loss(1): 0.0000\tClassification Loss: 1.3471\r\n",
      "Train Epoch: 28 [128000/209539 (61%)]\tAll Loss: 1.3767\tTriple Loss(1): 0.0274\tClassification Loss: 1.3220\r\n",
      "Train Epoch: 28 [128640/209539 (61%)]\tAll Loss: 0.9293\tTriple Loss(1): 0.0104\tClassification Loss: 0.9085\r\n",
      "Train Epoch: 28 [129280/209539 (62%)]\tAll Loss: 1.9229\tTriple Loss(0): 0.2729\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 28 [129920/209539 (62%)]\tAll Loss: 1.1458\tTriple Loss(1): 0.0205\tClassification Loss: 1.1049\r\n",
      "Train Epoch: 28 [130560/209539 (62%)]\tAll Loss: 0.9170\tTriple Loss(1): 0.0000\tClassification Loss: 0.9170\r\n",
      "Train Epoch: 28 [131200/209539 (63%)]\tAll Loss: 1.0721\tTriple Loss(1): 0.0000\tClassification Loss: 1.0721\r\n",
      "Train Epoch: 28 [131840/209539 (63%)]\tAll Loss: 1.2150\tTriple Loss(1): 0.0451\tClassification Loss: 1.1248\r\n",
      "Train Epoch: 28 [132480/209539 (63%)]\tAll Loss: 1.1390\tTriple Loss(1): 0.0162\tClassification Loss: 1.1066\r\n",
      "Train Epoch: 28 [133120/209539 (64%)]\tAll Loss: 1.5264\tTriple Loss(0): 0.1404\tClassification Loss: 1.2455\r\n",
      "Train Epoch: 28 [133760/209539 (64%)]\tAll Loss: 1.6272\tTriple Loss(0): 0.1364\tClassification Loss: 1.3543\r\n",
      "Train Epoch: 28 [134400/209539 (64%)]\tAll Loss: 0.8996\tTriple Loss(1): 0.0153\tClassification Loss: 0.8690\r\n",
      "Train Epoch: 28 [135040/209539 (64%)]\tAll Loss: 1.1621\tTriple Loss(1): 0.0000\tClassification Loss: 1.1621\r\n",
      "Train Epoch: 28 [135680/209539 (65%)]\tAll Loss: 1.5060\tTriple Loss(1): 0.0544\tClassification Loss: 1.3972\r\n",
      "Train Epoch: 28 [136320/209539 (65%)]\tAll Loss: 1.6178\tTriple Loss(1): 0.0960\tClassification Loss: 1.4258\r\n",
      "Train Epoch: 28 [136960/209539 (65%)]\tAll Loss: 1.2957\tTriple Loss(1): 0.0106\tClassification Loss: 1.2745\r\n",
      "Train Epoch: 28 [137600/209539 (66%)]\tAll Loss: 1.2172\tTriple Loss(1): 0.0356\tClassification Loss: 1.1459\r\n",
      "Train Epoch: 28 [138240/209539 (66%)]\tAll Loss: 2.1371\tTriple Loss(0): 0.3394\tClassification Loss: 1.4584\r\n",
      "Train Epoch: 28 [138880/209539 (66%)]\tAll Loss: 1.1692\tTriple Loss(1): 0.0040\tClassification Loss: 1.1612\r\n",
      "Train Epoch: 28 [139520/209539 (67%)]\tAll Loss: 1.1073\tTriple Loss(1): 0.0037\tClassification Loss: 1.0999\r\n",
      "Train Epoch: 28 [140160/209539 (67%)]\tAll Loss: 1.3948\tTriple Loss(1): 0.0637\tClassification Loss: 1.2674\r\n",
      "Train Epoch: 28 [140800/209539 (67%)]\tAll Loss: 1.5542\tTriple Loss(1): 0.0255\tClassification Loss: 1.5033\r\n",
      "Train Epoch: 28 [141440/209539 (68%)]\tAll Loss: 1.3474\tTriple Loss(1): 0.0014\tClassification Loss: 1.3446\r\n",
      "Train Epoch: 28 [142080/209539 (68%)]\tAll Loss: 1.0391\tTriple Loss(1): 0.0000\tClassification Loss: 1.0391\r\n",
      "Train Epoch: 28 [142720/209539 (68%)]\tAll Loss: 1.8468\tTriple Loss(1): 0.1131\tClassification Loss: 1.6205\r\n",
      "Train Epoch: 28 [143360/209539 (68%)]\tAll Loss: 1.8196\tTriple Loss(0): 0.3544\tClassification Loss: 1.1109\r\n",
      "Train Epoch: 28 [144000/209539 (69%)]\tAll Loss: 1.1509\tTriple Loss(1): 0.0621\tClassification Loss: 1.0267\r\n",
      "Train Epoch: 28 [144640/209539 (69%)]\tAll Loss: 1.5224\tTriple Loss(0): 0.2090\tClassification Loss: 1.1044\r\n",
      "Train Epoch: 28 [145280/209539 (69%)]\tAll Loss: 1.9035\tTriple Loss(0): 0.2780\tClassification Loss: 1.3475\r\n",
      "Train Epoch: 28 [145920/209539 (70%)]\tAll Loss: 1.3168\tTriple Loss(1): 0.0374\tClassification Loss: 1.2421\r\n",
      "Train Epoch: 28 [146560/209539 (70%)]\tAll Loss: 1.0243\tTriple Loss(1): 0.0291\tClassification Loss: 0.9660\r\n",
      "Train Epoch: 28 [147200/209539 (70%)]\tAll Loss: 2.4403\tTriple Loss(0): 0.6041\tClassification Loss: 1.2321\r\n",
      "Train Epoch: 28 [147840/209539 (71%)]\tAll Loss: 1.7487\tTriple Loss(0): 0.3474\tClassification Loss: 1.0540\r\n",
      "Train Epoch: 28 [148480/209539 (71%)]\tAll Loss: 1.3647\tTriple Loss(1): 0.0629\tClassification Loss: 1.2390\r\n",
      "Train Epoch: 28 [149120/209539 (71%)]\tAll Loss: 1.2009\tTriple Loss(1): 0.0034\tClassification Loss: 1.1940\r\n",
      "Train Epoch: 28 [149760/209539 (71%)]\tAll Loss: 1.6309\tTriple Loss(0): 0.2891\tClassification Loss: 1.0528\r\n",
      "Train Epoch: 28 [150400/209539 (72%)]\tAll Loss: 1.5899\tTriple Loss(1): 0.0210\tClassification Loss: 1.5480\r\n",
      "Train Epoch: 28 [151040/209539 (72%)]\tAll Loss: 1.8500\tTriple Loss(0): 0.3747\tClassification Loss: 1.1007\r\n",
      "Train Epoch: 28 [151680/209539 (72%)]\tAll Loss: 1.4953\tTriple Loss(1): 0.0136\tClassification Loss: 1.4681\r\n",
      "Train Epoch: 28 [152320/209539 (73%)]\tAll Loss: 1.1307\tTriple Loss(1): 0.0169\tClassification Loss: 1.0969\r\n",
      "Train Epoch: 28 [152960/209539 (73%)]\tAll Loss: 1.1815\tTriple Loss(1): 0.0611\tClassification Loss: 1.0594\r\n",
      "Train Epoch: 28 [153600/209539 (73%)]\tAll Loss: 1.3922\tTriple Loss(1): 0.0387\tClassification Loss: 1.3147\r\n",
      "Train Epoch: 28 [154240/209539 (74%)]\tAll Loss: 1.2008\tTriple Loss(1): 0.0100\tClassification Loss: 1.1808\r\n",
      "Train Epoch: 28 [154880/209539 (74%)]\tAll Loss: 1.8472\tTriple Loss(0): 0.2582\tClassification Loss: 1.3307\r\n",
      "Train Epoch: 28 [155520/209539 (74%)]\tAll Loss: 1.2166\tTriple Loss(1): 0.0000\tClassification Loss: 1.2166\r\n",
      "Train Epoch: 28 [156160/209539 (75%)]\tAll Loss: 1.7233\tTriple Loss(0): 0.1826\tClassification Loss: 1.3582\r\n",
      "Train Epoch: 28 [156800/209539 (75%)]\tAll Loss: 1.6312\tTriple Loss(1): 0.0362\tClassification Loss: 1.5588\r\n",
      "Train Epoch: 28 [157440/209539 (75%)]\tAll Loss: 1.4819\tTriple Loss(1): 0.0380\tClassification Loss: 1.4059\r\n",
      "Train Epoch: 28 [158080/209539 (75%)]\tAll Loss: 1.3686\tTriple Loss(1): 0.0596\tClassification Loss: 1.2495\r\n",
      "Train Epoch: 28 [158720/209539 (76%)]\tAll Loss: 1.1323\tTriple Loss(1): 0.0133\tClassification Loss: 1.1057\r\n",
      "Train Epoch: 28 [159360/209539 (76%)]\tAll Loss: 0.9663\tTriple Loss(1): 0.0147\tClassification Loss: 0.9369\r\n",
      "Train Epoch: 28 [160000/209539 (76%)]\tAll Loss: 1.5548\tTriple Loss(1): 0.0714\tClassification Loss: 1.4121\r\n",
      "Train Epoch: 28 [160640/209539 (77%)]\tAll Loss: 0.9830\tTriple Loss(1): 0.0000\tClassification Loss: 0.9830\r\n",
      "Train Epoch: 28 [161280/209539 (77%)]\tAll Loss: 1.9563\tTriple Loss(0): 0.4244\tClassification Loss: 1.1075\r\n",
      "Train Epoch: 28 [161920/209539 (77%)]\tAll Loss: 1.2320\tTriple Loss(1): 0.0023\tClassification Loss: 1.2273\r\n",
      "Train Epoch: 28 [162560/209539 (78%)]\tAll Loss: 1.1949\tTriple Loss(1): 0.0101\tClassification Loss: 1.1748\r\n",
      "Train Epoch: 28 [163200/209539 (78%)]\tAll Loss: 1.0222\tTriple Loss(1): 0.0000\tClassification Loss: 1.0222\r\n",
      "Train Epoch: 28 [163840/209539 (78%)]\tAll Loss: 1.1714\tTriple Loss(1): 0.0186\tClassification Loss: 1.1342\r\n",
      "Train Epoch: 28 [164480/209539 (78%)]\tAll Loss: 2.0228\tTriple Loss(0): 0.4332\tClassification Loss: 1.1564\r\n",
      "Train Epoch: 28 [165120/209539 (79%)]\tAll Loss: 1.0827\tTriple Loss(1): 0.0294\tClassification Loss: 1.0238\r\n",
      "Train Epoch: 28 [165760/209539 (79%)]\tAll Loss: 1.4755\tTriple Loss(1): 0.0336\tClassification Loss: 1.4083\r\n",
      "Train Epoch: 28 [166400/209539 (79%)]\tAll Loss: 1.2663\tTriple Loss(1): 0.0435\tClassification Loss: 1.1794\r\n",
      "Train Epoch: 28 [167040/209539 (80%)]\tAll Loss: 1.5629\tTriple Loss(1): 0.0431\tClassification Loss: 1.4766\r\n",
      "Train Epoch: 28 [167680/209539 (80%)]\tAll Loss: 1.4547\tTriple Loss(1): 0.0308\tClassification Loss: 1.3930\r\n",
      "Train Epoch: 28 [168320/209539 (80%)]\tAll Loss: 1.1924\tTriple Loss(1): 0.0192\tClassification Loss: 1.1540\r\n",
      "Train Epoch: 28 [168960/209539 (81%)]\tAll Loss: 1.1708\tTriple Loss(1): 0.0190\tClassification Loss: 1.1328\r\n",
      "Train Epoch: 28 [169600/209539 (81%)]\tAll Loss: 1.2499\tTriple Loss(1): 0.0530\tClassification Loss: 1.1438\r\n",
      "Train Epoch: 28 [170240/209539 (81%)]\tAll Loss: 1.4610\tTriple Loss(1): 0.0541\tClassification Loss: 1.3528\r\n",
      "Train Epoch: 28 [170880/209539 (82%)]\tAll Loss: 1.2467\tTriple Loss(1): 0.0000\tClassification Loss: 1.2467\r\n",
      "Train Epoch: 28 [171520/209539 (82%)]\tAll Loss: 1.0324\tTriple Loss(1): 0.0170\tClassification Loss: 0.9984\r\n",
      "Train Epoch: 28 [172160/209539 (82%)]\tAll Loss: 2.0546\tTriple Loss(0): 0.3455\tClassification Loss: 1.3636\r\n",
      "Train Epoch: 28 [172800/209539 (82%)]\tAll Loss: 1.1011\tTriple Loss(1): 0.0000\tClassification Loss: 1.1011\r\n",
      "Train Epoch: 28 [173440/209539 (83%)]\tAll Loss: 1.5480\tTriple Loss(1): 0.0071\tClassification Loss: 1.5338\r\n",
      "Train Epoch: 28 [174080/209539 (83%)]\tAll Loss: 1.6487\tTriple Loss(0): 0.2437\tClassification Loss: 1.1613\r\n",
      "Train Epoch: 28 [174720/209539 (83%)]\tAll Loss: 1.2446\tTriple Loss(1): 0.0387\tClassification Loss: 1.1673\r\n",
      "Train Epoch: 28 [175360/209539 (84%)]\tAll Loss: 1.3400\tTriple Loss(1): 0.0112\tClassification Loss: 1.3175\r\n",
      "Train Epoch: 28 [176000/209539 (84%)]\tAll Loss: 1.2783\tTriple Loss(1): 0.0127\tClassification Loss: 1.2529\r\n",
      "Train Epoch: 28 [176640/209539 (84%)]\tAll Loss: 1.0271\tTriple Loss(1): 0.0570\tClassification Loss: 0.9131\r\n",
      "Train Epoch: 28 [177280/209539 (85%)]\tAll Loss: 1.5451\tTriple Loss(1): 0.0231\tClassification Loss: 1.4989\r\n",
      "Train Epoch: 28 [177920/209539 (85%)]\tAll Loss: 1.3844\tTriple Loss(1): 0.0531\tClassification Loss: 1.2783\r\n",
      "Train Epoch: 28 [178560/209539 (85%)]\tAll Loss: 1.6498\tTriple Loss(0): 0.2876\tClassification Loss: 1.0745\r\n",
      "Train Epoch: 28 [179200/209539 (86%)]\tAll Loss: 1.4576\tTriple Loss(1): 0.0428\tClassification Loss: 1.3720\r\n",
      "Train Epoch: 28 [179840/209539 (86%)]\tAll Loss: 1.3420\tTriple Loss(1): 0.0002\tClassification Loss: 1.3416\r\n",
      "Train Epoch: 28 [180480/209539 (86%)]\tAll Loss: 2.0653\tTriple Loss(0): 0.4105\tClassification Loss: 1.2443\r\n",
      "Train Epoch: 28 [181120/209539 (86%)]\tAll Loss: 1.4096\tTriple Loss(1): 0.0557\tClassification Loss: 1.2981\r\n",
      "Train Epoch: 28 [181760/209539 (87%)]\tAll Loss: 1.1920\tTriple Loss(1): 0.0582\tClassification Loss: 1.0755\r\n",
      "Train Epoch: 28 [182400/209539 (87%)]\tAll Loss: 1.4552\tTriple Loss(1): 0.0628\tClassification Loss: 1.3295\r\n",
      "Train Epoch: 28 [183040/209539 (87%)]\tAll Loss: 1.3276\tTriple Loss(1): 0.0156\tClassification Loss: 1.2963\r\n",
      "Train Epoch: 28 [183680/209539 (88%)]\tAll Loss: 1.0091\tTriple Loss(1): 0.0152\tClassification Loss: 0.9787\r\n",
      "Train Epoch: 28 [184320/209539 (88%)]\tAll Loss: 1.2030\tTriple Loss(1): 0.0129\tClassification Loss: 1.1772\r\n",
      "Train Epoch: 28 [184960/209539 (88%)]\tAll Loss: 1.2370\tTriple Loss(1): 0.0898\tClassification Loss: 1.0575\r\n",
      "Train Epoch: 28 [185600/209539 (89%)]\tAll Loss: 2.0504\tTriple Loss(0): 0.3137\tClassification Loss: 1.4231\r\n",
      "Train Epoch: 28 [186240/209539 (89%)]\tAll Loss: 1.3167\tTriple Loss(1): 0.0392\tClassification Loss: 1.2382\r\n",
      "Train Epoch: 28 [186880/209539 (89%)]\tAll Loss: 1.4383\tTriple Loss(1): 0.0668\tClassification Loss: 1.3048\r\n",
      "Train Epoch: 28 [187520/209539 (89%)]\tAll Loss: 1.8123\tTriple Loss(0): 0.1969\tClassification Loss: 1.4184\r\n",
      "Train Epoch: 28 [188160/209539 (90%)]\tAll Loss: 0.9138\tTriple Loss(1): 0.0000\tClassification Loss: 0.9138\r\n",
      "Train Epoch: 28 [188800/209539 (90%)]\tAll Loss: 1.1541\tTriple Loss(1): 0.0180\tClassification Loss: 1.1181\r\n",
      "Train Epoch: 28 [189440/209539 (90%)]\tAll Loss: 1.4002\tTriple Loss(1): 0.0755\tClassification Loss: 1.2492\r\n",
      "Train Epoch: 28 [190080/209539 (91%)]\tAll Loss: 1.6440\tTriple Loss(0): 0.2982\tClassification Loss: 1.0475\r\n",
      "Train Epoch: 28 [190720/209539 (91%)]\tAll Loss: 1.2077\tTriple Loss(1): 0.0000\tClassification Loss: 1.2077\r\n",
      "Train Epoch: 28 [191360/209539 (91%)]\tAll Loss: 0.9347\tTriple Loss(1): 0.0154\tClassification Loss: 0.9040\r\n",
      "Train Epoch: 28 [192000/209539 (92%)]\tAll Loss: 1.5919\tTriple Loss(1): 0.0249\tClassification Loss: 1.5422\r\n",
      "Train Epoch: 28 [192640/209539 (92%)]\tAll Loss: 1.3529\tTriple Loss(1): 0.0614\tClassification Loss: 1.2302\r\n",
      "Train Epoch: 28 [193280/209539 (92%)]\tAll Loss: 1.4805\tTriple Loss(1): 0.1527\tClassification Loss: 1.1752\r\n",
      "Train Epoch: 28 [193920/209539 (93%)]\tAll Loss: 1.8759\tTriple Loss(0): 0.3743\tClassification Loss: 1.1273\r\n",
      "Train Epoch: 28 [194560/209539 (93%)]\tAll Loss: 1.2074\tTriple Loss(1): 0.0240\tClassification Loss: 1.1595\r\n",
      "Train Epoch: 28 [195200/209539 (93%)]\tAll Loss: 1.4044\tTriple Loss(1): 0.0037\tClassification Loss: 1.3971\r\n",
      "Train Epoch: 28 [195840/209539 (93%)]\tAll Loss: 0.8620\tTriple Loss(1): 0.0121\tClassification Loss: 0.8377\r\n",
      "Train Epoch: 28 [196480/209539 (94%)]\tAll Loss: 2.1854\tTriple Loss(0): 0.4239\tClassification Loss: 1.3376\r\n",
      "Train Epoch: 28 [197120/209539 (94%)]\tAll Loss: 1.0802\tTriple Loss(1): 0.0000\tClassification Loss: 1.0802\r\n",
      "Train Epoch: 28 [197760/209539 (94%)]\tAll Loss: 1.4692\tTriple Loss(1): 0.0854\tClassification Loss: 1.2984\r\n",
      "Train Epoch: 28 [198400/209539 (95%)]\tAll Loss: 1.2679\tTriple Loss(1): 0.0235\tClassification Loss: 1.2210\r\n",
      "Train Epoch: 28 [199040/209539 (95%)]\tAll Loss: 1.7082\tTriple Loss(1): 0.0654\tClassification Loss: 1.5773\r\n",
      "Train Epoch: 28 [199680/209539 (95%)]\tAll Loss: 1.1607\tTriple Loss(1): 0.0256\tClassification Loss: 1.1096\r\n",
      "Train Epoch: 28 [200320/209539 (96%)]\tAll Loss: 1.3079\tTriple Loss(1): 0.0139\tClassification Loss: 1.2800\r\n",
      "Train Epoch: 28 [200960/209539 (96%)]\tAll Loss: 0.8486\tTriple Loss(1): 0.0000\tClassification Loss: 0.8486\r\n",
      "Train Epoch: 28 [201600/209539 (96%)]\tAll Loss: 1.1940\tTriple Loss(1): 0.0056\tClassification Loss: 1.1828\r\n",
      "Train Epoch: 28 [202240/209539 (97%)]\tAll Loss: 1.2253\tTriple Loss(1): 0.0147\tClassification Loss: 1.1959\r\n",
      "Train Epoch: 28 [202880/209539 (97%)]\tAll Loss: 1.0317\tTriple Loss(1): 0.0406\tClassification Loss: 0.9505\r\n",
      "Train Epoch: 28 [203520/209539 (97%)]\tAll Loss: 1.3425\tTriple Loss(1): 0.0083\tClassification Loss: 1.3260\r\n",
      "Train Epoch: 28 [204160/209539 (97%)]\tAll Loss: 1.6332\tTriple Loss(1): 0.0132\tClassification Loss: 1.6068\r\n",
      "Train Epoch: 28 [204800/209539 (98%)]\tAll Loss: 1.4935\tTriple Loss(1): 0.0518\tClassification Loss: 1.3900\r\n",
      "Train Epoch: 28 [205440/209539 (98%)]\tAll Loss: 0.8631\tTriple Loss(1): 0.0150\tClassification Loss: 0.8332\r\n",
      "Train Epoch: 28 [206080/209539 (98%)]\tAll Loss: 1.5766\tTriple Loss(0): 0.1810\tClassification Loss: 1.2145\r\n",
      "Train Epoch: 28 [206720/209539 (99%)]\tAll Loss: 1.1312\tTriple Loss(1): 0.0051\tClassification Loss: 1.1210\r\n",
      "Train Epoch: 28 [207360/209539 (99%)]\tAll Loss: 0.9879\tTriple Loss(1): 0.0160\tClassification Loss: 0.9558\r\n",
      "Train Epoch: 28 [208000/209539 (99%)]\tAll Loss: 1.0964\tTriple Loss(1): 0.0278\tClassification Loss: 1.0408\r\n",
      "Train Epoch: 28 [208640/209539 (100%)]\tAll Loss: 1.7868\tTriple Loss(0): 0.2802\tClassification Loss: 1.2264\r\n",
      "Train Epoch: 28 [209280/209539 (100%)]\tAll Loss: 1.2473\tTriple Loss(1): 0.0013\tClassification Loss: 1.2447\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/28_epochs\r\n",
      "Train Epoch: 29 [0/209539 (0%)]\tAll Loss: 1.8528\tTriple Loss(1): 0.1850\tClassification Loss: 1.4828\r\n",
      "\r\n",
      "Test set: Average loss: 1.0990\r\n",
      "Top 1 Accuracy: 54522/80128 (68%)\r\n",
      "Top 3 Accuracy: 69835/80128 (87%)\r\n",
      "Top 5 Accuracy: 74545/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 29 [640/209539 (0%)]\tAll Loss: 1.3560\tTriple Loss(1): 0.0405\tClassification Loss: 1.2750\r\n",
      "Train Epoch: 29 [1280/209539 (1%)]\tAll Loss: 1.5395\tTriple Loss(0): 0.2612\tClassification Loss: 1.0171\r\n",
      "Train Epoch: 29 [1920/209539 (1%)]\tAll Loss: 1.2533\tTriple Loss(1): 0.0000\tClassification Loss: 1.2533\r\n",
      "Train Epoch: 29 [2560/209539 (1%)]\tAll Loss: 1.5703\tTriple Loss(1): 0.0305\tClassification Loss: 1.5093\r\n",
      "Train Epoch: 29 [3200/209539 (2%)]\tAll Loss: 1.3050\tTriple Loss(1): 0.0099\tClassification Loss: 1.2852\r\n",
      "Train Epoch: 29 [3840/209539 (2%)]\tAll Loss: 1.9399\tTriple Loss(0): 0.3726\tClassification Loss: 1.1948\r\n",
      "Train Epoch: 29 [4480/209539 (2%)]\tAll Loss: 1.5228\tTriple Loss(1): 0.0466\tClassification Loss: 1.4295\r\n",
      "Train Epoch: 29 [5120/209539 (2%)]\tAll Loss: 1.1712\tTriple Loss(1): 0.0102\tClassification Loss: 1.1509\r\n",
      "Train Epoch: 29 [5760/209539 (3%)]\tAll Loss: 1.2829\tTriple Loss(1): 0.0111\tClassification Loss: 1.2607\r\n",
      "Train Epoch: 29 [6400/209539 (3%)]\tAll Loss: 1.1697\tTriple Loss(1): 0.0588\tClassification Loss: 1.0521\r\n",
      "Train Epoch: 29 [7040/209539 (3%)]\tAll Loss: 1.2227\tTriple Loss(1): 0.0178\tClassification Loss: 1.1871\r\n",
      "Train Epoch: 29 [7680/209539 (4%)]\tAll Loss: 1.1495\tTriple Loss(1): 0.0582\tClassification Loss: 1.0330\r\n",
      "Train Epoch: 29 [8320/209539 (4%)]\tAll Loss: 1.1940\tTriple Loss(1): 0.0000\tClassification Loss: 1.1940\r\n",
      "Train Epoch: 29 [8960/209539 (4%)]\tAll Loss: 1.1800\tTriple Loss(1): 0.0086\tClassification Loss: 1.1628\r\n",
      "Train Epoch: 29 [9600/209539 (5%)]\tAll Loss: 1.5284\tTriple Loss(1): 0.0251\tClassification Loss: 1.4782\r\n",
      "Train Epoch: 29 [10240/209539 (5%)]\tAll Loss: 1.0478\tTriple Loss(1): 0.0391\tClassification Loss: 0.9697\r\n",
      "Train Epoch: 29 [10880/209539 (5%)]\tAll Loss: 1.2493\tTriple Loss(1): 0.0057\tClassification Loss: 1.2379\r\n",
      "Train Epoch: 29 [11520/209539 (5%)]\tAll Loss: 1.7849\tTriple Loss(0): 0.3045\tClassification Loss: 1.1758\r\n",
      "Train Epoch: 29 [12160/209539 (6%)]\tAll Loss: 1.0653\tTriple Loss(1): 0.0022\tClassification Loss: 1.0608\r\n",
      "Train Epoch: 29 [12800/209539 (6%)]\tAll Loss: 0.8541\tTriple Loss(1): 0.0274\tClassification Loss: 0.7992\r\n",
      "Train Epoch: 29 [13440/209539 (6%)]\tAll Loss: 1.1006\tTriple Loss(1): 0.0075\tClassification Loss: 1.0856\r\n",
      "Train Epoch: 29 [14080/209539 (7%)]\tAll Loss: 1.3458\tTriple Loss(1): 0.0041\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 29 [14720/209539 (7%)]\tAll Loss: 1.2790\tTriple Loss(1): 0.0140\tClassification Loss: 1.2511\r\n",
      "Train Epoch: 29 [15360/209539 (7%)]\tAll Loss: 1.2025\tTriple Loss(1): 0.0255\tClassification Loss: 1.1516\r\n",
      "Train Epoch: 29 [16000/209539 (8%)]\tAll Loss: 1.3763\tTriple Loss(1): 0.0000\tClassification Loss: 1.3763\r\n",
      "Train Epoch: 29 [16640/209539 (8%)]\tAll Loss: 1.6556\tTriple Loss(1): 0.0247\tClassification Loss: 1.6061\r\n",
      "Train Epoch: 29 [17280/209539 (8%)]\tAll Loss: 1.4194\tTriple Loss(1): 0.0137\tClassification Loss: 1.3921\r\n",
      "Train Epoch: 29 [17920/209539 (9%)]\tAll Loss: 1.4667\tTriple Loss(1): 0.0731\tClassification Loss: 1.3204\r\n",
      "Train Epoch: 29 [18560/209539 (9%)]\tAll Loss: 1.1746\tTriple Loss(1): 0.0021\tClassification Loss: 1.1705\r\n",
      "Train Epoch: 29 [19200/209539 (9%)]\tAll Loss: 1.2801\tTriple Loss(1): 0.0416\tClassification Loss: 1.1969\r\n",
      "Train Epoch: 29 [19840/209539 (9%)]\tAll Loss: 1.3635\tTriple Loss(1): 0.0239\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 29 [20480/209539 (10%)]\tAll Loss: 1.1835\tTriple Loss(1): 0.0566\tClassification Loss: 1.0702\r\n",
      "Train Epoch: 29 [21120/209539 (10%)]\tAll Loss: 1.5165\tTriple Loss(1): 0.0164\tClassification Loss: 1.4836\r\n",
      "Train Epoch: 29 [21760/209539 (10%)]\tAll Loss: 1.2427\tTriple Loss(1): 0.0228\tClassification Loss: 1.1972\r\n",
      "Train Epoch: 29 [22400/209539 (11%)]\tAll Loss: 1.1595\tTriple Loss(1): 0.0303\tClassification Loss: 1.0990\r\n",
      "Train Epoch: 29 [23040/209539 (11%)]\tAll Loss: 1.5909\tTriple Loss(0): 0.1623\tClassification Loss: 1.2664\r\n",
      "Train Epoch: 29 [23680/209539 (11%)]\tAll Loss: 1.0460\tTriple Loss(1): 0.0225\tClassification Loss: 1.0011\r\n",
      "Train Epoch: 29 [24320/209539 (12%)]\tAll Loss: 1.2744\tTriple Loss(1): 0.0106\tClassification Loss: 1.2532\r\n",
      "Train Epoch: 29 [24960/209539 (12%)]\tAll Loss: 1.0795\tTriple Loss(1): 0.0187\tClassification Loss: 1.0420\r\n",
      "Train Epoch: 29 [25600/209539 (12%)]\tAll Loss: 1.2933\tTriple Loss(1): 0.0000\tClassification Loss: 1.2933\r\n",
      "Train Epoch: 29 [26240/209539 (13%)]\tAll Loss: 1.0711\tTriple Loss(1): 0.0271\tClassification Loss: 1.0168\r\n",
      "Train Epoch: 29 [26880/209539 (13%)]\tAll Loss: 1.2678\tTriple Loss(1): 0.0245\tClassification Loss: 1.2189\r\n",
      "Train Epoch: 29 [27520/209539 (13%)]\tAll Loss: 1.8848\tTriple Loss(0): 0.3210\tClassification Loss: 1.2428\r\n",
      "Train Epoch: 29 [28160/209539 (13%)]\tAll Loss: 1.5422\tTriple Loss(1): 0.0006\tClassification Loss: 1.5410\r\n",
      "Train Epoch: 29 [28800/209539 (14%)]\tAll Loss: 1.6348\tTriple Loss(1): 0.1000\tClassification Loss: 1.4347\r\n",
      "Train Epoch: 29 [29440/209539 (14%)]\tAll Loss: 2.2398\tTriple Loss(0): 0.4082\tClassification Loss: 1.4235\r\n",
      "Train Epoch: 29 [30080/209539 (14%)]\tAll Loss: 1.1830\tTriple Loss(1): 0.0179\tClassification Loss: 1.1472\r\n",
      "Train Epoch: 29 [30720/209539 (15%)]\tAll Loss: 2.2554\tTriple Loss(0): 0.4808\tClassification Loss: 1.2939\r\n",
      "Train Epoch: 29 [31360/209539 (15%)]\tAll Loss: 1.1421\tTriple Loss(1): 0.0432\tClassification Loss: 1.0556\r\n",
      "Train Epoch: 29 [32000/209539 (15%)]\tAll Loss: 1.4638\tTriple Loss(1): 0.0382\tClassification Loss: 1.3874\r\n",
      "Train Epoch: 29 [32640/209539 (16%)]\tAll Loss: 1.3320\tTriple Loss(1): 0.0183\tClassification Loss: 1.2955\r\n",
      "Train Epoch: 29 [33280/209539 (16%)]\tAll Loss: 1.2727\tTriple Loss(1): 0.0419\tClassification Loss: 1.1888\r\n",
      "Train Epoch: 29 [33920/209539 (16%)]\tAll Loss: 1.2332\tTriple Loss(1): 0.0079\tClassification Loss: 1.2174\r\n",
      "Train Epoch: 29 [34560/209539 (16%)]\tAll Loss: 1.0342\tTriple Loss(1): 0.0085\tClassification Loss: 1.0171\r\n",
      "Train Epoch: 29 [35200/209539 (17%)]\tAll Loss: 1.1329\tTriple Loss(1): 0.0310\tClassification Loss: 1.0708\r\n",
      "Train Epoch: 29 [35840/209539 (17%)]\tAll Loss: 1.0813\tTriple Loss(1): 0.0265\tClassification Loss: 1.0284\r\n",
      "Train Epoch: 29 [36480/209539 (17%)]\tAll Loss: 0.9081\tTriple Loss(1): 0.0386\tClassification Loss: 0.8310\r\n",
      "Train Epoch: 29 [37120/209539 (18%)]\tAll Loss: 1.8908\tTriple Loss(0): 0.2313\tClassification Loss: 1.4281\r\n",
      "Train Epoch: 29 [37760/209539 (18%)]\tAll Loss: 1.1366\tTriple Loss(1): 0.0000\tClassification Loss: 1.1366\r\n",
      "Train Epoch: 29 [38400/209539 (18%)]\tAll Loss: 1.7142\tTriple Loss(0): 0.2299\tClassification Loss: 1.2543\r\n",
      "Train Epoch: 29 [39040/209539 (19%)]\tAll Loss: 0.9860\tTriple Loss(1): 0.0638\tClassification Loss: 0.8583\r\n",
      "Train Epoch: 29 [39680/209539 (19%)]\tAll Loss: 1.1336\tTriple Loss(1): 0.0078\tClassification Loss: 1.1180\r\n",
      "Train Epoch: 29 [40320/209539 (19%)]\tAll Loss: 1.9165\tTriple Loss(0): 0.2858\tClassification Loss: 1.3448\r\n",
      "Train Epoch: 29 [40960/209539 (20%)]\tAll Loss: 1.2850\tTriple Loss(1): 0.0286\tClassification Loss: 1.2278\r\n",
      "Train Epoch: 29 [41600/209539 (20%)]\tAll Loss: 1.8983\tTriple Loss(0): 0.4200\tClassification Loss: 1.0583\r\n",
      "Train Epoch: 29 [42240/209539 (20%)]\tAll Loss: 1.5666\tTriple Loss(0): 0.2178\tClassification Loss: 1.1310\r\n",
      "Train Epoch: 29 [42880/209539 (20%)]\tAll Loss: 1.0270\tTriple Loss(1): 0.0217\tClassification Loss: 0.9836\r\n",
      "Train Epoch: 29 [43520/209539 (21%)]\tAll Loss: 1.5905\tTriple Loss(1): 0.0725\tClassification Loss: 1.4454\r\n",
      "Train Epoch: 29 [44160/209539 (21%)]\tAll Loss: 1.5683\tTriple Loss(1): 0.0140\tClassification Loss: 1.5403\r\n",
      "Train Epoch: 29 [44800/209539 (21%)]\tAll Loss: 1.4670\tTriple Loss(1): 0.0382\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 29 [45440/209539 (22%)]\tAll Loss: 1.7077\tTriple Loss(1): 0.0267\tClassification Loss: 1.6543\r\n",
      "Train Epoch: 29 [46080/209539 (22%)]\tAll Loss: 1.3286\tTriple Loss(1): 0.0471\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 29 [46720/209539 (22%)]\tAll Loss: 1.6441\tTriple Loss(1): 0.0183\tClassification Loss: 1.6075\r\n",
      "Train Epoch: 29 [47360/209539 (23%)]\tAll Loss: 0.9340\tTriple Loss(1): 0.0114\tClassification Loss: 0.9112\r\n",
      "Train Epoch: 29 [48000/209539 (23%)]\tAll Loss: 1.0979\tTriple Loss(1): 0.0078\tClassification Loss: 1.0823\r\n",
      "Train Epoch: 29 [48640/209539 (23%)]\tAll Loss: 2.1725\tTriple Loss(0): 0.3126\tClassification Loss: 1.5473\r\n",
      "Train Epoch: 29 [49280/209539 (24%)]\tAll Loss: 1.2063\tTriple Loss(1): 0.0000\tClassification Loss: 1.2063\r\n",
      "Train Epoch: 29 [49920/209539 (24%)]\tAll Loss: 1.2786\tTriple Loss(1): 0.0144\tClassification Loss: 1.2498\r\n",
      "Train Epoch: 29 [50560/209539 (24%)]\tAll Loss: 1.9501\tTriple Loss(0): 0.2600\tClassification Loss: 1.4301\r\n",
      "Train Epoch: 29 [51200/209539 (24%)]\tAll Loss: 1.2640\tTriple Loss(1): 0.0056\tClassification Loss: 1.2527\r\n",
      "Train Epoch: 29 [51840/209539 (25%)]\tAll Loss: 1.4058\tTriple Loss(1): 0.0705\tClassification Loss: 1.2649\r\n",
      "Train Epoch: 29 [52480/209539 (25%)]\tAll Loss: 1.2019\tTriple Loss(1): 0.0315\tClassification Loss: 1.1389\r\n",
      "Train Epoch: 29 [53120/209539 (25%)]\tAll Loss: 1.3626\tTriple Loss(1): 0.0217\tClassification Loss: 1.3192\r\n",
      "Train Epoch: 29 [53760/209539 (26%)]\tAll Loss: 1.4738\tTriple Loss(1): 0.0173\tClassification Loss: 1.4391\r\n",
      "Train Epoch: 29 [54400/209539 (26%)]\tAll Loss: 1.7068\tTriple Loss(1): 0.0335\tClassification Loss: 1.6397\r\n",
      "Train Epoch: 29 [55040/209539 (26%)]\tAll Loss: 1.0678\tTriple Loss(1): 0.0211\tClassification Loss: 1.0256\r\n",
      "Train Epoch: 29 [55680/209539 (27%)]\tAll Loss: 1.6403\tTriple Loss(1): 0.0057\tClassification Loss: 1.6288\r\n",
      "Train Epoch: 29 [56320/209539 (27%)]\tAll Loss: 1.0224\tTriple Loss(1): 0.0000\tClassification Loss: 1.0224\r\n",
      "Train Epoch: 29 [56960/209539 (27%)]\tAll Loss: 1.2708\tTriple Loss(1): 0.0066\tClassification Loss: 1.2575\r\n",
      "Train Epoch: 29 [57600/209539 (27%)]\tAll Loss: 1.2239\tTriple Loss(1): 0.0949\tClassification Loss: 1.0340\r\n",
      "Train Epoch: 29 [58240/209539 (28%)]\tAll Loss: 1.1700\tTriple Loss(1): 0.0400\tClassification Loss: 1.0899\r\n",
      "Train Epoch: 29 [58880/209539 (28%)]\tAll Loss: 1.4045\tTriple Loss(1): 0.0219\tClassification Loss: 1.3607\r\n",
      "Train Epoch: 29 [59520/209539 (28%)]\tAll Loss: 1.1191\tTriple Loss(1): 0.0048\tClassification Loss: 1.1095\r\n",
      "Train Epoch: 29 [60160/209539 (29%)]\tAll Loss: 1.4081\tTriple Loss(1): 0.0406\tClassification Loss: 1.3269\r\n",
      "Train Epoch: 29 [60800/209539 (29%)]\tAll Loss: 2.3571\tTriple Loss(0): 0.3525\tClassification Loss: 1.6521\r\n",
      "Train Epoch: 29 [61440/209539 (29%)]\tAll Loss: 1.1866\tTriple Loss(1): 0.0280\tClassification Loss: 1.1306\r\n",
      "Train Epoch: 29 [62080/209539 (30%)]\tAll Loss: 1.5597\tTriple Loss(1): 0.0358\tClassification Loss: 1.4880\r\n",
      "Train Epoch: 29 [62720/209539 (30%)]\tAll Loss: 1.2226\tTriple Loss(1): 0.0140\tClassification Loss: 1.1947\r\n",
      "Train Epoch: 29 [63360/209539 (30%)]\tAll Loss: 1.5371\tTriple Loss(0): 0.1516\tClassification Loss: 1.2340\r\n",
      "Train Epoch: 29 [64000/209539 (31%)]\tAll Loss: 1.3361\tTriple Loss(1): 0.0198\tClassification Loss: 1.2965\r\n",
      "Train Epoch: 29 [64640/209539 (31%)]\tAll Loss: 1.5657\tTriple Loss(1): 0.0122\tClassification Loss: 1.5413\r\n",
      "Train Epoch: 29 [65280/209539 (31%)]\tAll Loss: 1.3336\tTriple Loss(1): 0.0182\tClassification Loss: 1.2972\r\n",
      "Train Epoch: 29 [65920/209539 (31%)]\tAll Loss: 1.2845\tTriple Loss(1): 0.0133\tClassification Loss: 1.2579\r\n",
      "Train Epoch: 29 [66560/209539 (32%)]\tAll Loss: 1.3471\tTriple Loss(1): 0.0391\tClassification Loss: 1.2688\r\n",
      "Train Epoch: 29 [67200/209539 (32%)]\tAll Loss: 1.2374\tTriple Loss(1): 0.0715\tClassification Loss: 1.0944\r\n",
      "Train Epoch: 29 [67840/209539 (32%)]\tAll Loss: 1.1477\tTriple Loss(1): 0.0000\tClassification Loss: 1.1477\r\n",
      "Train Epoch: 29 [68480/209539 (33%)]\tAll Loss: 1.3985\tTriple Loss(1): 0.0563\tClassification Loss: 1.2859\r\n",
      "Train Epoch: 29 [69120/209539 (33%)]\tAll Loss: 1.4775\tTriple Loss(1): 0.0181\tClassification Loss: 1.4412\r\n",
      "Train Epoch: 29 [69760/209539 (33%)]\tAll Loss: 1.6968\tTriple Loss(0): 0.3702\tClassification Loss: 0.9564\r\n",
      "Train Epoch: 29 [70400/209539 (34%)]\tAll Loss: 1.1244\tTriple Loss(1): 0.0260\tClassification Loss: 1.0724\r\n",
      "Train Epoch: 29 [71040/209539 (34%)]\tAll Loss: 1.6483\tTriple Loss(1): 0.0373\tClassification Loss: 1.5737\r\n",
      "Train Epoch: 29 [71680/209539 (34%)]\tAll Loss: 1.5798\tTriple Loss(1): 0.0187\tClassification Loss: 1.5424\r\n",
      "Train Epoch: 29 [72320/209539 (35%)]\tAll Loss: 1.6181\tTriple Loss(0): 0.2255\tClassification Loss: 1.1672\r\n",
      "Train Epoch: 29 [72960/209539 (35%)]\tAll Loss: 1.8562\tTriple Loss(0): 0.3184\tClassification Loss: 1.2194\r\n",
      "Train Epoch: 29 [73600/209539 (35%)]\tAll Loss: 1.5028\tTriple Loss(1): 0.0020\tClassification Loss: 1.4988\r\n",
      "Train Epoch: 29 [74240/209539 (35%)]\tAll Loss: 1.5673\tTriple Loss(1): 0.0383\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 29 [74880/209539 (36%)]\tAll Loss: 1.7217\tTriple Loss(1): 0.0071\tClassification Loss: 1.7076\r\n",
      "Train Epoch: 29 [75520/209539 (36%)]\tAll Loss: 1.0679\tTriple Loss(1): 0.0024\tClassification Loss: 1.0630\r\n",
      "Train Epoch: 29 [76160/209539 (36%)]\tAll Loss: 1.1019\tTriple Loss(1): 0.0842\tClassification Loss: 0.9335\r\n",
      "Train Epoch: 29 [76800/209539 (37%)]\tAll Loss: 1.3177\tTriple Loss(1): 0.0262\tClassification Loss: 1.2653\r\n",
      "Train Epoch: 29 [77440/209539 (37%)]\tAll Loss: 1.3321\tTriple Loss(1): 0.0405\tClassification Loss: 1.2511\r\n",
      "Train Epoch: 29 [78080/209539 (37%)]\tAll Loss: 2.1786\tTriple Loss(0): 0.4311\tClassification Loss: 1.3163\r\n",
      "Train Epoch: 29 [78720/209539 (38%)]\tAll Loss: 1.4819\tTriple Loss(1): 0.0406\tClassification Loss: 1.4008\r\n",
      "Train Epoch: 29 [79360/209539 (38%)]\tAll Loss: 1.4949\tTriple Loss(0): 0.1592\tClassification Loss: 1.1766\r\n",
      "Train Epoch: 29 [80000/209539 (38%)]\tAll Loss: 1.2320\tTriple Loss(1): 0.0126\tClassification Loss: 1.2068\r\n",
      "Train Epoch: 29 [80640/209539 (38%)]\tAll Loss: 1.8337\tTriple Loss(0): 0.1815\tClassification Loss: 1.4708\r\n",
      "Train Epoch: 29 [81280/209539 (39%)]\tAll Loss: 1.4744\tTriple Loss(1): 0.0044\tClassification Loss: 1.4657\r\n",
      "Train Epoch: 29 [81920/209539 (39%)]\tAll Loss: 1.2147\tTriple Loss(1): 0.0385\tClassification Loss: 1.1376\r\n",
      "Train Epoch: 29 [82560/209539 (39%)]\tAll Loss: 1.3646\tTriple Loss(1): 0.0304\tClassification Loss: 1.3038\r\n",
      "Train Epoch: 29 [83200/209539 (40%)]\tAll Loss: 1.4470\tTriple Loss(1): 0.0424\tClassification Loss: 1.3622\r\n",
      "Train Epoch: 29 [83840/209539 (40%)]\tAll Loss: 2.0486\tTriple Loss(0): 0.3521\tClassification Loss: 1.3445\r\n",
      "Train Epoch: 29 [84480/209539 (40%)]\tAll Loss: 1.2458\tTriple Loss(1): 0.0182\tClassification Loss: 1.2093\r\n",
      "Train Epoch: 29 [85120/209539 (41%)]\tAll Loss: 1.5876\tTriple Loss(1): 0.0123\tClassification Loss: 1.5630\r\n",
      "Train Epoch: 29 [85760/209539 (41%)]\tAll Loss: 1.9949\tTriple Loss(0): 0.3657\tClassification Loss: 1.2635\r\n",
      "Train Epoch: 29 [86400/209539 (41%)]\tAll Loss: 1.4938\tTriple Loss(1): 0.0405\tClassification Loss: 1.4127\r\n",
      "Train Epoch: 29 [87040/209539 (42%)]\tAll Loss: 1.8359\tTriple Loss(0): 0.3325\tClassification Loss: 1.1709\r\n",
      "Train Epoch: 29 [87680/209539 (42%)]\tAll Loss: 1.2483\tTriple Loss(1): 0.0247\tClassification Loss: 1.1989\r\n",
      "Train Epoch: 29 [88320/209539 (42%)]\tAll Loss: 1.8902\tTriple Loss(0): 0.2837\tClassification Loss: 1.3229\r\n",
      "Train Epoch: 29 [88960/209539 (42%)]\tAll Loss: 1.4414\tTriple Loss(1): 0.0616\tClassification Loss: 1.3182\r\n",
      "Train Epoch: 29 [89600/209539 (43%)]\tAll Loss: 1.2881\tTriple Loss(1): 0.0050\tClassification Loss: 1.2782\r\n",
      "Train Epoch: 29 [90240/209539 (43%)]\tAll Loss: 1.2416\tTriple Loss(1): 0.0757\tClassification Loss: 1.0902\r\n",
      "Train Epoch: 29 [90880/209539 (43%)]\tAll Loss: 2.0863\tTriple Loss(0): 0.2645\tClassification Loss: 1.5573\r\n",
      "Train Epoch: 29 [91520/209539 (44%)]\tAll Loss: 1.3949\tTriple Loss(1): 0.0098\tClassification Loss: 1.3752\r\n",
      "Train Epoch: 29 [92160/209539 (44%)]\tAll Loss: 1.4017\tTriple Loss(1): 0.0683\tClassification Loss: 1.2650\r\n",
      "Train Epoch: 29 [92800/209539 (44%)]\tAll Loss: 1.3064\tTriple Loss(1): 0.0361\tClassification Loss: 1.2341\r\n",
      "Train Epoch: 29 [93440/209539 (45%)]\tAll Loss: 1.3278\tTriple Loss(1): 0.0096\tClassification Loss: 1.3087\r\n",
      "Train Epoch: 29 [94080/209539 (45%)]\tAll Loss: 1.1133\tTriple Loss(1): 0.0304\tClassification Loss: 1.0525\r\n",
      "Train Epoch: 29 [94720/209539 (45%)]\tAll Loss: 1.6341\tTriple Loss(1): 0.0547\tClassification Loss: 1.5247\r\n",
      "Train Epoch: 29 [95360/209539 (46%)]\tAll Loss: 1.3663\tTriple Loss(1): 0.0080\tClassification Loss: 1.3503\r\n",
      "Train Epoch: 29 [96000/209539 (46%)]\tAll Loss: 1.9144\tTriple Loss(0): 0.2948\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 29 [96640/209539 (46%)]\tAll Loss: 1.6157\tTriple Loss(1): 0.0000\tClassification Loss: 1.6157\r\n",
      "Train Epoch: 29 [97280/209539 (46%)]\tAll Loss: 1.3216\tTriple Loss(0): 0.1003\tClassification Loss: 1.1211\r\n",
      "Train Epoch: 29 [97920/209539 (47%)]\tAll Loss: 1.2836\tTriple Loss(1): 0.0000\tClassification Loss: 1.2836\r\n",
      "Train Epoch: 29 [98560/209539 (47%)]\tAll Loss: 1.5495\tTriple Loss(1): 0.0435\tClassification Loss: 1.4625\r\n",
      "Train Epoch: 29 [99200/209539 (47%)]\tAll Loss: 1.2610\tTriple Loss(1): 0.0155\tClassification Loss: 1.2301\r\n",
      "Train Epoch: 29 [99840/209539 (48%)]\tAll Loss: 1.2276\tTriple Loss(1): 0.0000\tClassification Loss: 1.2276\r\n",
      "Train Epoch: 29 [100480/209539 (48%)]\tAll Loss: 1.5399\tTriple Loss(1): 0.1497\tClassification Loss: 1.2405\r\n",
      "Train Epoch: 29 [101120/209539 (48%)]\tAll Loss: 1.4757\tTriple Loss(1): 0.0879\tClassification Loss: 1.2998\r\n",
      "Train Epoch: 29 [101760/209539 (49%)]\tAll Loss: 1.1864\tTriple Loss(1): 0.0000\tClassification Loss: 1.1864\r\n",
      "Train Epoch: 29 [102400/209539 (49%)]\tAll Loss: 1.7186\tTriple Loss(0): 0.2816\tClassification Loss: 1.1554\r\n",
      "Train Epoch: 29 [103040/209539 (49%)]\tAll Loss: 1.9179\tTriple Loss(0): 0.4069\tClassification Loss: 1.1040\r\n",
      "Train Epoch: 29 [103680/209539 (49%)]\tAll Loss: 1.3795\tTriple Loss(1): 0.0358\tClassification Loss: 1.3078\r\n",
      "Train Epoch: 29 [104320/209539 (50%)]\tAll Loss: 0.9675\tTriple Loss(1): 0.0237\tClassification Loss: 0.9202\r\n",
      "Train Epoch: 29 [104960/209539 (50%)]\tAll Loss: 1.8913\tTriple Loss(0): 0.3319\tClassification Loss: 1.2274\r\n",
      "Train Epoch: 29 [105600/209539 (50%)]\tAll Loss: 1.2569\tTriple Loss(1): 0.0157\tClassification Loss: 1.2254\r\n",
      "Train Epoch: 29 [106240/209539 (51%)]\tAll Loss: 1.1921\tTriple Loss(1): 0.0360\tClassification Loss: 1.1201\r\n",
      "Train Epoch: 29 [106880/209539 (51%)]\tAll Loss: 1.4860\tTriple Loss(0): 0.2015\tClassification Loss: 1.0829\r\n",
      "Train Epoch: 29 [107520/209539 (51%)]\tAll Loss: 1.2101\tTriple Loss(1): 0.0075\tClassification Loss: 1.1951\r\n",
      "Train Epoch: 29 [108160/209539 (52%)]\tAll Loss: 1.3851\tTriple Loss(1): 0.0445\tClassification Loss: 1.2961\r\n",
      "Train Epoch: 29 [108800/209539 (52%)]\tAll Loss: 1.1548\tTriple Loss(1): 0.0754\tClassification Loss: 1.0040\r\n",
      "Train Epoch: 29 [109440/209539 (52%)]\tAll Loss: 1.7411\tTriple Loss(1): 0.0829\tClassification Loss: 1.5752\r\n",
      "Train Epoch: 29 [110080/209539 (53%)]\tAll Loss: 1.4371\tTriple Loss(1): 0.0054\tClassification Loss: 1.4263\r\n",
      "Train Epoch: 29 [110720/209539 (53%)]\tAll Loss: 1.3115\tTriple Loss(1): 0.0000\tClassification Loss: 1.3115\r\n",
      "Train Epoch: 29 [111360/209539 (53%)]\tAll Loss: 1.0961\tTriple Loss(1): 0.0223\tClassification Loss: 1.0515\r\n",
      "Train Epoch: 29 [112000/209539 (53%)]\tAll Loss: 1.2315\tTriple Loss(1): 0.0377\tClassification Loss: 1.1562\r\n",
      "Train Epoch: 29 [112640/209539 (54%)]\tAll Loss: 1.3316\tTriple Loss(1): 0.0259\tClassification Loss: 1.2797\r\n",
      "Train Epoch: 29 [113280/209539 (54%)]\tAll Loss: 1.1628\tTriple Loss(1): 0.0035\tClassification Loss: 1.1558\r\n",
      "Train Epoch: 29 [113920/209539 (54%)]\tAll Loss: 1.2329\tTriple Loss(1): 0.0124\tClassification Loss: 1.2082\r\n",
      "Train Epoch: 29 [114560/209539 (55%)]\tAll Loss: 1.2779\tTriple Loss(1): 0.0234\tClassification Loss: 1.2312\r\n",
      "Train Epoch: 29 [115200/209539 (55%)]\tAll Loss: 1.3675\tTriple Loss(1): 0.0007\tClassification Loss: 1.3660\r\n",
      "Train Epoch: 29 [115840/209539 (55%)]\tAll Loss: 1.6164\tTriple Loss(0): 0.2319\tClassification Loss: 1.1526\r\n",
      "Train Epoch: 29 [116480/209539 (56%)]\tAll Loss: 1.4371\tTriple Loss(1): 0.0455\tClassification Loss: 1.3461\r\n",
      "Train Epoch: 29 [117120/209539 (56%)]\tAll Loss: 1.4469\tTriple Loss(1): 0.0770\tClassification Loss: 1.2929\r\n",
      "Train Epoch: 29 [117760/209539 (56%)]\tAll Loss: 1.6617\tTriple Loss(0): 0.1714\tClassification Loss: 1.3189\r\n",
      "Train Epoch: 29 [118400/209539 (57%)]\tAll Loss: 1.0366\tTriple Loss(1): 0.0482\tClassification Loss: 0.9402\r\n",
      "Train Epoch: 29 [119040/209539 (57%)]\tAll Loss: 1.4392\tTriple Loss(1): 0.0079\tClassification Loss: 1.4235\r\n",
      "Train Epoch: 29 [119680/209539 (57%)]\tAll Loss: 1.2398\tTriple Loss(1): 0.0000\tClassification Loss: 1.2398\r\n",
      "Train Epoch: 29 [120320/209539 (57%)]\tAll Loss: 1.2630\tTriple Loss(1): 0.0295\tClassification Loss: 1.2040\r\n",
      "Train Epoch: 29 [120960/209539 (58%)]\tAll Loss: 1.1638\tTriple Loss(1): 0.0036\tClassification Loss: 1.1566\r\n",
      "Train Epoch: 29 [121600/209539 (58%)]\tAll Loss: 1.7621\tTriple Loss(0): 0.3117\tClassification Loss: 1.1388\r\n",
      "Train Epoch: 29 [122240/209539 (58%)]\tAll Loss: 1.1226\tTriple Loss(1): 0.0163\tClassification Loss: 1.0900\r\n",
      "Train Epoch: 29 [122880/209539 (59%)]\tAll Loss: 1.0993\tTriple Loss(1): 0.0377\tClassification Loss: 1.0239\r\n",
      "Train Epoch: 29 [123520/209539 (59%)]\tAll Loss: 1.2579\tTriple Loss(1): 0.0109\tClassification Loss: 1.2361\r\n",
      "Train Epoch: 29 [124160/209539 (59%)]\tAll Loss: 1.1631\tTriple Loss(1): 0.0119\tClassification Loss: 1.1392\r\n",
      "Train Epoch: 29 [124800/209539 (60%)]\tAll Loss: 1.1111\tTriple Loss(1): 0.0620\tClassification Loss: 0.9871\r\n",
      "Train Epoch: 29 [125440/209539 (60%)]\tAll Loss: 1.5679\tTriple Loss(0): 0.2272\tClassification Loss: 1.1135\r\n",
      "Train Epoch: 29 [126080/209539 (60%)]\tAll Loss: 1.1157\tTriple Loss(1): 0.0000\tClassification Loss: 1.1157\r\n",
      "Train Epoch: 29 [126720/209539 (60%)]\tAll Loss: 1.2693\tTriple Loss(1): 0.0133\tClassification Loss: 1.2427\r\n",
      "Train Epoch: 29 [127360/209539 (61%)]\tAll Loss: 2.0171\tTriple Loss(0): 0.2932\tClassification Loss: 1.4308\r\n",
      "Train Epoch: 29 [128000/209539 (61%)]\tAll Loss: 1.6800\tTriple Loss(1): 0.0110\tClassification Loss: 1.6579\r\n",
      "Train Epoch: 29 [128640/209539 (61%)]\tAll Loss: 1.0103\tTriple Loss(1): 0.0198\tClassification Loss: 0.9708\r\n",
      "Train Epoch: 29 [129280/209539 (62%)]\tAll Loss: 1.5183\tTriple Loss(1): 0.0200\tClassification Loss: 1.4784\r\n",
      "Train Epoch: 29 [129920/209539 (62%)]\tAll Loss: 1.1540\tTriple Loss(1): 0.0102\tClassification Loss: 1.1336\r\n",
      "Train Epoch: 29 [130560/209539 (62%)]\tAll Loss: 0.8897\tTriple Loss(1): 0.0013\tClassification Loss: 0.8870\r\n",
      "Train Epoch: 29 [131200/209539 (63%)]\tAll Loss: 1.1520\tTriple Loss(1): 0.0492\tClassification Loss: 1.0536\r\n",
      "Train Epoch: 29 [131840/209539 (63%)]\tAll Loss: 1.2527\tTriple Loss(1): 0.0200\tClassification Loss: 1.2128\r\n",
      "Train Epoch: 29 [132480/209539 (63%)]\tAll Loss: 1.6570\tTriple Loss(0): 0.3481\tClassification Loss: 0.9607\r\n",
      "Train Epoch: 29 [133120/209539 (64%)]\tAll Loss: 1.6289\tTriple Loss(0): 0.2373\tClassification Loss: 1.1544\r\n",
      "Train Epoch: 29 [133760/209539 (64%)]\tAll Loss: 1.9584\tTriple Loss(0): 0.2947\tClassification Loss: 1.3691\r\n",
      "Train Epoch: 29 [134400/209539 (64%)]\tAll Loss: 1.5939\tTriple Loss(0): 0.1565\tClassification Loss: 1.2809\r\n",
      "Train Epoch: 29 [135040/209539 (64%)]\tAll Loss: 2.0461\tTriple Loss(0): 0.3448\tClassification Loss: 1.3565\r\n",
      "Train Epoch: 29 [135680/209539 (65%)]\tAll Loss: 1.5351\tTriple Loss(1): 0.0000\tClassification Loss: 1.5351\r\n",
      "Train Epoch: 29 [136320/209539 (65%)]\tAll Loss: 1.2694\tTriple Loss(1): 0.0193\tClassification Loss: 1.2309\r\n",
      "Train Epoch: 29 [136960/209539 (65%)]\tAll Loss: 2.0243\tTriple Loss(0): 0.4565\tClassification Loss: 1.1113\r\n",
      "Train Epoch: 29 [137600/209539 (66%)]\tAll Loss: 1.2593\tTriple Loss(1): 0.0374\tClassification Loss: 1.1846\r\n",
      "Train Epoch: 29 [138240/209539 (66%)]\tAll Loss: 1.7053\tTriple Loss(0): 0.1833\tClassification Loss: 1.3386\r\n",
      "Train Epoch: 29 [138880/209539 (66%)]\tAll Loss: 1.4420\tTriple Loss(1): 0.0817\tClassification Loss: 1.2786\r\n",
      "Train Epoch: 29 [139520/209539 (67%)]\tAll Loss: 1.2239\tTriple Loss(1): 0.0114\tClassification Loss: 1.2011\r\n",
      "Train Epoch: 29 [140160/209539 (67%)]\tAll Loss: 1.3796\tTriple Loss(1): 0.0000\tClassification Loss: 1.3796\r\n",
      "Train Epoch: 29 [140800/209539 (67%)]\tAll Loss: 1.4574\tTriple Loss(1): 0.0286\tClassification Loss: 1.4003\r\n",
      "Train Epoch: 29 [141440/209539 (68%)]\tAll Loss: 1.3691\tTriple Loss(1): 0.0000\tClassification Loss: 1.3691\r\n",
      "Train Epoch: 29 [142080/209539 (68%)]\tAll Loss: 1.1979\tTriple Loss(1): 0.0055\tClassification Loss: 1.1869\r\n",
      "Train Epoch: 29 [142720/209539 (68%)]\tAll Loss: 1.6783\tTriple Loss(1): 0.0354\tClassification Loss: 1.6074\r\n",
      "Train Epoch: 29 [143360/209539 (68%)]\tAll Loss: 1.0148\tTriple Loss(1): 0.0093\tClassification Loss: 0.9962\r\n",
      "Train Epoch: 29 [144000/209539 (69%)]\tAll Loss: 1.5760\tTriple Loss(0): 0.2315\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 29 [144640/209539 (69%)]\tAll Loss: 1.1771\tTriple Loss(1): 0.0284\tClassification Loss: 1.1202\r\n",
      "Train Epoch: 29 [145280/209539 (69%)]\tAll Loss: 1.4846\tTriple Loss(1): 0.0052\tClassification Loss: 1.4742\r\n",
      "Train Epoch: 29 [145920/209539 (70%)]\tAll Loss: 1.2454\tTriple Loss(1): 0.0167\tClassification Loss: 1.2119\r\n",
      "Train Epoch: 29 [146560/209539 (70%)]\tAll Loss: 1.7340\tTriple Loss(0): 0.3779\tClassification Loss: 0.9781\r\n",
      "Train Epoch: 29 [147200/209539 (70%)]\tAll Loss: 1.3821\tTriple Loss(1): 0.0000\tClassification Loss: 1.3821\r\n",
      "Train Epoch: 29 [147840/209539 (71%)]\tAll Loss: 1.7279\tTriple Loss(0): 0.3317\tClassification Loss: 1.0646\r\n",
      "Train Epoch: 29 [148480/209539 (71%)]\tAll Loss: 0.9369\tTriple Loss(1): 0.0354\tClassification Loss: 0.8661\r\n",
      "Train Epoch: 29 [149120/209539 (71%)]\tAll Loss: 1.3285\tTriple Loss(1): 0.0479\tClassification Loss: 1.2327\r\n",
      "Train Epoch: 29 [149760/209539 (71%)]\tAll Loss: 1.1897\tTriple Loss(1): 0.0494\tClassification Loss: 1.0910\r\n",
      "Train Epoch: 29 [150400/209539 (72%)]\tAll Loss: 1.6982\tTriple Loss(0): 0.2007\tClassification Loss: 1.2968\r\n",
      "Train Epoch: 29 [151040/209539 (72%)]\tAll Loss: 1.7667\tTriple Loss(0): 0.3109\tClassification Loss: 1.1450\r\n",
      "Train Epoch: 29 [151680/209539 (72%)]\tAll Loss: 1.4605\tTriple Loss(1): 0.0395\tClassification Loss: 1.3815\r\n",
      "Train Epoch: 29 [152320/209539 (73%)]\tAll Loss: 1.2543\tTriple Loss(1): 0.0077\tClassification Loss: 1.2390\r\n",
      "Train Epoch: 29 [152960/209539 (73%)]\tAll Loss: 1.2517\tTriple Loss(1): 0.0817\tClassification Loss: 1.0883\r\n",
      "Train Epoch: 29 [153600/209539 (73%)]\tAll Loss: 1.4228\tTriple Loss(1): 0.0788\tClassification Loss: 1.2652\r\n",
      "Train Epoch: 29 [154240/209539 (74%)]\tAll Loss: 2.0234\tTriple Loss(0): 0.4031\tClassification Loss: 1.2173\r\n",
      "Train Epoch: 29 [154880/209539 (74%)]\tAll Loss: 1.3312\tTriple Loss(1): 0.0064\tClassification Loss: 1.3185\r\n",
      "Train Epoch: 29 [155520/209539 (74%)]\tAll Loss: 1.3396\tTriple Loss(1): 0.0962\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 29 [156160/209539 (75%)]\tAll Loss: 1.2235\tTriple Loss(1): 0.0162\tClassification Loss: 1.1912\r\n",
      "Train Epoch: 29 [156800/209539 (75%)]\tAll Loss: 1.6690\tTriple Loss(1): 0.0192\tClassification Loss: 1.6306\r\n",
      "Train Epoch: 29 [157440/209539 (75%)]\tAll Loss: 1.4162\tTriple Loss(1): 0.0018\tClassification Loss: 1.4127\r\n",
      "Train Epoch: 29 [158080/209539 (75%)]\tAll Loss: 1.3274\tTriple Loss(1): 0.0568\tClassification Loss: 1.2137\r\n",
      "Train Epoch: 29 [158720/209539 (76%)]\tAll Loss: 1.0628\tTriple Loss(1): 0.0237\tClassification Loss: 1.0155\r\n",
      "Train Epoch: 29 [159360/209539 (76%)]\tAll Loss: 0.9919\tTriple Loss(1): 0.0005\tClassification Loss: 0.9910\r\n",
      "Train Epoch: 29 [160000/209539 (76%)]\tAll Loss: 1.6827\tTriple Loss(1): 0.0000\tClassification Loss: 1.6827\r\n",
      "Train Epoch: 29 [160640/209539 (77%)]\tAll Loss: 1.6186\tTriple Loss(0): 0.2573\tClassification Loss: 1.1040\r\n",
      "Train Epoch: 29 [161280/209539 (77%)]\tAll Loss: 1.9306\tTriple Loss(0): 0.3436\tClassification Loss: 1.2434\r\n",
      "Train Epoch: 29 [161920/209539 (77%)]\tAll Loss: 1.1753\tTriple Loss(1): 0.0126\tClassification Loss: 1.1501\r\n",
      "Train Epoch: 29 [162560/209539 (78%)]\tAll Loss: 1.2936\tTriple Loss(1): 0.0224\tClassification Loss: 1.2488\r\n",
      "Train Epoch: 29 [163200/209539 (78%)]\tAll Loss: 1.0391\tTriple Loss(1): 0.0227\tClassification Loss: 0.9937\r\n",
      "Train Epoch: 29 [163840/209539 (78%)]\tAll Loss: 1.8505\tTriple Loss(0): 0.3307\tClassification Loss: 1.1891\r\n",
      "Train Epoch: 29 [164480/209539 (78%)]\tAll Loss: 1.0729\tTriple Loss(1): 0.0190\tClassification Loss: 1.0350\r\n",
      "Train Epoch: 29 [165120/209539 (79%)]\tAll Loss: 1.1666\tTriple Loss(1): 0.0019\tClassification Loss: 1.1627\r\n",
      "Train Epoch: 29 [165760/209539 (79%)]\tAll Loss: 2.0918\tTriple Loss(0): 0.3309\tClassification Loss: 1.4301\r\n",
      "Train Epoch: 29 [166400/209539 (79%)]\tAll Loss: 1.5095\tTriple Loss(1): 0.0590\tClassification Loss: 1.3915\r\n",
      "Train Epoch: 29 [167040/209539 (80%)]\tAll Loss: 1.3587\tTriple Loss(1): 0.0298\tClassification Loss: 1.2990\r\n",
      "Train Epoch: 29 [167680/209539 (80%)]\tAll Loss: 1.3021\tTriple Loss(1): 0.0162\tClassification Loss: 1.2698\r\n",
      "Train Epoch: 29 [168320/209539 (80%)]\tAll Loss: 1.0107\tTriple Loss(1): 0.0050\tClassification Loss: 1.0007\r\n",
      "Train Epoch: 29 [168960/209539 (81%)]\tAll Loss: 1.7052\tTriple Loss(0): 0.2422\tClassification Loss: 1.2208\r\n",
      "Train Epoch: 29 [169600/209539 (81%)]\tAll Loss: 1.3608\tTriple Loss(1): 0.0723\tClassification Loss: 1.2161\r\n",
      "Train Epoch: 29 [170240/209539 (81%)]\tAll Loss: 1.3009\tTriple Loss(1): 0.0078\tClassification Loss: 1.2854\r\n",
      "Train Epoch: 29 [170880/209539 (82%)]\tAll Loss: 1.3859\tTriple Loss(1): 0.0694\tClassification Loss: 1.2470\r\n",
      "Train Epoch: 29 [171520/209539 (82%)]\tAll Loss: 1.1272\tTriple Loss(1): 0.0352\tClassification Loss: 1.0568\r\n",
      "Train Epoch: 29 [172160/209539 (82%)]\tAll Loss: 1.4884\tTriple Loss(1): 0.0203\tClassification Loss: 1.4478\r\n",
      "Train Epoch: 29 [172800/209539 (82%)]\tAll Loss: 1.2623\tTriple Loss(1): 0.0034\tClassification Loss: 1.2555\r\n",
      "Train Epoch: 29 [173440/209539 (83%)]\tAll Loss: 1.6554\tTriple Loss(1): 0.0489\tClassification Loss: 1.5577\r\n",
      "Train Epoch: 29 [174080/209539 (83%)]\tAll Loss: 1.2398\tTriple Loss(1): 0.0194\tClassification Loss: 1.2011\r\n",
      "Train Epoch: 29 [174720/209539 (83%)]\tAll Loss: 1.6564\tTriple Loss(0): 0.2014\tClassification Loss: 1.2536\r\n",
      "Train Epoch: 29 [175360/209539 (84%)]\tAll Loss: 1.3644\tTriple Loss(1): 0.0095\tClassification Loss: 1.3454\r\n",
      "Train Epoch: 29 [176000/209539 (84%)]\tAll Loss: 1.2028\tTriple Loss(1): 0.0078\tClassification Loss: 1.1872\r\n",
      "Train Epoch: 29 [176640/209539 (84%)]\tAll Loss: 1.6169\tTriple Loss(0): 0.3623\tClassification Loss: 0.8922\r\n",
      "Train Epoch: 29 [177280/209539 (85%)]\tAll Loss: 1.5416\tTriple Loss(1): 0.0462\tClassification Loss: 1.4492\r\n",
      "Train Epoch: 29 [177920/209539 (85%)]\tAll Loss: 1.5565\tTriple Loss(1): 0.0000\tClassification Loss: 1.5565\r\n",
      "Train Epoch: 29 [178560/209539 (85%)]\tAll Loss: 1.1421\tTriple Loss(1): 0.0231\tClassification Loss: 1.0958\r\n",
      "Train Epoch: 29 [179200/209539 (86%)]\tAll Loss: 1.4402\tTriple Loss(1): 0.0058\tClassification Loss: 1.4286\r\n",
      "Train Epoch: 29 [179840/209539 (86%)]\tAll Loss: 1.3292\tTriple Loss(1): 0.0164\tClassification Loss: 1.2964\r\n",
      "Train Epoch: 29 [180480/209539 (86%)]\tAll Loss: 1.4329\tTriple Loss(1): 0.0584\tClassification Loss: 1.3161\r\n",
      "Train Epoch: 29 [181120/209539 (86%)]\tAll Loss: 1.5804\tTriple Loss(1): 0.0110\tClassification Loss: 1.5585\r\n",
      "Train Epoch: 29 [181760/209539 (87%)]\tAll Loss: 1.0635\tTriple Loss(1): 0.0000\tClassification Loss: 1.0635\r\n",
      "Train Epoch: 29 [182400/209539 (87%)]\tAll Loss: 1.5709\tTriple Loss(1): 0.0302\tClassification Loss: 1.5106\r\n",
      "Train Epoch: 29 [183040/209539 (87%)]\tAll Loss: 1.8796\tTriple Loss(0): 0.3086\tClassification Loss: 1.2624\r\n",
      "Train Epoch: 29 [183680/209539 (88%)]\tAll Loss: 1.2789\tTriple Loss(1): 0.0732\tClassification Loss: 1.1325\r\n",
      "Train Epoch: 29 [184320/209539 (88%)]\tAll Loss: 1.2351\tTriple Loss(1): 0.0152\tClassification Loss: 1.2047\r\n",
      "Train Epoch: 29 [184960/209539 (88%)]\tAll Loss: 1.1783\tTriple Loss(1): 0.0004\tClassification Loss: 1.1776\r\n",
      "Train Epoch: 29 [185600/209539 (89%)]\tAll Loss: 1.2402\tTriple Loss(1): 0.0448\tClassification Loss: 1.1506\r\n",
      "Train Epoch: 29 [186240/209539 (89%)]\tAll Loss: 1.4194\tTriple Loss(1): 0.0474\tClassification Loss: 1.3247\r\n",
      "Train Epoch: 29 [186880/209539 (89%)]\tAll Loss: 1.4555\tTriple Loss(0): 0.2205\tClassification Loss: 1.0145\r\n",
      "Train Epoch: 29 [187520/209539 (89%)]\tAll Loss: 1.3831\tTriple Loss(1): 0.0149\tClassification Loss: 1.3533\r\n",
      "Train Epoch: 29 [188160/209539 (90%)]\tAll Loss: 0.9467\tTriple Loss(1): 0.0368\tClassification Loss: 0.8732\r\n",
      "Train Epoch: 29 [188800/209539 (90%)]\tAll Loss: 1.3055\tTriple Loss(1): 0.0382\tClassification Loss: 1.2291\r\n",
      "Train Epoch: 29 [189440/209539 (90%)]\tAll Loss: 1.0079\tTriple Loss(1): 0.0148\tClassification Loss: 0.9782\r\n",
      "Train Epoch: 29 [190080/209539 (91%)]\tAll Loss: 1.8255\tTriple Loss(0): 0.3967\tClassification Loss: 1.0321\r\n",
      "Train Epoch: 29 [190720/209539 (91%)]\tAll Loss: 1.2821\tTriple Loss(1): 0.0546\tClassification Loss: 1.1730\r\n",
      "Train Epoch: 29 [191360/209539 (91%)]\tAll Loss: 1.2102\tTriple Loss(1): 0.0018\tClassification Loss: 1.2065\r\n",
      "Train Epoch: 29 [192000/209539 (92%)]\tAll Loss: 1.6441\tTriple Loss(1): 0.0588\tClassification Loss: 1.5265\r\n",
      "Train Epoch: 29 [192640/209539 (92%)]\tAll Loss: 1.4838\tTriple Loss(1): 0.1283\tClassification Loss: 1.2272\r\n",
      "Train Epoch: 29 [193280/209539 (92%)]\tAll Loss: 1.2909\tTriple Loss(1): 0.0467\tClassification Loss: 1.1976\r\n",
      "Train Epoch: 29 [193920/209539 (93%)]\tAll Loss: 1.1881\tTriple Loss(1): 0.0221\tClassification Loss: 1.1439\r\n",
      "Train Epoch: 29 [194560/209539 (93%)]\tAll Loss: 1.9788\tTriple Loss(0): 0.3887\tClassification Loss: 1.2015\r\n",
      "Train Epoch: 29 [195200/209539 (93%)]\tAll Loss: 1.2732\tTriple Loss(1): 0.0000\tClassification Loss: 1.2732\r\n",
      "Train Epoch: 29 [195840/209539 (93%)]\tAll Loss: 1.6157\tTriple Loss(0): 0.2974\tClassification Loss: 1.0208\r\n",
      "Train Epoch: 29 [196480/209539 (94%)]\tAll Loss: 1.6276\tTriple Loss(1): 0.0231\tClassification Loss: 1.5814\r\n",
      "Train Epoch: 29 [197120/209539 (94%)]\tAll Loss: 1.2389\tTriple Loss(1): 0.0194\tClassification Loss: 1.2002\r\n",
      "Train Epoch: 29 [197760/209539 (94%)]\tAll Loss: 1.3147\tTriple Loss(1): 0.0123\tClassification Loss: 1.2900\r\n",
      "Train Epoch: 29 [198400/209539 (95%)]\tAll Loss: 1.4156\tTriple Loss(1): 0.0692\tClassification Loss: 1.2773\r\n",
      "Train Epoch: 29 [199040/209539 (95%)]\tAll Loss: 2.1095\tTriple Loss(0): 0.3578\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 29 [199680/209539 (95%)]\tAll Loss: 1.2680\tTriple Loss(1): 0.0145\tClassification Loss: 1.2389\r\n",
      "Train Epoch: 29 [200320/209539 (96%)]\tAll Loss: 1.1696\tTriple Loss(1): 0.0000\tClassification Loss: 1.1696\r\n",
      "Train Epoch: 29 [200960/209539 (96%)]\tAll Loss: 1.0728\tTriple Loss(1): 0.0210\tClassification Loss: 1.0307\r\n",
      "Train Epoch: 29 [201600/209539 (96%)]\tAll Loss: 1.2305\tTriple Loss(1): 0.0435\tClassification Loss: 1.1434\r\n",
      "Train Epoch: 29 [202240/209539 (97%)]\tAll Loss: 1.7183\tTriple Loss(0): 0.2793\tClassification Loss: 1.1596\r\n",
      "Train Epoch: 29 [202880/209539 (97%)]\tAll Loss: 1.1045\tTriple Loss(1): 0.0503\tClassification Loss: 1.0040\r\n",
      "Train Epoch: 29 [203520/209539 (97%)]\tAll Loss: 1.5019\tTriple Loss(1): 0.0122\tClassification Loss: 1.4776\r\n",
      "Train Epoch: 29 [204160/209539 (97%)]\tAll Loss: 2.1577\tTriple Loss(0): 0.2699\tClassification Loss: 1.6179\r\n",
      "Train Epoch: 29 [204800/209539 (98%)]\tAll Loss: 2.1993\tTriple Loss(0): 0.3720\tClassification Loss: 1.4554\r\n",
      "Train Epoch: 29 [205440/209539 (98%)]\tAll Loss: 1.0767\tTriple Loss(1): 0.0146\tClassification Loss: 1.0475\r\n",
      "Train Epoch: 29 [206080/209539 (98%)]\tAll Loss: 1.3966\tTriple Loss(1): 0.0304\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 29 [206720/209539 (99%)]\tAll Loss: 1.5500\tTriple Loss(0): 0.2297\tClassification Loss: 1.0906\r\n",
      "Train Epoch: 29 [207360/209539 (99%)]\tAll Loss: 1.2159\tTriple Loss(1): 0.0656\tClassification Loss: 1.0847\r\n",
      "Train Epoch: 29 [208000/209539 (99%)]\tAll Loss: 1.3886\tTriple Loss(0): 0.2002\tClassification Loss: 0.9881\r\n",
      "Train Epoch: 29 [208640/209539 (100%)]\tAll Loss: 1.3577\tTriple Loss(1): 0.0212\tClassification Loss: 1.3154\r\n",
      "Train Epoch: 29 [209280/209539 (100%)]\tAll Loss: 1.5563\tTriple Loss(1): 0.0599\tClassification Loss: 1.4366\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/29_epochs\r\n",
      "Train Epoch: 30 [0/209539 (0%)]\tAll Loss: 1.5283\tTriple Loss(1): 0.0430\tClassification Loss: 1.4423\r\n",
      "\r\n",
      "Test set: Average loss: 1.0996\r\n",
      "Top 1 Accuracy: 54560/80128 (68%)\r\n",
      "Top 3 Accuracy: 69873/80128 (87%)\r\n",
      "Top 5 Accuracy: 74525/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 30 [640/209539 (0%)]\tAll Loss: 1.2538\tTriple Loss(1): 0.0000\tClassification Loss: 1.2538\r\n",
      "Train Epoch: 30 [1280/209539 (1%)]\tAll Loss: 1.5967\tTriple Loss(0): 0.2408\tClassification Loss: 1.1151\r\n",
      "Train Epoch: 30 [1920/209539 (1%)]\tAll Loss: 1.6536\tTriple Loss(0): 0.1992\tClassification Loss: 1.2553\r\n",
      "Train Epoch: 30 [2560/209539 (1%)]\tAll Loss: 2.0880\tTriple Loss(0): 0.2922\tClassification Loss: 1.5036\r\n",
      "Train Epoch: 30 [3200/209539 (2%)]\tAll Loss: 1.2311\tTriple Loss(1): 0.0137\tClassification Loss: 1.2038\r\n",
      "Train Epoch: 30 [3840/209539 (2%)]\tAll Loss: 1.3095\tTriple Loss(1): 0.0049\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 30 [4480/209539 (2%)]\tAll Loss: 1.8012\tTriple Loss(0): 0.2505\tClassification Loss: 1.3002\r\n",
      "Train Epoch: 30 [5120/209539 (2%)]\tAll Loss: 1.1734\tTriple Loss(1): 0.0033\tClassification Loss: 1.1668\r\n",
      "Train Epoch: 30 [5760/209539 (3%)]\tAll Loss: 1.4638\tTriple Loss(1): 0.0553\tClassification Loss: 1.3532\r\n",
      "Train Epoch: 30 [6400/209539 (3%)]\tAll Loss: 1.1159\tTriple Loss(1): 0.0471\tClassification Loss: 1.0217\r\n",
      "Train Epoch: 30 [7040/209539 (3%)]\tAll Loss: 1.1408\tTriple Loss(1): 0.0114\tClassification Loss: 1.1179\r\n",
      "Train Epoch: 30 [7680/209539 (4%)]\tAll Loss: 1.3058\tTriple Loss(1): 0.0954\tClassification Loss: 1.1149\r\n",
      "Train Epoch: 30 [8320/209539 (4%)]\tAll Loss: 1.2273\tTriple Loss(1): 0.0000\tClassification Loss: 1.2273\r\n",
      "Train Epoch: 30 [8960/209539 (4%)]\tAll Loss: 1.2379\tTriple Loss(1): 0.0060\tClassification Loss: 1.2259\r\n",
      "Train Epoch: 30 [9600/209539 (5%)]\tAll Loss: 1.2985\tTriple Loss(1): 0.0000\tClassification Loss: 1.2985\r\n",
      "Train Epoch: 30 [10240/209539 (5%)]\tAll Loss: 1.3157\tTriple Loss(0): 0.1992\tClassification Loss: 0.9174\r\n",
      "Train Epoch: 30 [10880/209539 (5%)]\tAll Loss: 1.9141\tTriple Loss(0): 0.3730\tClassification Loss: 1.1682\r\n",
      "Train Epoch: 30 [11520/209539 (5%)]\tAll Loss: 1.2199\tTriple Loss(1): 0.0174\tClassification Loss: 1.1852\r\n",
      "Train Epoch: 30 [12160/209539 (6%)]\tAll Loss: 1.1302\tTriple Loss(1): 0.0115\tClassification Loss: 1.1071\r\n",
      "Train Epoch: 30 [12800/209539 (6%)]\tAll Loss: 1.0239\tTriple Loss(1): 0.0477\tClassification Loss: 0.9286\r\n",
      "Train Epoch: 30 [13440/209539 (6%)]\tAll Loss: 1.1342\tTriple Loss(1): 0.0679\tClassification Loss: 0.9984\r\n",
      "Train Epoch: 30 [14080/209539 (7%)]\tAll Loss: 1.7621\tTriple Loss(0): 0.2266\tClassification Loss: 1.3090\r\n",
      "Train Epoch: 30 [14720/209539 (7%)]\tAll Loss: 1.7540\tTriple Loss(0): 0.2331\tClassification Loss: 1.2877\r\n",
      "Train Epoch: 30 [15360/209539 (7%)]\tAll Loss: 1.2938\tTriple Loss(1): 0.0000\tClassification Loss: 1.2938\r\n",
      "Train Epoch: 30 [16000/209539 (8%)]\tAll Loss: 1.5675\tTriple Loss(1): 0.0717\tClassification Loss: 1.4241\r\n",
      "Train Epoch: 30 [16640/209539 (8%)]\tAll Loss: 2.1581\tTriple Loss(0): 0.2654\tClassification Loss: 1.6274\r\n",
      "Train Epoch: 30 [17280/209539 (8%)]\tAll Loss: 1.2080\tTriple Loss(1): 0.0114\tClassification Loss: 1.1852\r\n",
      "Train Epoch: 30 [17920/209539 (9%)]\tAll Loss: 1.4483\tTriple Loss(1): 0.0924\tClassification Loss: 1.2634\r\n",
      "Train Epoch: 30 [18560/209539 (9%)]\tAll Loss: 1.9296\tTriple Loss(0): 0.3767\tClassification Loss: 1.1762\r\n",
      "Train Epoch: 30 [19200/209539 (9%)]\tAll Loss: 1.6420\tTriple Loss(0): 0.2438\tClassification Loss: 1.1544\r\n",
      "Train Epoch: 30 [19840/209539 (9%)]\tAll Loss: 1.1762\tTriple Loss(1): 0.0000\tClassification Loss: 1.1762\r\n",
      "Train Epoch: 30 [20480/209539 (10%)]\tAll Loss: 1.1255\tTriple Loss(1): 0.0144\tClassification Loss: 1.0967\r\n",
      "Train Epoch: 30 [21120/209539 (10%)]\tAll Loss: 1.5245\tTriple Loss(1): 0.0268\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 30 [21760/209539 (10%)]\tAll Loss: 1.2347\tTriple Loss(1): 0.0000\tClassification Loss: 1.2347\r\n",
      "Train Epoch: 30 [22400/209539 (11%)]\tAll Loss: 1.2629\tTriple Loss(1): 0.0299\tClassification Loss: 1.2032\r\n",
      "Train Epoch: 30 [23040/209539 (11%)]\tAll Loss: 1.6775\tTriple Loss(0): 0.2950\tClassification Loss: 1.0876\r\n",
      "Train Epoch: 30 [23680/209539 (11%)]\tAll Loss: 1.5540\tTriple Loss(0): 0.2761\tClassification Loss: 1.0018\r\n",
      "Train Epoch: 30 [24320/209539 (12%)]\tAll Loss: 1.1716\tTriple Loss(1): 0.0072\tClassification Loss: 1.1573\r\n",
      "Train Epoch: 30 [24960/209539 (12%)]\tAll Loss: 1.0760\tTriple Loss(1): 0.0433\tClassification Loss: 0.9894\r\n",
      "Train Epoch: 30 [25600/209539 (12%)]\tAll Loss: 1.2047\tTriple Loss(1): 0.0292\tClassification Loss: 1.1462\r\n",
      "Train Epoch: 30 [26240/209539 (13%)]\tAll Loss: 1.3621\tTriple Loss(0): 0.1804\tClassification Loss: 1.0014\r\n",
      "Train Epoch: 30 [26880/209539 (13%)]\tAll Loss: 1.3888\tTriple Loss(1): 0.0290\tClassification Loss: 1.3308\r\n",
      "Train Epoch: 30 [27520/209539 (13%)]\tAll Loss: 2.0277\tTriple Loss(0): 0.4519\tClassification Loss: 1.1239\r\n",
      "Train Epoch: 30 [28160/209539 (13%)]\tAll Loss: 1.9539\tTriple Loss(0): 0.3145\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 30 [28800/209539 (14%)]\tAll Loss: 1.5712\tTriple Loss(1): 0.0000\tClassification Loss: 1.5712\r\n",
      "Train Epoch: 30 [29440/209539 (14%)]\tAll Loss: 2.1416\tTriple Loss(0): 0.3665\tClassification Loss: 1.4086\r\n",
      "Train Epoch: 30 [30080/209539 (14%)]\tAll Loss: 1.1797\tTriple Loss(1): 0.0000\tClassification Loss: 1.1797\r\n",
      "Train Epoch: 30 [30720/209539 (15%)]\tAll Loss: 1.3191\tTriple Loss(0): 0.1497\tClassification Loss: 1.0198\r\n",
      "Train Epoch: 30 [31360/209539 (15%)]\tAll Loss: 1.5669\tTriple Loss(0): 0.2248\tClassification Loss: 1.1173\r\n",
      "Train Epoch: 30 [32000/209539 (15%)]\tAll Loss: 1.4742\tTriple Loss(1): 0.0293\tClassification Loss: 1.4157\r\n",
      "Train Epoch: 30 [32640/209539 (16%)]\tAll Loss: 1.3957\tTriple Loss(1): 0.0000\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 30 [33280/209539 (16%)]\tAll Loss: 1.2471\tTriple Loss(1): 0.0000\tClassification Loss: 1.2471\r\n",
      "Train Epoch: 30 [33920/209539 (16%)]\tAll Loss: 1.3191\tTriple Loss(1): 0.0116\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 30 [34560/209539 (16%)]\tAll Loss: 1.2171\tTriple Loss(1): 0.0025\tClassification Loss: 1.2121\r\n",
      "Train Epoch: 30 [35200/209539 (17%)]\tAll Loss: 1.2140\tTriple Loss(1): 0.0583\tClassification Loss: 1.0974\r\n",
      "Train Epoch: 30 [35840/209539 (17%)]\tAll Loss: 1.1228\tTriple Loss(1): 0.0237\tClassification Loss: 1.0753\r\n",
      "Train Epoch: 30 [36480/209539 (17%)]\tAll Loss: 1.2335\tTriple Loss(1): 0.0000\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 30 [37120/209539 (18%)]\tAll Loss: 2.1749\tTriple Loss(0): 0.3722\tClassification Loss: 1.4305\r\n",
      "Train Epoch: 30 [37760/209539 (18%)]\tAll Loss: 1.4299\tTriple Loss(1): 0.0355\tClassification Loss: 1.3589\r\n",
      "Train Epoch: 30 [38400/209539 (18%)]\tAll Loss: 1.5477\tTriple Loss(1): 0.0441\tClassification Loss: 1.4594\r\n",
      "Train Epoch: 30 [39040/209539 (19%)]\tAll Loss: 1.0777\tTriple Loss(1): 0.0413\tClassification Loss: 0.9950\r\n",
      "Train Epoch: 30 [39680/209539 (19%)]\tAll Loss: 1.5717\tTriple Loss(0): 0.2030\tClassification Loss: 1.1656\r\n",
      "Train Epoch: 30 [40320/209539 (19%)]\tAll Loss: 1.3095\tTriple Loss(1): 0.0407\tClassification Loss: 1.2281\r\n",
      "Train Epoch: 30 [40960/209539 (20%)]\tAll Loss: 1.2473\tTriple Loss(1): 0.0807\tClassification Loss: 1.0858\r\n",
      "Train Epoch: 30 [41600/209539 (20%)]\tAll Loss: 1.1989\tTriple Loss(1): 0.0268\tClassification Loss: 1.1453\r\n",
      "Train Epoch: 30 [42240/209539 (20%)]\tAll Loss: 1.4018\tTriple Loss(0): 0.1406\tClassification Loss: 1.1206\r\n",
      "Train Epoch: 30 [42880/209539 (20%)]\tAll Loss: 1.1334\tTriple Loss(1): 0.0715\tClassification Loss: 0.9904\r\n",
      "Train Epoch: 30 [43520/209539 (21%)]\tAll Loss: 2.4169\tTriple Loss(0): 0.5241\tClassification Loss: 1.3686\r\n",
      "Train Epoch: 30 [44160/209539 (21%)]\tAll Loss: 1.6684\tTriple Loss(1): 0.0617\tClassification Loss: 1.5451\r\n",
      "Train Epoch: 30 [44800/209539 (21%)]\tAll Loss: 1.5362\tTriple Loss(1): 0.0006\tClassification Loss: 1.5350\r\n",
      "Train Epoch: 30 [45440/209539 (22%)]\tAll Loss: 1.7437\tTriple Loss(1): 0.0024\tClassification Loss: 1.7388\r\n",
      "Train Epoch: 30 [46080/209539 (22%)]\tAll Loss: 1.0534\tTriple Loss(1): 0.0246\tClassification Loss: 1.0043\r\n",
      "Train Epoch: 30 [46720/209539 (22%)]\tAll Loss: 1.7687\tTriple Loss(1): 0.1084\tClassification Loss: 1.5518\r\n",
      "Train Epoch: 30 [47360/209539 (23%)]\tAll Loss: 0.8880\tTriple Loss(1): 0.0219\tClassification Loss: 0.8441\r\n",
      "Train Epoch: 30 [48000/209539 (23%)]\tAll Loss: 1.0598\tTriple Loss(1): 0.0015\tClassification Loss: 1.0568\r\n",
      "Train Epoch: 30 [48640/209539 (23%)]\tAll Loss: 1.5679\tTriple Loss(1): 0.0256\tClassification Loss: 1.5166\r\n",
      "Train Epoch: 30 [49280/209539 (24%)]\tAll Loss: 1.3311\tTriple Loss(1): 0.0215\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 30 [49920/209539 (24%)]\tAll Loss: 1.3408\tTriple Loss(1): 0.0259\tClassification Loss: 1.2890\r\n",
      "Train Epoch: 30 [50560/209539 (24%)]\tAll Loss: 1.4916\tTriple Loss(1): 0.0300\tClassification Loss: 1.4316\r\n",
      "Train Epoch: 30 [51200/209539 (24%)]\tAll Loss: 1.4223\tTriple Loss(1): 0.0176\tClassification Loss: 1.3872\r\n",
      "Train Epoch: 30 [51840/209539 (25%)]\tAll Loss: 1.1612\tTriple Loss(1): 0.0034\tClassification Loss: 1.1544\r\n",
      "Train Epoch: 30 [52480/209539 (25%)]\tAll Loss: 1.9909\tTriple Loss(0): 0.4112\tClassification Loss: 1.1684\r\n",
      "Train Epoch: 30 [53120/209539 (25%)]\tAll Loss: 1.4453\tTriple Loss(1): 0.0821\tClassification Loss: 1.2812\r\n",
      "Train Epoch: 30 [53760/209539 (26%)]\tAll Loss: 1.8576\tTriple Loss(1): 0.1620\tClassification Loss: 1.5336\r\n",
      "Train Epoch: 30 [54400/209539 (26%)]\tAll Loss: 1.7282\tTriple Loss(1): 0.0370\tClassification Loss: 1.6542\r\n",
      "Train Epoch: 30 [55040/209539 (26%)]\tAll Loss: 0.9076\tTriple Loss(1): 0.0161\tClassification Loss: 0.8755\r\n",
      "Train Epoch: 30 [55680/209539 (27%)]\tAll Loss: 2.5381\tTriple Loss(0): 0.5380\tClassification Loss: 1.4622\r\n",
      "Train Epoch: 30 [56320/209539 (27%)]\tAll Loss: 1.0157\tTriple Loss(1): 0.0453\tClassification Loss: 0.9252\r\n",
      "Train Epoch: 30 [56960/209539 (27%)]\tAll Loss: 1.3369\tTriple Loss(1): 0.0146\tClassification Loss: 1.3078\r\n",
      "Train Epoch: 30 [57600/209539 (27%)]\tAll Loss: 1.1714\tTriple Loss(1): 0.0108\tClassification Loss: 1.1497\r\n",
      "Train Epoch: 30 [58240/209539 (28%)]\tAll Loss: 1.3726\tTriple Loss(1): 0.1436\tClassification Loss: 1.0853\r\n",
      "Train Epoch: 30 [58880/209539 (28%)]\tAll Loss: 1.6534\tTriple Loss(0): 0.1938\tClassification Loss: 1.2659\r\n",
      "Train Epoch: 30 [59520/209539 (28%)]\tAll Loss: 1.3706\tTriple Loss(1): 0.0742\tClassification Loss: 1.2223\r\n",
      "Train Epoch: 30 [60160/209539 (29%)]\tAll Loss: 1.5395\tTriple Loss(1): 0.1018\tClassification Loss: 1.3359\r\n",
      "Train Epoch: 30 [60800/209539 (29%)]\tAll Loss: 1.3270\tTriple Loss(1): 0.0000\tClassification Loss: 1.3270\r\n",
      "Train Epoch: 30 [61440/209539 (29%)]\tAll Loss: 2.3529\tTriple Loss(0): 0.6047\tClassification Loss: 1.1434\r\n",
      "Train Epoch: 30 [62080/209539 (30%)]\tAll Loss: 1.4168\tTriple Loss(1): 0.0032\tClassification Loss: 1.4103\r\n",
      "Train Epoch: 30 [62720/209539 (30%)]\tAll Loss: 1.1703\tTriple Loss(1): 0.0089\tClassification Loss: 1.1525\r\n",
      "Train Epoch: 30 [63360/209539 (30%)]\tAll Loss: 1.2491\tTriple Loss(1): 0.0000\tClassification Loss: 1.2491\r\n",
      "Train Epoch: 30 [64000/209539 (31%)]\tAll Loss: 1.5014\tTriple Loss(1): 0.0000\tClassification Loss: 1.5014\r\n",
      "Train Epoch: 30 [64640/209539 (31%)]\tAll Loss: 2.1952\tTriple Loss(0): 0.2495\tClassification Loss: 1.6962\r\n",
      "Train Epoch: 30 [65280/209539 (31%)]\tAll Loss: 1.3262\tTriple Loss(1): 0.0032\tClassification Loss: 1.3197\r\n",
      "Train Epoch: 30 [65920/209539 (31%)]\tAll Loss: 1.4083\tTriple Loss(1): 0.0015\tClassification Loss: 1.4054\r\n",
      "Train Epoch: 30 [66560/209539 (32%)]\tAll Loss: 1.2787\tTriple Loss(1): 0.0310\tClassification Loss: 1.2166\r\n",
      "Train Epoch: 30 [67200/209539 (32%)]\tAll Loss: 1.2408\tTriple Loss(1): 0.0622\tClassification Loss: 1.1164\r\n",
      "Train Epoch: 30 [67840/209539 (32%)]\tAll Loss: 1.2544\tTriple Loss(1): 0.0027\tClassification Loss: 1.2489\r\n",
      "Train Epoch: 30 [68480/209539 (33%)]\tAll Loss: 1.3916\tTriple Loss(1): 0.0628\tClassification Loss: 1.2660\r\n",
      "Train Epoch: 30 [69120/209539 (33%)]\tAll Loss: 1.4685\tTriple Loss(1): 0.0322\tClassification Loss: 1.4041\r\n",
      "Train Epoch: 30 [69760/209539 (33%)]\tAll Loss: 1.0944\tTriple Loss(1): 0.0000\tClassification Loss: 1.0944\r\n",
      "Train Epoch: 30 [70400/209539 (34%)]\tAll Loss: 1.6883\tTriple Loss(0): 0.2078\tClassification Loss: 1.2727\r\n",
      "Train Epoch: 30 [71040/209539 (34%)]\tAll Loss: 1.8205\tTriple Loss(1): 0.1204\tClassification Loss: 1.5797\r\n",
      "Train Epoch: 30 [71680/209539 (34%)]\tAll Loss: 2.1028\tTriple Loss(0): 0.2512\tClassification Loss: 1.6003\r\n",
      "Train Epoch: 30 [72320/209539 (35%)]\tAll Loss: 1.1829\tTriple Loss(1): 0.0205\tClassification Loss: 1.1420\r\n",
      "Train Epoch: 30 [72960/209539 (35%)]\tAll Loss: 2.2758\tTriple Loss(0): 0.5116\tClassification Loss: 1.2526\r\n",
      "Train Epoch: 30 [73600/209539 (35%)]\tAll Loss: 1.8573\tTriple Loss(0): 0.2564\tClassification Loss: 1.3445\r\n",
      "Train Epoch: 30 [74240/209539 (35%)]\tAll Loss: 1.5421\tTriple Loss(1): 0.0508\tClassification Loss: 1.4406\r\n",
      "Train Epoch: 30 [74880/209539 (36%)]\tAll Loss: 1.7679\tTriple Loss(1): 0.0372\tClassification Loss: 1.6935\r\n",
      "Train Epoch: 30 [75520/209539 (36%)]\tAll Loss: 1.1249\tTriple Loss(1): 0.0278\tClassification Loss: 1.0693\r\n",
      "Train Epoch: 30 [76160/209539 (36%)]\tAll Loss: 1.6591\tTriple Loss(0): 0.3541\tClassification Loss: 0.9509\r\n",
      "Train Epoch: 30 [76800/209539 (37%)]\tAll Loss: 1.2161\tTriple Loss(1): 0.0153\tClassification Loss: 1.1856\r\n",
      "Train Epoch: 30 [77440/209539 (37%)]\tAll Loss: 1.2500\tTriple Loss(1): 0.0034\tClassification Loss: 1.2431\r\n",
      "Train Epoch: 30 [78080/209539 (37%)]\tAll Loss: 2.0376\tTriple Loss(0): 0.3786\tClassification Loss: 1.2803\r\n",
      "Train Epoch: 30 [78720/209539 (38%)]\tAll Loss: 1.2846\tTriple Loss(1): 0.0129\tClassification Loss: 1.2588\r\n",
      "Train Epoch: 30 [79360/209539 (38%)]\tAll Loss: 1.8877\tTriple Loss(0): 0.2716\tClassification Loss: 1.3444\r\n",
      "Train Epoch: 30 [80000/209539 (38%)]\tAll Loss: 1.2932\tTriple Loss(1): 0.0193\tClassification Loss: 1.2545\r\n",
      "Train Epoch: 30 [80640/209539 (38%)]\tAll Loss: 1.8716\tTriple Loss(0): 0.2661\tClassification Loss: 1.3395\r\n",
      "Train Epoch: 30 [81280/209539 (39%)]\tAll Loss: 1.4087\tTriple Loss(1): 0.0990\tClassification Loss: 1.2107\r\n",
      "Train Epoch: 30 [81920/209539 (39%)]\tAll Loss: 1.1664\tTriple Loss(1): 0.0262\tClassification Loss: 1.1140\r\n",
      "Train Epoch: 30 [82560/209539 (39%)]\tAll Loss: 1.3765\tTriple Loss(1): 0.0132\tClassification Loss: 1.3501\r\n",
      "Train Epoch: 30 [83200/209539 (40%)]\tAll Loss: 2.2124\tTriple Loss(0): 0.4095\tClassification Loss: 1.3933\r\n",
      "Train Epoch: 30 [83840/209539 (40%)]\tAll Loss: 1.3234\tTriple Loss(1): 0.0000\tClassification Loss: 1.3234\r\n",
      "Train Epoch: 30 [84480/209539 (40%)]\tAll Loss: 1.1930\tTriple Loss(1): 0.0425\tClassification Loss: 1.1080\r\n",
      "Train Epoch: 30 [85120/209539 (41%)]\tAll Loss: 1.6643\tTriple Loss(1): 0.0341\tClassification Loss: 1.5960\r\n",
      "Train Epoch: 30 [85760/209539 (41%)]\tAll Loss: 1.3639\tTriple Loss(1): 0.0036\tClassification Loss: 1.3568\r\n",
      "Train Epoch: 30 [86400/209539 (41%)]\tAll Loss: 1.3733\tTriple Loss(1): 0.0040\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 30 [87040/209539 (42%)]\tAll Loss: 1.0895\tTriple Loss(1): 0.0296\tClassification Loss: 1.0304\r\n",
      "Train Epoch: 30 [87680/209539 (42%)]\tAll Loss: 1.2727\tTriple Loss(1): 0.0828\tClassification Loss: 1.1072\r\n",
      "Train Epoch: 30 [88320/209539 (42%)]\tAll Loss: 1.7747\tTriple Loss(0): 0.2497\tClassification Loss: 1.2754\r\n",
      "Train Epoch: 30 [88960/209539 (42%)]\tAll Loss: 1.5319\tTriple Loss(1): 0.1020\tClassification Loss: 1.3278\r\n",
      "Train Epoch: 30 [89600/209539 (43%)]\tAll Loss: 1.4635\tTriple Loss(1): 0.0137\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 30 [90240/209539 (43%)]\tAll Loss: 1.1593\tTriple Loss(1): 0.0000\tClassification Loss: 1.1593\r\n",
      "Train Epoch: 30 [90880/209539 (43%)]\tAll Loss: 1.7474\tTriple Loss(1): 0.1293\tClassification Loss: 1.4888\r\n",
      "Train Epoch: 30 [91520/209539 (44%)]\tAll Loss: 1.5746\tTriple Loss(0): 0.2105\tClassification Loss: 1.1536\r\n",
      "Train Epoch: 30 [92160/209539 (44%)]\tAll Loss: 1.1434\tTriple Loss(1): 0.0000\tClassification Loss: 1.1434\r\n",
      "Train Epoch: 30 [92800/209539 (44%)]\tAll Loss: 1.1211\tTriple Loss(1): 0.0138\tClassification Loss: 1.0935\r\n",
      "Train Epoch: 30 [93440/209539 (45%)]\tAll Loss: 1.4029\tTriple Loss(1): 0.0446\tClassification Loss: 1.3138\r\n",
      "Train Epoch: 30 [94080/209539 (45%)]\tAll Loss: 1.3484\tTriple Loss(1): 0.0885\tClassification Loss: 1.1715\r\n",
      "Train Epoch: 30 [94720/209539 (45%)]\tAll Loss: 1.7487\tTriple Loss(0): 0.2477\tClassification Loss: 1.2534\r\n",
      "Train Epoch: 30 [95360/209539 (46%)]\tAll Loss: 1.2853\tTriple Loss(1): 0.0300\tClassification Loss: 1.2254\r\n",
      "Train Epoch: 30 [96000/209539 (46%)]\tAll Loss: 1.6663\tTriple Loss(1): 0.0440\tClassification Loss: 1.5783\r\n",
      "Train Epoch: 30 [96640/209539 (46%)]\tAll Loss: 1.4124\tTriple Loss(1): 0.0153\tClassification Loss: 1.3817\r\n",
      "Train Epoch: 30 [97280/209539 (46%)]\tAll Loss: 1.1890\tTriple Loss(1): 0.0557\tClassification Loss: 1.0775\r\n",
      "Train Epoch: 30 [97920/209539 (47%)]\tAll Loss: 2.0649\tTriple Loss(0): 0.2910\tClassification Loss: 1.4829\r\n",
      "Train Epoch: 30 [98560/209539 (47%)]\tAll Loss: 1.6035\tTriple Loss(1): 0.0000\tClassification Loss: 1.6035\r\n",
      "Train Epoch: 30 [99200/209539 (47%)]\tAll Loss: 1.5448\tTriple Loss(1): 0.1594\tClassification Loss: 1.2259\r\n",
      "Train Epoch: 30 [99840/209539 (48%)]\tAll Loss: 1.3641\tTriple Loss(1): 0.0191\tClassification Loss: 1.3258\r\n",
      "Train Epoch: 30 [100480/209539 (48%)]\tAll Loss: 2.0264\tTriple Loss(0): 0.4005\tClassification Loss: 1.2253\r\n",
      "Train Epoch: 30 [101120/209539 (48%)]\tAll Loss: 1.8573\tTriple Loss(0): 0.3118\tClassification Loss: 1.2336\r\n",
      "Train Epoch: 30 [101760/209539 (49%)]\tAll Loss: 1.2800\tTriple Loss(1): 0.0000\tClassification Loss: 1.2800\r\n",
      "Train Epoch: 30 [102400/209539 (49%)]\tAll Loss: 1.0718\tTriple Loss(1): 0.0729\tClassification Loss: 0.9259\r\n",
      "Train Epoch: 30 [103040/209539 (49%)]\tAll Loss: 1.1446\tTriple Loss(1): 0.0510\tClassification Loss: 1.0425\r\n",
      "Train Epoch: 30 [103680/209539 (49%)]\tAll Loss: 1.4162\tTriple Loss(1): 0.0012\tClassification Loss: 1.4138\r\n",
      "Train Epoch: 30 [104320/209539 (50%)]\tAll Loss: 1.0346\tTriple Loss(1): 0.0093\tClassification Loss: 1.0159\r\n",
      "Train Epoch: 30 [104960/209539 (50%)]\tAll Loss: 1.2191\tTriple Loss(1): 0.0513\tClassification Loss: 1.1165\r\n",
      "Train Epoch: 30 [105600/209539 (50%)]\tAll Loss: 1.2624\tTriple Loss(1): 0.0000\tClassification Loss: 1.2624\r\n",
      "Train Epoch: 30 [106240/209539 (51%)]\tAll Loss: 1.6889\tTriple Loss(0): 0.2936\tClassification Loss: 1.1017\r\n",
      "Train Epoch: 30 [106880/209539 (51%)]\tAll Loss: 1.1854\tTriple Loss(1): 0.0002\tClassification Loss: 1.1851\r\n",
      "Train Epoch: 30 [107520/209539 (51%)]\tAll Loss: 1.8674\tTriple Loss(0): 0.3251\tClassification Loss: 1.2172\r\n",
      "Train Epoch: 30 [108160/209539 (52%)]\tAll Loss: 1.2812\tTriple Loss(1): 0.0166\tClassification Loss: 1.2480\r\n",
      "Train Epoch: 30 [108800/209539 (52%)]\tAll Loss: 1.1267\tTriple Loss(1): 0.0128\tClassification Loss: 1.1010\r\n",
      "Train Epoch: 30 [109440/209539 (52%)]\tAll Loss: 1.5989\tTriple Loss(1): 0.0327\tClassification Loss: 1.5335\r\n",
      "Train Epoch: 30 [110080/209539 (53%)]\tAll Loss: 1.4381\tTriple Loss(1): 0.0032\tClassification Loss: 1.4317\r\n",
      "Train Epoch: 30 [110720/209539 (53%)]\tAll Loss: 1.1984\tTriple Loss(1): 0.0052\tClassification Loss: 1.1880\r\n",
      "Train Epoch: 30 [111360/209539 (53%)]\tAll Loss: 0.9269\tTriple Loss(1): 0.0000\tClassification Loss: 0.9269\r\n",
      "Train Epoch: 30 [112000/209539 (53%)]\tAll Loss: 1.0494\tTriple Loss(1): 0.0222\tClassification Loss: 1.0051\r\n",
      "Train Epoch: 30 [112640/209539 (54%)]\tAll Loss: 1.5320\tTriple Loss(0): 0.1870\tClassification Loss: 1.1580\r\n",
      "Train Epoch: 30 [113280/209539 (54%)]\tAll Loss: 0.9089\tTriple Loss(1): 0.0000\tClassification Loss: 0.9089\r\n",
      "Train Epoch: 30 [113920/209539 (54%)]\tAll Loss: 1.1828\tTriple Loss(1): 0.0130\tClassification Loss: 1.1567\r\n",
      "Train Epoch: 30 [114560/209539 (55%)]\tAll Loss: 1.2654\tTriple Loss(1): 0.0000\tClassification Loss: 1.2654\r\n",
      "Train Epoch: 30 [115200/209539 (55%)]\tAll Loss: 1.4796\tTriple Loss(1): 0.0430\tClassification Loss: 1.3936\r\n",
      "Train Epoch: 30 [115840/209539 (55%)]\tAll Loss: 1.2522\tTriple Loss(1): 0.0559\tClassification Loss: 1.1404\r\n",
      "Train Epoch: 30 [116480/209539 (56%)]\tAll Loss: 1.2572\tTriple Loss(1): 0.0288\tClassification Loss: 1.1996\r\n",
      "Train Epoch: 30 [117120/209539 (56%)]\tAll Loss: 1.3177\tTriple Loss(1): 0.0157\tClassification Loss: 1.2863\r\n",
      "Train Epoch: 30 [117760/209539 (56%)]\tAll Loss: 1.2659\tTriple Loss(1): 0.0000\tClassification Loss: 1.2659\r\n",
      "Train Epoch: 30 [118400/209539 (57%)]\tAll Loss: 1.0421\tTriple Loss(1): 0.0307\tClassification Loss: 0.9806\r\n",
      "Train Epoch: 30 [119040/209539 (57%)]\tAll Loss: 1.2949\tTriple Loss(1): 0.0055\tClassification Loss: 1.2840\r\n",
      "Train Epoch: 30 [119680/209539 (57%)]\tAll Loss: 1.8970\tTriple Loss(0): 0.3269\tClassification Loss: 1.2432\r\n",
      "Train Epoch: 30 [120320/209539 (57%)]\tAll Loss: 1.0908\tTriple Loss(1): 0.0000\tClassification Loss: 1.0908\r\n",
      "Train Epoch: 30 [120960/209539 (58%)]\tAll Loss: 1.1369\tTriple Loss(1): 0.0085\tClassification Loss: 1.1198\r\n",
      "Train Epoch: 30 [121600/209539 (58%)]\tAll Loss: 1.0717\tTriple Loss(1): 0.0107\tClassification Loss: 1.0504\r\n",
      "Train Epoch: 30 [122240/209539 (58%)]\tAll Loss: 1.6889\tTriple Loss(0): 0.3977\tClassification Loss: 0.8934\r\n",
      "Train Epoch: 30 [122880/209539 (59%)]\tAll Loss: 1.0566\tTriple Loss(1): 0.0462\tClassification Loss: 0.9643\r\n",
      "Train Epoch: 30 [123520/209539 (59%)]\tAll Loss: 2.2044\tTriple Loss(0): 0.4075\tClassification Loss: 1.3895\r\n",
      "Train Epoch: 30 [124160/209539 (59%)]\tAll Loss: 1.3450\tTriple Loss(1): 0.0132\tClassification Loss: 1.3186\r\n",
      "Train Epoch: 30 [124800/209539 (60%)]\tAll Loss: 1.0873\tTriple Loss(1): 0.0027\tClassification Loss: 1.0818\r\n",
      "Train Epoch: 30 [125440/209539 (60%)]\tAll Loss: 1.1427\tTriple Loss(1): 0.0000\tClassification Loss: 1.1427\r\n",
      "Train Epoch: 30 [126080/209539 (60%)]\tAll Loss: 1.3534\tTriple Loss(1): 0.0101\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 30 [126720/209539 (60%)]\tAll Loss: 1.5248\tTriple Loss(1): 0.0073\tClassification Loss: 1.5102\r\n",
      "Train Epoch: 30 [127360/209539 (61%)]\tAll Loss: 1.3123\tTriple Loss(1): 0.0000\tClassification Loss: 1.3123\r\n",
      "Train Epoch: 30 [128000/209539 (61%)]\tAll Loss: 2.1524\tTriple Loss(0): 0.3376\tClassification Loss: 1.4772\r\n",
      "Train Epoch: 30 [128640/209539 (61%)]\tAll Loss: 0.8196\tTriple Loss(1): 0.0073\tClassification Loss: 0.8050\r\n",
      "Train Epoch: 30 [129280/209539 (62%)]\tAll Loss: 1.3085\tTriple Loss(1): 0.0130\tClassification Loss: 1.2826\r\n",
      "Train Epoch: 30 [129920/209539 (62%)]\tAll Loss: 2.0387\tTriple Loss(0): 0.4197\tClassification Loss: 1.1993\r\n",
      "Train Epoch: 30 [130560/209539 (62%)]\tAll Loss: 1.1491\tTriple Loss(1): 0.0486\tClassification Loss: 1.0519\r\n",
      "Train Epoch: 30 [131200/209539 (63%)]\tAll Loss: 1.1106\tTriple Loss(1): 0.0210\tClassification Loss: 1.0686\r\n",
      "Train Epoch: 30 [131840/209539 (63%)]\tAll Loss: 1.0906\tTriple Loss(1): 0.0094\tClassification Loss: 1.0719\r\n",
      "Train Epoch: 30 [132480/209539 (63%)]\tAll Loss: 1.1230\tTriple Loss(1): 0.0316\tClassification Loss: 1.0597\r\n",
      "Train Epoch: 30 [133120/209539 (64%)]\tAll Loss: 1.0773\tTriple Loss(1): 0.0225\tClassification Loss: 1.0324\r\n",
      "Train Epoch: 30 [133760/209539 (64%)]\tAll Loss: 1.4126\tTriple Loss(1): 0.0248\tClassification Loss: 1.3631\r\n",
      "Train Epoch: 30 [134400/209539 (64%)]\tAll Loss: 1.1763\tTriple Loss(1): 0.0445\tClassification Loss: 1.0873\r\n",
      "Train Epoch: 30 [135040/209539 (64%)]\tAll Loss: 1.9748\tTriple Loss(0): 0.3581\tClassification Loss: 1.2586\r\n",
      "Train Epoch: 30 [135680/209539 (65%)]\tAll Loss: 1.4432\tTriple Loss(1): 0.0000\tClassification Loss: 1.4432\r\n",
      "Train Epoch: 30 [136320/209539 (65%)]\tAll Loss: 1.1710\tTriple Loss(1): 0.0265\tClassification Loss: 1.1181\r\n",
      "Train Epoch: 30 [136960/209539 (65%)]\tAll Loss: 1.1133\tTriple Loss(1): 0.0175\tClassification Loss: 1.0783\r\n",
      "Train Epoch: 30 [137600/209539 (66%)]\tAll Loss: 1.2841\tTriple Loss(1): 0.0128\tClassification Loss: 1.2586\r\n",
      "Train Epoch: 30 [138240/209539 (66%)]\tAll Loss: 1.8709\tTriple Loss(0): 0.2744\tClassification Loss: 1.3221\r\n",
      "Train Epoch: 30 [138880/209539 (66%)]\tAll Loss: 1.8885\tTriple Loss(0): 0.3439\tClassification Loss: 1.2008\r\n",
      "Train Epoch: 30 [139520/209539 (67%)]\tAll Loss: 1.1301\tTriple Loss(1): 0.0150\tClassification Loss: 1.1001\r\n",
      "Train Epoch: 30 [140160/209539 (67%)]\tAll Loss: 1.3389\tTriple Loss(1): 0.0351\tClassification Loss: 1.2687\r\n",
      "Train Epoch: 30 [140800/209539 (67%)]\tAll Loss: 1.4284\tTriple Loss(1): 0.0254\tClassification Loss: 1.3775\r\n",
      "Train Epoch: 30 [141440/209539 (68%)]\tAll Loss: 1.2335\tTriple Loss(1): 0.0000\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 30 [142080/209539 (68%)]\tAll Loss: 1.0425\tTriple Loss(1): 0.0000\tClassification Loss: 1.0425\r\n",
      "Train Epoch: 30 [142720/209539 (68%)]\tAll Loss: 1.9946\tTriple Loss(0): 0.2647\tClassification Loss: 1.4652\r\n",
      "Train Epoch: 30 [143360/209539 (68%)]\tAll Loss: 1.1679\tTriple Loss(1): 0.0726\tClassification Loss: 1.0226\r\n",
      "Train Epoch: 30 [144000/209539 (69%)]\tAll Loss: 1.7480\tTriple Loss(0): 0.2782\tClassification Loss: 1.1916\r\n",
      "Train Epoch: 30 [144640/209539 (69%)]\tAll Loss: 1.2329\tTriple Loss(1): 0.0506\tClassification Loss: 1.1316\r\n",
      "Train Epoch: 30 [145280/209539 (69%)]\tAll Loss: 1.3992\tTriple Loss(1): 0.0814\tClassification Loss: 1.2364\r\n",
      "Train Epoch: 30 [145920/209539 (70%)]\tAll Loss: 1.2826\tTriple Loss(1): 0.0554\tClassification Loss: 1.1719\r\n",
      "Train Epoch: 30 [146560/209539 (70%)]\tAll Loss: 1.1390\tTriple Loss(1): 0.0453\tClassification Loss: 1.0485\r\n",
      "Train Epoch: 30 [147200/209539 (70%)]\tAll Loss: 1.4653\tTriple Loss(1): 0.0067\tClassification Loss: 1.4519\r\n",
      "Train Epoch: 30 [147840/209539 (71%)]\tAll Loss: 0.8208\tTriple Loss(1): 0.0000\tClassification Loss: 0.8208\r\n",
      "Train Epoch: 30 [148480/209539 (71%)]\tAll Loss: 0.9931\tTriple Loss(1): 0.0097\tClassification Loss: 0.9737\r\n",
      "Train Epoch: 30 [149120/209539 (71%)]\tAll Loss: 1.3012\tTriple Loss(1): 0.0059\tClassification Loss: 1.2893\r\n",
      "Train Epoch: 30 [149760/209539 (71%)]\tAll Loss: 1.1368\tTriple Loss(1): 0.0055\tClassification Loss: 1.1258\r\n",
      "Train Epoch: 30 [150400/209539 (72%)]\tAll Loss: 1.3466\tTriple Loss(1): 0.0000\tClassification Loss: 1.3466\r\n",
      "Train Epoch: 30 [151040/209539 (72%)]\tAll Loss: 1.1813\tTriple Loss(1): 0.0178\tClassification Loss: 1.1457\r\n",
      "Train Epoch: 30 [151680/209539 (72%)]\tAll Loss: 1.4146\tTriple Loss(1): 0.0151\tClassification Loss: 1.3843\r\n",
      "Train Epoch: 30 [152320/209539 (73%)]\tAll Loss: 1.8647\tTriple Loss(0): 0.4033\tClassification Loss: 1.0580\r\n",
      "Train Epoch: 30 [152960/209539 (73%)]\tAll Loss: 1.0628\tTriple Loss(1): 0.0490\tClassification Loss: 0.9647\r\n",
      "Train Epoch: 30 [153600/209539 (73%)]\tAll Loss: 1.9388\tTriple Loss(0): 0.4141\tClassification Loss: 1.1107\r\n",
      "Train Epoch: 30 [154240/209539 (74%)]\tAll Loss: 1.2398\tTriple Loss(1): 0.0343\tClassification Loss: 1.1712\r\n",
      "Train Epoch: 30 [154880/209539 (74%)]\tAll Loss: 1.4006\tTriple Loss(1): 0.0046\tClassification Loss: 1.3914\r\n",
      "Train Epoch: 30 [155520/209539 (74%)]\tAll Loss: 1.1505\tTriple Loss(1): 0.0371\tClassification Loss: 1.0764\r\n",
      "Train Epoch: 30 [156160/209539 (75%)]\tAll Loss: 1.2726\tTriple Loss(1): 0.0155\tClassification Loss: 1.2416\r\n",
      "Train Epoch: 30 [156800/209539 (75%)]\tAll Loss: 1.7557\tTriple Loss(1): 0.0606\tClassification Loss: 1.6345\r\n",
      "Train Epoch: 30 [157440/209539 (75%)]\tAll Loss: 1.5596\tTriple Loss(1): 0.0422\tClassification Loss: 1.4751\r\n",
      "Train Epoch: 30 [158080/209539 (75%)]\tAll Loss: 1.2631\tTriple Loss(1): 0.0000\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 30 [158720/209539 (76%)]\tAll Loss: 1.9703\tTriple Loss(0): 0.3991\tClassification Loss: 1.1720\r\n",
      "Train Epoch: 30 [159360/209539 (76%)]\tAll Loss: 0.9776\tTriple Loss(1): 0.0303\tClassification Loss: 0.9170\r\n",
      "Train Epoch: 30 [160000/209539 (76%)]\tAll Loss: 1.5358\tTriple Loss(1): 0.0003\tClassification Loss: 1.5352\r\n",
      "Train Epoch: 30 [160640/209539 (77%)]\tAll Loss: 1.6538\tTriple Loss(0): 0.2692\tClassification Loss: 1.1153\r\n",
      "Train Epoch: 30 [161280/209539 (77%)]\tAll Loss: 1.3409\tTriple Loss(1): 0.0202\tClassification Loss: 1.3005\r\n",
      "Train Epoch: 30 [161920/209539 (77%)]\tAll Loss: 1.2853\tTriple Loss(1): 0.0000\tClassification Loss: 1.2853\r\n",
      "Train Epoch: 30 [162560/209539 (78%)]\tAll Loss: 1.1338\tTriple Loss(1): 0.0051\tClassification Loss: 1.1237\r\n",
      "Train Epoch: 30 [163200/209539 (78%)]\tAll Loss: 1.0602\tTriple Loss(1): 0.0153\tClassification Loss: 1.0296\r\n",
      "Train Epoch: 30 [163840/209539 (78%)]\tAll Loss: 1.4724\tTriple Loss(0): 0.1594\tClassification Loss: 1.1536\r\n",
      "Train Epoch: 30 [164480/209539 (78%)]\tAll Loss: 1.9187\tTriple Loss(0): 0.2711\tClassification Loss: 1.3765\r\n",
      "Train Epoch: 30 [165120/209539 (79%)]\tAll Loss: 1.8093\tTriple Loss(0): 0.3902\tClassification Loss: 1.0290\r\n",
      "Train Epoch: 30 [165760/209539 (79%)]\tAll Loss: 2.0230\tTriple Loss(0): 0.3839\tClassification Loss: 1.2553\r\n",
      "Train Epoch: 30 [166400/209539 (79%)]\tAll Loss: 1.4416\tTriple Loss(1): 0.0810\tClassification Loss: 1.2796\r\n",
      "Train Epoch: 30 [167040/209539 (80%)]\tAll Loss: 1.4172\tTriple Loss(1): 0.0197\tClassification Loss: 1.3777\r\n",
      "Train Epoch: 30 [167680/209539 (80%)]\tAll Loss: 1.3771\tTriple Loss(1): 0.0000\tClassification Loss: 1.3771\r\n",
      "Train Epoch: 30 [168320/209539 (80%)]\tAll Loss: 1.1308\tTriple Loss(1): 0.0639\tClassification Loss: 1.0029\r\n",
      "Train Epoch: 30 [168960/209539 (81%)]\tAll Loss: 1.1357\tTriple Loss(1): 0.0562\tClassification Loss: 1.0233\r\n",
      "Train Epoch: 30 [169600/209539 (81%)]\tAll Loss: 1.5040\tTriple Loss(0): 0.2666\tClassification Loss: 0.9708\r\n",
      "Train Epoch: 30 [170240/209539 (81%)]\tAll Loss: 1.3241\tTriple Loss(1): 0.0193\tClassification Loss: 1.2855\r\n",
      "Train Epoch: 30 [170880/209539 (82%)]\tAll Loss: 1.3406\tTriple Loss(1): 0.0846\tClassification Loss: 1.1714\r\n",
      "Train Epoch: 30 [171520/209539 (82%)]\tAll Loss: 1.1660\tTriple Loss(1): 0.0179\tClassification Loss: 1.1302\r\n",
      "Train Epoch: 30 [172160/209539 (82%)]\tAll Loss: 1.3360\tTriple Loss(1): 0.0562\tClassification Loss: 1.2236\r\n",
      "Train Epoch: 30 [172800/209539 (82%)]\tAll Loss: 1.2031\tTriple Loss(1): 0.0138\tClassification Loss: 1.1756\r\n",
      "Train Epoch: 30 [173440/209539 (83%)]\tAll Loss: 1.5606\tTriple Loss(1): 0.0231\tClassification Loss: 1.5145\r\n",
      "Train Epoch: 30 [174080/209539 (83%)]\tAll Loss: 1.8161\tTriple Loss(0): 0.3586\tClassification Loss: 1.0988\r\n",
      "Train Epoch: 30 [174720/209539 (83%)]\tAll Loss: 1.5794\tTriple Loss(0): 0.1965\tClassification Loss: 1.1864\r\n",
      "Train Epoch: 30 [175360/209539 (84%)]\tAll Loss: 1.3251\tTriple Loss(1): 0.0041\tClassification Loss: 1.3170\r\n",
      "Train Epoch: 30 [176000/209539 (84%)]\tAll Loss: 1.6809\tTriple Loss(0): 0.2215\tClassification Loss: 1.2378\r\n",
      "Train Epoch: 30 [176640/209539 (84%)]\tAll Loss: 1.2067\tTriple Loss(0): 0.2184\tClassification Loss: 0.7700\r\n",
      "Train Epoch: 30 [177280/209539 (85%)]\tAll Loss: 1.5259\tTriple Loss(1): 0.0214\tClassification Loss: 1.4831\r\n",
      "Train Epoch: 30 [177920/209539 (85%)]\tAll Loss: 1.3587\tTriple Loss(1): 0.0591\tClassification Loss: 1.2405\r\n",
      "Train Epoch: 30 [178560/209539 (85%)]\tAll Loss: 1.3107\tTriple Loss(0): 0.1065\tClassification Loss: 1.0977\r\n",
      "Train Epoch: 30 [179200/209539 (86%)]\tAll Loss: 1.4423\tTriple Loss(1): 0.0892\tClassification Loss: 1.2638\r\n",
      "Train Epoch: 30 [179840/209539 (86%)]\tAll Loss: 2.0202\tTriple Loss(0): 0.3684\tClassification Loss: 1.2835\r\n",
      "Train Epoch: 30 [180480/209539 (86%)]\tAll Loss: 1.3070\tTriple Loss(1): 0.0261\tClassification Loss: 1.2549\r\n",
      "Train Epoch: 30 [181120/209539 (86%)]\tAll Loss: 1.6088\tTriple Loss(0): 0.1613\tClassification Loss: 1.2863\r\n",
      "Train Epoch: 30 [181760/209539 (87%)]\tAll Loss: 1.0763\tTriple Loss(1): 0.0211\tClassification Loss: 1.0341\r\n",
      "Train Epoch: 30 [182400/209539 (87%)]\tAll Loss: 1.5522\tTriple Loss(1): 0.0000\tClassification Loss: 1.5522\r\n",
      "Train Epoch: 30 [183040/209539 (87%)]\tAll Loss: 1.8338\tTriple Loss(0): 0.2800\tClassification Loss: 1.2738\r\n",
      "Train Epoch: 30 [183680/209539 (88%)]\tAll Loss: 1.1221\tTriple Loss(1): 0.0693\tClassification Loss: 0.9835\r\n",
      "Train Epoch: 30 [184320/209539 (88%)]\tAll Loss: 1.9183\tTriple Loss(0): 0.3979\tClassification Loss: 1.1225\r\n",
      "Train Epoch: 30 [184960/209539 (88%)]\tAll Loss: 1.1627\tTriple Loss(1): 0.0362\tClassification Loss: 1.0902\r\n",
      "Train Epoch: 30 [185600/209539 (89%)]\tAll Loss: 1.3866\tTriple Loss(1): 0.0408\tClassification Loss: 1.3050\r\n",
      "Train Epoch: 30 [186240/209539 (89%)]\tAll Loss: 1.2703\tTriple Loss(1): 0.0306\tClassification Loss: 1.2090\r\n",
      "Train Epoch: 30 [186880/209539 (89%)]\tAll Loss: 1.3850\tTriple Loss(1): 0.0000\tClassification Loss: 1.3850\r\n",
      "Train Epoch: 30 [187520/209539 (89%)]\tAll Loss: 1.3660\tTriple Loss(1): 0.0285\tClassification Loss: 1.3090\r\n",
      "Train Epoch: 30 [188160/209539 (90%)]\tAll Loss: 1.0677\tTriple Loss(1): 0.0332\tClassification Loss: 1.0013\r\n",
      "Train Epoch: 30 [188800/209539 (90%)]\tAll Loss: 1.1141\tTriple Loss(1): 0.0015\tClassification Loss: 1.1112\r\n",
      "Train Epoch: 30 [189440/209539 (90%)]\tAll Loss: 1.2300\tTriple Loss(1): 0.0373\tClassification Loss: 1.1554\r\n",
      "Train Epoch: 30 [190080/209539 (91%)]\tAll Loss: 1.1720\tTriple Loss(1): 0.0075\tClassification Loss: 1.1569\r\n",
      "Train Epoch: 30 [190720/209539 (91%)]\tAll Loss: 1.2451\tTriple Loss(1): 0.0378\tClassification Loss: 1.1695\r\n",
      "Train Epoch: 30 [191360/209539 (91%)]\tAll Loss: 1.0713\tTriple Loss(1): 0.0047\tClassification Loss: 1.0619\r\n",
      "Train Epoch: 30 [192000/209539 (92%)]\tAll Loss: 1.5000\tTriple Loss(1): 0.0167\tClassification Loss: 1.4666\r\n",
      "Train Epoch: 30 [192640/209539 (92%)]\tAll Loss: 1.0353\tTriple Loss(1): 0.0269\tClassification Loss: 0.9814\r\n",
      "Train Epoch: 30 [193280/209539 (92%)]\tAll Loss: 1.1046\tTriple Loss(1): 0.0101\tClassification Loss: 1.0844\r\n",
      "Train Epoch: 30 [193920/209539 (93%)]\tAll Loss: 1.1970\tTriple Loss(1): 0.0178\tClassification Loss: 1.1614\r\n",
      "Train Epoch: 30 [194560/209539 (93%)]\tAll Loss: 1.2833\tTriple Loss(1): 0.0418\tClassification Loss: 1.1997\r\n",
      "Train Epoch: 30 [195200/209539 (93%)]\tAll Loss: 1.9467\tTriple Loss(0): 0.3256\tClassification Loss: 1.2955\r\n",
      "Train Epoch: 30 [195840/209539 (93%)]\tAll Loss: 0.9327\tTriple Loss(1): 0.0194\tClassification Loss: 0.8939\r\n",
      "Train Epoch: 30 [196480/209539 (94%)]\tAll Loss: 2.2904\tTriple Loss(0): 0.4044\tClassification Loss: 1.4816\r\n",
      "Train Epoch: 30 [197120/209539 (94%)]\tAll Loss: 1.1553\tTriple Loss(1): 0.0130\tClassification Loss: 1.1294\r\n",
      "Train Epoch: 30 [197760/209539 (94%)]\tAll Loss: 1.4168\tTriple Loss(1): 0.0228\tClassification Loss: 1.3712\r\n",
      "Train Epoch: 30 [198400/209539 (95%)]\tAll Loss: 1.2712\tTriple Loss(1): 0.0207\tClassification Loss: 1.2299\r\n",
      "Train Epoch: 30 [199040/209539 (95%)]\tAll Loss: 1.4863\tTriple Loss(1): 0.0913\tClassification Loss: 1.3038\r\n",
      "Train Epoch: 30 [199680/209539 (95%)]\tAll Loss: 1.2619\tTriple Loss(1): 0.0421\tClassification Loss: 1.1777\r\n",
      "Train Epoch: 30 [200320/209539 (96%)]\tAll Loss: 1.2385\tTriple Loss(1): 0.0412\tClassification Loss: 1.1561\r\n",
      "Train Epoch: 30 [200960/209539 (96%)]\tAll Loss: 0.8771\tTriple Loss(1): 0.0000\tClassification Loss: 0.8771\r\n",
      "Train Epoch: 30 [201600/209539 (96%)]\tAll Loss: 1.1189\tTriple Loss(1): 0.0310\tClassification Loss: 1.0569\r\n",
      "Train Epoch: 30 [202240/209539 (97%)]\tAll Loss: 1.9956\tTriple Loss(0): 0.4130\tClassification Loss: 1.1696\r\n",
      "Train Epoch: 30 [202880/209539 (97%)]\tAll Loss: 1.2564\tTriple Loss(1): 0.0400\tClassification Loss: 1.1764\r\n",
      "Train Epoch: 30 [203520/209539 (97%)]\tAll Loss: 1.3950\tTriple Loss(1): 0.0017\tClassification Loss: 1.3916\r\n",
      "Train Epoch: 30 [204160/209539 (97%)]\tAll Loss: 2.0743\tTriple Loss(0): 0.2323\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 30 [204800/209539 (98%)]\tAll Loss: 1.5405\tTriple Loss(1): 0.0737\tClassification Loss: 1.3931\r\n",
      "Train Epoch: 30 [205440/209539 (98%)]\tAll Loss: 1.8090\tTriple Loss(0): 0.4087\tClassification Loss: 0.9916\r\n",
      "Train Epoch: 30 [206080/209539 (98%)]\tAll Loss: 1.2270\tTriple Loss(1): 0.0000\tClassification Loss: 1.2270\r\n",
      "Train Epoch: 30 [206720/209539 (99%)]\tAll Loss: 1.0707\tTriple Loss(1): 0.0047\tClassification Loss: 1.0614\r\n",
      "Train Epoch: 30 [207360/209539 (99%)]\tAll Loss: 1.0154\tTriple Loss(1): 0.0310\tClassification Loss: 0.9533\r\n",
      "Train Epoch: 30 [208000/209539 (99%)]\tAll Loss: 1.0708\tTriple Loss(1): 0.0206\tClassification Loss: 1.0296\r\n",
      "Train Epoch: 30 [208640/209539 (100%)]\tAll Loss: 1.7998\tTriple Loss(0): 0.2070\tClassification Loss: 1.3857\r\n",
      "Train Epoch: 30 [209280/209539 (100%)]\tAll Loss: 1.9062\tTriple Loss(0): 0.3114\tClassification Loss: 1.2835\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/30_epochs\r\n",
      "Train Epoch: 31 [0/209539 (0%)]\tAll Loss: 1.7294\tTriple Loss(1): 0.0948\tClassification Loss: 1.5399\r\n",
      "\r\n",
      "Test set: Average loss: 1.0916\r\n",
      "Top 1 Accuracy: 54599/80128 (68%)\r\n",
      "Top 3 Accuracy: 70035/80128 (87%)\r\n",
      "Top 5 Accuracy: 74598/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 31 [640/209539 (0%)]\tAll Loss: 1.4028\tTriple Loss(1): 0.0282\tClassification Loss: 1.3465\r\n",
      "Train Epoch: 31 [1280/209539 (1%)]\tAll Loss: 1.0845\tTriple Loss(1): 0.0206\tClassification Loss: 1.0433\r\n",
      "Train Epoch: 31 [1920/209539 (1%)]\tAll Loss: 1.3622\tTriple Loss(1): 0.0359\tClassification Loss: 1.2905\r\n",
      "Train Epoch: 31 [2560/209539 (1%)]\tAll Loss: 1.5897\tTriple Loss(1): 0.0185\tClassification Loss: 1.5527\r\n",
      "Train Epoch: 31 [3200/209539 (2%)]\tAll Loss: 1.3381\tTriple Loss(1): 0.0047\tClassification Loss: 1.3286\r\n",
      "Train Epoch: 31 [3840/209539 (2%)]\tAll Loss: 1.2810\tTriple Loss(1): 0.0940\tClassification Loss: 1.0931\r\n",
      "Train Epoch: 31 [4480/209539 (2%)]\tAll Loss: 1.2776\tTriple Loss(1): 0.0152\tClassification Loss: 1.2473\r\n",
      "Train Epoch: 31 [5120/209539 (2%)]\tAll Loss: 1.2427\tTriple Loss(1): 0.0271\tClassification Loss: 1.1885\r\n",
      "Train Epoch: 31 [5760/209539 (3%)]\tAll Loss: 1.3040\tTriple Loss(1): 0.0000\tClassification Loss: 1.3040\r\n",
      "Train Epoch: 31 [6400/209539 (3%)]\tAll Loss: 1.4556\tTriple Loss(0): 0.2767\tClassification Loss: 0.9023\r\n",
      "Train Epoch: 31 [7040/209539 (3%)]\tAll Loss: 1.5145\tTriple Loss(0): 0.2263\tClassification Loss: 1.0620\r\n",
      "Train Epoch: 31 [7680/209539 (4%)]\tAll Loss: 1.0733\tTriple Loss(1): 0.0286\tClassification Loss: 1.0161\r\n",
      "Train Epoch: 31 [8320/209539 (4%)]\tAll Loss: 1.2353\tTriple Loss(1): 0.0635\tClassification Loss: 1.1083\r\n",
      "Train Epoch: 31 [8960/209539 (4%)]\tAll Loss: 1.2511\tTriple Loss(1): 0.0626\tClassification Loss: 1.1259\r\n",
      "Train Epoch: 31 [9600/209539 (5%)]\tAll Loss: 1.8940\tTriple Loss(0): 0.2958\tClassification Loss: 1.3024\r\n",
      "Train Epoch: 31 [10240/209539 (5%)]\tAll Loss: 1.0330\tTriple Loss(1): 0.0042\tClassification Loss: 1.0245\r\n",
      "Train Epoch: 31 [10880/209539 (5%)]\tAll Loss: 1.0776\tTriple Loss(1): 0.0110\tClassification Loss: 1.0557\r\n",
      "Train Epoch: 31 [11520/209539 (5%)]\tAll Loss: 1.3664\tTriple Loss(1): 0.0498\tClassification Loss: 1.2667\r\n",
      "Train Epoch: 31 [12160/209539 (6%)]\tAll Loss: 1.2601\tTriple Loss(0): 0.1317\tClassification Loss: 0.9967\r\n",
      "Train Epoch: 31 [12800/209539 (6%)]\tAll Loss: 1.3359\tTriple Loss(0): 0.2839\tClassification Loss: 0.7680\r\n",
      "Train Epoch: 31 [13440/209539 (6%)]\tAll Loss: 1.0882\tTriple Loss(1): 0.0323\tClassification Loss: 1.0236\r\n",
      "Train Epoch: 31 [14080/209539 (7%)]\tAll Loss: 1.1950\tTriple Loss(1): 0.0499\tClassification Loss: 1.0951\r\n",
      "Train Epoch: 31 [14720/209539 (7%)]\tAll Loss: 1.3255\tTriple Loss(1): 0.0204\tClassification Loss: 1.2846\r\n",
      "Train Epoch: 31 [15360/209539 (7%)]\tAll Loss: 1.1646\tTriple Loss(1): 0.0208\tClassification Loss: 1.1231\r\n",
      "Train Epoch: 31 [16000/209539 (8%)]\tAll Loss: 1.4658\tTriple Loss(1): 0.0059\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 31 [16640/209539 (8%)]\tAll Loss: 2.3659\tTriple Loss(0): 0.4654\tClassification Loss: 1.4351\r\n",
      "Train Epoch: 31 [17280/209539 (8%)]\tAll Loss: 1.1538\tTriple Loss(1): 0.0072\tClassification Loss: 1.1394\r\n",
      "Train Epoch: 31 [17920/209539 (9%)]\tAll Loss: 1.3191\tTriple Loss(1): 0.0457\tClassification Loss: 1.2278\r\n",
      "Train Epoch: 31 [18560/209539 (9%)]\tAll Loss: 1.2709\tTriple Loss(1): 0.0521\tClassification Loss: 1.1666\r\n",
      "Train Epoch: 31 [19200/209539 (9%)]\tAll Loss: 1.2811\tTriple Loss(1): 0.0055\tClassification Loss: 1.2701\r\n",
      "Train Epoch: 31 [19840/209539 (9%)]\tAll Loss: 1.1707\tTriple Loss(1): 0.0001\tClassification Loss: 1.1705\r\n",
      "Train Epoch: 31 [20480/209539 (10%)]\tAll Loss: 1.1094\tTriple Loss(1): 0.0227\tClassification Loss: 1.0640\r\n",
      "Train Epoch: 31 [21120/209539 (10%)]\tAll Loss: 1.5224\tTriple Loss(1): 0.0473\tClassification Loss: 1.4279\r\n",
      "Train Epoch: 31 [21760/209539 (10%)]\tAll Loss: 1.1714\tTriple Loss(1): 0.0101\tClassification Loss: 1.1512\r\n",
      "Train Epoch: 31 [22400/209539 (11%)]\tAll Loss: 1.7042\tTriple Loss(0): 0.2701\tClassification Loss: 1.1639\r\n",
      "Train Epoch: 31 [23040/209539 (11%)]\tAll Loss: 1.3873\tTriple Loss(1): 0.0199\tClassification Loss: 1.3476\r\n",
      "Train Epoch: 31 [23680/209539 (11%)]\tAll Loss: 1.1115\tTriple Loss(1): 0.0094\tClassification Loss: 1.0927\r\n",
      "Train Epoch: 31 [24320/209539 (12%)]\tAll Loss: 1.1266\tTriple Loss(1): 0.0051\tClassification Loss: 1.1164\r\n",
      "Train Epoch: 31 [24960/209539 (12%)]\tAll Loss: 1.0583\tTriple Loss(1): 0.0000\tClassification Loss: 1.0583\r\n",
      "Train Epoch: 31 [25600/209539 (12%)]\tAll Loss: 1.6946\tTriple Loss(0): 0.3256\tClassification Loss: 1.0434\r\n",
      "Train Epoch: 31 [26240/209539 (13%)]\tAll Loss: 1.1264\tTriple Loss(1): 0.0130\tClassification Loss: 1.1003\r\n",
      "Train Epoch: 31 [26880/209539 (13%)]\tAll Loss: 1.3205\tTriple Loss(1): 0.0013\tClassification Loss: 1.3180\r\n",
      "Train Epoch: 31 [27520/209539 (13%)]\tAll Loss: 1.3091\tTriple Loss(1): 0.0204\tClassification Loss: 1.2683\r\n",
      "Train Epoch: 31 [28160/209539 (13%)]\tAll Loss: 1.4771\tTriple Loss(1): 0.0296\tClassification Loss: 1.4178\r\n",
      "Train Epoch: 31 [28800/209539 (14%)]\tAll Loss: 1.6369\tTriple Loss(1): 0.0829\tClassification Loss: 1.4710\r\n",
      "Train Epoch: 31 [29440/209539 (14%)]\tAll Loss: 1.3024\tTriple Loss(1): 0.0195\tClassification Loss: 1.2634\r\n",
      "Train Epoch: 31 [30080/209539 (14%)]\tAll Loss: 1.3560\tTriple Loss(1): 0.0513\tClassification Loss: 1.2535\r\n",
      "Train Epoch: 31 [30720/209539 (15%)]\tAll Loss: 1.1247\tTriple Loss(1): 0.0000\tClassification Loss: 1.1247\r\n",
      "Train Epoch: 31 [31360/209539 (15%)]\tAll Loss: 1.3504\tTriple Loss(1): 0.0496\tClassification Loss: 1.2513\r\n",
      "Train Epoch: 31 [32000/209539 (15%)]\tAll Loss: 1.1803\tTriple Loss(1): 0.0372\tClassification Loss: 1.1058\r\n",
      "Train Epoch: 31 [32640/209539 (16%)]\tAll Loss: 1.3079\tTriple Loss(1): 0.0049\tClassification Loss: 1.2981\r\n",
      "Train Epoch: 31 [33280/209539 (16%)]\tAll Loss: 1.1126\tTriple Loss(1): 0.0035\tClassification Loss: 1.1057\r\n",
      "Train Epoch: 31 [33920/209539 (16%)]\tAll Loss: 1.2124\tTriple Loss(1): 0.0185\tClassification Loss: 1.1753\r\n",
      "Train Epoch: 31 [34560/209539 (16%)]\tAll Loss: 1.1099\tTriple Loss(1): 0.0263\tClassification Loss: 1.0573\r\n",
      "Train Epoch: 31 [35200/209539 (17%)]\tAll Loss: 1.1197\tTriple Loss(1): 0.0203\tClassification Loss: 1.0792\r\n",
      "Train Epoch: 31 [35840/209539 (17%)]\tAll Loss: 1.2462\tTriple Loss(1): 0.0553\tClassification Loss: 1.1356\r\n",
      "Train Epoch: 31 [36480/209539 (17%)]\tAll Loss: 1.6014\tTriple Loss(0): 0.3322\tClassification Loss: 0.9370\r\n",
      "Train Epoch: 31 [37120/209539 (18%)]\tAll Loss: 1.5392\tTriple Loss(1): 0.0296\tClassification Loss: 1.4800\r\n",
      "Train Epoch: 31 [37760/209539 (18%)]\tAll Loss: 1.2083\tTriple Loss(1): 0.0008\tClassification Loss: 1.2067\r\n",
      "Train Epoch: 31 [38400/209539 (18%)]\tAll Loss: 1.3933\tTriple Loss(1): 0.0342\tClassification Loss: 1.3249\r\n",
      "Train Epoch: 31 [39040/209539 (19%)]\tAll Loss: 0.9860\tTriple Loss(1): 0.0625\tClassification Loss: 0.8609\r\n",
      "Train Epoch: 31 [39680/209539 (19%)]\tAll Loss: 1.1271\tTriple Loss(1): 0.0023\tClassification Loss: 1.1226\r\n",
      "Train Epoch: 31 [40320/209539 (19%)]\tAll Loss: 1.2422\tTriple Loss(1): 0.0000\tClassification Loss: 1.2422\r\n",
      "Train Epoch: 31 [40960/209539 (20%)]\tAll Loss: 1.1065\tTriple Loss(1): 0.0271\tClassification Loss: 1.0522\r\n",
      "Train Epoch: 31 [41600/209539 (20%)]\tAll Loss: 1.6265\tTriple Loss(0): 0.2547\tClassification Loss: 1.1171\r\n",
      "Train Epoch: 31 [42240/209539 (20%)]\tAll Loss: 1.8232\tTriple Loss(0): 0.3700\tClassification Loss: 1.0832\r\n",
      "Train Epoch: 31 [42880/209539 (20%)]\tAll Loss: 0.9672\tTriple Loss(1): 0.0251\tClassification Loss: 0.9170\r\n",
      "Train Epoch: 31 [43520/209539 (21%)]\tAll Loss: 1.3197\tTriple Loss(1): 0.0078\tClassification Loss: 1.3041\r\n",
      "Train Epoch: 31 [44160/209539 (21%)]\tAll Loss: 1.6229\tTriple Loss(1): 0.1299\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 31 [44800/209539 (21%)]\tAll Loss: 2.2077\tTriple Loss(0): 0.3583\tClassification Loss: 1.4910\r\n",
      "Train Epoch: 31 [45440/209539 (22%)]\tAll Loss: 1.7184\tTriple Loss(1): 0.0098\tClassification Loss: 1.6989\r\n",
      "Train Epoch: 31 [46080/209539 (22%)]\tAll Loss: 1.3301\tTriple Loss(1): 0.0869\tClassification Loss: 1.1563\r\n",
      "Train Epoch: 31 [46720/209539 (22%)]\tAll Loss: 1.7692\tTriple Loss(0): 0.1315\tClassification Loss: 1.5063\r\n",
      "Train Epoch: 31 [47360/209539 (23%)]\tAll Loss: 1.0295\tTriple Loss(1): 0.0153\tClassification Loss: 0.9989\r\n",
      "Train Epoch: 31 [48000/209539 (23%)]\tAll Loss: 1.5558\tTriple Loss(1): 0.0619\tClassification Loss: 1.4319\r\n",
      "Train Epoch: 31 [48640/209539 (23%)]\tAll Loss: 1.6103\tTriple Loss(1): 0.0368\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 31 [49280/209539 (24%)]\tAll Loss: 1.5469\tTriple Loss(0): 0.2577\tClassification Loss: 1.0316\r\n",
      "Train Epoch: 31 [49920/209539 (24%)]\tAll Loss: 1.3223\tTriple Loss(1): 0.0346\tClassification Loss: 1.2531\r\n",
      "Train Epoch: 31 [50560/209539 (24%)]\tAll Loss: 2.0520\tTriple Loss(0): 0.3484\tClassification Loss: 1.3551\r\n",
      "Train Epoch: 31 [51200/209539 (24%)]\tAll Loss: 1.4686\tTriple Loss(1): 0.0620\tClassification Loss: 1.3446\r\n",
      "Train Epoch: 31 [51840/209539 (25%)]\tAll Loss: 1.3180\tTriple Loss(1): 0.0211\tClassification Loss: 1.2758\r\n",
      "Train Epoch: 31 [52480/209539 (25%)]\tAll Loss: 1.1565\tTriple Loss(1): 0.0018\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 31 [53120/209539 (25%)]\tAll Loss: 1.1245\tTriple Loss(1): 0.0000\tClassification Loss: 1.1245\r\n",
      "Train Epoch: 31 [53760/209539 (26%)]\tAll Loss: 1.3586\tTriple Loss(1): 0.0234\tClassification Loss: 1.3119\r\n",
      "Train Epoch: 31 [54400/209539 (26%)]\tAll Loss: 1.5176\tTriple Loss(1): 0.0319\tClassification Loss: 1.4538\r\n",
      "Train Epoch: 31 [55040/209539 (26%)]\tAll Loss: 1.4097\tTriple Loss(0): 0.2410\tClassification Loss: 0.9278\r\n",
      "Train Epoch: 31 [55680/209539 (27%)]\tAll Loss: 1.4362\tTriple Loss(1): 0.0000\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 31 [56320/209539 (27%)]\tAll Loss: 0.8292\tTriple Loss(1): 0.0000\tClassification Loss: 0.8292\r\n",
      "Train Epoch: 31 [56960/209539 (27%)]\tAll Loss: 1.2331\tTriple Loss(1): 0.0000\tClassification Loss: 1.2331\r\n",
      "Train Epoch: 31 [57600/209539 (27%)]\tAll Loss: 0.9452\tTriple Loss(1): 0.0000\tClassification Loss: 0.9452\r\n",
      "Train Epoch: 31 [58240/209539 (28%)]\tAll Loss: 1.0603\tTriple Loss(1): 0.0247\tClassification Loss: 1.0110\r\n",
      "Train Epoch: 31 [58880/209539 (28%)]\tAll Loss: 1.5387\tTriple Loss(1): 0.0594\tClassification Loss: 1.4199\r\n",
      "Train Epoch: 31 [59520/209539 (28%)]\tAll Loss: 1.8154\tTriple Loss(0): 0.3797\tClassification Loss: 1.0560\r\n",
      "Train Epoch: 31 [60160/209539 (29%)]\tAll Loss: 1.4755\tTriple Loss(1): 0.0360\tClassification Loss: 1.4034\r\n",
      "Train Epoch: 31 [60800/209539 (29%)]\tAll Loss: 1.8748\tTriple Loss(0): 0.2244\tClassification Loss: 1.4259\r\n",
      "Train Epoch: 31 [61440/209539 (29%)]\tAll Loss: 1.2446\tTriple Loss(1): 0.0324\tClassification Loss: 1.1798\r\n",
      "Train Epoch: 31 [62080/209539 (30%)]\tAll Loss: 1.4530\tTriple Loss(1): 0.0257\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 31 [62720/209539 (30%)]\tAll Loss: 1.1990\tTriple Loss(1): 0.0194\tClassification Loss: 1.1602\r\n",
      "Train Epoch: 31 [63360/209539 (30%)]\tAll Loss: 1.1813\tTriple Loss(1): 0.0116\tClassification Loss: 1.1581\r\n",
      "Train Epoch: 31 [64000/209539 (31%)]\tAll Loss: 1.4060\tTriple Loss(1): 0.0149\tClassification Loss: 1.3761\r\n",
      "Train Epoch: 31 [64640/209539 (31%)]\tAll Loss: 1.4872\tTriple Loss(1): 0.0000\tClassification Loss: 1.4872\r\n",
      "Train Epoch: 31 [65280/209539 (31%)]\tAll Loss: 1.2280\tTriple Loss(1): 0.0354\tClassification Loss: 1.1572\r\n",
      "Train Epoch: 31 [65920/209539 (31%)]\tAll Loss: 1.4621\tTriple Loss(1): 0.0170\tClassification Loss: 1.4282\r\n",
      "Train Epoch: 31 [66560/209539 (32%)]\tAll Loss: 1.1414\tTriple Loss(1): 0.0000\tClassification Loss: 1.1414\r\n",
      "Train Epoch: 31 [67200/209539 (32%)]\tAll Loss: 1.0648\tTriple Loss(1): 0.0309\tClassification Loss: 1.0031\r\n",
      "Train Epoch: 31 [67840/209539 (32%)]\tAll Loss: 1.6034\tTriple Loss(0): 0.2504\tClassification Loss: 1.1027\r\n",
      "Train Epoch: 31 [68480/209539 (33%)]\tAll Loss: 1.3551\tTriple Loss(1): 0.0475\tClassification Loss: 1.2601\r\n",
      "Train Epoch: 31 [69120/209539 (33%)]\tAll Loss: 1.5897\tTriple Loss(1): 0.0690\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 31 [69760/209539 (33%)]\tAll Loss: 1.0778\tTriple Loss(1): 0.0187\tClassification Loss: 1.0403\r\n",
      "Train Epoch: 31 [70400/209539 (34%)]\tAll Loss: 1.1822\tTriple Loss(1): 0.0723\tClassification Loss: 1.0376\r\n",
      "Train Epoch: 31 [71040/209539 (34%)]\tAll Loss: 1.5064\tTriple Loss(1): 0.0337\tClassification Loss: 1.4390\r\n",
      "Train Epoch: 31 [71680/209539 (34%)]\tAll Loss: 1.6340\tTriple Loss(1): 0.0345\tClassification Loss: 1.5650\r\n",
      "Train Epoch: 31 [72320/209539 (35%)]\tAll Loss: 1.2526\tTriple Loss(1): 0.0889\tClassification Loss: 1.0748\r\n",
      "Train Epoch: 31 [72960/209539 (35%)]\tAll Loss: 1.1263\tTriple Loss(1): 0.0154\tClassification Loss: 1.0956\r\n",
      "Train Epoch: 31 [73600/209539 (35%)]\tAll Loss: 1.2818\tTriple Loss(1): 0.0799\tClassification Loss: 1.1220\r\n",
      "Train Epoch: 31 [74240/209539 (35%)]\tAll Loss: 1.4383\tTriple Loss(1): 0.0098\tClassification Loss: 1.4186\r\n",
      "Train Epoch: 31 [74880/209539 (36%)]\tAll Loss: 1.8034\tTriple Loss(1): 0.0129\tClassification Loss: 1.7777\r\n",
      "Train Epoch: 31 [75520/209539 (36%)]\tAll Loss: 0.9706\tTriple Loss(1): 0.0000\tClassification Loss: 0.9706\r\n",
      "Train Epoch: 31 [76160/209539 (36%)]\tAll Loss: 1.4959\tTriple Loss(0): 0.2079\tClassification Loss: 1.0801\r\n",
      "Train Epoch: 31 [76800/209539 (37%)]\tAll Loss: 1.5817\tTriple Loss(0): 0.2344\tClassification Loss: 1.1128\r\n",
      "Train Epoch: 31 [77440/209539 (37%)]\tAll Loss: 1.1409\tTriple Loss(1): 0.0196\tClassification Loss: 1.1016\r\n",
      "Train Epoch: 31 [78080/209539 (37%)]\tAll Loss: 1.7075\tTriple Loss(0): 0.2845\tClassification Loss: 1.1385\r\n",
      "Train Epoch: 31 [78720/209539 (38%)]\tAll Loss: 1.5406\tTriple Loss(1): 0.0329\tClassification Loss: 1.4748\r\n",
      "Train Epoch: 31 [79360/209539 (38%)]\tAll Loss: 1.2750\tTriple Loss(1): 0.0878\tClassification Loss: 1.0993\r\n",
      "Train Epoch: 31 [80000/209539 (38%)]\tAll Loss: 1.2377\tTriple Loss(1): 0.0602\tClassification Loss: 1.1173\r\n",
      "Train Epoch: 31 [80640/209539 (38%)]\tAll Loss: 2.0127\tTriple Loss(0): 0.3442\tClassification Loss: 1.3244\r\n",
      "Train Epoch: 31 [81280/209539 (39%)]\tAll Loss: 1.2983\tTriple Loss(1): 0.0051\tClassification Loss: 1.2881\r\n",
      "Train Epoch: 31 [81920/209539 (39%)]\tAll Loss: 1.0046\tTriple Loss(1): 0.0134\tClassification Loss: 0.9778\r\n",
      "Train Epoch: 31 [82560/209539 (39%)]\tAll Loss: 1.5364\tTriple Loss(1): 0.0422\tClassification Loss: 1.4521\r\n",
      "Train Epoch: 31 [83200/209539 (40%)]\tAll Loss: 1.4932\tTriple Loss(1): 0.0896\tClassification Loss: 1.3141\r\n",
      "Train Epoch: 31 [83840/209539 (40%)]\tAll Loss: 1.3460\tTriple Loss(1): 0.0271\tClassification Loss: 1.2917\r\n",
      "Train Epoch: 31 [84480/209539 (40%)]\tAll Loss: 1.1601\tTriple Loss(1): 0.0135\tClassification Loss: 1.1332\r\n",
      "Train Epoch: 31 [85120/209539 (41%)]\tAll Loss: 2.1898\tTriple Loss(0): 0.2975\tClassification Loss: 1.5949\r\n",
      "Train Epoch: 31 [85760/209539 (41%)]\tAll Loss: 1.1003\tTriple Loss(1): 0.0048\tClassification Loss: 1.0908\r\n",
      "Train Epoch: 31 [86400/209539 (41%)]\tAll Loss: 2.3024\tTriple Loss(0): 0.4430\tClassification Loss: 1.4163\r\n",
      "Train Epoch: 31 [87040/209539 (42%)]\tAll Loss: 1.1084\tTriple Loss(1): 0.0374\tClassification Loss: 1.0335\r\n",
      "Train Epoch: 31 [87680/209539 (42%)]\tAll Loss: 1.4685\tTriple Loss(0): 0.2615\tClassification Loss: 0.9456\r\n",
      "Train Epoch: 31 [88320/209539 (42%)]\tAll Loss: 1.2740\tTriple Loss(1): 0.0363\tClassification Loss: 1.2013\r\n",
      "Train Epoch: 31 [88960/209539 (42%)]\tAll Loss: 1.8016\tTriple Loss(0): 0.3095\tClassification Loss: 1.1826\r\n",
      "Train Epoch: 31 [89600/209539 (43%)]\tAll Loss: 1.4508\tTriple Loss(1): 0.0024\tClassification Loss: 1.4459\r\n",
      "Train Epoch: 31 [90240/209539 (43%)]\tAll Loss: 1.2631\tTriple Loss(1): 0.0000\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 31 [90880/209539 (43%)]\tAll Loss: 1.5363\tTriple Loss(1): 0.0000\tClassification Loss: 1.5363\r\n",
      "Train Epoch: 31 [91520/209539 (44%)]\tAll Loss: 1.2113\tTriple Loss(1): 0.0000\tClassification Loss: 1.2113\r\n",
      "Train Epoch: 31 [92160/209539 (44%)]\tAll Loss: 1.2658\tTriple Loss(1): 0.0149\tClassification Loss: 1.2359\r\n",
      "Train Epoch: 31 [92800/209539 (44%)]\tAll Loss: 1.1540\tTriple Loss(1): 0.0479\tClassification Loss: 1.0582\r\n",
      "Train Epoch: 31 [93440/209539 (45%)]\tAll Loss: 2.0885\tTriple Loss(0): 0.3779\tClassification Loss: 1.3327\r\n",
      "Train Epoch: 31 [94080/209539 (45%)]\tAll Loss: 1.1985\tTriple Loss(1): 0.0221\tClassification Loss: 1.1543\r\n",
      "Train Epoch: 31 [94720/209539 (45%)]\tAll Loss: 1.5470\tTriple Loss(1): 0.0649\tClassification Loss: 1.4172\r\n",
      "Train Epoch: 31 [95360/209539 (46%)]\tAll Loss: 1.5999\tTriple Loss(1): 0.0711\tClassification Loss: 1.4576\r\n",
      "Train Epoch: 31 [96000/209539 (46%)]\tAll Loss: 1.3506\tTriple Loss(1): 0.0316\tClassification Loss: 1.2873\r\n",
      "Train Epoch: 31 [96640/209539 (46%)]\tAll Loss: 1.4834\tTriple Loss(1): 0.0308\tClassification Loss: 1.4218\r\n",
      "Train Epoch: 31 [97280/209539 (46%)]\tAll Loss: 1.2550\tTriple Loss(1): 0.0517\tClassification Loss: 1.1517\r\n",
      "Train Epoch: 31 [97920/209539 (47%)]\tAll Loss: 1.4186\tTriple Loss(1): 0.0453\tClassification Loss: 1.3279\r\n",
      "Train Epoch: 31 [98560/209539 (47%)]\tAll Loss: 1.9791\tTriple Loss(0): 0.2123\tClassification Loss: 1.5544\r\n",
      "Train Epoch: 31 [99200/209539 (47%)]\tAll Loss: 1.1208\tTriple Loss(1): 0.0068\tClassification Loss: 1.1072\r\n",
      "Train Epoch: 31 [99840/209539 (48%)]\tAll Loss: 1.3911\tTriple Loss(1): 0.0784\tClassification Loss: 1.2343\r\n",
      "Train Epoch: 31 [100480/209539 (48%)]\tAll Loss: 1.3755\tTriple Loss(1): 0.0632\tClassification Loss: 1.2490\r\n",
      "Train Epoch: 31 [101120/209539 (48%)]\tAll Loss: 1.3473\tTriple Loss(1): 0.0169\tClassification Loss: 1.3136\r\n",
      "Train Epoch: 31 [101760/209539 (49%)]\tAll Loss: 1.2993\tTriple Loss(1): 0.0242\tClassification Loss: 1.2508\r\n",
      "Train Epoch: 31 [102400/209539 (49%)]\tAll Loss: 1.9423\tTriple Loss(0): 0.4092\tClassification Loss: 1.1238\r\n",
      "Train Epoch: 31 [103040/209539 (49%)]\tAll Loss: 1.1834\tTriple Loss(1): 0.0101\tClassification Loss: 1.1631\r\n",
      "Train Epoch: 31 [103680/209539 (49%)]\tAll Loss: 1.7144\tTriple Loss(0): 0.2317\tClassification Loss: 1.2510\r\n",
      "Train Epoch: 31 [104320/209539 (50%)]\tAll Loss: 1.0266\tTriple Loss(1): 0.0061\tClassification Loss: 1.0144\r\n",
      "Train Epoch: 31 [104960/209539 (50%)]\tAll Loss: 2.3463\tTriple Loss(0): 0.4630\tClassification Loss: 1.4203\r\n",
      "Train Epoch: 31 [105600/209539 (50%)]\tAll Loss: 1.2782\tTriple Loss(1): 0.0000\tClassification Loss: 1.2782\r\n",
      "Train Epoch: 31 [106240/209539 (51%)]\tAll Loss: 1.4228\tTriple Loss(1): 0.0953\tClassification Loss: 1.2322\r\n",
      "Train Epoch: 31 [106880/209539 (51%)]\tAll Loss: 1.1079\tTriple Loss(1): 0.0281\tClassification Loss: 1.0516\r\n",
      "Train Epoch: 31 [107520/209539 (51%)]\tAll Loss: 1.2326\tTriple Loss(1): 0.0187\tClassification Loss: 1.1953\r\n",
      "Train Epoch: 31 [108160/209539 (52%)]\tAll Loss: 1.1904\tTriple Loss(1): 0.0048\tClassification Loss: 1.1807\r\n",
      "Train Epoch: 31 [108800/209539 (52%)]\tAll Loss: 1.4237\tTriple Loss(0): 0.2614\tClassification Loss: 0.9009\r\n",
      "Train Epoch: 31 [109440/209539 (52%)]\tAll Loss: 1.4558\tTriple Loss(1): 0.0000\tClassification Loss: 1.4558\r\n",
      "Train Epoch: 31 [110080/209539 (53%)]\tAll Loss: 1.9223\tTriple Loss(0): 0.2303\tClassification Loss: 1.4618\r\n",
      "Train Epoch: 31 [110720/209539 (53%)]\tAll Loss: 1.3647\tTriple Loss(1): 0.0143\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 31 [111360/209539 (53%)]\tAll Loss: 1.2522\tTriple Loss(1): 0.1333\tClassification Loss: 0.9856\r\n",
      "Train Epoch: 31 [112000/209539 (53%)]\tAll Loss: 0.9896\tTriple Loss(1): 0.0220\tClassification Loss: 0.9456\r\n",
      "Train Epoch: 31 [112640/209539 (54%)]\tAll Loss: 1.3133\tTriple Loss(1): 0.0762\tClassification Loss: 1.1610\r\n",
      "Train Epoch: 31 [113280/209539 (54%)]\tAll Loss: 0.9887\tTriple Loss(1): 0.0208\tClassification Loss: 0.9470\r\n",
      "Train Epoch: 31 [113920/209539 (54%)]\tAll Loss: 1.3424\tTriple Loss(1): 0.0219\tClassification Loss: 1.2985\r\n",
      "Train Epoch: 31 [114560/209539 (55%)]\tAll Loss: 1.4395\tTriple Loss(1): 0.0365\tClassification Loss: 1.3665\r\n",
      "Train Epoch: 31 [115200/209539 (55%)]\tAll Loss: 1.4796\tTriple Loss(1): 0.1030\tClassification Loss: 1.2736\r\n",
      "Train Epoch: 31 [115840/209539 (55%)]\tAll Loss: 1.6635\tTriple Loss(0): 0.2526\tClassification Loss: 1.1583\r\n",
      "Train Epoch: 31 [116480/209539 (56%)]\tAll Loss: 1.2844\tTriple Loss(1): 0.0091\tClassification Loss: 1.2662\r\n",
      "Train Epoch: 31 [117120/209539 (56%)]\tAll Loss: 1.2528\tTriple Loss(1): 0.0374\tClassification Loss: 1.1780\r\n",
      "Train Epoch: 31 [117760/209539 (56%)]\tAll Loss: 1.5702\tTriple Loss(0): 0.1955\tClassification Loss: 1.1791\r\n",
      "Train Epoch: 31 [118400/209539 (57%)]\tAll Loss: 0.9220\tTriple Loss(1): 0.0143\tClassification Loss: 0.8934\r\n",
      "Train Epoch: 31 [119040/209539 (57%)]\tAll Loss: 1.5345\tTriple Loss(1): 0.0399\tClassification Loss: 1.4546\r\n",
      "Train Epoch: 31 [119680/209539 (57%)]\tAll Loss: 1.6533\tTriple Loss(0): 0.2592\tClassification Loss: 1.1348\r\n",
      "Train Epoch: 31 [120320/209539 (57%)]\tAll Loss: 1.4030\tTriple Loss(1): 0.1025\tClassification Loss: 1.1981\r\n",
      "Train Epoch: 31 [120960/209539 (58%)]\tAll Loss: 1.4866\tTriple Loss(0): 0.1967\tClassification Loss: 1.0933\r\n",
      "Train Epoch: 31 [121600/209539 (58%)]\tAll Loss: 1.0946\tTriple Loss(1): 0.0162\tClassification Loss: 1.0621\r\n",
      "Train Epoch: 31 [122240/209539 (58%)]\tAll Loss: 1.1973\tTriple Loss(0): 0.1048\tClassification Loss: 0.9877\r\n",
      "Train Epoch: 31 [122880/209539 (59%)]\tAll Loss: 1.2105\tTriple Loss(1): 0.0194\tClassification Loss: 1.1717\r\n",
      "Train Epoch: 31 [123520/209539 (59%)]\tAll Loss: 1.0918\tTriple Loss(1): 0.0002\tClassification Loss: 1.0913\r\n",
      "Train Epoch: 31 [124160/209539 (59%)]\tAll Loss: 1.3165\tTriple Loss(1): 0.0864\tClassification Loss: 1.1437\r\n",
      "Train Epoch: 31 [124800/209539 (60%)]\tAll Loss: 1.0175\tTriple Loss(1): 0.0352\tClassification Loss: 0.9470\r\n",
      "Train Epoch: 31 [125440/209539 (60%)]\tAll Loss: 1.2728\tTriple Loss(1): 0.0263\tClassification Loss: 1.2202\r\n",
      "Train Epoch: 31 [126080/209539 (60%)]\tAll Loss: 1.2352\tTriple Loss(1): 0.0119\tClassification Loss: 1.2114\r\n",
      "Train Epoch: 31 [126720/209539 (60%)]\tAll Loss: 1.8284\tTriple Loss(0): 0.2027\tClassification Loss: 1.4230\r\n",
      "Train Epoch: 31 [127360/209539 (61%)]\tAll Loss: 1.2511\tTriple Loss(1): 0.0019\tClassification Loss: 1.2473\r\n",
      "Train Epoch: 31 [128000/209539 (61%)]\tAll Loss: 1.8200\tTriple Loss(0): 0.1830\tClassification Loss: 1.4540\r\n",
      "Train Epoch: 31 [128640/209539 (61%)]\tAll Loss: 1.1868\tTriple Loss(1): 0.0666\tClassification Loss: 1.0536\r\n",
      "Train Epoch: 31 [129280/209539 (62%)]\tAll Loss: 1.3800\tTriple Loss(1): 0.0257\tClassification Loss: 1.3286\r\n",
      "Train Epoch: 31 [129920/209539 (62%)]\tAll Loss: 1.1915\tTriple Loss(1): 0.0601\tClassification Loss: 1.0712\r\n",
      "Train Epoch: 31 [130560/209539 (62%)]\tAll Loss: 1.2125\tTriple Loss(1): 0.0000\tClassification Loss: 1.2125\r\n",
      "Train Epoch: 31 [131200/209539 (63%)]\tAll Loss: 1.3488\tTriple Loss(1): 0.0074\tClassification Loss: 1.3340\r\n",
      "Train Epoch: 31 [131840/209539 (63%)]\tAll Loss: 1.3800\tTriple Loss(1): 0.0282\tClassification Loss: 1.3236\r\n",
      "Train Epoch: 31 [132480/209539 (63%)]\tAll Loss: 1.0550\tTriple Loss(1): 0.0124\tClassification Loss: 1.0302\r\n",
      "Train Epoch: 31 [133120/209539 (64%)]\tAll Loss: 1.1060\tTriple Loss(1): 0.0000\tClassification Loss: 1.1060\r\n",
      "Train Epoch: 31 [133760/209539 (64%)]\tAll Loss: 1.2092\tTriple Loss(1): 0.0206\tClassification Loss: 1.1681\r\n",
      "Train Epoch: 31 [134400/209539 (64%)]\tAll Loss: 1.4907\tTriple Loss(0): 0.2187\tClassification Loss: 1.0534\r\n",
      "Train Epoch: 31 [135040/209539 (64%)]\tAll Loss: 1.3401\tTriple Loss(1): 0.0417\tClassification Loss: 1.2567\r\n",
      "Train Epoch: 31 [135680/209539 (65%)]\tAll Loss: 1.4861\tTriple Loss(1): 0.0262\tClassification Loss: 1.4337\r\n",
      "Train Epoch: 31 [136320/209539 (65%)]\tAll Loss: 1.2629\tTriple Loss(1): 0.0111\tClassification Loss: 1.2408\r\n",
      "Train Epoch: 31 [136960/209539 (65%)]\tAll Loss: 1.2096\tTriple Loss(1): 0.0442\tClassification Loss: 1.1212\r\n",
      "Train Epoch: 31 [137600/209539 (66%)]\tAll Loss: 1.3159\tTriple Loss(1): 0.0706\tClassification Loss: 1.1747\r\n",
      "Train Epoch: 31 [138240/209539 (66%)]\tAll Loss: 2.3850\tTriple Loss(0): 0.4780\tClassification Loss: 1.4291\r\n",
      "Train Epoch: 31 [138880/209539 (66%)]\tAll Loss: 1.2343\tTriple Loss(1): 0.0262\tClassification Loss: 1.1818\r\n",
      "Train Epoch: 31 [139520/209539 (67%)]\tAll Loss: 2.0706\tTriple Loss(0): 0.4368\tClassification Loss: 1.1970\r\n",
      "Train Epoch: 31 [140160/209539 (67%)]\tAll Loss: 1.1781\tTriple Loss(1): 0.0008\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 31 [140800/209539 (67%)]\tAll Loss: 1.3673\tTriple Loss(1): 0.0000\tClassification Loss: 1.3673\r\n",
      "Train Epoch: 31 [141440/209539 (68%)]\tAll Loss: 1.2637\tTriple Loss(1): 0.0297\tClassification Loss: 1.2043\r\n",
      "Train Epoch: 31 [142080/209539 (68%)]\tAll Loss: 1.1077\tTriple Loss(1): 0.0209\tClassification Loss: 1.0660\r\n",
      "Train Epoch: 31 [142720/209539 (68%)]\tAll Loss: 2.3155\tTriple Loss(0): 0.3145\tClassification Loss: 1.6865\r\n",
      "Train Epoch: 31 [143360/209539 (68%)]\tAll Loss: 0.9758\tTriple Loss(1): 0.0000\tClassification Loss: 0.9758\r\n",
      "Train Epoch: 31 [144000/209539 (69%)]\tAll Loss: 1.2671\tTriple Loss(1): 0.0061\tClassification Loss: 1.2549\r\n",
      "Train Epoch: 31 [144640/209539 (69%)]\tAll Loss: 1.1860\tTriple Loss(1): 0.0118\tClassification Loss: 1.1625\r\n",
      "Train Epoch: 31 [145280/209539 (69%)]\tAll Loss: 1.2630\tTriple Loss(1): 0.0217\tClassification Loss: 1.2197\r\n",
      "Train Epoch: 31 [145920/209539 (70%)]\tAll Loss: 1.1679\tTriple Loss(1): 0.0070\tClassification Loss: 1.1540\r\n",
      "Train Epoch: 31 [146560/209539 (70%)]\tAll Loss: 1.1832\tTriple Loss(1): 0.0105\tClassification Loss: 1.1623\r\n",
      "Train Epoch: 31 [147200/209539 (70%)]\tAll Loss: 1.2709\tTriple Loss(1): 0.0315\tClassification Loss: 1.2078\r\n",
      "Train Epoch: 31 [147840/209539 (71%)]\tAll Loss: 1.1146\tTriple Loss(1): 0.0782\tClassification Loss: 0.9582\r\n",
      "Train Epoch: 31 [148480/209539 (71%)]\tAll Loss: 1.1280\tTriple Loss(1): 0.0124\tClassification Loss: 1.1031\r\n",
      "Train Epoch: 31 [149120/209539 (71%)]\tAll Loss: 1.7189\tTriple Loss(0): 0.1621\tClassification Loss: 1.3947\r\n",
      "Train Epoch: 31 [149760/209539 (71%)]\tAll Loss: 1.0433\tTriple Loss(1): 0.0005\tClassification Loss: 1.0424\r\n",
      "Train Epoch: 31 [150400/209539 (72%)]\tAll Loss: 1.5478\tTriple Loss(1): 0.0139\tClassification Loss: 1.5200\r\n",
      "Train Epoch: 31 [151040/209539 (72%)]\tAll Loss: 1.1111\tTriple Loss(1): 0.0098\tClassification Loss: 1.0915\r\n",
      "Train Epoch: 31 [151680/209539 (72%)]\tAll Loss: 1.5200\tTriple Loss(1): 0.0683\tClassification Loss: 1.3834\r\n",
      "Train Epoch: 31 [152320/209539 (73%)]\tAll Loss: 1.0335\tTriple Loss(1): 0.0320\tClassification Loss: 0.9696\r\n",
      "Train Epoch: 31 [152960/209539 (73%)]\tAll Loss: 1.0508\tTriple Loss(1): 0.0000\tClassification Loss: 1.0508\r\n",
      "Train Epoch: 31 [153600/209539 (73%)]\tAll Loss: 1.2976\tTriple Loss(1): 0.0233\tClassification Loss: 1.2511\r\n",
      "Train Epoch: 31 [154240/209539 (74%)]\tAll Loss: 1.1343\tTriple Loss(1): 0.0000\tClassification Loss: 1.1343\r\n",
      "Train Epoch: 31 [154880/209539 (74%)]\tAll Loss: 1.5794\tTriple Loss(0): 0.1423\tClassification Loss: 1.2948\r\n",
      "Train Epoch: 31 [155520/209539 (74%)]\tAll Loss: 1.4128\tTriple Loss(1): 0.0195\tClassification Loss: 1.3738\r\n",
      "Train Epoch: 31 [156160/209539 (75%)]\tAll Loss: 1.8575\tTriple Loss(0): 0.2868\tClassification Loss: 1.2839\r\n",
      "Train Epoch: 31 [156800/209539 (75%)]\tAll Loss: 1.9774\tTriple Loss(1): 0.1479\tClassification Loss: 1.6816\r\n",
      "Train Epoch: 31 [157440/209539 (75%)]\tAll Loss: 1.7434\tTriple Loss(1): 0.0405\tClassification Loss: 1.6624\r\n",
      "Train Epoch: 31 [158080/209539 (75%)]\tAll Loss: 1.3836\tTriple Loss(1): 0.0611\tClassification Loss: 1.2615\r\n",
      "Train Epoch: 31 [158720/209539 (76%)]\tAll Loss: 1.2308\tTriple Loss(1): 0.0311\tClassification Loss: 1.1686\r\n",
      "Train Epoch: 31 [159360/209539 (76%)]\tAll Loss: 1.1569\tTriple Loss(1): 0.0343\tClassification Loss: 1.0884\r\n",
      "Train Epoch: 31 [160000/209539 (76%)]\tAll Loss: 1.5225\tTriple Loss(1): 0.0196\tClassification Loss: 1.4834\r\n",
      "Train Epoch: 31 [160640/209539 (77%)]\tAll Loss: 1.3436\tTriple Loss(1): 0.0978\tClassification Loss: 1.1480\r\n",
      "Train Epoch: 31 [161280/209539 (77%)]\tAll Loss: 1.4413\tTriple Loss(0): 0.1990\tClassification Loss: 1.0432\r\n",
      "Train Epoch: 31 [161920/209539 (77%)]\tAll Loss: 1.2449\tTriple Loss(1): 0.0162\tClassification Loss: 1.2124\r\n",
      "Train Epoch: 31 [162560/209539 (78%)]\tAll Loss: 1.1768\tTriple Loss(1): 0.0016\tClassification Loss: 1.1736\r\n",
      "Train Epoch: 31 [163200/209539 (78%)]\tAll Loss: 1.0336\tTriple Loss(1): 0.0357\tClassification Loss: 0.9622\r\n",
      "Train Epoch: 31 [163840/209539 (78%)]\tAll Loss: 1.0673\tTriple Loss(1): 0.0021\tClassification Loss: 1.0631\r\n",
      "Train Epoch: 31 [164480/209539 (78%)]\tAll Loss: 1.0417\tTriple Loss(1): 0.0108\tClassification Loss: 1.0202\r\n",
      "Train Epoch: 31 [165120/209539 (79%)]\tAll Loss: 1.3849\tTriple Loss(0): 0.2839\tClassification Loss: 0.8172\r\n",
      "Train Epoch: 31 [165760/209539 (79%)]\tAll Loss: 1.9543\tTriple Loss(0): 0.3483\tClassification Loss: 1.2577\r\n",
      "Train Epoch: 31 [166400/209539 (79%)]\tAll Loss: 1.4719\tTriple Loss(1): 0.0385\tClassification Loss: 1.3949\r\n",
      "Train Epoch: 31 [167040/209539 (80%)]\tAll Loss: 1.3173\tTriple Loss(1): 0.0152\tClassification Loss: 1.2870\r\n",
      "Train Epoch: 31 [167680/209539 (80%)]\tAll Loss: 1.5029\tTriple Loss(1): 0.0135\tClassification Loss: 1.4759\r\n",
      "Train Epoch: 31 [168320/209539 (80%)]\tAll Loss: 1.1084\tTriple Loss(1): 0.0440\tClassification Loss: 1.0204\r\n",
      "Train Epoch: 31 [168960/209539 (81%)]\tAll Loss: 1.1692\tTriple Loss(1): 0.0464\tClassification Loss: 1.0764\r\n",
      "Train Epoch: 31 [169600/209539 (81%)]\tAll Loss: 1.1402\tTriple Loss(1): 0.0185\tClassification Loss: 1.1032\r\n",
      "Train Epoch: 31 [170240/209539 (81%)]\tAll Loss: 1.8182\tTriple Loss(0): 0.2920\tClassification Loss: 1.2342\r\n",
      "Train Epoch: 31 [170880/209539 (82%)]\tAll Loss: 1.2359\tTriple Loss(1): 0.0000\tClassification Loss: 1.2359\r\n",
      "Train Epoch: 31 [171520/209539 (82%)]\tAll Loss: 1.1990\tTriple Loss(1): 0.0137\tClassification Loss: 1.1717\r\n",
      "Train Epoch: 31 [172160/209539 (82%)]\tAll Loss: 1.6305\tTriple Loss(1): 0.0223\tClassification Loss: 1.5859\r\n",
      "Train Epoch: 31 [172800/209539 (82%)]\tAll Loss: 1.8580\tTriple Loss(0): 0.2037\tClassification Loss: 1.4507\r\n",
      "Train Epoch: 31 [173440/209539 (83%)]\tAll Loss: 1.6638\tTriple Loss(1): 0.0616\tClassification Loss: 1.5406\r\n",
      "Train Epoch: 31 [174080/209539 (83%)]\tAll Loss: 1.2749\tTriple Loss(1): 0.0000\tClassification Loss: 1.2749\r\n",
      "Train Epoch: 31 [174720/209539 (83%)]\tAll Loss: 1.2228\tTriple Loss(1): 0.0057\tClassification Loss: 1.2115\r\n",
      "Train Epoch: 31 [175360/209539 (84%)]\tAll Loss: 1.7468\tTriple Loss(0): 0.1645\tClassification Loss: 1.4178\r\n",
      "Train Epoch: 31 [176000/209539 (84%)]\tAll Loss: 1.2820\tTriple Loss(1): 0.0179\tClassification Loss: 1.2461\r\n",
      "Train Epoch: 31 [176640/209539 (84%)]\tAll Loss: 1.5479\tTriple Loss(0): 0.3477\tClassification Loss: 0.8526\r\n",
      "Train Epoch: 31 [177280/209539 (85%)]\tAll Loss: 1.6312\tTriple Loss(1): 0.0544\tClassification Loss: 1.5224\r\n",
      "Train Epoch: 31 [177920/209539 (85%)]\tAll Loss: 1.4686\tTriple Loss(1): 0.0176\tClassification Loss: 1.4335\r\n",
      "Train Epoch: 31 [178560/209539 (85%)]\tAll Loss: 2.1685\tTriple Loss(0): 0.4237\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 31 [179200/209539 (86%)]\tAll Loss: 1.5851\tTriple Loss(1): 0.0622\tClassification Loss: 1.4607\r\n",
      "Train Epoch: 31 [179840/209539 (86%)]\tAll Loss: 1.4261\tTriple Loss(1): 0.0157\tClassification Loss: 1.3947\r\n",
      "Train Epoch: 31 [180480/209539 (86%)]\tAll Loss: 1.8898\tTriple Loss(0): 0.2879\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 31 [181120/209539 (86%)]\tAll Loss: 2.0564\tTriple Loss(0): 0.2907\tClassification Loss: 1.4749\r\n",
      "Train Epoch: 31 [181760/209539 (87%)]\tAll Loss: 1.0696\tTriple Loss(1): 0.0091\tClassification Loss: 1.0514\r\n",
      "Train Epoch: 31 [182400/209539 (87%)]\tAll Loss: 1.7214\tTriple Loss(1): 0.0516\tClassification Loss: 1.6181\r\n",
      "Train Epoch: 31 [183040/209539 (87%)]\tAll Loss: 1.6887\tTriple Loss(0): 0.2095\tClassification Loss: 1.2697\r\n",
      "Train Epoch: 31 [183680/209539 (88%)]\tAll Loss: 0.9956\tTriple Loss(1): 0.0095\tClassification Loss: 0.9766\r\n",
      "Train Epoch: 31 [184320/209539 (88%)]\tAll Loss: 1.6252\tTriple Loss(0): 0.2560\tClassification Loss: 1.1132\r\n",
      "Train Epoch: 31 [184960/209539 (88%)]\tAll Loss: 1.2165\tTriple Loss(1): 0.0675\tClassification Loss: 1.0814\r\n",
      "Train Epoch: 31 [185600/209539 (89%)]\tAll Loss: 1.2633\tTriple Loss(1): 0.0166\tClassification Loss: 1.2301\r\n",
      "Train Epoch: 31 [186240/209539 (89%)]\tAll Loss: 1.3823\tTriple Loss(1): 0.0672\tClassification Loss: 1.2480\r\n",
      "Train Epoch: 31 [186880/209539 (89%)]\tAll Loss: 1.3230\tTriple Loss(1): 0.0243\tClassification Loss: 1.2745\r\n",
      "Train Epoch: 31 [187520/209539 (89%)]\tAll Loss: 1.6051\tTriple Loss(0): 0.2127\tClassification Loss: 1.1798\r\n",
      "Train Epoch: 31 [188160/209539 (90%)]\tAll Loss: 1.0750\tTriple Loss(1): 0.0381\tClassification Loss: 0.9987\r\n",
      "Train Epoch: 31 [188800/209539 (90%)]\tAll Loss: 1.6101\tTriple Loss(0): 0.2435\tClassification Loss: 1.1231\r\n",
      "Train Epoch: 31 [189440/209539 (90%)]\tAll Loss: 1.1786\tTriple Loss(1): 0.0437\tClassification Loss: 1.0911\r\n",
      "Train Epoch: 31 [190080/209539 (91%)]\tAll Loss: 1.2007\tTriple Loss(1): 0.0064\tClassification Loss: 1.1879\r\n",
      "Train Epoch: 31 [190720/209539 (91%)]\tAll Loss: 1.2128\tTriple Loss(1): 0.0130\tClassification Loss: 1.1869\r\n",
      "Train Epoch: 31 [191360/209539 (91%)]\tAll Loss: 1.4679\tTriple Loss(0): 0.2646\tClassification Loss: 0.9387\r\n",
      "Train Epoch: 31 [192000/209539 (92%)]\tAll Loss: 2.0671\tTriple Loss(0): 0.2079\tClassification Loss: 1.6513\r\n",
      "Train Epoch: 31 [192640/209539 (92%)]\tAll Loss: 1.8895\tTriple Loss(0): 0.4093\tClassification Loss: 1.0710\r\n",
      "Train Epoch: 31 [193280/209539 (92%)]\tAll Loss: 1.2097\tTriple Loss(1): 0.0084\tClassification Loss: 1.1929\r\n",
      "Train Epoch: 31 [193920/209539 (93%)]\tAll Loss: 1.1635\tTriple Loss(1): 0.0289\tClassification Loss: 1.1056\r\n",
      "Train Epoch: 31 [194560/209539 (93%)]\tAll Loss: 1.1344\tTriple Loss(1): 0.0000\tClassification Loss: 1.1344\r\n",
      "Train Epoch: 31 [195200/209539 (93%)]\tAll Loss: 1.3054\tTriple Loss(1): 0.0316\tClassification Loss: 1.2422\r\n",
      "Train Epoch: 31 [195840/209539 (93%)]\tAll Loss: 1.0340\tTriple Loss(1): 0.0238\tClassification Loss: 0.9864\r\n",
      "Train Epoch: 31 [196480/209539 (94%)]\tAll Loss: 1.3539\tTriple Loss(1): 0.0068\tClassification Loss: 1.3403\r\n",
      "Train Epoch: 31 [197120/209539 (94%)]\tAll Loss: 1.1483\tTriple Loss(1): 0.0000\tClassification Loss: 1.1483\r\n",
      "Train Epoch: 31 [197760/209539 (94%)]\tAll Loss: 1.8017\tTriple Loss(0): 0.2644\tClassification Loss: 1.2730\r\n",
      "Train Epoch: 31 [198400/209539 (95%)]\tAll Loss: 1.4461\tTriple Loss(0): 0.1675\tClassification Loss: 1.1110\r\n",
      "Train Epoch: 31 [199040/209539 (95%)]\tAll Loss: 1.4677\tTriple Loss(1): 0.0896\tClassification Loss: 1.2885\r\n",
      "Train Epoch: 31 [199680/209539 (95%)]\tAll Loss: 1.1091\tTriple Loss(1): 0.0254\tClassification Loss: 1.0583\r\n",
      "Train Epoch: 31 [200320/209539 (96%)]\tAll Loss: 1.4068\tTriple Loss(1): 0.0000\tClassification Loss: 1.4068\r\n",
      "Train Epoch: 31 [200960/209539 (96%)]\tAll Loss: 1.0375\tTriple Loss(1): 0.0115\tClassification Loss: 1.0146\r\n",
      "Train Epoch: 31 [201600/209539 (96%)]\tAll Loss: 1.7768\tTriple Loss(0): 0.3310\tClassification Loss: 1.1149\r\n",
      "Train Epoch: 31 [202240/209539 (97%)]\tAll Loss: 1.1969\tTriple Loss(1): 0.0084\tClassification Loss: 1.1801\r\n",
      "Train Epoch: 31 [202880/209539 (97%)]\tAll Loss: 0.9322\tTriple Loss(1): 0.0006\tClassification Loss: 0.9311\r\n",
      "Train Epoch: 31 [203520/209539 (97%)]\tAll Loss: 1.2601\tTriple Loss(1): 0.0067\tClassification Loss: 1.2468\r\n",
      "Train Epoch: 31 [204160/209539 (97%)]\tAll Loss: 1.4211\tTriple Loss(1): 0.0035\tClassification Loss: 1.4141\r\n",
      "Train Epoch: 31 [204800/209539 (98%)]\tAll Loss: 1.4229\tTriple Loss(1): 0.0000\tClassification Loss: 1.4229\r\n",
      "Train Epoch: 31 [205440/209539 (98%)]\tAll Loss: 1.2990\tTriple Loss(0): 0.2422\tClassification Loss: 0.8146\r\n",
      "Train Epoch: 31 [206080/209539 (98%)]\tAll Loss: 1.1762\tTriple Loss(1): 0.0040\tClassification Loss: 1.1682\r\n",
      "Train Epoch: 31 [206720/209539 (99%)]\tAll Loss: 1.0309\tTriple Loss(1): 0.0181\tClassification Loss: 0.9946\r\n",
      "Train Epoch: 31 [207360/209539 (99%)]\tAll Loss: 1.3212\tTriple Loss(1): 0.0788\tClassification Loss: 1.1636\r\n",
      "Train Epoch: 31 [208000/209539 (99%)]\tAll Loss: 1.1466\tTriple Loss(1): 0.0394\tClassification Loss: 1.0677\r\n",
      "Train Epoch: 31 [208640/209539 (100%)]\tAll Loss: 1.7986\tTriple Loss(0): 0.2438\tClassification Loss: 1.3111\r\n",
      "Train Epoch: 31 [209280/209539 (100%)]\tAll Loss: 1.2343\tTriple Loss(1): 0.0000\tClassification Loss: 1.2343\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/31_epochs\r\n",
      "Train Epoch: 32 [0/209539 (0%)]\tAll Loss: 2.4931\tTriple Loss(0): 0.4250\tClassification Loss: 1.6432\r\n",
      "\r\n",
      "Test set: Average loss: 1.0964\r\n",
      "Top 1 Accuracy: 54512/80128 (68%)\r\n",
      "Top 3 Accuracy: 69887/80128 (87%)\r\n",
      "Top 5 Accuracy: 74633/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 32 [640/209539 (0%)]\tAll Loss: 1.4110\tTriple Loss(1): 0.0391\tClassification Loss: 1.3327\r\n",
      "Train Epoch: 32 [1280/209539 (1%)]\tAll Loss: 1.1043\tTriple Loss(1): 0.0206\tClassification Loss: 1.0631\r\n",
      "Train Epoch: 32 [1920/209539 (1%)]\tAll Loss: 1.5073\tTriple Loss(0): 0.1380\tClassification Loss: 1.2314\r\n",
      "Train Epoch: 32 [2560/209539 (1%)]\tAll Loss: 1.4014\tTriple Loss(1): 0.0033\tClassification Loss: 1.3948\r\n",
      "Train Epoch: 32 [3200/209539 (2%)]\tAll Loss: 1.2280\tTriple Loss(1): 0.0397\tClassification Loss: 1.1485\r\n",
      "Train Epoch: 32 [3840/209539 (2%)]\tAll Loss: 1.7429\tTriple Loss(0): 0.3097\tClassification Loss: 1.1234\r\n",
      "Train Epoch: 32 [4480/209539 (2%)]\tAll Loss: 1.1853\tTriple Loss(1): 0.0202\tClassification Loss: 1.1449\r\n",
      "Train Epoch: 32 [5120/209539 (2%)]\tAll Loss: 1.8770\tTriple Loss(0): 0.3565\tClassification Loss: 1.1640\r\n",
      "Train Epoch: 32 [5760/209539 (3%)]\tAll Loss: 1.8669\tTriple Loss(0): 0.2518\tClassification Loss: 1.3632\r\n",
      "Train Epoch: 32 [6400/209539 (3%)]\tAll Loss: 1.0383\tTriple Loss(1): 0.0000\tClassification Loss: 1.0383\r\n",
      "Train Epoch: 32 [7040/209539 (3%)]\tAll Loss: 1.2123\tTriple Loss(1): 0.0187\tClassification Loss: 1.1749\r\n",
      "Train Epoch: 32 [7680/209539 (4%)]\tAll Loss: 1.0339\tTriple Loss(1): 0.0000\tClassification Loss: 1.0339\r\n",
      "Train Epoch: 32 [8320/209539 (4%)]\tAll Loss: 1.8083\tTriple Loss(0): 0.3340\tClassification Loss: 1.1404\r\n",
      "Train Epoch: 32 [8960/209539 (4%)]\tAll Loss: 1.7191\tTriple Loss(0): 0.3303\tClassification Loss: 1.0585\r\n",
      "Train Epoch: 32 [9600/209539 (5%)]\tAll Loss: 1.7396\tTriple Loss(1): 0.0707\tClassification Loss: 1.5981\r\n",
      "Train Epoch: 32 [10240/209539 (5%)]\tAll Loss: 1.1261\tTriple Loss(1): 0.0005\tClassification Loss: 1.1251\r\n",
      "Train Epoch: 32 [10880/209539 (5%)]\tAll Loss: 1.5975\tTriple Loss(0): 0.2347\tClassification Loss: 1.1281\r\n",
      "Train Epoch: 32 [11520/209539 (5%)]\tAll Loss: 1.2659\tTriple Loss(1): 0.0000\tClassification Loss: 1.2659\r\n",
      "Train Epoch: 32 [12160/209539 (6%)]\tAll Loss: 1.1181\tTriple Loss(1): 0.0019\tClassification Loss: 1.1144\r\n",
      "Train Epoch: 32 [12800/209539 (6%)]\tAll Loss: 0.8882\tTriple Loss(1): 0.0000\tClassification Loss: 0.8882\r\n",
      "Train Epoch: 32 [13440/209539 (6%)]\tAll Loss: 1.2717\tTriple Loss(1): 0.0282\tClassification Loss: 1.2153\r\n",
      "Train Epoch: 32 [14080/209539 (7%)]\tAll Loss: 1.5178\tTriple Loss(1): 0.0221\tClassification Loss: 1.4736\r\n",
      "Train Epoch: 32 [14720/209539 (7%)]\tAll Loss: 1.3463\tTriple Loss(1): 0.0114\tClassification Loss: 1.3234\r\n",
      "Train Epoch: 32 [15360/209539 (7%)]\tAll Loss: 1.1339\tTriple Loss(1): 0.0109\tClassification Loss: 1.1120\r\n",
      "Train Epoch: 32 [16000/209539 (8%)]\tAll Loss: 1.6775\tTriple Loss(0): 0.0893\tClassification Loss: 1.4989\r\n",
      "Train Epoch: 32 [16640/209539 (8%)]\tAll Loss: 1.4872\tTriple Loss(1): 0.0176\tClassification Loss: 1.4521\r\n",
      "Train Epoch: 32 [17280/209539 (8%)]\tAll Loss: 1.1321\tTriple Loss(1): 0.0146\tClassification Loss: 1.1028\r\n",
      "Train Epoch: 32 [17920/209539 (9%)]\tAll Loss: 1.2574\tTriple Loss(1): 0.0265\tClassification Loss: 1.2044\r\n",
      "Train Epoch: 32 [18560/209539 (9%)]\tAll Loss: 1.1720\tTriple Loss(1): 0.0049\tClassification Loss: 1.1621\r\n",
      "Train Epoch: 32 [19200/209539 (9%)]\tAll Loss: 1.1770\tTriple Loss(1): 0.0183\tClassification Loss: 1.1404\r\n",
      "Train Epoch: 32 [19840/209539 (9%)]\tAll Loss: 1.8020\tTriple Loss(0): 0.2526\tClassification Loss: 1.2968\r\n",
      "Train Epoch: 32 [20480/209539 (10%)]\tAll Loss: 1.2958\tTriple Loss(1): 0.0000\tClassification Loss: 1.2958\r\n",
      "Train Epoch: 32 [21120/209539 (10%)]\tAll Loss: 1.4485\tTriple Loss(1): 0.0000\tClassification Loss: 1.4485\r\n",
      "Train Epoch: 32 [21760/209539 (10%)]\tAll Loss: 1.1044\tTriple Loss(1): 0.0302\tClassification Loss: 1.0439\r\n",
      "Train Epoch: 32 [22400/209539 (11%)]\tAll Loss: 1.0697\tTriple Loss(1): 0.0000\tClassification Loss: 1.0697\r\n",
      "Train Epoch: 32 [23040/209539 (11%)]\tAll Loss: 1.3009\tTriple Loss(1): 0.0250\tClassification Loss: 1.2509\r\n",
      "Train Epoch: 32 [23680/209539 (11%)]\tAll Loss: 1.0455\tTriple Loss(1): 0.0324\tClassification Loss: 0.9808\r\n",
      "Train Epoch: 32 [24320/209539 (12%)]\tAll Loss: 1.5414\tTriple Loss(0): 0.1929\tClassification Loss: 1.1556\r\n",
      "Train Epoch: 32 [24960/209539 (12%)]\tAll Loss: 1.1461\tTriple Loss(1): 0.0127\tClassification Loss: 1.1206\r\n",
      "Train Epoch: 32 [25600/209539 (12%)]\tAll Loss: 1.8871\tTriple Loss(0): 0.3773\tClassification Loss: 1.1326\r\n",
      "Train Epoch: 32 [26240/209539 (13%)]\tAll Loss: 1.0654\tTriple Loss(1): 0.0199\tClassification Loss: 1.0256\r\n",
      "Train Epoch: 32 [26880/209539 (13%)]\tAll Loss: 1.4395\tTriple Loss(1): 0.0454\tClassification Loss: 1.3486\r\n",
      "Train Epoch: 32 [27520/209539 (13%)]\tAll Loss: 1.7091\tTriple Loss(0): 0.3160\tClassification Loss: 1.0772\r\n",
      "Train Epoch: 32 [28160/209539 (13%)]\tAll Loss: 2.0857\tTriple Loss(0): 0.3829\tClassification Loss: 1.3198\r\n",
      "Train Epoch: 32 [28800/209539 (14%)]\tAll Loss: 1.5614\tTriple Loss(1): 0.0382\tClassification Loss: 1.4850\r\n",
      "Train Epoch: 32 [29440/209539 (14%)]\tAll Loss: 1.1768\tTriple Loss(1): 0.0152\tClassification Loss: 1.1464\r\n",
      "Train Epoch: 32 [30080/209539 (14%)]\tAll Loss: 1.0516\tTriple Loss(1): 0.0151\tClassification Loss: 1.0214\r\n",
      "Train Epoch: 32 [30720/209539 (15%)]\tAll Loss: 1.1907\tTriple Loss(1): 0.0236\tClassification Loss: 1.1435\r\n",
      "Train Epoch: 32 [31360/209539 (15%)]\tAll Loss: 1.2069\tTriple Loss(1): 0.0103\tClassification Loss: 1.1863\r\n",
      "Train Epoch: 32 [32000/209539 (15%)]\tAll Loss: 1.5378\tTriple Loss(1): 0.0658\tClassification Loss: 1.4063\r\n",
      "Train Epoch: 32 [32640/209539 (16%)]\tAll Loss: 1.2867\tTriple Loss(1): 0.0247\tClassification Loss: 1.2374\r\n",
      "Train Epoch: 32 [33280/209539 (16%)]\tAll Loss: 1.2956\tTriple Loss(1): 0.0570\tClassification Loss: 1.1816\r\n",
      "Train Epoch: 32 [33920/209539 (16%)]\tAll Loss: 1.1988\tTriple Loss(1): 0.0000\tClassification Loss: 1.1988\r\n",
      "Train Epoch: 32 [34560/209539 (16%)]\tAll Loss: 1.1876\tTriple Loss(1): 0.0000\tClassification Loss: 1.1876\r\n",
      "Train Epoch: 32 [35200/209539 (17%)]\tAll Loss: 1.0100\tTriple Loss(1): 0.0548\tClassification Loss: 0.9004\r\n",
      "Train Epoch: 32 [35840/209539 (17%)]\tAll Loss: 1.1096\tTriple Loss(1): 0.0117\tClassification Loss: 1.0862\r\n",
      "Train Epoch: 32 [36480/209539 (17%)]\tAll Loss: 1.2985\tTriple Loss(0): 0.1958\tClassification Loss: 0.9069\r\n",
      "Train Epoch: 32 [37120/209539 (18%)]\tAll Loss: 1.4942\tTriple Loss(1): 0.0297\tClassification Loss: 1.4348\r\n",
      "Train Epoch: 32 [37760/209539 (18%)]\tAll Loss: 1.3078\tTriple Loss(1): 0.0150\tClassification Loss: 1.2778\r\n",
      "Train Epoch: 32 [38400/209539 (18%)]\tAll Loss: 1.4219\tTriple Loss(1): 0.0117\tClassification Loss: 1.3985\r\n",
      "Train Epoch: 32 [39040/209539 (19%)]\tAll Loss: 2.0888\tTriple Loss(0): 0.5634\tClassification Loss: 0.9619\r\n",
      "Train Epoch: 32 [39680/209539 (19%)]\tAll Loss: 1.2932\tTriple Loss(1): 0.0627\tClassification Loss: 1.1677\r\n",
      "Train Epoch: 32 [40320/209539 (19%)]\tAll Loss: 1.8559\tTriple Loss(0): 0.3584\tClassification Loss: 1.1391\r\n",
      "Train Epoch: 32 [40960/209539 (20%)]\tAll Loss: 1.1784\tTriple Loss(1): 0.0142\tClassification Loss: 1.1500\r\n",
      "Train Epoch: 32 [41600/209539 (20%)]\tAll Loss: 1.2517\tTriple Loss(1): 0.0338\tClassification Loss: 1.1841\r\n",
      "Train Epoch: 32 [42240/209539 (20%)]\tAll Loss: 1.1452\tTriple Loss(1): 0.0187\tClassification Loss: 1.1078\r\n",
      "Train Epoch: 32 [42880/209539 (20%)]\tAll Loss: 0.9860\tTriple Loss(1): 0.0321\tClassification Loss: 0.9217\r\n",
      "Train Epoch: 32 [43520/209539 (21%)]\tAll Loss: 1.2854\tTriple Loss(1): 0.0170\tClassification Loss: 1.2513\r\n",
      "Train Epoch: 32 [44160/209539 (21%)]\tAll Loss: 1.4292\tTriple Loss(1): 0.0213\tClassification Loss: 1.3867\r\n",
      "Train Epoch: 32 [44800/209539 (21%)]\tAll Loss: 1.5430\tTriple Loss(1): 0.0261\tClassification Loss: 1.4908\r\n",
      "Train Epoch: 32 [45440/209539 (22%)]\tAll Loss: 1.7068\tTriple Loss(1): 0.0555\tClassification Loss: 1.5959\r\n",
      "Train Epoch: 32 [46080/209539 (22%)]\tAll Loss: 1.3534\tTriple Loss(1): 0.0593\tClassification Loss: 1.2348\r\n",
      "Train Epoch: 32 [46720/209539 (22%)]\tAll Loss: 1.5848\tTriple Loss(1): 0.0612\tClassification Loss: 1.4625\r\n",
      "Train Epoch: 32 [47360/209539 (23%)]\tAll Loss: 0.9723\tTriple Loss(1): 0.0779\tClassification Loss: 0.8165\r\n",
      "Train Epoch: 32 [48000/209539 (23%)]\tAll Loss: 1.2172\tTriple Loss(1): 0.0175\tClassification Loss: 1.1823\r\n",
      "Train Epoch: 32 [48640/209539 (23%)]\tAll Loss: 1.6861\tTriple Loss(1): 0.0726\tClassification Loss: 1.5409\r\n",
      "Train Epoch: 32 [49280/209539 (24%)]\tAll Loss: 2.2363\tTriple Loss(0): 0.5294\tClassification Loss: 1.1775\r\n",
      "Train Epoch: 32 [49920/209539 (24%)]\tAll Loss: 1.3924\tTriple Loss(1): 0.0638\tClassification Loss: 1.2649\r\n",
      "Train Epoch: 32 [50560/209539 (24%)]\tAll Loss: 2.1601\tTriple Loss(0): 0.3871\tClassification Loss: 1.3859\r\n",
      "Train Epoch: 32 [51200/209539 (24%)]\tAll Loss: 1.6841\tTriple Loss(1): 0.0773\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 32 [51840/209539 (25%)]\tAll Loss: 1.3556\tTriple Loss(1): 0.0858\tClassification Loss: 1.1840\r\n",
      "Train Epoch: 32 [52480/209539 (25%)]\tAll Loss: 1.2442\tTriple Loss(1): 0.0524\tClassification Loss: 1.1394\r\n",
      "Train Epoch: 32 [53120/209539 (25%)]\tAll Loss: 2.1018\tTriple Loss(0): 0.4472\tClassification Loss: 1.2074\r\n",
      "Train Epoch: 32 [53760/209539 (26%)]\tAll Loss: 1.5580\tTriple Loss(1): 0.0591\tClassification Loss: 1.4397\r\n",
      "Train Epoch: 32 [54400/209539 (26%)]\tAll Loss: 1.7492\tTriple Loss(1): 0.0760\tClassification Loss: 1.5971\r\n",
      "Train Epoch: 32 [55040/209539 (26%)]\tAll Loss: 0.9540\tTriple Loss(1): 0.0393\tClassification Loss: 0.8754\r\n",
      "Train Epoch: 32 [55680/209539 (27%)]\tAll Loss: 1.5017\tTriple Loss(1): 0.0252\tClassification Loss: 1.4513\r\n",
      "Train Epoch: 32 [56320/209539 (27%)]\tAll Loss: 1.0265\tTriple Loss(1): 0.0220\tClassification Loss: 0.9825\r\n",
      "Train Epoch: 32 [56960/209539 (27%)]\tAll Loss: 1.3247\tTriple Loss(1): 0.1040\tClassification Loss: 1.1167\r\n",
      "Train Epoch: 32 [57600/209539 (27%)]\tAll Loss: 1.1250\tTriple Loss(1): 0.0484\tClassification Loss: 1.0282\r\n",
      "Train Epoch: 32 [58240/209539 (28%)]\tAll Loss: 0.9566\tTriple Loss(1): 0.0596\tClassification Loss: 0.8374\r\n",
      "Train Epoch: 32 [58880/209539 (28%)]\tAll Loss: 1.4435\tTriple Loss(1): 0.0389\tClassification Loss: 1.3658\r\n",
      "Train Epoch: 32 [59520/209539 (28%)]\tAll Loss: 1.1553\tTriple Loss(1): 0.0191\tClassification Loss: 1.1171\r\n",
      "Train Epoch: 32 [60160/209539 (29%)]\tAll Loss: 1.2829\tTriple Loss(1): 0.0143\tClassification Loss: 1.2543\r\n",
      "Train Epoch: 32 [60800/209539 (29%)]\tAll Loss: 1.2391\tTriple Loss(1): 0.0346\tClassification Loss: 1.1699\r\n",
      "Train Epoch: 32 [61440/209539 (29%)]\tAll Loss: 1.4256\tTriple Loss(1): 0.0422\tClassification Loss: 1.3413\r\n",
      "Train Epoch: 32 [62080/209539 (30%)]\tAll Loss: 1.3389\tTriple Loss(1): 0.0372\tClassification Loss: 1.2645\r\n",
      "Train Epoch: 32 [62720/209539 (30%)]\tAll Loss: 1.0194\tTriple Loss(1): 0.0000\tClassification Loss: 1.0194\r\n",
      "Train Epoch: 32 [63360/209539 (30%)]\tAll Loss: 1.4072\tTriple Loss(1): 0.0238\tClassification Loss: 1.3595\r\n",
      "Train Epoch: 32 [64000/209539 (31%)]\tAll Loss: 1.4251\tTriple Loss(1): 0.0350\tClassification Loss: 1.3550\r\n",
      "Train Epoch: 32 [64640/209539 (31%)]\tAll Loss: 1.5471\tTriple Loss(1): 0.0580\tClassification Loss: 1.4312\r\n",
      "Train Epoch: 32 [65280/209539 (31%)]\tAll Loss: 1.2994\tTriple Loss(1): 0.0842\tClassification Loss: 1.1309\r\n",
      "Train Epoch: 32 [65920/209539 (31%)]\tAll Loss: 1.7104\tTriple Loss(1): 0.1814\tClassification Loss: 1.3477\r\n",
      "Train Epoch: 32 [66560/209539 (32%)]\tAll Loss: 1.1718\tTriple Loss(1): 0.0357\tClassification Loss: 1.1004\r\n",
      "Train Epoch: 32 [67200/209539 (32%)]\tAll Loss: 1.1502\tTriple Loss(1): 0.0080\tClassification Loss: 1.1342\r\n",
      "Train Epoch: 32 [67840/209539 (32%)]\tAll Loss: 1.7889\tTriple Loss(0): 0.3614\tClassification Loss: 1.0661\r\n",
      "Train Epoch: 32 [68480/209539 (33%)]\tAll Loss: 1.4973\tTriple Loss(1): 0.0271\tClassification Loss: 1.4432\r\n",
      "Train Epoch: 32 [69120/209539 (33%)]\tAll Loss: 1.3694\tTriple Loss(1): 0.0111\tClassification Loss: 1.3472\r\n",
      "Train Epoch: 32 [69760/209539 (33%)]\tAll Loss: 1.2740\tTriple Loss(1): 0.0825\tClassification Loss: 1.1091\r\n",
      "Train Epoch: 32 [70400/209539 (34%)]\tAll Loss: 1.1107\tTriple Loss(1): 0.0000\tClassification Loss: 1.1107\r\n",
      "Train Epoch: 32 [71040/209539 (34%)]\tAll Loss: 1.5624\tTriple Loss(1): 0.0383\tClassification Loss: 1.4858\r\n",
      "Train Epoch: 32 [71680/209539 (34%)]\tAll Loss: 2.0273\tTriple Loss(0): 0.3416\tClassification Loss: 1.3441\r\n",
      "Train Epoch: 32 [72320/209539 (35%)]\tAll Loss: 1.2306\tTriple Loss(1): 0.0868\tClassification Loss: 1.0569\r\n",
      "Train Epoch: 32 [72960/209539 (35%)]\tAll Loss: 1.2692\tTriple Loss(1): 0.0000\tClassification Loss: 1.2692\r\n",
      "Train Epoch: 32 [73600/209539 (35%)]\tAll Loss: 1.2735\tTriple Loss(1): 0.0043\tClassification Loss: 1.2650\r\n",
      "Train Epoch: 32 [74240/209539 (35%)]\tAll Loss: 1.5845\tTriple Loss(1): 0.0234\tClassification Loss: 1.5378\r\n",
      "Train Epoch: 32 [74880/209539 (36%)]\tAll Loss: 2.3673\tTriple Loss(0): 0.3375\tClassification Loss: 1.6924\r\n",
      "Train Epoch: 32 [75520/209539 (36%)]\tAll Loss: 1.1947\tTriple Loss(1): 0.0303\tClassification Loss: 1.1341\r\n",
      "Train Epoch: 32 [76160/209539 (36%)]\tAll Loss: 1.1890\tTriple Loss(1): 0.0000\tClassification Loss: 1.1890\r\n",
      "Train Epoch: 32 [76800/209539 (37%)]\tAll Loss: 1.1745\tTriple Loss(1): 0.0125\tClassification Loss: 1.1495\r\n",
      "Train Epoch: 32 [77440/209539 (37%)]\tAll Loss: 1.2535\tTriple Loss(1): 0.0366\tClassification Loss: 1.1803\r\n",
      "Train Epoch: 32 [78080/209539 (37%)]\tAll Loss: 1.3213\tTriple Loss(1): 0.0219\tClassification Loss: 1.2775\r\n",
      "Train Epoch: 32 [78720/209539 (38%)]\tAll Loss: 1.4555\tTriple Loss(1): 0.0011\tClassification Loss: 1.4534\r\n",
      "Train Epoch: 32 [79360/209539 (38%)]\tAll Loss: 1.2168\tTriple Loss(1): 0.0504\tClassification Loss: 1.1161\r\n",
      "Train Epoch: 32 [80000/209539 (38%)]\tAll Loss: 1.0989\tTriple Loss(1): 0.0175\tClassification Loss: 1.0638\r\n",
      "Train Epoch: 32 [80640/209539 (38%)]\tAll Loss: 1.5149\tTriple Loss(1): 0.0712\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 32 [81280/209539 (39%)]\tAll Loss: 1.3058\tTriple Loss(1): 0.0250\tClassification Loss: 1.2558\r\n",
      "Train Epoch: 32 [81920/209539 (39%)]\tAll Loss: 1.0393\tTriple Loss(1): 0.0012\tClassification Loss: 1.0370\r\n",
      "Train Epoch: 32 [82560/209539 (39%)]\tAll Loss: 1.1718\tTriple Loss(1): 0.0000\tClassification Loss: 1.1718\r\n",
      "Train Epoch: 32 [83200/209539 (40%)]\tAll Loss: 1.4767\tTriple Loss(1): 0.0135\tClassification Loss: 1.4497\r\n",
      "Train Epoch: 32 [83840/209539 (40%)]\tAll Loss: 1.1790\tTriple Loss(1): 0.0231\tClassification Loss: 1.1329\r\n",
      "Train Epoch: 32 [84480/209539 (40%)]\tAll Loss: 1.2278\tTriple Loss(1): 0.0554\tClassification Loss: 1.1170\r\n",
      "Train Epoch: 32 [85120/209539 (41%)]\tAll Loss: 1.5582\tTriple Loss(1): 0.0000\tClassification Loss: 1.5582\r\n",
      "Train Epoch: 32 [85760/209539 (41%)]\tAll Loss: 1.9265\tTriple Loss(0): 0.3157\tClassification Loss: 1.2952\r\n",
      "Train Epoch: 32 [86400/209539 (41%)]\tAll Loss: 1.3629\tTriple Loss(1): 0.0075\tClassification Loss: 1.3478\r\n",
      "Train Epoch: 32 [87040/209539 (42%)]\tAll Loss: 1.1649\tTriple Loss(1): 0.0844\tClassification Loss: 0.9960\r\n",
      "Train Epoch: 32 [87680/209539 (42%)]\tAll Loss: 1.2045\tTriple Loss(1): 0.0569\tClassification Loss: 1.0906\r\n",
      "Train Epoch: 32 [88320/209539 (42%)]\tAll Loss: 1.7398\tTriple Loss(0): 0.3037\tClassification Loss: 1.1323\r\n",
      "Train Epoch: 32 [88960/209539 (42%)]\tAll Loss: 1.3269\tTriple Loss(1): 0.0233\tClassification Loss: 1.2803\r\n",
      "Train Epoch: 32 [89600/209539 (43%)]\tAll Loss: 1.5810\tTriple Loss(1): 0.0143\tClassification Loss: 1.5525\r\n",
      "Train Epoch: 32 [90240/209539 (43%)]\tAll Loss: 1.2869\tTriple Loss(1): 0.0851\tClassification Loss: 1.1166\r\n",
      "Train Epoch: 32 [90880/209539 (43%)]\tAll Loss: 1.5417\tTriple Loss(1): 0.0148\tClassification Loss: 1.5120\r\n",
      "Train Epoch: 32 [91520/209539 (44%)]\tAll Loss: 1.3906\tTriple Loss(1): 0.0704\tClassification Loss: 1.2498\r\n",
      "Train Epoch: 32 [92160/209539 (44%)]\tAll Loss: 1.5956\tTriple Loss(0): 0.2405\tClassification Loss: 1.1146\r\n",
      "Train Epoch: 32 [92800/209539 (44%)]\tAll Loss: 1.2418\tTriple Loss(1): 0.0382\tClassification Loss: 1.1654\r\n",
      "Train Epoch: 32 [93440/209539 (45%)]\tAll Loss: 1.2641\tTriple Loss(1): 0.0061\tClassification Loss: 1.2519\r\n",
      "Train Epoch: 32 [94080/209539 (45%)]\tAll Loss: 1.3165\tTriple Loss(1): 0.0748\tClassification Loss: 1.1669\r\n",
      "Train Epoch: 32 [94720/209539 (45%)]\tAll Loss: 1.7707\tTriple Loss(0): 0.1552\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 32 [95360/209539 (46%)]\tAll Loss: 1.5303\tTriple Loss(1): 0.0528\tClassification Loss: 1.4247\r\n",
      "Train Epoch: 32 [96000/209539 (46%)]\tAll Loss: 1.3058\tTriple Loss(1): 0.0141\tClassification Loss: 1.2776\r\n",
      "Train Epoch: 32 [96640/209539 (46%)]\tAll Loss: 1.6130\tTriple Loss(1): 0.0080\tClassification Loss: 1.5971\r\n",
      "Train Epoch: 32 [97280/209539 (46%)]\tAll Loss: 1.3195\tTriple Loss(1): 0.0481\tClassification Loss: 1.2233\r\n",
      "Train Epoch: 32 [97920/209539 (47%)]\tAll Loss: 1.1579\tTriple Loss(1): 0.0000\tClassification Loss: 1.1579\r\n",
      "Train Epoch: 32 [98560/209539 (47%)]\tAll Loss: 1.4910\tTriple Loss(1): 0.0573\tClassification Loss: 1.3764\r\n",
      "Train Epoch: 32 [99200/209539 (47%)]\tAll Loss: 1.1930\tTriple Loss(1): 0.0083\tClassification Loss: 1.1764\r\n",
      "Train Epoch: 32 [99840/209539 (48%)]\tAll Loss: 1.4621\tTriple Loss(1): 0.0658\tClassification Loss: 1.3305\r\n",
      "Train Epoch: 32 [100480/209539 (48%)]\tAll Loss: 1.5078\tTriple Loss(1): 0.0104\tClassification Loss: 1.4869\r\n",
      "Train Epoch: 32 [101120/209539 (48%)]\tAll Loss: 1.2685\tTriple Loss(1): 0.0312\tClassification Loss: 1.2061\r\n",
      "Train Epoch: 32 [101760/209539 (49%)]\tAll Loss: 1.2169\tTriple Loss(1): 0.0308\tClassification Loss: 1.1553\r\n",
      "Train Epoch: 32 [102400/209539 (49%)]\tAll Loss: 1.0629\tTriple Loss(1): 0.0304\tClassification Loss: 1.0021\r\n",
      "Train Epoch: 32 [103040/209539 (49%)]\tAll Loss: 1.2386\tTriple Loss(1): 0.0387\tClassification Loss: 1.1613\r\n",
      "Train Epoch: 32 [103680/209539 (49%)]\tAll Loss: 1.3027\tTriple Loss(1): 0.0456\tClassification Loss: 1.2115\r\n",
      "Train Epoch: 32 [104320/209539 (50%)]\tAll Loss: 1.0863\tTriple Loss(1): 0.0189\tClassification Loss: 1.0484\r\n",
      "Train Epoch: 32 [104960/209539 (50%)]\tAll Loss: 1.1362\tTriple Loss(1): 0.0157\tClassification Loss: 1.1048\r\n",
      "Train Epoch: 32 [105600/209539 (50%)]\tAll Loss: 1.3380\tTriple Loss(1): 0.0549\tClassification Loss: 1.2283\r\n",
      "Train Epoch: 32 [106240/209539 (51%)]\tAll Loss: 1.3017\tTriple Loss(1): 0.0838\tClassification Loss: 1.1340\r\n",
      "Train Epoch: 32 [106880/209539 (51%)]\tAll Loss: 1.2178\tTriple Loss(1): 0.0136\tClassification Loss: 1.1906\r\n",
      "Train Epoch: 32 [107520/209539 (51%)]\tAll Loss: 1.1837\tTriple Loss(1): 0.0154\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 32 [108160/209539 (52%)]\tAll Loss: 1.5811\tTriple Loss(0): 0.2972\tClassification Loss: 0.9868\r\n",
      "Train Epoch: 32 [108800/209539 (52%)]\tAll Loss: 1.2474\tTriple Loss(1): 0.0550\tClassification Loss: 1.1373\r\n",
      "Train Epoch: 32 [109440/209539 (52%)]\tAll Loss: 1.3708\tTriple Loss(1): 0.0000\tClassification Loss: 1.3708\r\n",
      "Train Epoch: 32 [110080/209539 (53%)]\tAll Loss: 1.2967\tTriple Loss(1): 0.0030\tClassification Loss: 1.2906\r\n",
      "Train Epoch: 32 [110720/209539 (53%)]\tAll Loss: 1.3925\tTriple Loss(1): 0.0164\tClassification Loss: 1.3596\r\n",
      "Train Epoch: 32 [111360/209539 (53%)]\tAll Loss: 1.7131\tTriple Loss(0): 0.3408\tClassification Loss: 1.0315\r\n",
      "Train Epoch: 32 [112000/209539 (53%)]\tAll Loss: 1.5326\tTriple Loss(0): 0.2032\tClassification Loss: 1.1263\r\n",
      "Train Epoch: 32 [112640/209539 (54%)]\tAll Loss: 1.2858\tTriple Loss(1): 0.0098\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 32 [113280/209539 (54%)]\tAll Loss: 0.9171\tTriple Loss(1): 0.0426\tClassification Loss: 0.8319\r\n",
      "Train Epoch: 32 [113920/209539 (54%)]\tAll Loss: 1.1545\tTriple Loss(1): 0.0093\tClassification Loss: 1.1358\r\n",
      "Train Epoch: 32 [114560/209539 (55%)]\tAll Loss: 1.3980\tTriple Loss(1): 0.0455\tClassification Loss: 1.3069\r\n",
      "Train Epoch: 32 [115200/209539 (55%)]\tAll Loss: 1.4038\tTriple Loss(1): 0.0354\tClassification Loss: 1.3331\r\n",
      "Train Epoch: 32 [115840/209539 (55%)]\tAll Loss: 1.3628\tTriple Loss(1): 0.0126\tClassification Loss: 1.3375\r\n",
      "Train Epoch: 32 [116480/209539 (56%)]\tAll Loss: 1.4170\tTriple Loss(1): 0.0059\tClassification Loss: 1.4052\r\n",
      "Train Epoch: 32 [117120/209539 (56%)]\tAll Loss: 1.8556\tTriple Loss(0): 0.2681\tClassification Loss: 1.3194\r\n",
      "Train Epoch: 32 [117760/209539 (56%)]\tAll Loss: 1.3101\tTriple Loss(1): 0.0405\tClassification Loss: 1.2292\r\n",
      "Train Epoch: 32 [118400/209539 (57%)]\tAll Loss: 1.1204\tTriple Loss(1): 0.1175\tClassification Loss: 0.8853\r\n",
      "Train Epoch: 32 [119040/209539 (57%)]\tAll Loss: 1.5784\tTriple Loss(1): 0.0522\tClassification Loss: 1.4740\r\n",
      "Train Epoch: 32 [119680/209539 (57%)]\tAll Loss: 1.2644\tTriple Loss(1): 0.0034\tClassification Loss: 1.2576\r\n",
      "Train Epoch: 32 [120320/209539 (57%)]\tAll Loss: 1.9990\tTriple Loss(0): 0.3875\tClassification Loss: 1.2240\r\n",
      "Train Epoch: 32 [120960/209539 (58%)]\tAll Loss: 1.0940\tTriple Loss(1): 0.0000\tClassification Loss: 1.0940\r\n",
      "Train Epoch: 32 [121600/209539 (58%)]\tAll Loss: 1.0844\tTriple Loss(1): 0.0117\tClassification Loss: 1.0610\r\n",
      "Train Epoch: 32 [122240/209539 (58%)]\tAll Loss: 1.1334\tTriple Loss(1): 0.0160\tClassification Loss: 1.1013\r\n",
      "Train Epoch: 32 [122880/209539 (59%)]\tAll Loss: 1.5942\tTriple Loss(0): 0.2454\tClassification Loss: 1.1034\r\n",
      "Train Epoch: 32 [123520/209539 (59%)]\tAll Loss: 2.2328\tTriple Loss(0): 0.4056\tClassification Loss: 1.4216\r\n",
      "Train Epoch: 32 [124160/209539 (59%)]\tAll Loss: 1.6610\tTriple Loss(0): 0.2851\tClassification Loss: 1.0908\r\n",
      "Train Epoch: 32 [124800/209539 (60%)]\tAll Loss: 0.9941\tTriple Loss(1): 0.0070\tClassification Loss: 0.9801\r\n",
      "Train Epoch: 32 [125440/209539 (60%)]\tAll Loss: 1.1842\tTriple Loss(1): 0.0000\tClassification Loss: 1.1842\r\n",
      "Train Epoch: 32 [126080/209539 (60%)]\tAll Loss: 1.3444\tTriple Loss(1): 0.0565\tClassification Loss: 1.2313\r\n",
      "Train Epoch: 32 [126720/209539 (60%)]\tAll Loss: 1.3974\tTriple Loss(1): 0.0140\tClassification Loss: 1.3695\r\n",
      "Train Epoch: 32 [127360/209539 (61%)]\tAll Loss: 1.2514\tTriple Loss(1): 0.0000\tClassification Loss: 1.2514\r\n",
      "Train Epoch: 32 [128000/209539 (61%)]\tAll Loss: 1.5194\tTriple Loss(1): 0.0030\tClassification Loss: 1.5133\r\n",
      "Train Epoch: 32 [128640/209539 (61%)]\tAll Loss: 1.5488\tTriple Loss(0): 0.3052\tClassification Loss: 0.9384\r\n",
      "Train Epoch: 32 [129280/209539 (62%)]\tAll Loss: 1.4113\tTriple Loss(1): 0.0793\tClassification Loss: 1.2527\r\n",
      "Train Epoch: 32 [129920/209539 (62%)]\tAll Loss: 1.2391\tTriple Loss(1): 0.0451\tClassification Loss: 1.1490\r\n",
      "Train Epoch: 32 [130560/209539 (62%)]\tAll Loss: 0.9977\tTriple Loss(1): 0.0000\tClassification Loss: 0.9977\r\n",
      "Train Epoch: 32 [131200/209539 (63%)]\tAll Loss: 0.9966\tTriple Loss(1): 0.0245\tClassification Loss: 0.9475\r\n",
      "Train Epoch: 32 [131840/209539 (63%)]\tAll Loss: 0.9826\tTriple Loss(1): 0.0008\tClassification Loss: 0.9810\r\n",
      "Train Epoch: 32 [132480/209539 (63%)]\tAll Loss: 1.1515\tTriple Loss(1): 0.0000\tClassification Loss: 1.1515\r\n",
      "Train Epoch: 32 [133120/209539 (64%)]\tAll Loss: 1.3049\tTriple Loss(1): 0.0579\tClassification Loss: 1.1891\r\n",
      "Train Epoch: 32 [133760/209539 (64%)]\tAll Loss: 1.4233\tTriple Loss(1): 0.0072\tClassification Loss: 1.4089\r\n",
      "Train Epoch: 32 [134400/209539 (64%)]\tAll Loss: 1.4706\tTriple Loss(0): 0.2540\tClassification Loss: 0.9626\r\n",
      "Train Epoch: 32 [135040/209539 (64%)]\tAll Loss: 1.5181\tTriple Loss(0): 0.2110\tClassification Loss: 1.0961\r\n",
      "Train Epoch: 32 [135680/209539 (65%)]\tAll Loss: 1.2756\tTriple Loss(1): 0.0000\tClassification Loss: 1.2756\r\n",
      "Train Epoch: 32 [136320/209539 (65%)]\tAll Loss: 1.3455\tTriple Loss(1): 0.0571\tClassification Loss: 1.2312\r\n",
      "Train Epoch: 32 [136960/209539 (65%)]\tAll Loss: 1.7227\tTriple Loss(0): 0.2823\tClassification Loss: 1.1582\r\n",
      "Train Epoch: 32 [137600/209539 (66%)]\tAll Loss: 1.2147\tTriple Loss(1): 0.0421\tClassification Loss: 1.1304\r\n",
      "Train Epoch: 32 [138240/209539 (66%)]\tAll Loss: 1.4814\tTriple Loss(1): 0.0063\tClassification Loss: 1.4687\r\n",
      "Train Epoch: 32 [138880/209539 (66%)]\tAll Loss: 1.2174\tTriple Loss(1): 0.0131\tClassification Loss: 1.1912\r\n",
      "Train Epoch: 32 [139520/209539 (67%)]\tAll Loss: 0.9724\tTriple Loss(1): 0.0017\tClassification Loss: 0.9690\r\n",
      "Train Epoch: 32 [140160/209539 (67%)]\tAll Loss: 2.4191\tTriple Loss(0): 0.6290\tClassification Loss: 1.1612\r\n",
      "Train Epoch: 32 [140800/209539 (67%)]\tAll Loss: 1.4219\tTriple Loss(1): 0.0370\tClassification Loss: 1.3479\r\n",
      "Train Epoch: 32 [141440/209539 (68%)]\tAll Loss: 1.9653\tTriple Loss(0): 0.3851\tClassification Loss: 1.1951\r\n",
      "Train Epoch: 32 [142080/209539 (68%)]\tAll Loss: 1.7138\tTriple Loss(0): 0.2938\tClassification Loss: 1.1263\r\n",
      "Train Epoch: 32 [142720/209539 (68%)]\tAll Loss: 1.9547\tTriple Loss(1): 0.0068\tClassification Loss: 1.9411\r\n",
      "Train Epoch: 32 [143360/209539 (68%)]\tAll Loss: 1.0794\tTriple Loss(1): 0.0143\tClassification Loss: 1.0507\r\n",
      "Train Epoch: 32 [144000/209539 (69%)]\tAll Loss: 1.2761\tTriple Loss(1): 0.0325\tClassification Loss: 1.2112\r\n",
      "Train Epoch: 32 [144640/209539 (69%)]\tAll Loss: 1.0321\tTriple Loss(1): 0.0000\tClassification Loss: 1.0321\r\n",
      "Train Epoch: 32 [145280/209539 (69%)]\tAll Loss: 1.9592\tTriple Loss(0): 0.3120\tClassification Loss: 1.3352\r\n",
      "Train Epoch: 32 [145920/209539 (70%)]\tAll Loss: 1.0690\tTriple Loss(1): 0.0000\tClassification Loss: 1.0690\r\n",
      "Train Epoch: 32 [146560/209539 (70%)]\tAll Loss: 0.9467\tTriple Loss(1): 0.0025\tClassification Loss: 0.9418\r\n",
      "Train Epoch: 32 [147200/209539 (70%)]\tAll Loss: 1.2963\tTriple Loss(1): 0.0025\tClassification Loss: 1.2913\r\n",
      "Train Epoch: 32 [147840/209539 (71%)]\tAll Loss: 1.0485\tTriple Loss(1): 0.0523\tClassification Loss: 0.9440\r\n",
      "Train Epoch: 32 [148480/209539 (71%)]\tAll Loss: 1.7112\tTriple Loss(0): 0.3290\tClassification Loss: 1.0532\r\n",
      "Train Epoch: 32 [149120/209539 (71%)]\tAll Loss: 1.3122\tTriple Loss(1): 0.0370\tClassification Loss: 1.2382\r\n",
      "Train Epoch: 32 [149760/209539 (71%)]\tAll Loss: 1.0413\tTriple Loss(1): 0.0171\tClassification Loss: 1.0071\r\n",
      "Train Epoch: 32 [150400/209539 (72%)]\tAll Loss: 1.3720\tTriple Loss(1): 0.0362\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 32 [151040/209539 (72%)]\tAll Loss: 1.1208\tTriple Loss(1): 0.0070\tClassification Loss: 1.1069\r\n",
      "Train Epoch: 32 [151680/209539 (72%)]\tAll Loss: 1.5549\tTriple Loss(1): 0.1051\tClassification Loss: 1.3447\r\n",
      "Train Epoch: 32 [152320/209539 (73%)]\tAll Loss: 1.1970\tTriple Loss(1): 0.0281\tClassification Loss: 1.1408\r\n",
      "Train Epoch: 32 [152960/209539 (73%)]\tAll Loss: 1.1696\tTriple Loss(1): 0.0000\tClassification Loss: 1.1696\r\n",
      "Train Epoch: 32 [153600/209539 (73%)]\tAll Loss: 1.2625\tTriple Loss(1): 0.0361\tClassification Loss: 1.1902\r\n",
      "Train Epoch: 32 [154240/209539 (74%)]\tAll Loss: 1.6833\tTriple Loss(0): 0.2771\tClassification Loss: 1.1291\r\n",
      "Train Epoch: 32 [154880/209539 (74%)]\tAll Loss: 1.3879\tTriple Loss(1): 0.0334\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 32 [155520/209539 (74%)]\tAll Loss: 1.2672\tTriple Loss(1): 0.0621\tClassification Loss: 1.1430\r\n",
      "Train Epoch: 32 [156160/209539 (75%)]\tAll Loss: 1.1775\tTriple Loss(1): 0.0190\tClassification Loss: 1.1396\r\n",
      "Train Epoch: 32 [156800/209539 (75%)]\tAll Loss: 1.9971\tTriple Loss(0): 0.2328\tClassification Loss: 1.5315\r\n",
      "Train Epoch: 32 [157440/209539 (75%)]\tAll Loss: 1.3551\tTriple Loss(1): 0.0000\tClassification Loss: 1.3551\r\n",
      "Train Epoch: 32 [158080/209539 (75%)]\tAll Loss: 1.2977\tTriple Loss(1): 0.0000\tClassification Loss: 1.2977\r\n",
      "Train Epoch: 32 [158720/209539 (76%)]\tAll Loss: 1.0228\tTriple Loss(1): 0.0458\tClassification Loss: 0.9313\r\n",
      "Train Epoch: 32 [159360/209539 (76%)]\tAll Loss: 1.0593\tTriple Loss(1): 0.0000\tClassification Loss: 1.0593\r\n",
      "Train Epoch: 32 [160000/209539 (76%)]\tAll Loss: 1.3940\tTriple Loss(1): 0.0065\tClassification Loss: 1.3809\r\n",
      "Train Epoch: 32 [160640/209539 (77%)]\tAll Loss: 1.9775\tTriple Loss(0): 0.4211\tClassification Loss: 1.1353\r\n",
      "Train Epoch: 32 [161280/209539 (77%)]\tAll Loss: 1.1706\tTriple Loss(1): 0.0320\tClassification Loss: 1.1067\r\n",
      "Train Epoch: 32 [161920/209539 (77%)]\tAll Loss: 1.9926\tTriple Loss(0): 0.3836\tClassification Loss: 1.2254\r\n",
      "Train Epoch: 32 [162560/209539 (78%)]\tAll Loss: 1.1005\tTriple Loss(1): 0.0101\tClassification Loss: 1.0803\r\n",
      "Train Epoch: 32 [163200/209539 (78%)]\tAll Loss: 0.9399\tTriple Loss(1): 0.0068\tClassification Loss: 0.9263\r\n",
      "Train Epoch: 32 [163840/209539 (78%)]\tAll Loss: 1.6602\tTriple Loss(0): 0.2566\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 32 [164480/209539 (78%)]\tAll Loss: 1.1635\tTriple Loss(1): 0.0189\tClassification Loss: 1.1256\r\n",
      "Train Epoch: 32 [165120/209539 (79%)]\tAll Loss: 1.0520\tTriple Loss(1): 0.0172\tClassification Loss: 1.0175\r\n",
      "Train Epoch: 32 [165760/209539 (79%)]\tAll Loss: 1.2862\tTriple Loss(1): 0.0207\tClassification Loss: 1.2449\r\n",
      "Train Epoch: 32 [166400/209539 (79%)]\tAll Loss: 1.9646\tTriple Loss(0): 0.3547\tClassification Loss: 1.2552\r\n",
      "Train Epoch: 32 [167040/209539 (80%)]\tAll Loss: 1.5466\tTriple Loss(1): 0.0662\tClassification Loss: 1.4141\r\n",
      "Train Epoch: 32 [167680/209539 (80%)]\tAll Loss: 1.4214\tTriple Loss(1): 0.0202\tClassification Loss: 1.3811\r\n",
      "Train Epoch: 32 [168320/209539 (80%)]\tAll Loss: 1.1919\tTriple Loss(1): 0.0658\tClassification Loss: 1.0602\r\n",
      "Train Epoch: 32 [168960/209539 (81%)]\tAll Loss: 1.6716\tTriple Loss(0): 0.3202\tClassification Loss: 1.0312\r\n",
      "Train Epoch: 32 [169600/209539 (81%)]\tAll Loss: 1.2576\tTriple Loss(1): 0.0000\tClassification Loss: 1.2576\r\n",
      "Train Epoch: 32 [170240/209539 (81%)]\tAll Loss: 1.2348\tTriple Loss(1): 0.0000\tClassification Loss: 1.2348\r\n",
      "Train Epoch: 32 [170880/209539 (82%)]\tAll Loss: 1.4404\tTriple Loss(1): 0.0369\tClassification Loss: 1.3665\r\n",
      "Train Epoch: 32 [171520/209539 (82%)]\tAll Loss: 1.0963\tTriple Loss(1): 0.0185\tClassification Loss: 1.0592\r\n",
      "Train Epoch: 32 [172160/209539 (82%)]\tAll Loss: 1.2753\tTriple Loss(1): 0.0075\tClassification Loss: 1.2603\r\n",
      "Train Epoch: 32 [172800/209539 (82%)]\tAll Loss: 1.1967\tTriple Loss(1): 0.0000\tClassification Loss: 1.1967\r\n",
      "Train Epoch: 32 [173440/209539 (83%)]\tAll Loss: 1.3823\tTriple Loss(1): 0.0238\tClassification Loss: 1.3346\r\n",
      "Train Epoch: 32 [174080/209539 (83%)]\tAll Loss: 0.9797\tTriple Loss(1): 0.0100\tClassification Loss: 0.9598\r\n",
      "Train Epoch: 32 [174720/209539 (83%)]\tAll Loss: 1.1662\tTriple Loss(1): 0.0127\tClassification Loss: 1.1407\r\n",
      "Train Epoch: 32 [175360/209539 (84%)]\tAll Loss: 1.6073\tTriple Loss(1): 0.0574\tClassification Loss: 1.4924\r\n",
      "Train Epoch: 32 [176000/209539 (84%)]\tAll Loss: 1.3051\tTriple Loss(1): 0.0123\tClassification Loss: 1.2805\r\n",
      "Train Epoch: 32 [176640/209539 (84%)]\tAll Loss: 0.9107\tTriple Loss(1): 0.0393\tClassification Loss: 0.8321\r\n",
      "Train Epoch: 32 [177280/209539 (85%)]\tAll Loss: 1.5810\tTriple Loss(1): 0.0452\tClassification Loss: 1.4905\r\n",
      "Train Epoch: 32 [177920/209539 (85%)]\tAll Loss: 1.9610\tTriple Loss(0): 0.2442\tClassification Loss: 1.4726\r\n",
      "Train Epoch: 32 [178560/209539 (85%)]\tAll Loss: 1.1934\tTriple Loss(1): 0.0368\tClassification Loss: 1.1198\r\n",
      "Train Epoch: 32 [179200/209539 (86%)]\tAll Loss: 1.2208\tTriple Loss(1): 0.0084\tClassification Loss: 1.2040\r\n",
      "Train Epoch: 32 [179840/209539 (86%)]\tAll Loss: 1.3143\tTriple Loss(1): 0.0500\tClassification Loss: 1.2143\r\n",
      "Train Epoch: 32 [180480/209539 (86%)]\tAll Loss: 1.3090\tTriple Loss(1): 0.0270\tClassification Loss: 1.2550\r\n",
      "Train Epoch: 32 [181120/209539 (86%)]\tAll Loss: 1.6357\tTriple Loss(1): 0.0425\tClassification Loss: 1.5507\r\n",
      "Train Epoch: 32 [181760/209539 (87%)]\tAll Loss: 1.1013\tTriple Loss(1): 0.0281\tClassification Loss: 1.0452\r\n",
      "Train Epoch: 32 [182400/209539 (87%)]\tAll Loss: 1.5873\tTriple Loss(1): 0.0064\tClassification Loss: 1.5745\r\n",
      "Train Epoch: 32 [183040/209539 (87%)]\tAll Loss: 1.4251\tTriple Loss(1): 0.0019\tClassification Loss: 1.4213\r\n",
      "Train Epoch: 32 [183680/209539 (88%)]\tAll Loss: 1.1235\tTriple Loss(1): 0.0172\tClassification Loss: 1.0891\r\n",
      "Train Epoch: 32 [184320/209539 (88%)]\tAll Loss: 1.2726\tTriple Loss(1): 0.0617\tClassification Loss: 1.1492\r\n",
      "Train Epoch: 32 [184960/209539 (88%)]\tAll Loss: 1.1780\tTriple Loss(1): 0.0212\tClassification Loss: 1.1355\r\n",
      "Train Epoch: 32 [185600/209539 (89%)]\tAll Loss: 1.5132\tTriple Loss(1): 0.0624\tClassification Loss: 1.3884\r\n",
      "Train Epoch: 32 [186240/209539 (89%)]\tAll Loss: 1.3922\tTriple Loss(1): 0.0342\tClassification Loss: 1.3239\r\n",
      "Train Epoch: 32 [186880/209539 (89%)]\tAll Loss: 1.0847\tTriple Loss(1): 0.0302\tClassification Loss: 1.0244\r\n",
      "Train Epoch: 32 [187520/209539 (89%)]\tAll Loss: 1.3067\tTriple Loss(1): 0.0162\tClassification Loss: 1.2743\r\n",
      "Train Epoch: 32 [188160/209539 (90%)]\tAll Loss: 1.1875\tTriple Loss(1): 0.0636\tClassification Loss: 1.0604\r\n",
      "Train Epoch: 32 [188800/209539 (90%)]\tAll Loss: 1.2829\tTriple Loss(1): 0.0342\tClassification Loss: 1.2145\r\n",
      "Train Epoch: 32 [189440/209539 (90%)]\tAll Loss: 1.1529\tTriple Loss(1): 0.0000\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 32 [190080/209539 (91%)]\tAll Loss: 1.1802\tTriple Loss(1): 0.0126\tClassification Loss: 1.1550\r\n",
      "Train Epoch: 32 [190720/209539 (91%)]\tAll Loss: 1.1897\tTriple Loss(1): 0.0000\tClassification Loss: 1.1897\r\n",
      "Train Epoch: 32 [191360/209539 (91%)]\tAll Loss: 0.9822\tTriple Loss(1): 0.0049\tClassification Loss: 0.9724\r\n",
      "Train Epoch: 32 [192000/209539 (92%)]\tAll Loss: 1.3711\tTriple Loss(1): 0.0039\tClassification Loss: 1.3633\r\n",
      "Train Epoch: 32 [192640/209539 (92%)]\tAll Loss: 1.2434\tTriple Loss(1): 0.0450\tClassification Loss: 1.1535\r\n",
      "Train Epoch: 32 [193280/209539 (92%)]\tAll Loss: 1.3040\tTriple Loss(1): 0.0000\tClassification Loss: 1.3040\r\n",
      "Train Epoch: 32 [193920/209539 (93%)]\tAll Loss: 1.1047\tTriple Loss(1): 0.0202\tClassification Loss: 1.0644\r\n",
      "Train Epoch: 32 [194560/209539 (93%)]\tAll Loss: 1.1448\tTriple Loss(1): 0.0000\tClassification Loss: 1.1448\r\n",
      "Train Epoch: 32 [195200/209539 (93%)]\tAll Loss: 1.2911\tTriple Loss(1): 0.0226\tClassification Loss: 1.2460\r\n",
      "Train Epoch: 32 [195840/209539 (93%)]\tAll Loss: 0.7747\tTriple Loss(1): 0.0174\tClassification Loss: 0.7398\r\n",
      "Train Epoch: 32 [196480/209539 (94%)]\tAll Loss: 1.4628\tTriple Loss(1): 0.0870\tClassification Loss: 1.2887\r\n",
      "Train Epoch: 32 [197120/209539 (94%)]\tAll Loss: 1.2353\tTriple Loss(1): 0.0630\tClassification Loss: 1.1092\r\n",
      "Train Epoch: 32 [197760/209539 (94%)]\tAll Loss: 1.4110\tTriple Loss(0): 0.1271\tClassification Loss: 1.1569\r\n",
      "Train Epoch: 32 [198400/209539 (95%)]\tAll Loss: 1.3141\tTriple Loss(1): 0.0000\tClassification Loss: 1.3141\r\n",
      "Train Epoch: 32 [199040/209539 (95%)]\tAll Loss: 1.4142\tTriple Loss(1): 0.0000\tClassification Loss: 1.4142\r\n",
      "Train Epoch: 32 [199680/209539 (95%)]\tAll Loss: 1.1000\tTriple Loss(1): 0.0063\tClassification Loss: 1.0874\r\n",
      "Train Epoch: 32 [200320/209539 (96%)]\tAll Loss: 1.0767\tTriple Loss(1): 0.0091\tClassification Loss: 1.0586\r\n",
      "Train Epoch: 32 [200960/209539 (96%)]\tAll Loss: 1.2012\tTriple Loss(1): 0.0709\tClassification Loss: 1.0594\r\n",
      "Train Epoch: 32 [201600/209539 (96%)]\tAll Loss: 1.0969\tTriple Loss(1): 0.0000\tClassification Loss: 1.0969\r\n",
      "Train Epoch: 32 [202240/209539 (97%)]\tAll Loss: 1.1125\tTriple Loss(1): 0.0320\tClassification Loss: 1.0484\r\n",
      "Train Epoch: 32 [202880/209539 (97%)]\tAll Loss: 0.9910\tTriple Loss(1): 0.0013\tClassification Loss: 0.9885\r\n",
      "Train Epoch: 32 [203520/209539 (97%)]\tAll Loss: 1.2923\tTriple Loss(1): 0.0400\tClassification Loss: 1.2123\r\n",
      "Train Epoch: 32 [204160/209539 (97%)]\tAll Loss: 1.7171\tTriple Loss(1): 0.0617\tClassification Loss: 1.5937\r\n",
      "Train Epoch: 32 [204800/209539 (98%)]\tAll Loss: 1.4276\tTriple Loss(1): 0.0054\tClassification Loss: 1.4168\r\n",
      "Train Epoch: 32 [205440/209539 (98%)]\tAll Loss: 1.2428\tTriple Loss(1): 0.0653\tClassification Loss: 1.1121\r\n",
      "Train Epoch: 32 [206080/209539 (98%)]\tAll Loss: 1.2113\tTriple Loss(1): 0.0297\tClassification Loss: 1.1518\r\n",
      "Train Epoch: 32 [206720/209539 (99%)]\tAll Loss: 1.1499\tTriple Loss(1): 0.0153\tClassification Loss: 1.1193\r\n",
      "Train Epoch: 32 [207360/209539 (99%)]\tAll Loss: 1.1591\tTriple Loss(1): 0.0220\tClassification Loss: 1.1152\r\n",
      "Train Epoch: 32 [208000/209539 (99%)]\tAll Loss: 1.1076\tTriple Loss(1): 0.0144\tClassification Loss: 1.0787\r\n",
      "Train Epoch: 32 [208640/209539 (100%)]\tAll Loss: 1.3192\tTriple Loss(1): 0.0689\tClassification Loss: 1.1813\r\n",
      "Train Epoch: 32 [209280/209539 (100%)]\tAll Loss: 1.3103\tTriple Loss(1): 0.0378\tClassification Loss: 1.2347\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/32_epochs\r\n",
      "Train Epoch: 33 [0/209539 (0%)]\tAll Loss: 1.7336\tTriple Loss(1): 0.1682\tClassification Loss: 1.3972\r\n",
      "\r\n",
      "Test set: Average loss: 1.0845\r\n",
      "Top 1 Accuracy: 54817/80128 (68%)\r\n",
      "Top 3 Accuracy: 70129/80128 (88%)\r\n",
      "Top 5 Accuracy: 74621/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 33 [640/209539 (0%)]\tAll Loss: 1.2827\tTriple Loss(1): 0.0110\tClassification Loss: 1.2608\r\n",
      "Train Epoch: 33 [1280/209539 (1%)]\tAll Loss: 1.1944\tTriple Loss(1): 0.0455\tClassification Loss: 1.1033\r\n",
      "Train Epoch: 33 [1920/209539 (1%)]\tAll Loss: 1.3427\tTriple Loss(1): 0.0563\tClassification Loss: 1.2302\r\n",
      "Train Epoch: 33 [2560/209539 (1%)]\tAll Loss: 1.9746\tTriple Loss(0): 0.2518\tClassification Loss: 1.4709\r\n",
      "Train Epoch: 33 [3200/209539 (2%)]\tAll Loss: 1.2671\tTriple Loss(1): 0.0023\tClassification Loss: 1.2624\r\n",
      "Train Epoch: 33 [3840/209539 (2%)]\tAll Loss: 1.1130\tTriple Loss(1): 0.0000\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 33 [4480/209539 (2%)]\tAll Loss: 1.2099\tTriple Loss(1): 0.0093\tClassification Loss: 1.1914\r\n",
      "Train Epoch: 33 [5120/209539 (2%)]\tAll Loss: 1.5813\tTriple Loss(0): 0.1896\tClassification Loss: 1.2021\r\n",
      "Train Epoch: 33 [5760/209539 (3%)]\tAll Loss: 1.3049\tTriple Loss(1): 0.0333\tClassification Loss: 1.2382\r\n",
      "Train Epoch: 33 [6400/209539 (3%)]\tAll Loss: 0.9194\tTriple Loss(1): 0.0162\tClassification Loss: 0.8870\r\n",
      "Train Epoch: 33 [7040/209539 (3%)]\tAll Loss: 1.1051\tTriple Loss(1): 0.0216\tClassification Loss: 1.0618\r\n",
      "Train Epoch: 33 [7680/209539 (4%)]\tAll Loss: 0.9082\tTriple Loss(1): 0.0000\tClassification Loss: 0.9082\r\n",
      "Train Epoch: 33 [8320/209539 (4%)]\tAll Loss: 1.4433\tTriple Loss(0): 0.1597\tClassification Loss: 1.1239\r\n",
      "Train Epoch: 33 [8960/209539 (4%)]\tAll Loss: 1.6224\tTriple Loss(0): 0.3100\tClassification Loss: 1.0024\r\n",
      "Train Epoch: 33 [9600/209539 (5%)]\tAll Loss: 1.6796\tTriple Loss(1): 0.1014\tClassification Loss: 1.4769\r\n",
      "Train Epoch: 33 [10240/209539 (5%)]\tAll Loss: 0.9672\tTriple Loss(1): 0.0118\tClassification Loss: 0.9436\r\n",
      "Train Epoch: 33 [10880/209539 (5%)]\tAll Loss: 1.1914\tTriple Loss(1): 0.0278\tClassification Loss: 1.1358\r\n",
      "Train Epoch: 33 [11520/209539 (5%)]\tAll Loss: 1.3896\tTriple Loss(1): 0.0000\tClassification Loss: 1.3896\r\n",
      "Train Epoch: 33 [12160/209539 (6%)]\tAll Loss: 0.9865\tTriple Loss(1): 0.0049\tClassification Loss: 0.9766\r\n",
      "Train Epoch: 33 [12800/209539 (6%)]\tAll Loss: 0.9383\tTriple Loss(1): 0.0451\tClassification Loss: 0.8481\r\n",
      "Train Epoch: 33 [13440/209539 (6%)]\tAll Loss: 1.1541\tTriple Loss(1): 0.0321\tClassification Loss: 1.0899\r\n",
      "Train Epoch: 33 [14080/209539 (7%)]\tAll Loss: 1.3887\tTriple Loss(1): 0.0062\tClassification Loss: 1.3764\r\n",
      "Train Epoch: 33 [14720/209539 (7%)]\tAll Loss: 1.2626\tTriple Loss(1): 0.0000\tClassification Loss: 1.2626\r\n",
      "Train Epoch: 33 [15360/209539 (7%)]\tAll Loss: 1.0632\tTriple Loss(1): 0.0602\tClassification Loss: 0.9428\r\n",
      "Train Epoch: 33 [16000/209539 (8%)]\tAll Loss: 1.4463\tTriple Loss(1): 0.0000\tClassification Loss: 1.4463\r\n",
      "Train Epoch: 33 [16640/209539 (8%)]\tAll Loss: 1.7021\tTriple Loss(1): 0.0280\tClassification Loss: 1.6460\r\n",
      "Train Epoch: 33 [17280/209539 (8%)]\tAll Loss: 1.2206\tTriple Loss(1): 0.0284\tClassification Loss: 1.1639\r\n",
      "Train Epoch: 33 [17920/209539 (9%)]\tAll Loss: 1.9076\tTriple Loss(0): 0.3681\tClassification Loss: 1.1714\r\n",
      "Train Epoch: 33 [18560/209539 (9%)]\tAll Loss: 1.0987\tTriple Loss(1): 0.0296\tClassification Loss: 1.0394\r\n",
      "Train Epoch: 33 [19200/209539 (9%)]\tAll Loss: 1.2124\tTriple Loss(1): 0.0016\tClassification Loss: 1.2091\r\n",
      "Train Epoch: 33 [19840/209539 (9%)]\tAll Loss: 1.1785\tTriple Loss(1): 0.0022\tClassification Loss: 1.1740\r\n",
      "Train Epoch: 33 [20480/209539 (10%)]\tAll Loss: 1.1534\tTriple Loss(1): 0.0265\tClassification Loss: 1.1003\r\n",
      "Train Epoch: 33 [21120/209539 (10%)]\tAll Loss: 1.3428\tTriple Loss(1): 0.0424\tClassification Loss: 1.2580\r\n",
      "Train Epoch: 33 [21760/209539 (10%)]\tAll Loss: 1.6628\tTriple Loss(0): 0.2965\tClassification Loss: 1.0697\r\n",
      "Train Epoch: 33 [22400/209539 (11%)]\tAll Loss: 1.2805\tTriple Loss(0): 0.1410\tClassification Loss: 0.9985\r\n",
      "Train Epoch: 33 [23040/209539 (11%)]\tAll Loss: 1.2525\tTriple Loss(1): 0.0000\tClassification Loss: 1.2525\r\n",
      "Train Epoch: 33 [23680/209539 (11%)]\tAll Loss: 0.9916\tTriple Loss(1): 0.0365\tClassification Loss: 0.9185\r\n",
      "Train Epoch: 33 [24320/209539 (12%)]\tAll Loss: 1.2021\tTriple Loss(1): 0.0115\tClassification Loss: 1.1792\r\n",
      "Train Epoch: 33 [24960/209539 (12%)]\tAll Loss: 1.4627\tTriple Loss(0): 0.1753\tClassification Loss: 1.1121\r\n",
      "Train Epoch: 33 [25600/209539 (12%)]\tAll Loss: 1.1478\tTriple Loss(1): 0.0000\tClassification Loss: 1.1478\r\n",
      "Train Epoch: 33 [26240/209539 (13%)]\tAll Loss: 1.2302\tTriple Loss(1): 0.0452\tClassification Loss: 1.1397\r\n",
      "Train Epoch: 33 [26880/209539 (13%)]\tAll Loss: 1.7805\tTriple Loss(0): 0.2745\tClassification Loss: 1.2315\r\n",
      "Train Epoch: 33 [27520/209539 (13%)]\tAll Loss: 1.2383\tTriple Loss(1): 0.0207\tClassification Loss: 1.1968\r\n",
      "Train Epoch: 33 [28160/209539 (13%)]\tAll Loss: 1.1531\tTriple Loss(1): 0.0108\tClassification Loss: 1.1315\r\n",
      "Train Epoch: 33 [28800/209539 (14%)]\tAll Loss: 1.3562\tTriple Loss(1): 0.0181\tClassification Loss: 1.3199\r\n",
      "Train Epoch: 33 [29440/209539 (14%)]\tAll Loss: 1.4436\tTriple Loss(1): 0.0226\tClassification Loss: 1.3983\r\n",
      "Train Epoch: 33 [30080/209539 (14%)]\tAll Loss: 1.0861\tTriple Loss(1): 0.0104\tClassification Loss: 1.0652\r\n",
      "Train Epoch: 33 [30720/209539 (15%)]\tAll Loss: 1.2257\tTriple Loss(1): 0.0661\tClassification Loss: 1.0934\r\n",
      "Train Epoch: 33 [31360/209539 (15%)]\tAll Loss: 1.0656\tTriple Loss(1): 0.0291\tClassification Loss: 1.0075\r\n",
      "Train Epoch: 33 [32000/209539 (15%)]\tAll Loss: 1.3599\tTriple Loss(1): 0.0131\tClassification Loss: 1.3337\r\n",
      "Train Epoch: 33 [32640/209539 (16%)]\tAll Loss: 1.2844\tTriple Loss(1): 0.0000\tClassification Loss: 1.2844\r\n",
      "Train Epoch: 33 [33280/209539 (16%)]\tAll Loss: 1.6326\tTriple Loss(0): 0.2474\tClassification Loss: 1.1378\r\n",
      "Train Epoch: 33 [33920/209539 (16%)]\tAll Loss: 1.1321\tTriple Loss(1): 0.0459\tClassification Loss: 1.0403\r\n",
      "Train Epoch: 33 [34560/209539 (16%)]\tAll Loss: 1.0325\tTriple Loss(1): 0.0149\tClassification Loss: 1.0027\r\n",
      "Train Epoch: 33 [35200/209539 (17%)]\tAll Loss: 1.0124\tTriple Loss(1): 0.0000\tClassification Loss: 1.0124\r\n",
      "Train Epoch: 33 [35840/209539 (17%)]\tAll Loss: 0.9942\tTriple Loss(1): 0.0000\tClassification Loss: 0.9942\r\n",
      "Train Epoch: 33 [36480/209539 (17%)]\tAll Loss: 0.9792\tTriple Loss(1): 0.0282\tClassification Loss: 0.9228\r\n",
      "Train Epoch: 33 [37120/209539 (18%)]\tAll Loss: 1.7045\tTriple Loss(1): 0.0685\tClassification Loss: 1.5676\r\n",
      "Train Epoch: 33 [37760/209539 (18%)]\tAll Loss: 1.1233\tTriple Loss(1): 0.0104\tClassification Loss: 1.1025\r\n",
      "Train Epoch: 33 [38400/209539 (18%)]\tAll Loss: 1.8779\tTriple Loss(0): 0.2690\tClassification Loss: 1.3399\r\n",
      "Train Epoch: 33 [39040/209539 (19%)]\tAll Loss: 0.8179\tTriple Loss(1): 0.0000\tClassification Loss: 0.8179\r\n",
      "Train Epoch: 33 [39680/209539 (19%)]\tAll Loss: 1.6716\tTriple Loss(0): 0.2894\tClassification Loss: 1.0928\r\n",
      "Train Epoch: 33 [40320/209539 (19%)]\tAll Loss: 1.2238\tTriple Loss(1): 0.0277\tClassification Loss: 1.1684\r\n",
      "Train Epoch: 33 [40960/209539 (20%)]\tAll Loss: 1.0735\tTriple Loss(1): 0.0462\tClassification Loss: 0.9810\r\n",
      "Train Epoch: 33 [41600/209539 (20%)]\tAll Loss: 1.3286\tTriple Loss(1): 0.0327\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 33 [42240/209539 (20%)]\tAll Loss: 1.1720\tTriple Loss(1): 0.0206\tClassification Loss: 1.1309\r\n",
      "Train Epoch: 33 [42880/209539 (20%)]\tAll Loss: 1.3737\tTriple Loss(0): 0.2604\tClassification Loss: 0.8529\r\n",
      "Train Epoch: 33 [43520/209539 (21%)]\tAll Loss: 1.3855\tTriple Loss(1): 0.0190\tClassification Loss: 1.3475\r\n",
      "Train Epoch: 33 [44160/209539 (21%)]\tAll Loss: 1.3701\tTriple Loss(1): 0.0045\tClassification Loss: 1.3612\r\n",
      "Train Epoch: 33 [44800/209539 (21%)]\tAll Loss: 1.9190\tTriple Loss(0): 0.3123\tClassification Loss: 1.2943\r\n",
      "Train Epoch: 33 [45440/209539 (22%)]\tAll Loss: 1.6605\tTriple Loss(1): 0.0063\tClassification Loss: 1.6479\r\n",
      "Train Epoch: 33 [46080/209539 (22%)]\tAll Loss: 1.5084\tTriple Loss(0): 0.2008\tClassification Loss: 1.1067\r\n",
      "Train Epoch: 33 [46720/209539 (22%)]\tAll Loss: 2.2785\tTriple Loss(0): 0.3673\tClassification Loss: 1.5439\r\n",
      "Train Epoch: 33 [47360/209539 (23%)]\tAll Loss: 0.9663\tTriple Loss(1): 0.0086\tClassification Loss: 0.9491\r\n",
      "Train Epoch: 33 [48000/209539 (23%)]\tAll Loss: 1.7268\tTriple Loss(0): 0.3075\tClassification Loss: 1.1117\r\n",
      "Train Epoch: 33 [48640/209539 (23%)]\tAll Loss: 1.3674\tTriple Loss(1): 0.0000\tClassification Loss: 1.3674\r\n",
      "Train Epoch: 33 [49280/209539 (24%)]\tAll Loss: 1.3863\tTriple Loss(1): 0.0672\tClassification Loss: 1.2519\r\n",
      "Train Epoch: 33 [49920/209539 (24%)]\tAll Loss: 1.2723\tTriple Loss(1): 0.0014\tClassification Loss: 1.2694\r\n",
      "Train Epoch: 33 [50560/209539 (24%)]\tAll Loss: 1.9250\tTriple Loss(0): 0.2577\tClassification Loss: 1.4096\r\n",
      "Train Epoch: 33 [51200/209539 (24%)]\tAll Loss: 1.3674\tTriple Loss(1): 0.0065\tClassification Loss: 1.3544\r\n",
      "Train Epoch: 33 [51840/209539 (25%)]\tAll Loss: 1.1666\tTriple Loss(1): 0.0030\tClassification Loss: 1.1605\r\n",
      "Train Epoch: 33 [52480/209539 (25%)]\tAll Loss: 1.1511\tTriple Loss(1): 0.0635\tClassification Loss: 1.0242\r\n",
      "Train Epoch: 33 [53120/209539 (25%)]\tAll Loss: 1.0358\tTriple Loss(1): 0.0191\tClassification Loss: 0.9976\r\n",
      "Train Epoch: 33 [53760/209539 (26%)]\tAll Loss: 1.4083\tTriple Loss(1): 0.0221\tClassification Loss: 1.3642\r\n",
      "Train Epoch: 33 [54400/209539 (26%)]\tAll Loss: 1.6751\tTriple Loss(1): 0.0307\tClassification Loss: 1.6136\r\n",
      "Train Epoch: 33 [55040/209539 (26%)]\tAll Loss: 1.0839\tTriple Loss(1): 0.0151\tClassification Loss: 1.0538\r\n",
      "Train Epoch: 33 [55680/209539 (27%)]\tAll Loss: 1.5224\tTriple Loss(1): 0.0213\tClassification Loss: 1.4798\r\n",
      "Train Epoch: 33 [56320/209539 (27%)]\tAll Loss: 1.0723\tTriple Loss(1): 0.0260\tClassification Loss: 1.0202\r\n",
      "Train Epoch: 33 [56960/209539 (27%)]\tAll Loss: 1.5590\tTriple Loss(0): 0.2163\tClassification Loss: 1.1263\r\n",
      "Train Epoch: 33 [57600/209539 (27%)]\tAll Loss: 1.4487\tTriple Loss(0): 0.2252\tClassification Loss: 0.9983\r\n",
      "Train Epoch: 33 [58240/209539 (28%)]\tAll Loss: 1.2041\tTriple Loss(1): 0.1111\tClassification Loss: 0.9820\r\n",
      "Train Epoch: 33 [58880/209539 (28%)]\tAll Loss: 1.5303\tTriple Loss(1): 0.0456\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 33 [59520/209539 (28%)]\tAll Loss: 1.2213\tTriple Loss(1): 0.0505\tClassification Loss: 1.1203\r\n",
      "Train Epoch: 33 [60160/209539 (29%)]\tAll Loss: 1.5228\tTriple Loss(1): 0.0233\tClassification Loss: 1.4763\r\n",
      "Train Epoch: 33 [60800/209539 (29%)]\tAll Loss: 1.3846\tTriple Loss(1): 0.0109\tClassification Loss: 1.3627\r\n",
      "Train Epoch: 33 [61440/209539 (29%)]\tAll Loss: 1.3021\tTriple Loss(1): 0.1012\tClassification Loss: 1.0996\r\n",
      "Train Epoch: 33 [62080/209539 (30%)]\tAll Loss: 1.7019\tTriple Loss(0): 0.1735\tClassification Loss: 1.3548\r\n",
      "Train Epoch: 33 [62720/209539 (30%)]\tAll Loss: 1.2278\tTriple Loss(1): 0.0190\tClassification Loss: 1.1899\r\n",
      "Train Epoch: 33 [63360/209539 (30%)]\tAll Loss: 1.2960\tTriple Loss(1): 0.0568\tClassification Loss: 1.1823\r\n",
      "Train Epoch: 33 [64000/209539 (31%)]\tAll Loss: 1.4563\tTriple Loss(1): 0.0519\tClassification Loss: 1.3525\r\n",
      "Train Epoch: 33 [64640/209539 (31%)]\tAll Loss: 1.6257\tTriple Loss(1): 0.0767\tClassification Loss: 1.4723\r\n",
      "Train Epoch: 33 [65280/209539 (31%)]\tAll Loss: 1.1225\tTriple Loss(1): 0.0000\tClassification Loss: 1.1225\r\n",
      "Train Epoch: 33 [65920/209539 (31%)]\tAll Loss: 1.4706\tTriple Loss(1): 0.0146\tClassification Loss: 1.4413\r\n",
      "Train Epoch: 33 [66560/209539 (32%)]\tAll Loss: 1.1468\tTriple Loss(1): 0.0547\tClassification Loss: 1.0373\r\n",
      "Train Epoch: 33 [67200/209539 (32%)]\tAll Loss: 1.1700\tTriple Loss(1): 0.0153\tClassification Loss: 1.1394\r\n",
      "Train Epoch: 33 [67840/209539 (32%)]\tAll Loss: 1.0926\tTriple Loss(1): 0.0000\tClassification Loss: 1.0926\r\n",
      "Train Epoch: 33 [68480/209539 (33%)]\tAll Loss: 2.0003\tTriple Loss(0): 0.3375\tClassification Loss: 1.3254\r\n",
      "Train Epoch: 33 [69120/209539 (33%)]\tAll Loss: 1.8887\tTriple Loss(0): 0.2823\tClassification Loss: 1.3240\r\n",
      "Train Epoch: 33 [69760/209539 (33%)]\tAll Loss: 1.5470\tTriple Loss(0): 0.3021\tClassification Loss: 0.9428\r\n",
      "Train Epoch: 33 [70400/209539 (34%)]\tAll Loss: 1.8704\tTriple Loss(0): 0.4346\tClassification Loss: 1.0012\r\n",
      "Train Epoch: 33 [71040/209539 (34%)]\tAll Loss: 1.4427\tTriple Loss(1): 0.0413\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 33 [71680/209539 (34%)]\tAll Loss: 1.6181\tTriple Loss(1): 0.0202\tClassification Loss: 1.5778\r\n",
      "Train Epoch: 33 [72320/209539 (35%)]\tAll Loss: 1.3906\tTriple Loss(1): 0.1104\tClassification Loss: 1.1699\r\n",
      "Train Epoch: 33 [72960/209539 (35%)]\tAll Loss: 1.2314\tTriple Loss(1): 0.0256\tClassification Loss: 1.1802\r\n",
      "Train Epoch: 33 [73600/209539 (35%)]\tAll Loss: 1.2774\tTriple Loss(1): 0.0000\tClassification Loss: 1.2774\r\n",
      "Train Epoch: 33 [74240/209539 (35%)]\tAll Loss: 1.4101\tTriple Loss(1): 0.0000\tClassification Loss: 1.4101\r\n",
      "Train Epoch: 33 [74880/209539 (36%)]\tAll Loss: 1.6129\tTriple Loss(1): 0.0067\tClassification Loss: 1.5994\r\n",
      "Train Epoch: 33 [75520/209539 (36%)]\tAll Loss: 1.0873\tTriple Loss(1): 0.0073\tClassification Loss: 1.0726\r\n",
      "Train Epoch: 33 [76160/209539 (36%)]\tAll Loss: 1.0929\tTriple Loss(1): 0.0556\tClassification Loss: 0.9816\r\n",
      "Train Epoch: 33 [76800/209539 (37%)]\tAll Loss: 1.1552\tTriple Loss(1): 0.0538\tClassification Loss: 1.0475\r\n",
      "Train Epoch: 33 [77440/209539 (37%)]\tAll Loss: 1.2035\tTriple Loss(1): 0.0071\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 33 [78080/209539 (37%)]\tAll Loss: 1.1977\tTriple Loss(1): 0.0006\tClassification Loss: 1.1965\r\n",
      "Train Epoch: 33 [78720/209539 (38%)]\tAll Loss: 1.5349\tTriple Loss(1): 0.0412\tClassification Loss: 1.4526\r\n",
      "Train Epoch: 33 [79360/209539 (38%)]\tAll Loss: 1.1631\tTriple Loss(1): 0.0378\tClassification Loss: 1.0876\r\n",
      "Train Epoch: 33 [80000/209539 (38%)]\tAll Loss: 1.2006\tTriple Loss(1): 0.0240\tClassification Loss: 1.1526\r\n",
      "Train Epoch: 33 [80640/209539 (38%)]\tAll Loss: 1.2259\tTriple Loss(1): 0.0329\tClassification Loss: 1.1602\r\n",
      "Train Epoch: 33 [81280/209539 (39%)]\tAll Loss: 1.2782\tTriple Loss(1): 0.0000\tClassification Loss: 1.2782\r\n",
      "Train Epoch: 33 [81920/209539 (39%)]\tAll Loss: 1.3745\tTriple Loss(1): 0.0548\tClassification Loss: 1.2648\r\n",
      "Train Epoch: 33 [82560/209539 (39%)]\tAll Loss: 1.3814\tTriple Loss(1): 0.0159\tClassification Loss: 1.3496\r\n",
      "Train Epoch: 33 [83200/209539 (40%)]\tAll Loss: 1.5467\tTriple Loss(1): 0.0332\tClassification Loss: 1.4803\r\n",
      "Train Epoch: 33 [83840/209539 (40%)]\tAll Loss: 1.2183\tTriple Loss(1): 0.0053\tClassification Loss: 1.2077\r\n",
      "Train Epoch: 33 [84480/209539 (40%)]\tAll Loss: 1.1000\tTriple Loss(1): 0.0136\tClassification Loss: 1.0727\r\n",
      "Train Epoch: 33 [85120/209539 (41%)]\tAll Loss: 1.7306\tTriple Loss(1): 0.0107\tClassification Loss: 1.7091\r\n",
      "Train Epoch: 33 [85760/209539 (41%)]\tAll Loss: 1.2050\tTriple Loss(1): 0.0439\tClassification Loss: 1.1173\r\n",
      "Train Epoch: 33 [86400/209539 (41%)]\tAll Loss: 1.9355\tTriple Loss(0): 0.3419\tClassification Loss: 1.2517\r\n",
      "Train Epoch: 33 [87040/209539 (42%)]\tAll Loss: 1.1439\tTriple Loss(1): 0.0000\tClassification Loss: 1.1439\r\n",
      "Train Epoch: 33 [87680/209539 (42%)]\tAll Loss: 1.1186\tTriple Loss(1): 0.0682\tClassification Loss: 0.9821\r\n",
      "Train Epoch: 33 [88320/209539 (42%)]\tAll Loss: 1.2978\tTriple Loss(1): 0.0862\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 33 [88960/209539 (42%)]\tAll Loss: 1.2905\tTriple Loss(1): 0.0000\tClassification Loss: 1.2905\r\n",
      "Train Epoch: 33 [89600/209539 (43%)]\tAll Loss: 1.9535\tTriple Loss(0): 0.3287\tClassification Loss: 1.2961\r\n",
      "Train Epoch: 33 [90240/209539 (43%)]\tAll Loss: 1.9337\tTriple Loss(0): 0.3695\tClassification Loss: 1.1946\r\n",
      "Train Epoch: 33 [90880/209539 (43%)]\tAll Loss: 1.4883\tTriple Loss(1): 0.0058\tClassification Loss: 1.4768\r\n",
      "Train Epoch: 33 [91520/209539 (44%)]\tAll Loss: 1.2617\tTriple Loss(1): 0.0000\tClassification Loss: 1.2617\r\n",
      "Train Epoch: 33 [92160/209539 (44%)]\tAll Loss: 1.0811\tTriple Loss(1): 0.0558\tClassification Loss: 0.9696\r\n",
      "Train Epoch: 33 [92800/209539 (44%)]\tAll Loss: 1.0809\tTriple Loss(1): 0.0330\tClassification Loss: 1.0149\r\n",
      "Train Epoch: 33 [93440/209539 (45%)]\tAll Loss: 1.3732\tTriple Loss(1): 0.0100\tClassification Loss: 1.3533\r\n",
      "Train Epoch: 33 [94080/209539 (45%)]\tAll Loss: 1.1529\tTriple Loss(1): 0.0000\tClassification Loss: 1.1529\r\n",
      "Train Epoch: 33 [94720/209539 (45%)]\tAll Loss: 1.2898\tTriple Loss(1): 0.0188\tClassification Loss: 1.2522\r\n",
      "Train Epoch: 33 [95360/209539 (46%)]\tAll Loss: 1.2913\tTriple Loss(1): 0.0510\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 33 [96000/209539 (46%)]\tAll Loss: 1.2694\tTriple Loss(1): 0.0000\tClassification Loss: 1.2694\r\n",
      "Train Epoch: 33 [96640/209539 (46%)]\tAll Loss: 1.2486\tTriple Loss(1): 0.0117\tClassification Loss: 1.2253\r\n",
      "Train Epoch: 33 [97280/209539 (46%)]\tAll Loss: 1.4817\tTriple Loss(0): 0.1815\tClassification Loss: 1.1187\r\n",
      "Train Epoch: 33 [97920/209539 (47%)]\tAll Loss: 1.8000\tTriple Loss(0): 0.1732\tClassification Loss: 1.4536\r\n",
      "Train Epoch: 33 [98560/209539 (47%)]\tAll Loss: 1.5421\tTriple Loss(1): 0.0407\tClassification Loss: 1.4608\r\n",
      "Train Epoch: 33 [99200/209539 (47%)]\tAll Loss: 1.2194\tTriple Loss(1): 0.0527\tClassification Loss: 1.1139\r\n",
      "Train Epoch: 33 [99840/209539 (48%)]\tAll Loss: 2.1288\tTriple Loss(0): 0.4457\tClassification Loss: 1.2375\r\n",
      "Train Epoch: 33 [100480/209539 (48%)]\tAll Loss: 1.2111\tTriple Loss(1): 0.0491\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 33 [101120/209539 (48%)]\tAll Loss: 1.1085\tTriple Loss(1): 0.0088\tClassification Loss: 1.0909\r\n",
      "Train Epoch: 33 [101760/209539 (49%)]\tAll Loss: 1.2447\tTriple Loss(1): 0.0520\tClassification Loss: 1.1408\r\n",
      "Train Epoch: 33 [102400/209539 (49%)]\tAll Loss: 1.4054\tTriple Loss(0): 0.2152\tClassification Loss: 0.9750\r\n",
      "Train Epoch: 33 [103040/209539 (49%)]\tAll Loss: 1.2469\tTriple Loss(1): 0.0333\tClassification Loss: 1.1803\r\n",
      "Train Epoch: 33 [103680/209539 (49%)]\tAll Loss: 1.4098\tTriple Loss(1): 0.0660\tClassification Loss: 1.2778\r\n",
      "Train Epoch: 33 [104320/209539 (50%)]\tAll Loss: 1.3341\tTriple Loss(0): 0.1695\tClassification Loss: 0.9950\r\n",
      "Train Epoch: 33 [104960/209539 (50%)]\tAll Loss: 1.2504\tTriple Loss(1): 0.0238\tClassification Loss: 1.2028\r\n",
      "Train Epoch: 33 [105600/209539 (50%)]\tAll Loss: 1.2866\tTriple Loss(1): 0.0080\tClassification Loss: 1.2706\r\n",
      "Train Epoch: 33 [106240/209539 (51%)]\tAll Loss: 0.9107\tTriple Loss(1): 0.0000\tClassification Loss: 0.9107\r\n",
      "Train Epoch: 33 [106880/209539 (51%)]\tAll Loss: 1.7208\tTriple Loss(0): 0.2731\tClassification Loss: 1.1747\r\n",
      "Train Epoch: 33 [107520/209539 (51%)]\tAll Loss: 1.2769\tTriple Loss(1): 0.0773\tClassification Loss: 1.1222\r\n",
      "Train Epoch: 33 [108160/209539 (52%)]\tAll Loss: 0.8662\tTriple Loss(1): 0.0000\tClassification Loss: 0.8662\r\n",
      "Train Epoch: 33 [108800/209539 (52%)]\tAll Loss: 0.9624\tTriple Loss(1): 0.0000\tClassification Loss: 0.9624\r\n",
      "Train Epoch: 33 [109440/209539 (52%)]\tAll Loss: 1.3607\tTriple Loss(1): 0.0100\tClassification Loss: 1.3408\r\n",
      "Train Epoch: 33 [110080/209539 (53%)]\tAll Loss: 1.4398\tTriple Loss(1): 0.0390\tClassification Loss: 1.3617\r\n",
      "Train Epoch: 33 [110720/209539 (53%)]\tAll Loss: 1.2172\tTriple Loss(1): 0.0017\tClassification Loss: 1.2137\r\n",
      "Train Epoch: 33 [111360/209539 (53%)]\tAll Loss: 0.9753\tTriple Loss(1): 0.0197\tClassification Loss: 0.9359\r\n",
      "Train Epoch: 33 [112000/209539 (53%)]\tAll Loss: 1.0486\tTriple Loss(1): 0.0000\tClassification Loss: 1.0486\r\n",
      "Train Epoch: 33 [112640/209539 (54%)]\tAll Loss: 1.1029\tTriple Loss(1): 0.0000\tClassification Loss: 1.1029\r\n",
      "Train Epoch: 33 [113280/209539 (54%)]\tAll Loss: 1.0212\tTriple Loss(1): 0.0029\tClassification Loss: 1.0154\r\n",
      "Train Epoch: 33 [113920/209539 (54%)]\tAll Loss: 1.0427\tTriple Loss(1): 0.0047\tClassification Loss: 1.0332\r\n",
      "Train Epoch: 33 [114560/209539 (55%)]\tAll Loss: 1.2460\tTriple Loss(1): 0.0529\tClassification Loss: 1.1403\r\n",
      "Train Epoch: 33 [115200/209539 (55%)]\tAll Loss: 1.9809\tTriple Loss(0): 0.3496\tClassification Loss: 1.2817\r\n",
      "Train Epoch: 33 [115840/209539 (55%)]\tAll Loss: 1.2973\tTriple Loss(1): 0.0539\tClassification Loss: 1.1894\r\n",
      "Train Epoch: 33 [116480/209539 (56%)]\tAll Loss: 1.3498\tTriple Loss(1): 0.0366\tClassification Loss: 1.2766\r\n",
      "Train Epoch: 33 [117120/209539 (56%)]\tAll Loss: 1.6228\tTriple Loss(1): 0.0853\tClassification Loss: 1.4521\r\n",
      "Train Epoch: 33 [117760/209539 (56%)]\tAll Loss: 1.1254\tTriple Loss(1): 0.0136\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 33 [118400/209539 (57%)]\tAll Loss: 1.3114\tTriple Loss(0): 0.1248\tClassification Loss: 1.0618\r\n",
      "Train Epoch: 33 [119040/209539 (57%)]\tAll Loss: 1.9359\tTriple Loss(0): 0.2261\tClassification Loss: 1.4837\r\n",
      "Train Epoch: 33 [119680/209539 (57%)]\tAll Loss: 1.0885\tTriple Loss(1): 0.0332\tClassification Loss: 1.0222\r\n",
      "Train Epoch: 33 [120320/209539 (57%)]\tAll Loss: 1.2624\tTriple Loss(1): 0.0165\tClassification Loss: 1.2293\r\n",
      "Train Epoch: 33 [120960/209539 (58%)]\tAll Loss: 1.1395\tTriple Loss(1): 0.0130\tClassification Loss: 1.1134\r\n",
      "Train Epoch: 33 [121600/209539 (58%)]\tAll Loss: 1.6887\tTriple Loss(0): 0.3054\tClassification Loss: 1.0778\r\n",
      "Train Epoch: 33 [122240/209539 (58%)]\tAll Loss: 1.0068\tTriple Loss(1): 0.0064\tClassification Loss: 0.9939\r\n",
      "Train Epoch: 33 [122880/209539 (59%)]\tAll Loss: 1.5255\tTriple Loss(0): 0.2704\tClassification Loss: 0.9847\r\n",
      "Train Epoch: 33 [123520/209539 (59%)]\tAll Loss: 1.7662\tTriple Loss(0): 0.3521\tClassification Loss: 1.0621\r\n",
      "Train Epoch: 33 [124160/209539 (59%)]\tAll Loss: 1.8019\tTriple Loss(0): 0.2568\tClassification Loss: 1.2883\r\n",
      "Train Epoch: 33 [124800/209539 (60%)]\tAll Loss: 0.7985\tTriple Loss(1): 0.0209\tClassification Loss: 0.7566\r\n",
      "Train Epoch: 33 [125440/209539 (60%)]\tAll Loss: 1.1313\tTriple Loss(1): 0.0000\tClassification Loss: 1.1313\r\n",
      "Train Epoch: 33 [126080/209539 (60%)]\tAll Loss: 1.1697\tTriple Loss(1): 0.0064\tClassification Loss: 1.1569\r\n",
      "Train Epoch: 33 [126720/209539 (60%)]\tAll Loss: 1.3163\tTriple Loss(1): 0.0158\tClassification Loss: 1.2846\r\n",
      "Train Epoch: 33 [127360/209539 (61%)]\tAll Loss: 1.7605\tTriple Loss(0): 0.2320\tClassification Loss: 1.2965\r\n",
      "Train Epoch: 33 [128000/209539 (61%)]\tAll Loss: 1.7914\tTriple Loss(0): 0.1894\tClassification Loss: 1.4127\r\n",
      "Train Epoch: 33 [128640/209539 (61%)]\tAll Loss: 1.0718\tTriple Loss(1): 0.0054\tClassification Loss: 1.0610\r\n",
      "Train Epoch: 33 [129280/209539 (62%)]\tAll Loss: 1.3422\tTriple Loss(1): 0.0053\tClassification Loss: 1.3317\r\n",
      "Train Epoch: 33 [129920/209539 (62%)]\tAll Loss: 1.0572\tTriple Loss(1): 0.0000\tClassification Loss: 1.0572\r\n",
      "Train Epoch: 33 [130560/209539 (62%)]\tAll Loss: 1.0423\tTriple Loss(1): 0.0000\tClassification Loss: 1.0423\r\n",
      "Train Epoch: 33 [131200/209539 (63%)]\tAll Loss: 1.1093\tTriple Loss(1): 0.0326\tClassification Loss: 1.0442\r\n",
      "Train Epoch: 33 [131840/209539 (63%)]\tAll Loss: 1.1631\tTriple Loss(1): 0.0413\tClassification Loss: 1.0806\r\n",
      "Train Epoch: 33 [132480/209539 (63%)]\tAll Loss: 1.6868\tTriple Loss(0): 0.2760\tClassification Loss: 1.1348\r\n",
      "Train Epoch: 33 [133120/209539 (64%)]\tAll Loss: 1.1977\tTriple Loss(1): 0.0132\tClassification Loss: 1.1713\r\n",
      "Train Epoch: 33 [133760/209539 (64%)]\tAll Loss: 2.0258\tTriple Loss(0): 0.3260\tClassification Loss: 1.3739\r\n",
      "Train Epoch: 33 [134400/209539 (64%)]\tAll Loss: 1.4458\tTriple Loss(0): 0.2553\tClassification Loss: 0.9351\r\n",
      "Train Epoch: 33 [135040/209539 (64%)]\tAll Loss: 1.1615\tTriple Loss(1): 0.0009\tClassification Loss: 1.1597\r\n",
      "Train Epoch: 33 [135680/209539 (65%)]\tAll Loss: 1.5152\tTriple Loss(1): 0.0416\tClassification Loss: 1.4320\r\n",
      "Train Epoch: 33 [136320/209539 (65%)]\tAll Loss: 1.5212\tTriple Loss(1): 0.0332\tClassification Loss: 1.4548\r\n",
      "Train Epoch: 33 [136960/209539 (65%)]\tAll Loss: 1.1497\tTriple Loss(1): 0.0102\tClassification Loss: 1.1294\r\n",
      "Train Epoch: 33 [137600/209539 (66%)]\tAll Loss: 1.6023\tTriple Loss(0): 0.2152\tClassification Loss: 1.1719\r\n",
      "Train Epoch: 33 [138240/209539 (66%)]\tAll Loss: 1.4614\tTriple Loss(1): 0.0178\tClassification Loss: 1.4259\r\n",
      "Train Epoch: 33 [138880/209539 (66%)]\tAll Loss: 1.4943\tTriple Loss(0): 0.1942\tClassification Loss: 1.1058\r\n",
      "Train Epoch: 33 [139520/209539 (67%)]\tAll Loss: 1.1329\tTriple Loss(1): 0.0000\tClassification Loss: 1.1329\r\n",
      "Train Epoch: 33 [140160/209539 (67%)]\tAll Loss: 1.2511\tTriple Loss(1): 0.0296\tClassification Loss: 1.1920\r\n",
      "Train Epoch: 33 [140800/209539 (67%)]\tAll Loss: 1.5917\tTriple Loss(1): 0.1184\tClassification Loss: 1.3550\r\n",
      "Train Epoch: 33 [141440/209539 (68%)]\tAll Loss: 1.2914\tTriple Loss(1): 0.0562\tClassification Loss: 1.1791\r\n",
      "Train Epoch: 33 [142080/209539 (68%)]\tAll Loss: 1.1716\tTriple Loss(1): 0.0202\tClassification Loss: 1.1312\r\n",
      "Train Epoch: 33 [142720/209539 (68%)]\tAll Loss: 1.5671\tTriple Loss(1): 0.0000\tClassification Loss: 1.5671\r\n",
      "Train Epoch: 33 [143360/209539 (68%)]\tAll Loss: 1.0155\tTriple Loss(1): 0.0399\tClassification Loss: 0.9356\r\n",
      "Train Epoch: 33 [144000/209539 (69%)]\tAll Loss: 1.4702\tTriple Loss(1): 0.1045\tClassification Loss: 1.2613\r\n",
      "Train Epoch: 33 [144640/209539 (69%)]\tAll Loss: 1.6614\tTriple Loss(0): 0.2732\tClassification Loss: 1.1151\r\n",
      "Train Epoch: 33 [145280/209539 (69%)]\tAll Loss: 2.0740\tTriple Loss(0): 0.3928\tClassification Loss: 1.2884\r\n",
      "Train Epoch: 33 [145920/209539 (70%)]\tAll Loss: 1.2038\tTriple Loss(1): 0.0300\tClassification Loss: 1.1437\r\n",
      "Train Epoch: 33 [146560/209539 (70%)]\tAll Loss: 0.8991\tTriple Loss(1): 0.0000\tClassification Loss: 0.8991\r\n",
      "Train Epoch: 33 [147200/209539 (70%)]\tAll Loss: 1.3799\tTriple Loss(1): 0.0445\tClassification Loss: 1.2910\r\n",
      "Train Epoch: 33 [147840/209539 (71%)]\tAll Loss: 1.1822\tTriple Loss(1): 0.0750\tClassification Loss: 1.0322\r\n",
      "Train Epoch: 33 [148480/209539 (71%)]\tAll Loss: 1.1021\tTriple Loss(1): 0.0185\tClassification Loss: 1.0651\r\n",
      "Train Epoch: 33 [149120/209539 (71%)]\tAll Loss: 1.4680\tTriple Loss(1): 0.0746\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 33 [149760/209539 (71%)]\tAll Loss: 1.0228\tTriple Loss(1): 0.0000\tClassification Loss: 1.0228\r\n",
      "Train Epoch: 33 [150400/209539 (72%)]\tAll Loss: 2.2243\tTriple Loss(0): 0.4174\tClassification Loss: 1.3894\r\n",
      "Train Epoch: 33 [151040/209539 (72%)]\tAll Loss: 1.0637\tTriple Loss(1): 0.0007\tClassification Loss: 1.0623\r\n",
      "Train Epoch: 33 [151680/209539 (72%)]\tAll Loss: 1.4162\tTriple Loss(1): 0.0180\tClassification Loss: 1.3802\r\n",
      "Train Epoch: 33 [152320/209539 (73%)]\tAll Loss: 1.0357\tTriple Loss(1): 0.0015\tClassification Loss: 1.0327\r\n",
      "Train Epoch: 33 [152960/209539 (73%)]\tAll Loss: 1.1406\tTriple Loss(1): 0.0000\tClassification Loss: 1.1406\r\n",
      "Train Epoch: 33 [153600/209539 (73%)]\tAll Loss: 1.2889\tTriple Loss(1): 0.0227\tClassification Loss: 1.2434\r\n",
      "Train Epoch: 33 [154240/209539 (74%)]\tAll Loss: 1.1492\tTriple Loss(1): 0.0000\tClassification Loss: 1.1492\r\n",
      "Train Epoch: 33 [154880/209539 (74%)]\tAll Loss: 1.3434\tTriple Loss(1): 0.0147\tClassification Loss: 1.3140\r\n",
      "Train Epoch: 33 [155520/209539 (74%)]\tAll Loss: 1.2230\tTriple Loss(1): 0.0135\tClassification Loss: 1.1961\r\n",
      "Train Epoch: 33 [156160/209539 (75%)]\tAll Loss: 1.3534\tTriple Loss(1): 0.0101\tClassification Loss: 1.3332\r\n",
      "Train Epoch: 33 [156800/209539 (75%)]\tAll Loss: 1.4925\tTriple Loss(1): 0.0110\tClassification Loss: 1.4705\r\n",
      "Train Epoch: 33 [157440/209539 (75%)]\tAll Loss: 1.2629\tTriple Loss(1): 0.0389\tClassification Loss: 1.1851\r\n",
      "Train Epoch: 33 [158080/209539 (75%)]\tAll Loss: 1.3129\tTriple Loss(1): 0.0860\tClassification Loss: 1.1409\r\n",
      "Train Epoch: 33 [158720/209539 (76%)]\tAll Loss: 1.0936\tTriple Loss(1): 0.0119\tClassification Loss: 1.0698\r\n",
      "Train Epoch: 33 [159360/209539 (76%)]\tAll Loss: 1.1757\tTriple Loss(1): 0.0545\tClassification Loss: 1.0668\r\n",
      "Train Epoch: 33 [160000/209539 (76%)]\tAll Loss: 1.4689\tTriple Loss(1): 0.0360\tClassification Loss: 1.3970\r\n",
      "Train Epoch: 33 [160640/209539 (77%)]\tAll Loss: 1.0089\tTriple Loss(1): 0.0112\tClassification Loss: 0.9864\r\n",
      "Train Epoch: 33 [161280/209539 (77%)]\tAll Loss: 1.0447\tTriple Loss(1): 0.0000\tClassification Loss: 1.0447\r\n",
      "Train Epoch: 33 [161920/209539 (77%)]\tAll Loss: 1.1907\tTriple Loss(1): 0.0121\tClassification Loss: 1.1664\r\n",
      "Train Epoch: 33 [162560/209539 (78%)]\tAll Loss: 1.7291\tTriple Loss(0): 0.2387\tClassification Loss: 1.2517\r\n",
      "Train Epoch: 33 [163200/209539 (78%)]\tAll Loss: 0.8879\tTriple Loss(1): 0.0148\tClassification Loss: 0.8583\r\n",
      "Train Epoch: 33 [163840/209539 (78%)]\tAll Loss: 1.2965\tTriple Loss(1): 0.0039\tClassification Loss: 1.2887\r\n",
      "Train Epoch: 33 [164480/209539 (78%)]\tAll Loss: 0.9842\tTriple Loss(1): 0.0166\tClassification Loss: 0.9510\r\n",
      "Train Epoch: 33 [165120/209539 (79%)]\tAll Loss: 1.7013\tTriple Loss(0): 0.2762\tClassification Loss: 1.1490\r\n",
      "Train Epoch: 33 [165760/209539 (79%)]\tAll Loss: 1.2075\tTriple Loss(1): 0.0165\tClassification Loss: 1.1744\r\n",
      "Train Epoch: 33 [166400/209539 (79%)]\tAll Loss: 1.1042\tTriple Loss(1): 0.0038\tClassification Loss: 1.0966\r\n",
      "Train Epoch: 33 [167040/209539 (80%)]\tAll Loss: 1.3001\tTriple Loss(1): 0.0000\tClassification Loss: 1.3001\r\n",
      "Train Epoch: 33 [167680/209539 (80%)]\tAll Loss: 1.6148\tTriple Loss(0): 0.1161\tClassification Loss: 1.3827\r\n",
      "Train Epoch: 33 [168320/209539 (80%)]\tAll Loss: 1.3800\tTriple Loss(1): 0.0787\tClassification Loss: 1.2226\r\n",
      "Train Epoch: 33 [168960/209539 (81%)]\tAll Loss: 1.2991\tTriple Loss(1): 0.1061\tClassification Loss: 1.0868\r\n",
      "Train Epoch: 33 [169600/209539 (81%)]\tAll Loss: 1.6621\tTriple Loss(0): 0.2431\tClassification Loss: 1.1760\r\n",
      "Train Epoch: 33 [170240/209539 (81%)]\tAll Loss: 1.4044\tTriple Loss(1): 0.0460\tClassification Loss: 1.3123\r\n",
      "Train Epoch: 33 [170880/209539 (82%)]\tAll Loss: 1.2344\tTriple Loss(1): 0.0000\tClassification Loss: 1.2344\r\n",
      "Train Epoch: 33 [171520/209539 (82%)]\tAll Loss: 1.0000\tTriple Loss(1): 0.0000\tClassification Loss: 1.0000\r\n",
      "Train Epoch: 33 [172160/209539 (82%)]\tAll Loss: 2.1540\tTriple Loss(0): 0.4437\tClassification Loss: 1.2667\r\n",
      "Train Epoch: 33 [172800/209539 (82%)]\tAll Loss: 1.2552\tTriple Loss(1): 0.0153\tClassification Loss: 1.2246\r\n",
      "Train Epoch: 33 [173440/209539 (83%)]\tAll Loss: 1.2644\tTriple Loss(1): 0.0127\tClassification Loss: 1.2391\r\n",
      "Train Epoch: 33 [174080/209539 (83%)]\tAll Loss: 1.3548\tTriple Loss(1): 0.0951\tClassification Loss: 1.1646\r\n",
      "Train Epoch: 33 [174720/209539 (83%)]\tAll Loss: 1.2037\tTriple Loss(1): 0.0446\tClassification Loss: 1.1145\r\n",
      "Train Epoch: 33 [175360/209539 (84%)]\tAll Loss: 1.3711\tTriple Loss(1): 0.0121\tClassification Loss: 1.3470\r\n",
      "Train Epoch: 33 [176000/209539 (84%)]\tAll Loss: 1.3224\tTriple Loss(1): 0.0540\tClassification Loss: 1.2144\r\n",
      "Train Epoch: 33 [176640/209539 (84%)]\tAll Loss: 0.9591\tTriple Loss(1): 0.0017\tClassification Loss: 0.9557\r\n",
      "Train Epoch: 33 [177280/209539 (85%)]\tAll Loss: 1.7570\tTriple Loss(1): 0.0655\tClassification Loss: 1.6261\r\n",
      "Train Epoch: 33 [177920/209539 (85%)]\tAll Loss: 2.0925\tTriple Loss(0): 0.3381\tClassification Loss: 1.4162\r\n",
      "Train Epoch: 33 [178560/209539 (85%)]\tAll Loss: 1.7536\tTriple Loss(0): 0.3502\tClassification Loss: 1.0532\r\n",
      "Train Epoch: 33 [179200/209539 (86%)]\tAll Loss: 1.2908\tTriple Loss(1): 0.0184\tClassification Loss: 1.2540\r\n",
      "Train Epoch: 33 [179840/209539 (86%)]\tAll Loss: 1.7358\tTriple Loss(0): 0.2799\tClassification Loss: 1.1760\r\n",
      "Train Epoch: 33 [180480/209539 (86%)]\tAll Loss: 1.3373\tTriple Loss(1): 0.0633\tClassification Loss: 1.2107\r\n",
      "Train Epoch: 33 [181120/209539 (86%)]\tAll Loss: 1.5279\tTriple Loss(1): 0.0371\tClassification Loss: 1.4538\r\n",
      "Train Epoch: 33 [181760/209539 (87%)]\tAll Loss: 1.2814\tTriple Loss(1): 0.0000\tClassification Loss: 1.2814\r\n",
      "Train Epoch: 33 [182400/209539 (87%)]\tAll Loss: 1.3715\tTriple Loss(1): 0.0275\tClassification Loss: 1.3165\r\n",
      "Train Epoch: 33 [183040/209539 (87%)]\tAll Loss: 1.3209\tTriple Loss(1): 0.0527\tClassification Loss: 1.2155\r\n",
      "Train Epoch: 33 [183680/209539 (88%)]\tAll Loss: 1.6052\tTriple Loss(0): 0.3454\tClassification Loss: 0.9144\r\n",
      "Train Epoch: 33 [184320/209539 (88%)]\tAll Loss: 1.4104\tTriple Loss(0): 0.2036\tClassification Loss: 1.0033\r\n",
      "Train Epoch: 33 [184960/209539 (88%)]\tAll Loss: 1.7854\tTriple Loss(0): 0.3620\tClassification Loss: 1.0614\r\n",
      "Train Epoch: 33 [185600/209539 (89%)]\tAll Loss: 1.6031\tTriple Loss(1): 0.0648\tClassification Loss: 1.4734\r\n",
      "Train Epoch: 33 [186240/209539 (89%)]\tAll Loss: 1.3482\tTriple Loss(1): 0.0000\tClassification Loss: 1.3482\r\n",
      "Train Epoch: 33 [186880/209539 (89%)]\tAll Loss: 1.1673\tTriple Loss(1): 0.0023\tClassification Loss: 1.1626\r\n",
      "Train Epoch: 33 [187520/209539 (89%)]\tAll Loss: 1.4671\tTriple Loss(1): 0.0000\tClassification Loss: 1.4671\r\n",
      "Train Epoch: 33 [188160/209539 (90%)]\tAll Loss: 1.2554\tTriple Loss(0): 0.1479\tClassification Loss: 0.9596\r\n",
      "Train Epoch: 33 [188800/209539 (90%)]\tAll Loss: 1.1106\tTriple Loss(1): 0.0012\tClassification Loss: 1.1082\r\n",
      "Train Epoch: 33 [189440/209539 (90%)]\tAll Loss: 2.2474\tTriple Loss(0): 0.4884\tClassification Loss: 1.2706\r\n",
      "Train Epoch: 33 [190080/209539 (91%)]\tAll Loss: 1.0789\tTriple Loss(1): 0.0000\tClassification Loss: 1.0789\r\n",
      "Train Epoch: 33 [190720/209539 (91%)]\tAll Loss: 1.1547\tTriple Loss(1): 0.0000\tClassification Loss: 1.1547\r\n",
      "Train Epoch: 33 [191360/209539 (91%)]\tAll Loss: 1.0915\tTriple Loss(1): 0.0205\tClassification Loss: 1.0505\r\n",
      "Train Epoch: 33 [192000/209539 (92%)]\tAll Loss: 1.6339\tTriple Loss(1): 0.0217\tClassification Loss: 1.5905\r\n",
      "Train Epoch: 33 [192640/209539 (92%)]\tAll Loss: 1.2309\tTriple Loss(1): 0.0091\tClassification Loss: 1.2128\r\n",
      "Train Epoch: 33 [193280/209539 (92%)]\tAll Loss: 1.2315\tTriple Loss(1): 0.0331\tClassification Loss: 1.1652\r\n",
      "Train Epoch: 33 [193920/209539 (93%)]\tAll Loss: 1.0565\tTriple Loss(1): 0.0368\tClassification Loss: 0.9829\r\n",
      "Train Epoch: 33 [194560/209539 (93%)]\tAll Loss: 1.2351\tTriple Loss(1): 0.0606\tClassification Loss: 1.1140\r\n",
      "Train Epoch: 33 [195200/209539 (93%)]\tAll Loss: 1.2666\tTriple Loss(1): 0.0145\tClassification Loss: 1.2377\r\n",
      "Train Epoch: 33 [195840/209539 (93%)]\tAll Loss: 0.8600\tTriple Loss(1): 0.0037\tClassification Loss: 0.8525\r\n",
      "Train Epoch: 33 [196480/209539 (94%)]\tAll Loss: 1.6268\tTriple Loss(1): 0.0701\tClassification Loss: 1.4866\r\n",
      "Train Epoch: 33 [197120/209539 (94%)]\tAll Loss: 1.3335\tTriple Loss(1): 0.0886\tClassification Loss: 1.1562\r\n",
      "Train Epoch: 33 [197760/209539 (94%)]\tAll Loss: 1.0562\tTriple Loss(1): 0.0000\tClassification Loss: 1.0562\r\n",
      "Train Epoch: 33 [198400/209539 (95%)]\tAll Loss: 2.0372\tTriple Loss(0): 0.4231\tClassification Loss: 1.1910\r\n",
      "Train Epoch: 33 [199040/209539 (95%)]\tAll Loss: 1.3865\tTriple Loss(1): 0.0302\tClassification Loss: 1.3261\r\n",
      "Train Epoch: 33 [199680/209539 (95%)]\tAll Loss: 1.2900\tTriple Loss(1): 0.0196\tClassification Loss: 1.2507\r\n",
      "Train Epoch: 33 [200320/209539 (96%)]\tAll Loss: 1.1103\tTriple Loss(1): 0.0000\tClassification Loss: 1.1103\r\n",
      "Train Epoch: 33 [200960/209539 (96%)]\tAll Loss: 1.4904\tTriple Loss(0): 0.2297\tClassification Loss: 1.0310\r\n",
      "Train Epoch: 33 [201600/209539 (96%)]\tAll Loss: 1.6493\tTriple Loss(0): 0.3076\tClassification Loss: 1.0341\r\n",
      "Train Epoch: 33 [202240/209539 (97%)]\tAll Loss: 1.0623\tTriple Loss(1): 0.0181\tClassification Loss: 1.0261\r\n",
      "Train Epoch: 33 [202880/209539 (97%)]\tAll Loss: 1.2194\tTriple Loss(0): 0.1948\tClassification Loss: 0.8299\r\n",
      "Train Epoch: 33 [203520/209539 (97%)]\tAll Loss: 1.2389\tTriple Loss(1): 0.0126\tClassification Loss: 1.2137\r\n",
      "Train Epoch: 33 [204160/209539 (97%)]\tAll Loss: 1.7498\tTriple Loss(1): 0.0284\tClassification Loss: 1.6929\r\n",
      "Train Epoch: 33 [204800/209539 (98%)]\tAll Loss: 1.4929\tTriple Loss(1): 0.0107\tClassification Loss: 1.4714\r\n",
      "Train Epoch: 33 [205440/209539 (98%)]\tAll Loss: 0.8533\tTriple Loss(1): 0.0219\tClassification Loss: 0.8095\r\n",
      "Train Epoch: 33 [206080/209539 (98%)]\tAll Loss: 1.1615\tTriple Loss(1): 0.0136\tClassification Loss: 1.1342\r\n",
      "Train Epoch: 33 [206720/209539 (99%)]\tAll Loss: 1.0257\tTriple Loss(1): 0.0000\tClassification Loss: 1.0257\r\n",
      "Train Epoch: 33 [207360/209539 (99%)]\tAll Loss: 0.9244\tTriple Loss(1): 0.0000\tClassification Loss: 0.9244\r\n",
      "Train Epoch: 33 [208000/209539 (99%)]\tAll Loss: 1.1649\tTriple Loss(1): 0.0560\tClassification Loss: 1.0528\r\n",
      "Train Epoch: 33 [208640/209539 (100%)]\tAll Loss: 1.6253\tTriple Loss(0): 0.2105\tClassification Loss: 1.2042\r\n",
      "Train Epoch: 33 [209280/209539 (100%)]\tAll Loss: 1.1954\tTriple Loss(1): 0.0060\tClassification Loss: 1.1834\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/33_epochs\r\n",
      "Train Epoch: 34 [0/209539 (0%)]\tAll Loss: 1.7391\tTriple Loss(1): 0.1059\tClassification Loss: 1.5273\r\n",
      "\r\n",
      "Test set: Average loss: 1.0900\r\n",
      "Top 1 Accuracy: 54600/80128 (68%)\r\n",
      "Top 3 Accuracy: 70009/80128 (87%)\r\n",
      "Top 5 Accuracy: 74621/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 34 [640/209539 (0%)]\tAll Loss: 1.3373\tTriple Loss(1): 0.0190\tClassification Loss: 1.2993\r\n",
      "Train Epoch: 34 [1280/209539 (1%)]\tAll Loss: 1.1035\tTriple Loss(1): 0.0189\tClassification Loss: 1.0658\r\n",
      "Train Epoch: 34 [1920/209539 (1%)]\tAll Loss: 1.4251\tTriple Loss(1): 0.0599\tClassification Loss: 1.3054\r\n",
      "Train Epoch: 34 [2560/209539 (1%)]\tAll Loss: 2.2678\tTriple Loss(0): 0.4416\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 34 [3200/209539 (2%)]\tAll Loss: 1.8215\tTriple Loss(0): 0.2507\tClassification Loss: 1.3200\r\n",
      "Train Epoch: 34 [3840/209539 (2%)]\tAll Loss: 2.0625\tTriple Loss(0): 0.4856\tClassification Loss: 1.0914\r\n",
      "Train Epoch: 34 [4480/209539 (2%)]\tAll Loss: 1.2515\tTriple Loss(1): 0.0021\tClassification Loss: 1.2473\r\n",
      "Train Epoch: 34 [5120/209539 (2%)]\tAll Loss: 2.0077\tTriple Loss(0): 0.4759\tClassification Loss: 1.0559\r\n",
      "Train Epoch: 34 [5760/209539 (3%)]\tAll Loss: 1.2224\tTriple Loss(1): 0.0113\tClassification Loss: 1.1998\r\n",
      "Train Epoch: 34 [6400/209539 (3%)]\tAll Loss: 1.0082\tTriple Loss(1): 0.0249\tClassification Loss: 0.9583\r\n",
      "Train Epoch: 34 [7040/209539 (3%)]\tAll Loss: 1.1187\tTriple Loss(1): 0.0298\tClassification Loss: 1.0591\r\n",
      "Train Epoch: 34 [7680/209539 (4%)]\tAll Loss: 1.6565\tTriple Loss(0): 0.3629\tClassification Loss: 0.9307\r\n",
      "Train Epoch: 34 [8320/209539 (4%)]\tAll Loss: 1.2657\tTriple Loss(1): 0.0137\tClassification Loss: 1.2382\r\n",
      "Train Epoch: 34 [8960/209539 (4%)]\tAll Loss: 1.1571\tTriple Loss(1): 0.0046\tClassification Loss: 1.1479\r\n",
      "Train Epoch: 34 [9600/209539 (5%)]\tAll Loss: 1.9061\tTriple Loss(0): 0.1782\tClassification Loss: 1.5497\r\n",
      "Train Epoch: 34 [10240/209539 (5%)]\tAll Loss: 1.1066\tTriple Loss(1): 0.0690\tClassification Loss: 0.9685\r\n",
      "Train Epoch: 34 [10880/209539 (5%)]\tAll Loss: 1.1827\tTriple Loss(1): 0.0000\tClassification Loss: 1.1827\r\n",
      "Train Epoch: 34 [11520/209539 (5%)]\tAll Loss: 1.2079\tTriple Loss(1): 0.0109\tClassification Loss: 1.1862\r\n",
      "Train Epoch: 34 [12160/209539 (6%)]\tAll Loss: 0.9863\tTriple Loss(1): 0.0275\tClassification Loss: 0.9313\r\n",
      "Train Epoch: 34 [12800/209539 (6%)]\tAll Loss: 0.9939\tTriple Loss(1): 0.0270\tClassification Loss: 0.9399\r\n",
      "Train Epoch: 34 [13440/209539 (6%)]\tAll Loss: 1.0427\tTriple Loss(1): 0.0000\tClassification Loss: 1.0427\r\n",
      "Train Epoch: 34 [14080/209539 (7%)]\tAll Loss: 1.2202\tTriple Loss(1): 0.0036\tClassification Loss: 1.2130\r\n",
      "Train Epoch: 34 [14720/209539 (7%)]\tAll Loss: 1.8047\tTriple Loss(0): 0.3237\tClassification Loss: 1.1572\r\n",
      "Train Epoch: 34 [15360/209539 (7%)]\tAll Loss: 1.1045\tTriple Loss(1): 0.0249\tClassification Loss: 1.0546\r\n",
      "Train Epoch: 34 [16000/209539 (8%)]\tAll Loss: 1.6108\tTriple Loss(1): 0.0397\tClassification Loss: 1.5314\r\n",
      "Train Epoch: 34 [16640/209539 (8%)]\tAll Loss: 1.6262\tTriple Loss(1): 0.0140\tClassification Loss: 1.5982\r\n",
      "Train Epoch: 34 [17280/209539 (8%)]\tAll Loss: 1.6267\tTriple Loss(0): 0.1795\tClassification Loss: 1.2676\r\n",
      "Train Epoch: 34 [17920/209539 (9%)]\tAll Loss: 1.2327\tTriple Loss(1): 0.0000\tClassification Loss: 1.2327\r\n",
      "Train Epoch: 34 [18560/209539 (9%)]\tAll Loss: 1.1914\tTriple Loss(1): 0.0314\tClassification Loss: 1.1286\r\n",
      "Train Epoch: 34 [19200/209539 (9%)]\tAll Loss: 1.2177\tTriple Loss(1): 0.0842\tClassification Loss: 1.0493\r\n",
      "Train Epoch: 34 [19840/209539 (9%)]\tAll Loss: 1.3121\tTriple Loss(1): 0.0632\tClassification Loss: 1.1856\r\n",
      "Train Epoch: 34 [20480/209539 (10%)]\tAll Loss: 0.9607\tTriple Loss(1): 0.0128\tClassification Loss: 0.9351\r\n",
      "Train Epoch: 34 [21120/209539 (10%)]\tAll Loss: 1.9562\tTriple Loss(0): 0.2538\tClassification Loss: 1.4486\r\n",
      "Train Epoch: 34 [21760/209539 (10%)]\tAll Loss: 1.3550\tTriple Loss(0): 0.1219\tClassification Loss: 1.1112\r\n",
      "Train Epoch: 34 [22400/209539 (11%)]\tAll Loss: 1.1014\tTriple Loss(1): 0.0078\tClassification Loss: 1.0858\r\n",
      "Train Epoch: 34 [23040/209539 (11%)]\tAll Loss: 1.2109\tTriple Loss(1): 0.0000\tClassification Loss: 1.2109\r\n",
      "Train Epoch: 34 [23680/209539 (11%)]\tAll Loss: 1.0450\tTriple Loss(1): 0.0326\tClassification Loss: 0.9798\r\n",
      "Train Epoch: 34 [24320/209539 (12%)]\tAll Loss: 1.2850\tTriple Loss(1): 0.0002\tClassification Loss: 1.2846\r\n",
      "Train Epoch: 34 [24960/209539 (12%)]\tAll Loss: 1.0121\tTriple Loss(1): 0.0000\tClassification Loss: 1.0121\r\n",
      "Train Epoch: 34 [25600/209539 (12%)]\tAll Loss: 1.3661\tTriple Loss(1): 0.0712\tClassification Loss: 1.2237\r\n",
      "Train Epoch: 34 [26240/209539 (13%)]\tAll Loss: 0.9644\tTriple Loss(1): 0.0184\tClassification Loss: 0.9275\r\n",
      "Train Epoch: 34 [26880/209539 (13%)]\tAll Loss: 1.1782\tTriple Loss(1): 0.0110\tClassification Loss: 1.1561\r\n",
      "Train Epoch: 34 [27520/209539 (13%)]\tAll Loss: 1.0722\tTriple Loss(1): 0.0338\tClassification Loss: 1.0047\r\n",
      "Train Epoch: 34 [28160/209539 (13%)]\tAll Loss: 1.2942\tTriple Loss(1): 0.0000\tClassification Loss: 1.2942\r\n",
      "Train Epoch: 34 [28800/209539 (14%)]\tAll Loss: 1.4895\tTriple Loss(1): 0.0205\tClassification Loss: 1.4485\r\n",
      "Train Epoch: 34 [29440/209539 (14%)]\tAll Loss: 1.3126\tTriple Loss(1): 0.0288\tClassification Loss: 1.2551\r\n",
      "Train Epoch: 34 [30080/209539 (14%)]\tAll Loss: 1.1018\tTriple Loss(1): 0.0000\tClassification Loss: 1.1018\r\n",
      "Train Epoch: 34 [30720/209539 (15%)]\tAll Loss: 1.1359\tTriple Loss(1): 0.0008\tClassification Loss: 1.1344\r\n",
      "Train Epoch: 34 [31360/209539 (15%)]\tAll Loss: 1.1765\tTriple Loss(1): 0.0000\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 34 [32000/209539 (15%)]\tAll Loss: 1.2817\tTriple Loss(1): 0.0525\tClassification Loss: 1.1768\r\n",
      "Train Epoch: 34 [32640/209539 (16%)]\tAll Loss: 1.5140\tTriple Loss(1): 0.0831\tClassification Loss: 1.3478\r\n",
      "Train Epoch: 34 [33280/209539 (16%)]\tAll Loss: 1.7280\tTriple Loss(0): 0.2527\tClassification Loss: 1.2226\r\n",
      "Train Epoch: 34 [33920/209539 (16%)]\tAll Loss: 1.2548\tTriple Loss(1): 0.0186\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 34 [34560/209539 (16%)]\tAll Loss: 1.1755\tTriple Loss(1): 0.0232\tClassification Loss: 1.1291\r\n",
      "Train Epoch: 34 [35200/209539 (17%)]\tAll Loss: 0.9875\tTriple Loss(1): 0.0000\tClassification Loss: 0.9875\r\n",
      "Train Epoch: 34 [35840/209539 (17%)]\tAll Loss: 0.9814\tTriple Loss(1): 0.0005\tClassification Loss: 0.9805\r\n",
      "Train Epoch: 34 [36480/209539 (17%)]\tAll Loss: 1.1497\tTriple Loss(1): 0.0740\tClassification Loss: 1.0017\r\n",
      "Train Epoch: 34 [37120/209539 (18%)]\tAll Loss: 1.3655\tTriple Loss(1): 0.0000\tClassification Loss: 1.3655\r\n",
      "Train Epoch: 34 [37760/209539 (18%)]\tAll Loss: 1.3157\tTriple Loss(1): 0.0047\tClassification Loss: 1.3063\r\n",
      "Train Epoch: 34 [38400/209539 (18%)]\tAll Loss: 1.7161\tTriple Loss(0): 0.1942\tClassification Loss: 1.3277\r\n",
      "Train Epoch: 34 [39040/209539 (19%)]\tAll Loss: 0.9199\tTriple Loss(1): 0.0000\tClassification Loss: 0.9199\r\n",
      "Train Epoch: 34 [39680/209539 (19%)]\tAll Loss: 1.0284\tTriple Loss(1): 0.0000\tClassification Loss: 1.0284\r\n",
      "Train Epoch: 34 [40320/209539 (19%)]\tAll Loss: 1.3007\tTriple Loss(1): 0.0162\tClassification Loss: 1.2683\r\n",
      "Train Epoch: 34 [40960/209539 (20%)]\tAll Loss: 1.0814\tTriple Loss(1): 0.0000\tClassification Loss: 1.0814\r\n",
      "Train Epoch: 34 [41600/209539 (20%)]\tAll Loss: 2.2887\tTriple Loss(0): 0.5956\tClassification Loss: 1.0974\r\n",
      "Train Epoch: 34 [42240/209539 (20%)]\tAll Loss: 1.5524\tTriple Loss(0): 0.1757\tClassification Loss: 1.2009\r\n",
      "Train Epoch: 34 [42880/209539 (20%)]\tAll Loss: 0.9003\tTriple Loss(1): 0.0078\tClassification Loss: 0.8847\r\n",
      "Train Epoch: 34 [43520/209539 (21%)]\tAll Loss: 2.1198\tTriple Loss(0): 0.4038\tClassification Loss: 1.3122\r\n",
      "Train Epoch: 34 [44160/209539 (21%)]\tAll Loss: 1.3102\tTriple Loss(1): 0.0000\tClassification Loss: 1.3102\r\n",
      "Train Epoch: 34 [44800/209539 (21%)]\tAll Loss: 1.3347\tTriple Loss(1): 0.0008\tClassification Loss: 1.3331\r\n",
      "Train Epoch: 34 [45440/209539 (22%)]\tAll Loss: 1.6134\tTriple Loss(1): 0.0000\tClassification Loss: 1.6134\r\n",
      "Train Epoch: 34 [46080/209539 (22%)]\tAll Loss: 1.2593\tTriple Loss(1): 0.0155\tClassification Loss: 1.2282\r\n",
      "Train Epoch: 34 [46720/209539 (22%)]\tAll Loss: 2.1553\tTriple Loss(0): 0.3129\tClassification Loss: 1.5295\r\n",
      "Train Epoch: 34 [47360/209539 (23%)]\tAll Loss: 1.9327\tTriple Loss(0): 0.4667\tClassification Loss: 0.9992\r\n",
      "Train Epoch: 34 [48000/209539 (23%)]\tAll Loss: 1.3160\tTriple Loss(1): 0.1200\tClassification Loss: 1.0760\r\n",
      "Train Epoch: 34 [48640/209539 (23%)]\tAll Loss: 1.3721\tTriple Loss(1): 0.0144\tClassification Loss: 1.3433\r\n",
      "Train Epoch: 34 [49280/209539 (24%)]\tAll Loss: 1.2250\tTriple Loss(1): 0.0275\tClassification Loss: 1.1700\r\n",
      "Train Epoch: 34 [49920/209539 (24%)]\tAll Loss: 1.2531\tTriple Loss(1): 0.0269\tClassification Loss: 1.1992\r\n",
      "Train Epoch: 34 [50560/209539 (24%)]\tAll Loss: 1.3676\tTriple Loss(1): 0.0352\tClassification Loss: 1.2973\r\n",
      "Train Epoch: 34 [51200/209539 (24%)]\tAll Loss: 1.4491\tTriple Loss(1): 0.0494\tClassification Loss: 1.3504\r\n",
      "Train Epoch: 34 [51840/209539 (25%)]\tAll Loss: 1.0047\tTriple Loss(1): 0.0404\tClassification Loss: 0.9238\r\n",
      "Train Epoch: 34 [52480/209539 (25%)]\tAll Loss: 1.0775\tTriple Loss(1): 0.0038\tClassification Loss: 1.0700\r\n",
      "Train Epoch: 34 [53120/209539 (25%)]\tAll Loss: 1.1825\tTriple Loss(1): 0.0052\tClassification Loss: 1.1722\r\n",
      "Train Epoch: 34 [53760/209539 (26%)]\tAll Loss: 1.4758\tTriple Loss(1): 0.0398\tClassification Loss: 1.3963\r\n",
      "Train Epoch: 34 [54400/209539 (26%)]\tAll Loss: 1.7643\tTriple Loss(1): 0.0517\tClassification Loss: 1.6609\r\n",
      "Train Epoch: 34 [55040/209539 (26%)]\tAll Loss: 0.9557\tTriple Loss(1): 0.0150\tClassification Loss: 0.9258\r\n",
      "Train Epoch: 34 [55680/209539 (27%)]\tAll Loss: 1.4599\tTriple Loss(1): 0.0000\tClassification Loss: 1.4599\r\n",
      "Train Epoch: 34 [56320/209539 (27%)]\tAll Loss: 1.0908\tTriple Loss(1): 0.0267\tClassification Loss: 1.0374\r\n",
      "Train Epoch: 34 [56960/209539 (27%)]\tAll Loss: 1.1025\tTriple Loss(1): 0.0137\tClassification Loss: 1.0752\r\n",
      "Train Epoch: 34 [57600/209539 (27%)]\tAll Loss: 1.1459\tTriple Loss(1): 0.1083\tClassification Loss: 0.9294\r\n",
      "Train Epoch: 34 [58240/209539 (28%)]\tAll Loss: 1.0122\tTriple Loss(1): 0.0154\tClassification Loss: 0.9813\r\n",
      "Train Epoch: 34 [58880/209539 (28%)]\tAll Loss: 1.4019\tTriple Loss(1): 0.0352\tClassification Loss: 1.3315\r\n",
      "Train Epoch: 34 [59520/209539 (28%)]\tAll Loss: 1.1815\tTriple Loss(1): 0.0523\tClassification Loss: 1.0770\r\n",
      "Train Epoch: 34 [60160/209539 (29%)]\tAll Loss: 1.2456\tTriple Loss(1): 0.0152\tClassification Loss: 1.2152\r\n",
      "Train Epoch: 34 [60800/209539 (29%)]\tAll Loss: 1.2467\tTriple Loss(1): 0.0061\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 34 [61440/209539 (29%)]\tAll Loss: 1.2401\tTriple Loss(1): 0.0212\tClassification Loss: 1.1977\r\n",
      "Train Epoch: 34 [62080/209539 (30%)]\tAll Loss: 1.4020\tTriple Loss(1): 0.0297\tClassification Loss: 1.3425\r\n",
      "Train Epoch: 34 [62720/209539 (30%)]\tAll Loss: 1.0905\tTriple Loss(1): 0.0046\tClassification Loss: 1.0812\r\n",
      "Train Epoch: 34 [63360/209539 (30%)]\tAll Loss: 1.1303\tTriple Loss(1): 0.0160\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 34 [64000/209539 (31%)]\tAll Loss: 1.4071\tTriple Loss(1): 0.0000\tClassification Loss: 1.4071\r\n",
      "Train Epoch: 34 [64640/209539 (31%)]\tAll Loss: 1.3003\tTriple Loss(1): 0.0000\tClassification Loss: 1.3003\r\n",
      "Train Epoch: 34 [65280/209539 (31%)]\tAll Loss: 1.2164\tTriple Loss(1): 0.0303\tClassification Loss: 1.1559\r\n",
      "Train Epoch: 34 [65920/209539 (31%)]\tAll Loss: 1.4452\tTriple Loss(1): 0.0223\tClassification Loss: 1.4005\r\n",
      "Train Epoch: 34 [66560/209539 (32%)]\tAll Loss: 1.3131\tTriple Loss(1): 0.0521\tClassification Loss: 1.2090\r\n",
      "Train Epoch: 34 [67200/209539 (32%)]\tAll Loss: 1.7877\tTriple Loss(0): 0.3951\tClassification Loss: 0.9976\r\n",
      "Train Epoch: 34 [67840/209539 (32%)]\tAll Loss: 1.4560\tTriple Loss(1): 0.0674\tClassification Loss: 1.3212\r\n",
      "Train Epoch: 34 [68480/209539 (33%)]\tAll Loss: 1.3547\tTriple Loss(1): 0.0091\tClassification Loss: 1.3366\r\n",
      "Train Epoch: 34 [69120/209539 (33%)]\tAll Loss: 1.3603\tTriple Loss(1): 0.0000\tClassification Loss: 1.3603\r\n",
      "Train Epoch: 34 [69760/209539 (33%)]\tAll Loss: 0.9457\tTriple Loss(1): 0.0101\tClassification Loss: 0.9255\r\n",
      "Train Epoch: 34 [70400/209539 (34%)]\tAll Loss: 1.2051\tTriple Loss(1): 0.0703\tClassification Loss: 1.0644\r\n",
      "Train Epoch: 34 [71040/209539 (34%)]\tAll Loss: 1.5535\tTriple Loss(1): 0.0241\tClassification Loss: 1.5052\r\n",
      "Train Epoch: 34 [71680/209539 (34%)]\tAll Loss: 1.5392\tTriple Loss(1): 0.0035\tClassification Loss: 1.5322\r\n",
      "Train Epoch: 34 [72320/209539 (35%)]\tAll Loss: 1.0960\tTriple Loss(1): 0.0190\tClassification Loss: 1.0580\r\n",
      "Train Epoch: 34 [72960/209539 (35%)]\tAll Loss: 1.2835\tTriple Loss(1): 0.0274\tClassification Loss: 1.2288\r\n",
      "Train Epoch: 34 [73600/209539 (35%)]\tAll Loss: 1.2540\tTriple Loss(1): 0.0197\tClassification Loss: 1.2146\r\n",
      "Train Epoch: 34 [74240/209539 (35%)]\tAll Loss: 2.1600\tTriple Loss(0): 0.3130\tClassification Loss: 1.5339\r\n",
      "Train Epoch: 34 [74880/209539 (36%)]\tAll Loss: 1.5757\tTriple Loss(1): 0.0214\tClassification Loss: 1.5328\r\n",
      "Train Epoch: 34 [75520/209539 (36%)]\tAll Loss: 1.0580\tTriple Loss(1): 0.0373\tClassification Loss: 0.9834\r\n",
      "Train Epoch: 34 [76160/209539 (36%)]\tAll Loss: 1.2357\tTriple Loss(1): 0.0682\tClassification Loss: 1.0993\r\n",
      "Train Epoch: 34 [76800/209539 (37%)]\tAll Loss: 0.9601\tTriple Loss(1): 0.0508\tClassification Loss: 0.8584\r\n",
      "Train Epoch: 34 [77440/209539 (37%)]\tAll Loss: 1.1634\tTriple Loss(1): 0.0419\tClassification Loss: 1.0796\r\n",
      "Train Epoch: 34 [78080/209539 (37%)]\tAll Loss: 1.2537\tTriple Loss(1): 0.0026\tClassification Loss: 1.2485\r\n",
      "Train Epoch: 34 [78720/209539 (38%)]\tAll Loss: 2.5016\tTriple Loss(0): 0.5393\tClassification Loss: 1.4231\r\n",
      "Train Epoch: 34 [79360/209539 (38%)]\tAll Loss: 1.9761\tTriple Loss(0): 0.4011\tClassification Loss: 1.1740\r\n",
      "Train Epoch: 34 [80000/209539 (38%)]\tAll Loss: 1.3879\tTriple Loss(1): 0.0670\tClassification Loss: 1.2539\r\n",
      "Train Epoch: 34 [80640/209539 (38%)]\tAll Loss: 1.4253\tTriple Loss(1): 0.0687\tClassification Loss: 1.2878\r\n",
      "Train Epoch: 34 [81280/209539 (39%)]\tAll Loss: 1.3670\tTriple Loss(1): 0.0494\tClassification Loss: 1.2681\r\n",
      "Train Epoch: 34 [81920/209539 (39%)]\tAll Loss: 1.5895\tTriple Loss(0): 0.2516\tClassification Loss: 1.0862\r\n",
      "Train Epoch: 34 [82560/209539 (39%)]\tAll Loss: 1.1557\tTriple Loss(1): 0.0151\tClassification Loss: 1.1254\r\n",
      "Train Epoch: 34 [83200/209539 (40%)]\tAll Loss: 2.2651\tTriple Loss(0): 0.4488\tClassification Loss: 1.3674\r\n",
      "Train Epoch: 34 [83840/209539 (40%)]\tAll Loss: 1.5007\tTriple Loss(1): 0.0982\tClassification Loss: 1.3044\r\n",
      "Train Epoch: 34 [84480/209539 (40%)]\tAll Loss: 1.2060\tTriple Loss(1): 0.0000\tClassification Loss: 1.2060\r\n",
      "Train Epoch: 34 [85120/209539 (41%)]\tAll Loss: 1.6447\tTriple Loss(1): 0.0138\tClassification Loss: 1.6170\r\n",
      "Train Epoch: 34 [85760/209539 (41%)]\tAll Loss: 1.5502\tTriple Loss(0): 0.1656\tClassification Loss: 1.2189\r\n",
      "Train Epoch: 34 [86400/209539 (41%)]\tAll Loss: 1.6322\tTriple Loss(0): 0.1801\tClassification Loss: 1.2721\r\n",
      "Train Epoch: 34 [87040/209539 (42%)]\tAll Loss: 1.0577\tTriple Loss(1): 0.0154\tClassification Loss: 1.0269\r\n",
      "Train Epoch: 34 [87680/209539 (42%)]\tAll Loss: 1.2003\tTriple Loss(1): 0.0157\tClassification Loss: 1.1690\r\n",
      "Train Epoch: 34 [88320/209539 (42%)]\tAll Loss: 1.1310\tTriple Loss(1): 0.0251\tClassification Loss: 1.0809\r\n",
      "Train Epoch: 34 [88960/209539 (42%)]\tAll Loss: 1.2600\tTriple Loss(1): 0.0434\tClassification Loss: 1.1732\r\n",
      "Train Epoch: 34 [89600/209539 (43%)]\tAll Loss: 1.3518\tTriple Loss(1): 0.0000\tClassification Loss: 1.3518\r\n",
      "Train Epoch: 34 [90240/209539 (43%)]\tAll Loss: 1.7288\tTriple Loss(0): 0.3166\tClassification Loss: 1.0956\r\n",
      "Train Epoch: 34 [90880/209539 (43%)]\tAll Loss: 1.4594\tTriple Loss(1): 0.0000\tClassification Loss: 1.4594\r\n",
      "Train Epoch: 34 [91520/209539 (44%)]\tAll Loss: 1.9935\tTriple Loss(0): 0.4991\tClassification Loss: 0.9954\r\n",
      "Train Epoch: 34 [92160/209539 (44%)]\tAll Loss: 1.4721\tTriple Loss(0): 0.2186\tClassification Loss: 1.0349\r\n",
      "Train Epoch: 34 [92800/209539 (44%)]\tAll Loss: 1.8034\tTriple Loss(0): 0.3235\tClassification Loss: 1.1563\r\n",
      "Train Epoch: 34 [93440/209539 (45%)]\tAll Loss: 1.3859\tTriple Loss(1): 0.0317\tClassification Loss: 1.3224\r\n",
      "Train Epoch: 34 [94080/209539 (45%)]\tAll Loss: 1.1672\tTriple Loss(1): 0.0077\tClassification Loss: 1.1519\r\n",
      "Train Epoch: 34 [94720/209539 (45%)]\tAll Loss: 1.2751\tTriple Loss(1): 0.0000\tClassification Loss: 1.2751\r\n",
      "Train Epoch: 34 [95360/209539 (46%)]\tAll Loss: 1.1664\tTriple Loss(1): 0.0000\tClassification Loss: 1.1664\r\n",
      "Train Epoch: 34 [96000/209539 (46%)]\tAll Loss: 1.5017\tTriple Loss(1): 0.0499\tClassification Loss: 1.4018\r\n",
      "Train Epoch: 34 [96640/209539 (46%)]\tAll Loss: 2.0260\tTriple Loss(0): 0.2934\tClassification Loss: 1.4392\r\n",
      "Train Epoch: 34 [97280/209539 (46%)]\tAll Loss: 1.2876\tTriple Loss(1): 0.0090\tClassification Loss: 1.2696\r\n",
      "Train Epoch: 34 [97920/209539 (47%)]\tAll Loss: 1.3226\tTriple Loss(1): 0.0000\tClassification Loss: 1.3226\r\n",
      "Train Epoch: 34 [98560/209539 (47%)]\tAll Loss: 1.2842\tTriple Loss(1): 0.0084\tClassification Loss: 1.2675\r\n",
      "Train Epoch: 34 [99200/209539 (47%)]\tAll Loss: 1.2097\tTriple Loss(1): 0.0000\tClassification Loss: 1.2097\r\n",
      "Train Epoch: 34 [99840/209539 (48%)]\tAll Loss: 1.2595\tTriple Loss(1): 0.0000\tClassification Loss: 1.2595\r\n",
      "Train Epoch: 34 [100480/209539 (48%)]\tAll Loss: 1.3223\tTriple Loss(1): 0.1020\tClassification Loss: 1.1183\r\n",
      "Train Epoch: 34 [101120/209539 (48%)]\tAll Loss: 1.1937\tTriple Loss(1): 0.0293\tClassification Loss: 1.1351\r\n",
      "Train Epoch: 34 [101760/209539 (49%)]\tAll Loss: 1.2024\tTriple Loss(1): 0.0315\tClassification Loss: 1.1394\r\n",
      "Train Epoch: 34 [102400/209539 (49%)]\tAll Loss: 1.0266\tTriple Loss(1): 0.0139\tClassification Loss: 0.9987\r\n",
      "Train Epoch: 34 [103040/209539 (49%)]\tAll Loss: 1.0536\tTriple Loss(1): 0.0158\tClassification Loss: 1.0219\r\n",
      "Train Epoch: 34 [103680/209539 (49%)]\tAll Loss: 1.4199\tTriple Loss(1): 0.0127\tClassification Loss: 1.3944\r\n",
      "Train Epoch: 34 [104320/209539 (50%)]\tAll Loss: 0.9779\tTriple Loss(1): 0.0491\tClassification Loss: 0.8798\r\n",
      "Train Epoch: 34 [104960/209539 (50%)]\tAll Loss: 1.0571\tTriple Loss(1): 0.0367\tClassification Loss: 0.9836\r\n",
      "Train Epoch: 34 [105600/209539 (50%)]\tAll Loss: 1.3814\tTriple Loss(1): 0.0710\tClassification Loss: 1.2394\r\n",
      "Train Epoch: 34 [106240/209539 (51%)]\tAll Loss: 1.8961\tTriple Loss(0): 0.3460\tClassification Loss: 1.2041\r\n",
      "Train Epoch: 34 [106880/209539 (51%)]\tAll Loss: 1.0173\tTriple Loss(1): 0.0139\tClassification Loss: 0.9894\r\n",
      "Train Epoch: 34 [107520/209539 (51%)]\tAll Loss: 1.1777\tTriple Loss(1): 0.0078\tClassification Loss: 1.1620\r\n",
      "Train Epoch: 34 [108160/209539 (52%)]\tAll Loss: 1.6924\tTriple Loss(0): 0.2435\tClassification Loss: 1.2054\r\n",
      "Train Epoch: 34 [108800/209539 (52%)]\tAll Loss: 1.1214\tTriple Loss(1): 0.0000\tClassification Loss: 1.1214\r\n",
      "Train Epoch: 34 [109440/209539 (52%)]\tAll Loss: 1.9893\tTriple Loss(0): 0.2650\tClassification Loss: 1.4594\r\n",
      "Train Epoch: 34 [110080/209539 (53%)]\tAll Loss: 1.5005\tTriple Loss(1): 0.0000\tClassification Loss: 1.5005\r\n",
      "Train Epoch: 34 [110720/209539 (53%)]\tAll Loss: 1.3368\tTriple Loss(1): 0.0652\tClassification Loss: 1.2064\r\n",
      "Train Epoch: 34 [111360/209539 (53%)]\tAll Loss: 0.9892\tTriple Loss(1): 0.0614\tClassification Loss: 0.8664\r\n",
      "Train Epoch: 34 [112000/209539 (53%)]\tAll Loss: 1.1075\tTriple Loss(1): 0.0400\tClassification Loss: 1.0274\r\n",
      "Train Epoch: 34 [112640/209539 (54%)]\tAll Loss: 1.2047\tTriple Loss(1): 0.0000\tClassification Loss: 1.2047\r\n",
      "Train Epoch: 34 [113280/209539 (54%)]\tAll Loss: 1.3790\tTriple Loss(0): 0.2415\tClassification Loss: 0.8960\r\n",
      "Train Epoch: 34 [113920/209539 (54%)]\tAll Loss: 1.2695\tTriple Loss(1): 0.0364\tClassification Loss: 1.1967\r\n",
      "Train Epoch: 34 [114560/209539 (55%)]\tAll Loss: 1.2001\tTriple Loss(1): 0.0059\tClassification Loss: 1.1884\r\n",
      "Train Epoch: 34 [115200/209539 (55%)]\tAll Loss: 1.4460\tTriple Loss(1): 0.0126\tClassification Loss: 1.4208\r\n",
      "Train Epoch: 34 [115840/209539 (55%)]\tAll Loss: 1.3244\tTriple Loss(1): 0.0167\tClassification Loss: 1.2910\r\n",
      "Train Epoch: 34 [116480/209539 (56%)]\tAll Loss: 1.5479\tTriple Loss(1): 0.0660\tClassification Loss: 1.4160\r\n",
      "Train Epoch: 34 [117120/209539 (56%)]\tAll Loss: 1.9501\tTriple Loss(0): 0.2441\tClassification Loss: 1.4619\r\n",
      "Train Epoch: 34 [117760/209539 (56%)]\tAll Loss: 1.7161\tTriple Loss(0): 0.2156\tClassification Loss: 1.2849\r\n",
      "Train Epoch: 34 [118400/209539 (57%)]\tAll Loss: 0.8448\tTriple Loss(1): 0.0088\tClassification Loss: 0.8271\r\n",
      "Train Epoch: 34 [119040/209539 (57%)]\tAll Loss: 1.1474\tTriple Loss(1): 0.0000\tClassification Loss: 1.1474\r\n",
      "Train Epoch: 34 [119680/209539 (57%)]\tAll Loss: 1.2901\tTriple Loss(1): 0.0221\tClassification Loss: 1.2459\r\n",
      "Train Epoch: 34 [120320/209539 (57%)]\tAll Loss: 1.2775\tTriple Loss(1): 0.0000\tClassification Loss: 1.2775\r\n",
      "Train Epoch: 34 [120960/209539 (58%)]\tAll Loss: 1.0101\tTriple Loss(1): 0.0126\tClassification Loss: 0.9850\r\n",
      "Train Epoch: 34 [121600/209539 (58%)]\tAll Loss: 1.2279\tTriple Loss(1): 0.0241\tClassification Loss: 1.1796\r\n",
      "Train Epoch: 34 [122240/209539 (58%)]\tAll Loss: 1.2118\tTriple Loss(1): 0.0208\tClassification Loss: 1.1702\r\n",
      "Train Epoch: 34 [122880/209539 (59%)]\tAll Loss: 1.2214\tTriple Loss(1): 0.0446\tClassification Loss: 1.1321\r\n",
      "Train Epoch: 34 [123520/209539 (59%)]\tAll Loss: 1.4739\tTriple Loss(0): 0.1443\tClassification Loss: 1.1852\r\n",
      "Train Epoch: 34 [124160/209539 (59%)]\tAll Loss: 1.5823\tTriple Loss(0): 0.1992\tClassification Loss: 1.1839\r\n",
      "Train Epoch: 34 [124800/209539 (60%)]\tAll Loss: 0.9987\tTriple Loss(1): 0.0168\tClassification Loss: 0.9652\r\n",
      "Train Epoch: 34 [125440/209539 (60%)]\tAll Loss: 1.4233\tTriple Loss(1): 0.0884\tClassification Loss: 1.2466\r\n",
      "Train Epoch: 34 [126080/209539 (60%)]\tAll Loss: 1.2961\tTriple Loss(1): 0.0011\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 34 [126720/209539 (60%)]\tAll Loss: 1.8959\tTriple Loss(0): 0.2548\tClassification Loss: 1.3863\r\n",
      "Train Epoch: 34 [127360/209539 (61%)]\tAll Loss: 1.4023\tTriple Loss(1): 0.0118\tClassification Loss: 1.3787\r\n",
      "Train Epoch: 34 [128000/209539 (61%)]\tAll Loss: 1.2950\tTriple Loss(1): 0.0311\tClassification Loss: 1.2328\r\n",
      "Train Epoch: 34 [128640/209539 (61%)]\tAll Loss: 1.0863\tTriple Loss(1): 0.0106\tClassification Loss: 1.0651\r\n",
      "Train Epoch: 34 [129280/209539 (62%)]\tAll Loss: 1.4033\tTriple Loss(1): 0.0538\tClassification Loss: 1.2956\r\n",
      "Train Epoch: 34 [129920/209539 (62%)]\tAll Loss: 1.1978\tTriple Loss(1): 0.0265\tClassification Loss: 1.1448\r\n",
      "Train Epoch: 34 [130560/209539 (62%)]\tAll Loss: 1.4781\tTriple Loss(0): 0.2792\tClassification Loss: 0.9198\r\n",
      "Train Epoch: 34 [131200/209539 (63%)]\tAll Loss: 1.2323\tTriple Loss(1): 0.0535\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 34 [131840/209539 (63%)]\tAll Loss: 1.3373\tTriple Loss(1): 0.0420\tClassification Loss: 1.2533\r\n",
      "Train Epoch: 34 [132480/209539 (63%)]\tAll Loss: 1.1284\tTriple Loss(1): 0.0296\tClassification Loss: 1.0691\r\n",
      "Train Epoch: 34 [133120/209539 (64%)]\tAll Loss: 1.2943\tTriple Loss(1): 0.0286\tClassification Loss: 1.2370\r\n",
      "Train Epoch: 34 [133760/209539 (64%)]\tAll Loss: 1.3506\tTriple Loss(1): 0.0274\tClassification Loss: 1.2957\r\n",
      "Train Epoch: 34 [134400/209539 (64%)]\tAll Loss: 0.9801\tTriple Loss(1): 0.0193\tClassification Loss: 0.9415\r\n",
      "Train Epoch: 34 [135040/209539 (64%)]\tAll Loss: 1.9023\tTriple Loss(0): 0.3238\tClassification Loss: 1.2548\r\n",
      "Train Epoch: 34 [135680/209539 (65%)]\tAll Loss: 1.9415\tTriple Loss(0): 0.2144\tClassification Loss: 1.5128\r\n",
      "Train Epoch: 34 [136320/209539 (65%)]\tAll Loss: 1.2510\tTriple Loss(1): 0.0037\tClassification Loss: 1.2437\r\n",
      "Train Epoch: 34 [136960/209539 (65%)]\tAll Loss: 1.2109\tTriple Loss(1): 0.0085\tClassification Loss: 1.1939\r\n",
      "Train Epoch: 34 [137600/209539 (66%)]\tAll Loss: 1.1557\tTriple Loss(1): 0.0224\tClassification Loss: 1.1110\r\n",
      "Train Epoch: 34 [138240/209539 (66%)]\tAll Loss: 1.4546\tTriple Loss(1): 0.0247\tClassification Loss: 1.4051\r\n",
      "Train Epoch: 34 [138880/209539 (66%)]\tAll Loss: 1.2351\tTriple Loss(1): 0.0000\tClassification Loss: 1.2351\r\n",
      "Train Epoch: 34 [139520/209539 (67%)]\tAll Loss: 1.1626\tTriple Loss(1): 0.0000\tClassification Loss: 1.1626\r\n",
      "Train Epoch: 34 [140160/209539 (67%)]\tAll Loss: 1.5061\tTriple Loss(1): 0.1089\tClassification Loss: 1.2884\r\n",
      "Train Epoch: 34 [140800/209539 (67%)]\tAll Loss: 1.3854\tTriple Loss(1): 0.0087\tClassification Loss: 1.3680\r\n",
      "Train Epoch: 34 [141440/209539 (68%)]\tAll Loss: 1.2033\tTriple Loss(1): 0.0201\tClassification Loss: 1.1632\r\n",
      "Train Epoch: 34 [142080/209539 (68%)]\tAll Loss: 1.0836\tTriple Loss(1): 0.0000\tClassification Loss: 1.0836\r\n",
      "Train Epoch: 34 [142720/209539 (68%)]\tAll Loss: 1.7272\tTriple Loss(1): 0.0279\tClassification Loss: 1.6714\r\n",
      "Train Epoch: 34 [143360/209539 (68%)]\tAll Loss: 0.9504\tTriple Loss(1): 0.0000\tClassification Loss: 0.9504\r\n",
      "Train Epoch: 34 [144000/209539 (69%)]\tAll Loss: 1.3375\tTriple Loss(1): 0.0283\tClassification Loss: 1.2810\r\n",
      "Train Epoch: 34 [144640/209539 (69%)]\tAll Loss: 1.2623\tTriple Loss(1): 0.0249\tClassification Loss: 1.2125\r\n",
      "Train Epoch: 34 [145280/209539 (69%)]\tAll Loss: 1.3020\tTriple Loss(1): 0.0000\tClassification Loss: 1.3020\r\n",
      "Train Epoch: 34 [145920/209539 (70%)]\tAll Loss: 1.5578\tTriple Loss(0): 0.2443\tClassification Loss: 1.0692\r\n",
      "Train Epoch: 34 [146560/209539 (70%)]\tAll Loss: 1.0744\tTriple Loss(1): 0.0766\tClassification Loss: 0.9212\r\n",
      "Train Epoch: 34 [147200/209539 (70%)]\tAll Loss: 1.7451\tTriple Loss(0): 0.2324\tClassification Loss: 1.2803\r\n",
      "Train Epoch: 34 [147840/209539 (71%)]\tAll Loss: 0.9431\tTriple Loss(1): 0.0000\tClassification Loss: 0.9431\r\n",
      "Train Epoch: 34 [148480/209539 (71%)]\tAll Loss: 0.9994\tTriple Loss(1): 0.0115\tClassification Loss: 0.9763\r\n",
      "Train Epoch: 34 [149120/209539 (71%)]\tAll Loss: 1.3796\tTriple Loss(1): 0.0223\tClassification Loss: 1.3350\r\n",
      "Train Epoch: 34 [149760/209539 (71%)]\tAll Loss: 1.1588\tTriple Loss(1): 0.0043\tClassification Loss: 1.1502\r\n",
      "Train Epoch: 34 [150400/209539 (72%)]\tAll Loss: 1.8809\tTriple Loss(0): 0.2068\tClassification Loss: 1.4672\r\n",
      "Train Epoch: 34 [151040/209539 (72%)]\tAll Loss: 1.1353\tTriple Loss(1): 0.0030\tClassification Loss: 1.1293\r\n",
      "Train Epoch: 34 [151680/209539 (72%)]\tAll Loss: 1.4258\tTriple Loss(1): 0.0234\tClassification Loss: 1.3790\r\n",
      "Train Epoch: 34 [152320/209539 (73%)]\tAll Loss: 1.1138\tTriple Loss(1): 0.0037\tClassification Loss: 1.1063\r\n",
      "Train Epoch: 34 [152960/209539 (73%)]\tAll Loss: 1.1416\tTriple Loss(1): 0.0143\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 34 [153600/209539 (73%)]\tAll Loss: 1.1236\tTriple Loss(1): 0.0000\tClassification Loss: 1.1236\r\n",
      "Train Epoch: 34 [154240/209539 (74%)]\tAll Loss: 1.1226\tTriple Loss(1): 0.0321\tClassification Loss: 1.0584\r\n",
      "Train Epoch: 34 [154880/209539 (74%)]\tAll Loss: 1.5249\tTriple Loss(1): 0.1199\tClassification Loss: 1.2851\r\n",
      "Train Epoch: 34 [155520/209539 (74%)]\tAll Loss: 1.1752\tTriple Loss(1): 0.0000\tClassification Loss: 1.1752\r\n",
      "Train Epoch: 34 [156160/209539 (75%)]\tAll Loss: 1.1110\tTriple Loss(1): 0.0033\tClassification Loss: 1.1044\r\n",
      "Train Epoch: 34 [156800/209539 (75%)]\tAll Loss: 1.7528\tTriple Loss(1): 0.0446\tClassification Loss: 1.6636\r\n",
      "Train Epoch: 34 [157440/209539 (75%)]\tAll Loss: 1.5016\tTriple Loss(1): 0.0581\tClassification Loss: 1.3854\r\n",
      "Train Epoch: 34 [158080/209539 (75%)]\tAll Loss: 1.7051\tTriple Loss(0): 0.2785\tClassification Loss: 1.1481\r\n",
      "Train Epoch: 34 [158720/209539 (76%)]\tAll Loss: 1.1204\tTriple Loss(1): 0.0457\tClassification Loss: 1.0289\r\n",
      "Train Epoch: 34 [159360/209539 (76%)]\tAll Loss: 1.6165\tTriple Loss(0): 0.3206\tClassification Loss: 0.9754\r\n",
      "Train Epoch: 34 [160000/209539 (76%)]\tAll Loss: 1.4685\tTriple Loss(1): 0.0378\tClassification Loss: 1.3928\r\n",
      "Train Epoch: 34 [160640/209539 (77%)]\tAll Loss: 0.8959\tTriple Loss(1): 0.0000\tClassification Loss: 0.8959\r\n",
      "Train Epoch: 34 [161280/209539 (77%)]\tAll Loss: 1.1149\tTriple Loss(1): 0.0588\tClassification Loss: 0.9973\r\n",
      "Train Epoch: 34 [161920/209539 (77%)]\tAll Loss: 1.3468\tTriple Loss(1): 0.0552\tClassification Loss: 1.2364\r\n",
      "Train Epoch: 34 [162560/209539 (78%)]\tAll Loss: 1.7781\tTriple Loss(0): 0.2925\tClassification Loss: 1.1931\r\n",
      "Train Epoch: 34 [163200/209539 (78%)]\tAll Loss: 1.6750\tTriple Loss(0): 0.3419\tClassification Loss: 0.9912\r\n",
      "Train Epoch: 34 [163840/209539 (78%)]\tAll Loss: 1.1242\tTriple Loss(1): 0.0065\tClassification Loss: 1.1111\r\n",
      "Train Epoch: 34 [164480/209539 (78%)]\tAll Loss: 0.9730\tTriple Loss(1): 0.0008\tClassification Loss: 0.9714\r\n",
      "Train Epoch: 34 [165120/209539 (79%)]\tAll Loss: 0.9585\tTriple Loss(1): 0.0324\tClassification Loss: 0.8936\r\n",
      "Train Epoch: 34 [165760/209539 (79%)]\tAll Loss: 1.2859\tTriple Loss(1): 0.0175\tClassification Loss: 1.2508\r\n",
      "Train Epoch: 34 [166400/209539 (79%)]\tAll Loss: 1.2992\tTriple Loss(1): 0.0302\tClassification Loss: 1.2388\r\n",
      "Train Epoch: 34 [167040/209539 (80%)]\tAll Loss: 1.3034\tTriple Loss(1): 0.0000\tClassification Loss: 1.3034\r\n",
      "Train Epoch: 34 [167680/209539 (80%)]\tAll Loss: 1.3567\tTriple Loss(1): 0.0732\tClassification Loss: 1.2104\r\n",
      "Train Epoch: 34 [168320/209539 (80%)]\tAll Loss: 1.0946\tTriple Loss(1): 0.0292\tClassification Loss: 1.0363\r\n",
      "Train Epoch: 34 [168960/209539 (81%)]\tAll Loss: 1.0431\tTriple Loss(1): 0.0075\tClassification Loss: 1.0281\r\n",
      "Train Epoch: 34 [169600/209539 (81%)]\tAll Loss: 1.3058\tTriple Loss(1): 0.0000\tClassification Loss: 1.3058\r\n",
      "Train Epoch: 34 [170240/209539 (81%)]\tAll Loss: 1.5239\tTriple Loss(0): 0.2434\tClassification Loss: 1.0371\r\n",
      "Train Epoch: 34 [170880/209539 (82%)]\tAll Loss: 1.2207\tTriple Loss(1): 0.0057\tClassification Loss: 1.2094\r\n",
      "Train Epoch: 34 [171520/209539 (82%)]\tAll Loss: 1.1176\tTriple Loss(1): 0.0320\tClassification Loss: 1.0536\r\n",
      "Train Epoch: 34 [172160/209539 (82%)]\tAll Loss: 1.3921\tTriple Loss(1): 0.0000\tClassification Loss: 1.3921\r\n",
      "Train Epoch: 34 [172800/209539 (82%)]\tAll Loss: 1.5632\tTriple Loss(1): 0.0646\tClassification Loss: 1.4340\r\n",
      "Train Epoch: 34 [173440/209539 (83%)]\tAll Loss: 1.3920\tTriple Loss(1): 0.0295\tClassification Loss: 1.3329\r\n",
      "Train Epoch: 34 [174080/209539 (83%)]\tAll Loss: 1.0748\tTriple Loss(1): 0.0000\tClassification Loss: 1.0748\r\n",
      "Train Epoch: 34 [174720/209539 (83%)]\tAll Loss: 1.1246\tTriple Loss(1): 0.0172\tClassification Loss: 1.0902\r\n",
      "Train Epoch: 34 [175360/209539 (84%)]\tAll Loss: 1.2252\tTriple Loss(1): 0.0113\tClassification Loss: 1.2026\r\n",
      "Train Epoch: 34 [176000/209539 (84%)]\tAll Loss: 1.5425\tTriple Loss(0): 0.2075\tClassification Loss: 1.1274\r\n",
      "Train Epoch: 34 [176640/209539 (84%)]\tAll Loss: 0.8701\tTriple Loss(1): 0.0006\tClassification Loss: 0.8689\r\n",
      "Train Epoch: 34 [177280/209539 (85%)]\tAll Loss: 1.4128\tTriple Loss(1): 0.0152\tClassification Loss: 1.3823\r\n",
      "Train Epoch: 34 [177920/209539 (85%)]\tAll Loss: 1.5139\tTriple Loss(1): 0.0094\tClassification Loss: 1.4950\r\n",
      "Train Epoch: 34 [178560/209539 (85%)]\tAll Loss: 1.1761\tTriple Loss(1): 0.0721\tClassification Loss: 1.0319\r\n",
      "Train Epoch: 34 [179200/209539 (86%)]\tAll Loss: 2.0714\tTriple Loss(0): 0.3819\tClassification Loss: 1.3076\r\n",
      "Train Epoch: 34 [179840/209539 (86%)]\tAll Loss: 1.3557\tTriple Loss(1): 0.0006\tClassification Loss: 1.3545\r\n",
      "Train Epoch: 34 [180480/209539 (86%)]\tAll Loss: 1.4467\tTriple Loss(1): 0.0281\tClassification Loss: 1.3905\r\n",
      "Train Epoch: 34 [181120/209539 (86%)]\tAll Loss: 1.3513\tTriple Loss(1): 0.0229\tClassification Loss: 1.3056\r\n",
      "Train Epoch: 34 [181760/209539 (87%)]\tAll Loss: 1.2876\tTriple Loss(0): 0.0995\tClassification Loss: 1.0886\r\n",
      "Train Epoch: 34 [182400/209539 (87%)]\tAll Loss: 1.5201\tTriple Loss(1): 0.0153\tClassification Loss: 1.4896\r\n",
      "Train Epoch: 34 [183040/209539 (87%)]\tAll Loss: 1.8622\tTriple Loss(0): 0.2846\tClassification Loss: 1.2931\r\n",
      "Train Epoch: 34 [183680/209539 (88%)]\tAll Loss: 0.9876\tTriple Loss(1): 0.0134\tClassification Loss: 0.9607\r\n",
      "Train Epoch: 34 [184320/209539 (88%)]\tAll Loss: 1.1091\tTriple Loss(1): 0.0263\tClassification Loss: 1.0564\r\n",
      "Train Epoch: 34 [184960/209539 (88%)]\tAll Loss: 1.0125\tTriple Loss(1): 0.0105\tClassification Loss: 0.9916\r\n",
      "Train Epoch: 34 [185600/209539 (89%)]\tAll Loss: 1.9552\tTriple Loss(0): 0.3616\tClassification Loss: 1.2320\r\n",
      "Train Epoch: 34 [186240/209539 (89%)]\tAll Loss: 1.5973\tTriple Loss(1): 0.0987\tClassification Loss: 1.3999\r\n",
      "Train Epoch: 34 [186880/209539 (89%)]\tAll Loss: 1.2373\tTriple Loss(1): 0.0000\tClassification Loss: 1.2373\r\n",
      "Train Epoch: 34 [187520/209539 (89%)]\tAll Loss: 1.3415\tTriple Loss(1): 0.0000\tClassification Loss: 1.3415\r\n",
      "Train Epoch: 34 [188160/209539 (90%)]\tAll Loss: 1.0500\tTriple Loss(1): 0.0000\tClassification Loss: 1.0500\r\n",
      "Train Epoch: 34 [188800/209539 (90%)]\tAll Loss: 1.5349\tTriple Loss(0): 0.2434\tClassification Loss: 1.0481\r\n",
      "Train Epoch: 34 [189440/209539 (90%)]\tAll Loss: 1.1113\tTriple Loss(1): 0.0000\tClassification Loss: 1.1113\r\n",
      "Train Epoch: 34 [190080/209539 (91%)]\tAll Loss: 1.0948\tTriple Loss(1): 0.0000\tClassification Loss: 1.0948\r\n",
      "Train Epoch: 34 [190720/209539 (91%)]\tAll Loss: 1.1434\tTriple Loss(1): 0.0091\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 34 [191360/209539 (91%)]\tAll Loss: 1.0621\tTriple Loss(1): 0.0165\tClassification Loss: 1.0292\r\n",
      "Train Epoch: 34 [192000/209539 (92%)]\tAll Loss: 1.5441\tTriple Loss(1): 0.0251\tClassification Loss: 1.4940\r\n",
      "Train Epoch: 34 [192640/209539 (92%)]\tAll Loss: 1.6550\tTriple Loss(0): 0.2436\tClassification Loss: 1.1678\r\n",
      "Train Epoch: 34 [193280/209539 (92%)]\tAll Loss: 1.2791\tTriple Loss(1): 0.0438\tClassification Loss: 1.1915\r\n",
      "Train Epoch: 34 [193920/209539 (93%)]\tAll Loss: 1.2167\tTriple Loss(1): 0.0190\tClassification Loss: 1.1787\r\n",
      "Train Epoch: 34 [194560/209539 (93%)]\tAll Loss: 1.0133\tTriple Loss(1): 0.0191\tClassification Loss: 0.9752\r\n",
      "Train Epoch: 34 [195200/209539 (93%)]\tAll Loss: 1.2461\tTriple Loss(1): 0.0000\tClassification Loss: 1.2461\r\n",
      "Train Epoch: 34 [195840/209539 (93%)]\tAll Loss: 1.4097\tTriple Loss(0): 0.3403\tClassification Loss: 0.7291\r\n",
      "Train Epoch: 34 [196480/209539 (94%)]\tAll Loss: 1.9226\tTriple Loss(0): 0.2812\tClassification Loss: 1.3602\r\n",
      "Train Epoch: 34 [197120/209539 (94%)]\tAll Loss: 1.0729\tTriple Loss(1): 0.0046\tClassification Loss: 1.0637\r\n",
      "Train Epoch: 34 [197760/209539 (94%)]\tAll Loss: 2.1786\tTriple Loss(0): 0.5014\tClassification Loss: 1.1758\r\n",
      "Train Epoch: 34 [198400/209539 (95%)]\tAll Loss: 1.8874\tTriple Loss(0): 0.4515\tClassification Loss: 0.9844\r\n",
      "Train Epoch: 34 [199040/209539 (95%)]\tAll Loss: 1.3469\tTriple Loss(1): 0.0000\tClassification Loss: 1.3469\r\n",
      "Train Epoch: 34 [199680/209539 (95%)]\tAll Loss: 1.8101\tTriple Loss(0): 0.3293\tClassification Loss: 1.1515\r\n",
      "Train Epoch: 34 [200320/209539 (96%)]\tAll Loss: 1.4764\tTriple Loss(1): 0.0603\tClassification Loss: 1.3558\r\n",
      "Train Epoch: 34 [200960/209539 (96%)]\tAll Loss: 0.9870\tTriple Loss(1): 0.0422\tClassification Loss: 0.9026\r\n",
      "Train Epoch: 34 [201600/209539 (96%)]\tAll Loss: 1.0421\tTriple Loss(1): 0.0000\tClassification Loss: 1.0421\r\n",
      "Train Epoch: 34 [202240/209539 (97%)]\tAll Loss: 1.2406\tTriple Loss(1): 0.0000\tClassification Loss: 1.2406\r\n",
      "Train Epoch: 34 [202880/209539 (97%)]\tAll Loss: 1.0579\tTriple Loss(1): 0.0044\tClassification Loss: 1.0491\r\n",
      "Train Epoch: 34 [203520/209539 (97%)]\tAll Loss: 1.3806\tTriple Loss(1): 0.0456\tClassification Loss: 1.2894\r\n",
      "Train Epoch: 34 [204160/209539 (97%)]\tAll Loss: 1.5904\tTriple Loss(1): 0.0260\tClassification Loss: 1.5384\r\n",
      "Train Epoch: 34 [204800/209539 (98%)]\tAll Loss: 1.5408\tTriple Loss(1): 0.0101\tClassification Loss: 1.5207\r\n",
      "Train Epoch: 34 [205440/209539 (98%)]\tAll Loss: 1.0378\tTriple Loss(1): 0.0614\tClassification Loss: 0.9149\r\n",
      "Train Epoch: 34 [206080/209539 (98%)]\tAll Loss: 1.1459\tTriple Loss(1): 0.0065\tClassification Loss: 1.1330\r\n",
      "Train Epoch: 34 [206720/209539 (99%)]\tAll Loss: 1.0369\tTriple Loss(1): 0.0034\tClassification Loss: 1.0302\r\n",
      "Train Epoch: 34 [207360/209539 (99%)]\tAll Loss: 1.0485\tTriple Loss(1): 0.0344\tClassification Loss: 0.9797\r\n",
      "Train Epoch: 34 [208000/209539 (99%)]\tAll Loss: 1.0492\tTriple Loss(1): 0.0040\tClassification Loss: 1.0412\r\n",
      "Train Epoch: 34 [208640/209539 (100%)]\tAll Loss: 1.1439\tTriple Loss(1): 0.0000\tClassification Loss: 1.1439\r\n",
      "Train Epoch: 34 [209280/209539 (100%)]\tAll Loss: 1.4105\tTriple Loss(1): 0.0852\tClassification Loss: 1.2400\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/34_epochs\r\n",
      "Train Epoch: 35 [0/209539 (0%)]\tAll Loss: 1.4058\tTriple Loss(1): 0.0375\tClassification Loss: 1.3308\r\n",
      "\r\n",
      "Test set: Average loss: 1.0831\r\n",
      "Top 1 Accuracy: 54779/80128 (68%)\r\n",
      "Top 3 Accuracy: 70044/80128 (87%)\r\n",
      "Top 5 Accuracy: 74663/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 35 [640/209539 (0%)]\tAll Loss: 1.3297\tTriple Loss(1): 0.0168\tClassification Loss: 1.2961\r\n",
      "Train Epoch: 35 [1280/209539 (1%)]\tAll Loss: 1.1478\tTriple Loss(1): 0.0176\tClassification Loss: 1.1126\r\n",
      "Train Epoch: 35 [1920/209539 (1%)]\tAll Loss: 1.1859\tTriple Loss(1): 0.0121\tClassification Loss: 1.1616\r\n",
      "Train Epoch: 35 [2560/209539 (1%)]\tAll Loss: 1.4510\tTriple Loss(1): 0.0375\tClassification Loss: 1.3761\r\n",
      "Train Epoch: 35 [3200/209539 (2%)]\tAll Loss: 1.3016\tTriple Loss(1): 0.0385\tClassification Loss: 1.2247\r\n",
      "Train Epoch: 35 [3840/209539 (2%)]\tAll Loss: 1.0200\tTriple Loss(1): 0.0073\tClassification Loss: 1.0053\r\n",
      "Train Epoch: 35 [4480/209539 (2%)]\tAll Loss: 1.3384\tTriple Loss(1): 0.0222\tClassification Loss: 1.2941\r\n",
      "Train Epoch: 35 [5120/209539 (2%)]\tAll Loss: 1.2244\tTriple Loss(1): 0.0103\tClassification Loss: 1.2037\r\n",
      "Train Epoch: 35 [5760/209539 (3%)]\tAll Loss: 1.6127\tTriple Loss(0): 0.2330\tClassification Loss: 1.1466\r\n",
      "Train Epoch: 35 [6400/209539 (3%)]\tAll Loss: 1.0661\tTriple Loss(1): 0.0558\tClassification Loss: 0.9545\r\n",
      "Train Epoch: 35 [7040/209539 (3%)]\tAll Loss: 1.2233\tTriple Loss(1): 0.0457\tClassification Loss: 1.1319\r\n",
      "Train Epoch: 35 [7680/209539 (4%)]\tAll Loss: 1.3510\tTriple Loss(0): 0.1697\tClassification Loss: 1.0117\r\n",
      "Train Epoch: 35 [8320/209539 (4%)]\tAll Loss: 1.5426\tTriple Loss(0): 0.2000\tClassification Loss: 1.1427\r\n",
      "Train Epoch: 35 [8960/209539 (4%)]\tAll Loss: 1.2801\tTriple Loss(1): 0.0493\tClassification Loss: 1.1815\r\n",
      "Train Epoch: 35 [9600/209539 (5%)]\tAll Loss: 1.3774\tTriple Loss(1): 0.0000\tClassification Loss: 1.3774\r\n",
      "Train Epoch: 35 [10240/209539 (5%)]\tAll Loss: 0.8853\tTriple Loss(1): 0.0061\tClassification Loss: 0.8732\r\n",
      "Train Epoch: 35 [10880/209539 (5%)]\tAll Loss: 1.2448\tTriple Loss(1): 0.0115\tClassification Loss: 1.2219\r\n",
      "Train Epoch: 35 [11520/209539 (5%)]\tAll Loss: 1.3053\tTriple Loss(1): 0.0000\tClassification Loss: 1.3053\r\n",
      "Train Epoch: 35 [12160/209539 (6%)]\tAll Loss: 1.4260\tTriple Loss(0): 0.2456\tClassification Loss: 0.9349\r\n",
      "Train Epoch: 35 [12800/209539 (6%)]\tAll Loss: 1.3144\tTriple Loss(0): 0.2618\tClassification Loss: 0.7908\r\n",
      "Train Epoch: 35 [13440/209539 (6%)]\tAll Loss: 1.9537\tTriple Loss(0): 0.3915\tClassification Loss: 1.1706\r\n",
      "Train Epoch: 35 [14080/209539 (7%)]\tAll Loss: 1.2201\tTriple Loss(1): 0.0253\tClassification Loss: 1.1694\r\n",
      "Train Epoch: 35 [14720/209539 (7%)]\tAll Loss: 1.3356\tTriple Loss(1): 0.0356\tClassification Loss: 1.2644\r\n",
      "Train Epoch: 35 [15360/209539 (7%)]\tAll Loss: 1.0916\tTriple Loss(1): 0.0034\tClassification Loss: 1.0849\r\n",
      "Train Epoch: 35 [16000/209539 (8%)]\tAll Loss: 1.5069\tTriple Loss(1): 0.0152\tClassification Loss: 1.4764\r\n",
      "Train Epoch: 35 [16640/209539 (8%)]\tAll Loss: 1.5583\tTriple Loss(1): 0.0027\tClassification Loss: 1.5530\r\n",
      "Train Epoch: 35 [17280/209539 (8%)]\tAll Loss: 1.3781\tTriple Loss(1): 0.0210\tClassification Loss: 1.3361\r\n",
      "Train Epoch: 35 [17920/209539 (9%)]\tAll Loss: 1.3548\tTriple Loss(1): 0.0169\tClassification Loss: 1.3211\r\n",
      "Train Epoch: 35 [18560/209539 (9%)]\tAll Loss: 1.1122\tTriple Loss(1): 0.0010\tClassification Loss: 1.1102\r\n",
      "Train Epoch: 35 [19200/209539 (9%)]\tAll Loss: 1.1839\tTriple Loss(1): 0.0151\tClassification Loss: 1.1537\r\n",
      "Train Epoch: 35 [19840/209539 (9%)]\tAll Loss: 1.0905\tTriple Loss(1): 0.0000\tClassification Loss: 1.0905\r\n",
      "Train Epoch: 35 [20480/209539 (10%)]\tAll Loss: 1.5599\tTriple Loss(0): 0.2923\tClassification Loss: 0.9752\r\n",
      "Train Epoch: 35 [21120/209539 (10%)]\tAll Loss: 1.3477\tTriple Loss(1): 0.0110\tClassification Loss: 1.3258\r\n",
      "Train Epoch: 35 [21760/209539 (10%)]\tAll Loss: 1.1126\tTriple Loss(1): 0.0277\tClassification Loss: 1.0571\r\n",
      "Train Epoch: 35 [22400/209539 (11%)]\tAll Loss: 1.1022\tTriple Loss(1): 0.0772\tClassification Loss: 0.9478\r\n",
      "Train Epoch: 35 [23040/209539 (11%)]\tAll Loss: 1.4046\tTriple Loss(1): 0.0566\tClassification Loss: 1.2915\r\n",
      "Train Epoch: 35 [23680/209539 (11%)]\tAll Loss: 1.1260\tTriple Loss(1): 0.0000\tClassification Loss: 1.1260\r\n",
      "Train Epoch: 35 [24320/209539 (12%)]\tAll Loss: 1.1604\tTriple Loss(1): 0.0000\tClassification Loss: 1.1604\r\n",
      "Train Epoch: 35 [24960/209539 (12%)]\tAll Loss: 1.3967\tTriple Loss(0): 0.2013\tClassification Loss: 0.9941\r\n",
      "Train Epoch: 35 [25600/209539 (12%)]\tAll Loss: 1.2894\tTriple Loss(1): 0.0684\tClassification Loss: 1.1526\r\n",
      "Train Epoch: 35 [26240/209539 (13%)]\tAll Loss: 1.1142\tTriple Loss(1): 0.0456\tClassification Loss: 1.0231\r\n",
      "Train Epoch: 35 [26880/209539 (13%)]\tAll Loss: 1.3662\tTriple Loss(1): 0.0248\tClassification Loss: 1.3167\r\n",
      "Train Epoch: 35 [27520/209539 (13%)]\tAll Loss: 1.1238\tTriple Loss(1): 0.0209\tClassification Loss: 1.0820\r\n",
      "Train Epoch: 35 [28160/209539 (13%)]\tAll Loss: 1.3082\tTriple Loss(1): 0.0436\tClassification Loss: 1.2210\r\n",
      "Train Epoch: 35 [28800/209539 (14%)]\tAll Loss: 2.4708\tTriple Loss(0): 0.4177\tClassification Loss: 1.6354\r\n",
      "Train Epoch: 35 [29440/209539 (14%)]\tAll Loss: 1.3370\tTriple Loss(1): 0.0069\tClassification Loss: 1.3233\r\n",
      "Train Epoch: 35 [30080/209539 (14%)]\tAll Loss: 1.1550\tTriple Loss(1): 0.0280\tClassification Loss: 1.0989\r\n",
      "Train Epoch: 35 [30720/209539 (15%)]\tAll Loss: 1.1200\tTriple Loss(1): 0.0295\tClassification Loss: 1.0610\r\n",
      "Train Epoch: 35 [31360/209539 (15%)]\tAll Loss: 1.3959\tTriple Loss(0): 0.1952\tClassification Loss: 1.0055\r\n",
      "Train Epoch: 35 [32000/209539 (15%)]\tAll Loss: 1.2756\tTriple Loss(1): 0.0217\tClassification Loss: 1.2323\r\n",
      "Train Epoch: 35 [32640/209539 (16%)]\tAll Loss: 1.8633\tTriple Loss(0): 0.3778\tClassification Loss: 1.1076\r\n",
      "Train Epoch: 35 [33280/209539 (16%)]\tAll Loss: 1.2230\tTriple Loss(1): 0.0316\tClassification Loss: 1.1598\r\n",
      "Train Epoch: 35 [33920/209539 (16%)]\tAll Loss: 1.6973\tTriple Loss(0): 0.2954\tClassification Loss: 1.1064\r\n",
      "Train Epoch: 35 [34560/209539 (16%)]\tAll Loss: 1.1925\tTriple Loss(1): 0.0152\tClassification Loss: 1.1621\r\n",
      "Train Epoch: 35 [35200/209539 (17%)]\tAll Loss: 0.9645\tTriple Loss(1): 0.0048\tClassification Loss: 0.9549\r\n",
      "Train Epoch: 35 [35840/209539 (17%)]\tAll Loss: 0.9981\tTriple Loss(1): 0.0127\tClassification Loss: 0.9726\r\n",
      "Train Epoch: 35 [36480/209539 (17%)]\tAll Loss: 1.5451\tTriple Loss(0): 0.3111\tClassification Loss: 0.9229\r\n",
      "Train Epoch: 35 [37120/209539 (18%)]\tAll Loss: 1.9647\tTriple Loss(0): 0.2603\tClassification Loss: 1.4441\r\n",
      "Train Epoch: 35 [37760/209539 (18%)]\tAll Loss: 1.3799\tTriple Loss(1): 0.0206\tClassification Loss: 1.3387\r\n",
      "Train Epoch: 35 [38400/209539 (18%)]\tAll Loss: 1.8431\tTriple Loss(0): 0.2551\tClassification Loss: 1.3329\r\n",
      "Train Epoch: 35 [39040/209539 (19%)]\tAll Loss: 0.7634\tTriple Loss(1): 0.0019\tClassification Loss: 0.7596\r\n",
      "Train Epoch: 35 [39680/209539 (19%)]\tAll Loss: 1.6273\tTriple Loss(0): 0.2367\tClassification Loss: 1.1539\r\n",
      "Train Epoch: 35 [40320/209539 (19%)]\tAll Loss: 1.3285\tTriple Loss(1): 0.0502\tClassification Loss: 1.2281\r\n",
      "Train Epoch: 35 [40960/209539 (20%)]\tAll Loss: 1.1303\tTriple Loss(1): 0.0057\tClassification Loss: 1.1189\r\n",
      "Train Epoch: 35 [41600/209539 (20%)]\tAll Loss: 1.0955\tTriple Loss(1): 0.0792\tClassification Loss: 0.9371\r\n",
      "Train Epoch: 35 [42240/209539 (20%)]\tAll Loss: 1.1901\tTriple Loss(1): 0.0000\tClassification Loss: 1.1901\r\n",
      "Train Epoch: 35 [42880/209539 (20%)]\tAll Loss: 0.9167\tTriple Loss(1): 0.0103\tClassification Loss: 0.8962\r\n",
      "Train Epoch: 35 [43520/209539 (21%)]\tAll Loss: 1.2927\tTriple Loss(1): 0.0110\tClassification Loss: 1.2707\r\n",
      "Train Epoch: 35 [44160/209539 (21%)]\tAll Loss: 1.5189\tTriple Loss(1): 0.0126\tClassification Loss: 1.4937\r\n",
      "Train Epoch: 35 [44800/209539 (21%)]\tAll Loss: 1.6665\tTriple Loss(1): 0.0628\tClassification Loss: 1.5409\r\n",
      "Train Epoch: 35 [45440/209539 (22%)]\tAll Loss: 1.9687\tTriple Loss(0): 0.1254\tClassification Loss: 1.7179\r\n",
      "Train Epoch: 35 [46080/209539 (22%)]\tAll Loss: 1.7685\tTriple Loss(0): 0.3095\tClassification Loss: 1.1495\r\n",
      "Train Epoch: 35 [46720/209539 (22%)]\tAll Loss: 1.6575\tTriple Loss(1): 0.0538\tClassification Loss: 1.5499\r\n",
      "Train Epoch: 35 [47360/209539 (23%)]\tAll Loss: 0.9958\tTriple Loss(1): 0.0211\tClassification Loss: 0.9536\r\n",
      "Train Epoch: 35 [48000/209539 (23%)]\tAll Loss: 1.2092\tTriple Loss(1): 0.0383\tClassification Loss: 1.1326\r\n",
      "Train Epoch: 35 [48640/209539 (23%)]\tAll Loss: 1.7085\tTriple Loss(1): 0.0360\tClassification Loss: 1.6364\r\n",
      "Train Epoch: 35 [49280/209539 (24%)]\tAll Loss: 1.8168\tTriple Loss(0): 0.3697\tClassification Loss: 1.0774\r\n",
      "Train Epoch: 35 [49920/209539 (24%)]\tAll Loss: 1.2940\tTriple Loss(1): 0.0000\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 35 [50560/209539 (24%)]\tAll Loss: 1.3077\tTriple Loss(1): 0.0239\tClassification Loss: 1.2599\r\n",
      "Train Epoch: 35 [51200/209539 (24%)]\tAll Loss: 1.5559\tTriple Loss(1): 0.0223\tClassification Loss: 1.5114\r\n",
      "Train Epoch: 35 [51840/209539 (25%)]\tAll Loss: 1.2775\tTriple Loss(1): 0.0513\tClassification Loss: 1.1750\r\n",
      "Train Epoch: 35 [52480/209539 (25%)]\tAll Loss: 1.2472\tTriple Loss(1): 0.0434\tClassification Loss: 1.1603\r\n",
      "Train Epoch: 35 [53120/209539 (25%)]\tAll Loss: 1.3081\tTriple Loss(1): 0.0802\tClassification Loss: 1.1477\r\n",
      "Train Epoch: 35 [53760/209539 (26%)]\tAll Loss: 2.4483\tTriple Loss(0): 0.5372\tClassification Loss: 1.3739\r\n",
      "Train Epoch: 35 [54400/209539 (26%)]\tAll Loss: 1.6904\tTriple Loss(1): 0.0304\tClassification Loss: 1.6295\r\n",
      "Train Epoch: 35 [55040/209539 (26%)]\tAll Loss: 1.0768\tTriple Loss(1): 0.1176\tClassification Loss: 0.8415\r\n",
      "Train Epoch: 35 [55680/209539 (27%)]\tAll Loss: 1.5218\tTriple Loss(1): 0.0362\tClassification Loss: 1.4494\r\n",
      "Train Epoch: 35 [56320/209539 (27%)]\tAll Loss: 1.0223\tTriple Loss(1): 0.0501\tClassification Loss: 0.9222\r\n",
      "Train Epoch: 35 [56960/209539 (27%)]\tAll Loss: 1.9783\tTriple Loss(0): 0.3767\tClassification Loss: 1.2250\r\n",
      "Train Epoch: 35 [57600/209539 (27%)]\tAll Loss: 0.9407\tTriple Loss(1): 0.0000\tClassification Loss: 0.9407\r\n",
      "Train Epoch: 35 [58240/209539 (28%)]\tAll Loss: 1.0998\tTriple Loss(1): 0.0559\tClassification Loss: 0.9880\r\n",
      "Train Epoch: 35 [58880/209539 (28%)]\tAll Loss: 1.2746\tTriple Loss(1): 0.0000\tClassification Loss: 1.2746\r\n",
      "Train Epoch: 35 [59520/209539 (28%)]\tAll Loss: 1.1230\tTriple Loss(1): 0.0000\tClassification Loss: 1.1230\r\n",
      "Train Epoch: 35 [60160/209539 (29%)]\tAll Loss: 1.3453\tTriple Loss(1): 0.0384\tClassification Loss: 1.2685\r\n",
      "Train Epoch: 35 [60800/209539 (29%)]\tAll Loss: 1.4904\tTriple Loss(1): 0.0090\tClassification Loss: 1.4725\r\n",
      "Train Epoch: 35 [61440/209539 (29%)]\tAll Loss: 1.2531\tTriple Loss(1): 0.0187\tClassification Loss: 1.2157\r\n",
      "Train Epoch: 35 [62080/209539 (30%)]\tAll Loss: 1.4736\tTriple Loss(1): 0.0066\tClassification Loss: 1.4603\r\n",
      "Train Epoch: 35 [62720/209539 (30%)]\tAll Loss: 1.1248\tTriple Loss(1): 0.0028\tClassification Loss: 1.1191\r\n",
      "Train Epoch: 35 [63360/209539 (30%)]\tAll Loss: 1.2462\tTriple Loss(1): 0.0417\tClassification Loss: 1.1629\r\n",
      "Train Epoch: 35 [64000/209539 (31%)]\tAll Loss: 1.5845\tTriple Loss(1): 0.0474\tClassification Loss: 1.4898\r\n",
      "Train Epoch: 35 [64640/209539 (31%)]\tAll Loss: 1.5719\tTriple Loss(1): 0.0323\tClassification Loss: 1.5074\r\n",
      "Train Epoch: 35 [65280/209539 (31%)]\tAll Loss: 1.6835\tTriple Loss(0): 0.1993\tClassification Loss: 1.2848\r\n",
      "Train Epoch: 35 [65920/209539 (31%)]\tAll Loss: 1.3903\tTriple Loss(1): 0.0047\tClassification Loss: 1.3808\r\n",
      "Train Epoch: 35 [66560/209539 (32%)]\tAll Loss: 1.1217\tTriple Loss(1): 0.0000\tClassification Loss: 1.1217\r\n",
      "Train Epoch: 35 [67200/209539 (32%)]\tAll Loss: 1.5322\tTriple Loss(0): 0.2315\tClassification Loss: 1.0691\r\n",
      "Train Epoch: 35 [67840/209539 (32%)]\tAll Loss: 1.1077\tTriple Loss(1): 0.0181\tClassification Loss: 1.0715\r\n",
      "Train Epoch: 35 [68480/209539 (33%)]\tAll Loss: 1.4339\tTriple Loss(1): 0.0473\tClassification Loss: 1.3392\r\n",
      "Train Epoch: 35 [69120/209539 (33%)]\tAll Loss: 1.3559\tTriple Loss(1): 0.0212\tClassification Loss: 1.3136\r\n",
      "Train Epoch: 35 [69760/209539 (33%)]\tAll Loss: 1.2502\tTriple Loss(1): 0.0000\tClassification Loss: 1.2502\r\n",
      "Train Epoch: 35 [70400/209539 (34%)]\tAll Loss: 1.2141\tTriple Loss(1): 0.0001\tClassification Loss: 1.2139\r\n",
      "Train Epoch: 35 [71040/209539 (34%)]\tAll Loss: 1.9072\tTriple Loss(0): 0.2559\tClassification Loss: 1.3954\r\n",
      "Train Epoch: 35 [71680/209539 (34%)]\tAll Loss: 1.4548\tTriple Loss(1): 0.0314\tClassification Loss: 1.3921\r\n",
      "Train Epoch: 35 [72320/209539 (35%)]\tAll Loss: 1.0006\tTriple Loss(1): 0.0095\tClassification Loss: 0.9815\r\n",
      "Train Epoch: 35 [72960/209539 (35%)]\tAll Loss: 1.2911\tTriple Loss(1): 0.0472\tClassification Loss: 1.1966\r\n",
      "Train Epoch: 35 [73600/209539 (35%)]\tAll Loss: 1.2769\tTriple Loss(1): 0.0069\tClassification Loss: 1.2631\r\n",
      "Train Epoch: 35 [74240/209539 (35%)]\tAll Loss: 1.5075\tTriple Loss(1): 0.0000\tClassification Loss: 1.5075\r\n",
      "Train Epoch: 35 [74880/209539 (36%)]\tAll Loss: 1.6911\tTriple Loss(1): 0.0499\tClassification Loss: 1.5913\r\n",
      "Train Epoch: 35 [75520/209539 (36%)]\tAll Loss: 0.8593\tTriple Loss(1): 0.0318\tClassification Loss: 0.7956\r\n",
      "Train Epoch: 35 [76160/209539 (36%)]\tAll Loss: 1.1601\tTriple Loss(1): 0.0261\tClassification Loss: 1.1079\r\n",
      "Train Epoch: 35 [76800/209539 (37%)]\tAll Loss: 1.0119\tTriple Loss(1): 0.0101\tClassification Loss: 0.9917\r\n",
      "Train Epoch: 35 [77440/209539 (37%)]\tAll Loss: 1.1238\tTriple Loss(1): 0.0149\tClassification Loss: 1.0941\r\n",
      "Train Epoch: 35 [78080/209539 (37%)]\tAll Loss: 1.2653\tTriple Loss(1): 0.0000\tClassification Loss: 1.2653\r\n",
      "Train Epoch: 35 [78720/209539 (38%)]\tAll Loss: 1.9131\tTriple Loss(0): 0.2483\tClassification Loss: 1.4165\r\n",
      "Train Epoch: 35 [79360/209539 (38%)]\tAll Loss: 1.0726\tTriple Loss(1): 0.0581\tClassification Loss: 0.9564\r\n",
      "Train Epoch: 35 [80000/209539 (38%)]\tAll Loss: 1.1959\tTriple Loss(1): 0.0022\tClassification Loss: 1.1914\r\n",
      "Train Epoch: 35 [80640/209539 (38%)]\tAll Loss: 1.3503\tTriple Loss(1): 0.0283\tClassification Loss: 1.2937\r\n",
      "Train Epoch: 35 [81280/209539 (39%)]\tAll Loss: 1.5211\tTriple Loss(1): 0.0000\tClassification Loss: 1.5211\r\n",
      "Train Epoch: 35 [81920/209539 (39%)]\tAll Loss: 1.2817\tTriple Loss(0): 0.1648\tClassification Loss: 0.9521\r\n",
      "Train Epoch: 35 [82560/209539 (39%)]\tAll Loss: 1.4209\tTriple Loss(1): 0.0750\tClassification Loss: 1.2710\r\n",
      "Train Epoch: 35 [83200/209539 (40%)]\tAll Loss: 1.9143\tTriple Loss(0): 0.3457\tClassification Loss: 1.2229\r\n",
      "Train Epoch: 35 [83840/209539 (40%)]\tAll Loss: 1.0493\tTriple Loss(1): 0.0000\tClassification Loss: 1.0493\r\n",
      "Train Epoch: 35 [84480/209539 (40%)]\tAll Loss: 1.1492\tTriple Loss(1): 0.0000\tClassification Loss: 1.1492\r\n",
      "Train Epoch: 35 [85120/209539 (41%)]\tAll Loss: 1.8281\tTriple Loss(1): 0.0610\tClassification Loss: 1.7062\r\n",
      "Train Epoch: 35 [85760/209539 (41%)]\tAll Loss: 1.2655\tTriple Loss(1): 0.0008\tClassification Loss: 1.2639\r\n",
      "Train Epoch: 35 [86400/209539 (41%)]\tAll Loss: 1.2598\tTriple Loss(1): 0.0000\tClassification Loss: 1.2598\r\n",
      "Train Epoch: 35 [87040/209539 (42%)]\tAll Loss: 1.5010\tTriple Loss(1): 0.0324\tClassification Loss: 1.4362\r\n",
      "Train Epoch: 35 [87680/209539 (42%)]\tAll Loss: 1.3892\tTriple Loss(0): 0.1029\tClassification Loss: 1.1835\r\n",
      "Train Epoch: 35 [88320/209539 (42%)]\tAll Loss: 1.6902\tTriple Loss(0): 0.2610\tClassification Loss: 1.1681\r\n",
      "Train Epoch: 35 [88960/209539 (42%)]\tAll Loss: 1.2799\tTriple Loss(1): 0.0005\tClassification Loss: 1.2790\r\n",
      "Train Epoch: 35 [89600/209539 (43%)]\tAll Loss: 1.8302\tTriple Loss(0): 0.2826\tClassification Loss: 1.2649\r\n",
      "Train Epoch: 35 [90240/209539 (43%)]\tAll Loss: 1.0759\tTriple Loss(1): 0.0353\tClassification Loss: 1.0052\r\n",
      "Train Epoch: 35 [90880/209539 (43%)]\tAll Loss: 1.4128\tTriple Loss(1): 0.0317\tClassification Loss: 1.3494\r\n",
      "Train Epoch: 35 [91520/209539 (44%)]\tAll Loss: 1.1137\tTriple Loss(1): 0.0072\tClassification Loss: 1.0993\r\n",
      "Train Epoch: 35 [92160/209539 (44%)]\tAll Loss: 1.0437\tTriple Loss(1): 0.0000\tClassification Loss: 1.0437\r\n",
      "Train Epoch: 35 [92800/209539 (44%)]\tAll Loss: 1.0821\tTriple Loss(1): 0.0251\tClassification Loss: 1.0318\r\n",
      "Train Epoch: 35 [93440/209539 (45%)]\tAll Loss: 2.0481\tTriple Loss(0): 0.4067\tClassification Loss: 1.2346\r\n",
      "Train Epoch: 35 [94080/209539 (45%)]\tAll Loss: 1.1639\tTriple Loss(1): 0.0127\tClassification Loss: 1.1385\r\n",
      "Train Epoch: 35 [94720/209539 (45%)]\tAll Loss: 1.5351\tTriple Loss(0): 0.1479\tClassification Loss: 1.2393\r\n",
      "Train Epoch: 35 [95360/209539 (46%)]\tAll Loss: 1.3377\tTriple Loss(1): 0.0047\tClassification Loss: 1.3283\r\n",
      "Train Epoch: 35 [96000/209539 (46%)]\tAll Loss: 1.2991\tTriple Loss(1): 0.0000\tClassification Loss: 1.2991\r\n",
      "Train Epoch: 35 [96640/209539 (46%)]\tAll Loss: 1.3137\tTriple Loss(1): 0.0118\tClassification Loss: 1.2901\r\n",
      "Train Epoch: 35 [97280/209539 (46%)]\tAll Loss: 1.0817\tTriple Loss(1): 0.0087\tClassification Loss: 1.0643\r\n",
      "Train Epoch: 35 [97920/209539 (47%)]\tAll Loss: 1.2905\tTriple Loss(1): 0.0452\tClassification Loss: 1.2001\r\n",
      "Train Epoch: 35 [98560/209539 (47%)]\tAll Loss: 1.4959\tTriple Loss(1): 0.0560\tClassification Loss: 1.3838\r\n",
      "Train Epoch: 35 [99200/209539 (47%)]\tAll Loss: 1.4220\tTriple Loss(1): 0.0913\tClassification Loss: 1.2393\r\n",
      "Train Epoch: 35 [99840/209539 (48%)]\tAll Loss: 1.2010\tTriple Loss(1): 0.0000\tClassification Loss: 1.2010\r\n",
      "Train Epoch: 35 [100480/209539 (48%)]\tAll Loss: 1.4149\tTriple Loss(1): 0.0481\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 35 [101120/209539 (48%)]\tAll Loss: 1.3152\tTriple Loss(1): 0.0076\tClassification Loss: 1.3001\r\n",
      "Train Epoch: 35 [101760/209539 (49%)]\tAll Loss: 1.2454\tTriple Loss(1): 0.0765\tClassification Loss: 1.0924\r\n",
      "Train Epoch: 35 [102400/209539 (49%)]\tAll Loss: 1.6528\tTriple Loss(0): 0.3457\tClassification Loss: 0.9615\r\n",
      "Train Epoch: 35 [103040/209539 (49%)]\tAll Loss: 1.3116\tTriple Loss(1): 0.0507\tClassification Loss: 1.2102\r\n",
      "Train Epoch: 35 [103680/209539 (49%)]\tAll Loss: 1.4086\tTriple Loss(1): 0.0101\tClassification Loss: 1.3884\r\n",
      "Train Epoch: 35 [104320/209539 (50%)]\tAll Loss: 1.3253\tTriple Loss(0): 0.1901\tClassification Loss: 0.9450\r\n",
      "Train Epoch: 35 [104960/209539 (50%)]\tAll Loss: 1.1472\tTriple Loss(1): 0.0114\tClassification Loss: 1.1245\r\n",
      "Train Epoch: 35 [105600/209539 (50%)]\tAll Loss: 1.2656\tTriple Loss(1): 0.0000\tClassification Loss: 1.2656\r\n",
      "Train Epoch: 35 [106240/209539 (51%)]\tAll Loss: 1.1267\tTriple Loss(1): 0.0459\tClassification Loss: 1.0350\r\n",
      "Train Epoch: 35 [106880/209539 (51%)]\tAll Loss: 1.1143\tTriple Loss(1): 0.0343\tClassification Loss: 1.0456\r\n",
      "Train Epoch: 35 [107520/209539 (51%)]\tAll Loss: 1.0570\tTriple Loss(1): 0.0154\tClassification Loss: 1.0261\r\n",
      "Train Epoch: 35 [108160/209539 (52%)]\tAll Loss: 1.1724\tTriple Loss(1): 0.0647\tClassification Loss: 1.0430\r\n",
      "Train Epoch: 35 [108800/209539 (52%)]\tAll Loss: 1.1362\tTriple Loss(1): 0.0140\tClassification Loss: 1.1081\r\n",
      "Train Epoch: 35 [109440/209539 (52%)]\tAll Loss: 1.5399\tTriple Loss(1): 0.0000\tClassification Loss: 1.5399\r\n",
      "Train Epoch: 35 [110080/209539 (53%)]\tAll Loss: 1.4117\tTriple Loss(1): 0.0000\tClassification Loss: 1.4117\r\n",
      "Train Epoch: 35 [110720/209539 (53%)]\tAll Loss: 1.2427\tTriple Loss(1): 0.0111\tClassification Loss: 1.2205\r\n",
      "Train Epoch: 35 [111360/209539 (53%)]\tAll Loss: 1.0297\tTriple Loss(1): 0.0169\tClassification Loss: 0.9959\r\n",
      "Train Epoch: 35 [112000/209539 (53%)]\tAll Loss: 1.1313\tTriple Loss(1): 0.0039\tClassification Loss: 1.1235\r\n",
      "Train Epoch: 35 [112640/209539 (54%)]\tAll Loss: 1.4520\tTriple Loss(1): 0.0192\tClassification Loss: 1.4135\r\n",
      "Train Epoch: 35 [113280/209539 (54%)]\tAll Loss: 0.9042\tTriple Loss(1): 0.0197\tClassification Loss: 0.8648\r\n",
      "Train Epoch: 35 [113920/209539 (54%)]\tAll Loss: 1.1515\tTriple Loss(1): 0.0000\tClassification Loss: 1.1515\r\n",
      "Train Epoch: 35 [114560/209539 (55%)]\tAll Loss: 1.8766\tTriple Loss(0): 0.3692\tClassification Loss: 1.1383\r\n",
      "Train Epoch: 35 [115200/209539 (55%)]\tAll Loss: 1.5048\tTriple Loss(1): 0.0000\tClassification Loss: 1.5048\r\n",
      "Train Epoch: 35 [115840/209539 (55%)]\tAll Loss: 1.5647\tTriple Loss(0): 0.1571\tClassification Loss: 1.2506\r\n",
      "Train Epoch: 35 [116480/209539 (56%)]\tAll Loss: 1.4291\tTriple Loss(1): 0.0551\tClassification Loss: 1.3189\r\n",
      "Train Epoch: 35 [117120/209539 (56%)]\tAll Loss: 1.6456\tTriple Loss(0): 0.2207\tClassification Loss: 1.2043\r\n",
      "Train Epoch: 35 [117760/209539 (56%)]\tAll Loss: 1.7397\tTriple Loss(0): 0.3470\tClassification Loss: 1.0458\r\n",
      "Train Epoch: 35 [118400/209539 (57%)]\tAll Loss: 0.9918\tTriple Loss(1): 0.0332\tClassification Loss: 0.9254\r\n",
      "Train Epoch: 35 [119040/209539 (57%)]\tAll Loss: 1.3128\tTriple Loss(1): 0.0025\tClassification Loss: 1.3077\r\n",
      "Train Epoch: 35 [119680/209539 (57%)]\tAll Loss: 1.8614\tTriple Loss(0): 0.3234\tClassification Loss: 1.2147\r\n",
      "Train Epoch: 35 [120320/209539 (57%)]\tAll Loss: 1.1494\tTriple Loss(1): 0.0409\tClassification Loss: 1.0677\r\n",
      "Train Epoch: 35 [120960/209539 (58%)]\tAll Loss: 1.2106\tTriple Loss(1): 0.0220\tClassification Loss: 1.1666\r\n",
      "Train Epoch: 35 [121600/209539 (58%)]\tAll Loss: 1.1275\tTriple Loss(1): 0.0073\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 35 [122240/209539 (58%)]\tAll Loss: 1.2271\tTriple Loss(1): 0.0223\tClassification Loss: 1.1825\r\n",
      "Train Epoch: 35 [122880/209539 (59%)]\tAll Loss: 1.5595\tTriple Loss(0): 0.2190\tClassification Loss: 1.1216\r\n",
      "Train Epoch: 35 [123520/209539 (59%)]\tAll Loss: 1.1909\tTriple Loss(1): 0.0245\tClassification Loss: 1.1419\r\n",
      "Train Epoch: 35 [124160/209539 (59%)]\tAll Loss: 1.2860\tTriple Loss(1): 0.0395\tClassification Loss: 1.2070\r\n",
      "Train Epoch: 35 [124800/209539 (60%)]\tAll Loss: 1.0515\tTriple Loss(1): 0.0000\tClassification Loss: 1.0515\r\n",
      "Train Epoch: 35 [125440/209539 (60%)]\tAll Loss: 1.1106\tTriple Loss(1): 0.0047\tClassification Loss: 1.1012\r\n",
      "Train Epoch: 35 [126080/209539 (60%)]\tAll Loss: 1.2714\tTriple Loss(1): 0.0211\tClassification Loss: 1.2291\r\n",
      "Train Epoch: 35 [126720/209539 (60%)]\tAll Loss: 1.6949\tTriple Loss(1): 0.0190\tClassification Loss: 1.6568\r\n",
      "Train Epoch: 35 [127360/209539 (61%)]\tAll Loss: 1.2784\tTriple Loss(1): 0.0004\tClassification Loss: 1.2777\r\n",
      "Train Epoch: 35 [128000/209539 (61%)]\tAll Loss: 1.4485\tTriple Loss(1): 0.0276\tClassification Loss: 1.3932\r\n",
      "Train Epoch: 35 [128640/209539 (61%)]\tAll Loss: 0.8849\tTriple Loss(1): 0.0156\tClassification Loss: 0.8537\r\n",
      "Train Epoch: 35 [129280/209539 (62%)]\tAll Loss: 1.3726\tTriple Loss(1): 0.0000\tClassification Loss: 1.3726\r\n",
      "Train Epoch: 35 [129920/209539 (62%)]\tAll Loss: 1.0676\tTriple Loss(1): 0.0000\tClassification Loss: 1.0676\r\n",
      "Train Epoch: 35 [130560/209539 (62%)]\tAll Loss: 0.9339\tTriple Loss(1): 0.0397\tClassification Loss: 0.8546\r\n",
      "Train Epoch: 35 [131200/209539 (63%)]\tAll Loss: 1.0987\tTriple Loss(1): 0.0386\tClassification Loss: 1.0215\r\n",
      "Train Epoch: 35 [131840/209539 (63%)]\tAll Loss: 1.4315\tTriple Loss(0): 0.2270\tClassification Loss: 0.9774\r\n",
      "Train Epoch: 35 [132480/209539 (63%)]\tAll Loss: 1.1603\tTriple Loss(1): 0.0865\tClassification Loss: 0.9873\r\n",
      "Train Epoch: 35 [133120/209539 (64%)]\tAll Loss: 1.7529\tTriple Loss(0): 0.3391\tClassification Loss: 1.0747\r\n",
      "Train Epoch: 35 [133760/209539 (64%)]\tAll Loss: 2.0076\tTriple Loss(0): 0.3146\tClassification Loss: 1.3783\r\n",
      "Train Epoch: 35 [134400/209539 (64%)]\tAll Loss: 1.7392\tTriple Loss(0): 0.3768\tClassification Loss: 0.9855\r\n",
      "Train Epoch: 35 [135040/209539 (64%)]\tAll Loss: 1.7969\tTriple Loss(0): 0.2984\tClassification Loss: 1.2000\r\n",
      "Train Epoch: 35 [135680/209539 (65%)]\tAll Loss: 1.6777\tTriple Loss(1): 0.1016\tClassification Loss: 1.4746\r\n",
      "Train Epoch: 35 [136320/209539 (65%)]\tAll Loss: 1.1045\tTriple Loss(1): 0.0007\tClassification Loss: 1.1031\r\n",
      "Train Epoch: 35 [136960/209539 (65%)]\tAll Loss: 1.1456\tTriple Loss(1): 0.0000\tClassification Loss: 1.1456\r\n",
      "Train Epoch: 35 [137600/209539 (66%)]\tAll Loss: 1.0764\tTriple Loss(1): 0.0000\tClassification Loss: 1.0764\r\n",
      "Train Epoch: 35 [138240/209539 (66%)]\tAll Loss: 1.2852\tTriple Loss(1): 0.0000\tClassification Loss: 1.2852\r\n",
      "Train Epoch: 35 [138880/209539 (66%)]\tAll Loss: 1.1988\tTriple Loss(1): 0.0500\tClassification Loss: 1.0988\r\n",
      "Train Epoch: 35 [139520/209539 (67%)]\tAll Loss: 1.8195\tTriple Loss(0): 0.3076\tClassification Loss: 1.2042\r\n",
      "Train Epoch: 35 [140160/209539 (67%)]\tAll Loss: 1.7115\tTriple Loss(0): 0.1951\tClassification Loss: 1.3213\r\n",
      "Train Epoch: 35 [140800/209539 (67%)]\tAll Loss: 1.4424\tTriple Loss(1): 0.0049\tClassification Loss: 1.4325\r\n",
      "Train Epoch: 35 [141440/209539 (68%)]\tAll Loss: 1.2620\tTriple Loss(1): 0.0431\tClassification Loss: 1.1757\r\n",
      "Train Epoch: 35 [142080/209539 (68%)]\tAll Loss: 1.1120\tTriple Loss(1): 0.0000\tClassification Loss: 1.1120\r\n",
      "Train Epoch: 35 [142720/209539 (68%)]\tAll Loss: 1.6061\tTriple Loss(1): 0.0457\tClassification Loss: 1.5146\r\n",
      "Train Epoch: 35 [143360/209539 (68%)]\tAll Loss: 1.4024\tTriple Loss(0): 0.2249\tClassification Loss: 0.9527\r\n",
      "Train Epoch: 35 [144000/209539 (69%)]\tAll Loss: 1.2157\tTriple Loss(1): 0.0000\tClassification Loss: 1.2157\r\n",
      "Train Epoch: 35 [144640/209539 (69%)]\tAll Loss: 1.1780\tTriple Loss(1): 0.0284\tClassification Loss: 1.1213\r\n",
      "Train Epoch: 35 [145280/209539 (69%)]\tAll Loss: 1.3807\tTriple Loss(1): 0.0065\tClassification Loss: 1.3677\r\n",
      "Train Epoch: 35 [145920/209539 (70%)]\tAll Loss: 1.2639\tTriple Loss(1): 0.0161\tClassification Loss: 1.2317\r\n",
      "Train Epoch: 35 [146560/209539 (70%)]\tAll Loss: 0.9510\tTriple Loss(1): 0.0109\tClassification Loss: 0.9293\r\n",
      "Train Epoch: 35 [147200/209539 (70%)]\tAll Loss: 1.2443\tTriple Loss(1): 0.0033\tClassification Loss: 1.2377\r\n",
      "Train Epoch: 35 [147840/209539 (71%)]\tAll Loss: 0.9634\tTriple Loss(1): 0.0000\tClassification Loss: 0.9634\r\n",
      "Train Epoch: 35 [148480/209539 (71%)]\tAll Loss: 0.8280\tTriple Loss(1): 0.0000\tClassification Loss: 0.8280\r\n",
      "Train Epoch: 35 [149120/209539 (71%)]\tAll Loss: 1.4847\tTriple Loss(0): 0.1741\tClassification Loss: 1.1365\r\n",
      "Train Epoch: 35 [149760/209539 (71%)]\tAll Loss: 0.9890\tTriple Loss(1): 0.0000\tClassification Loss: 0.9890\r\n",
      "Train Epoch: 35 [150400/209539 (72%)]\tAll Loss: 1.4895\tTriple Loss(1): 0.0237\tClassification Loss: 1.4421\r\n",
      "Train Epoch: 35 [151040/209539 (72%)]\tAll Loss: 1.0207\tTriple Loss(1): 0.0000\tClassification Loss: 1.0207\r\n",
      "Train Epoch: 35 [151680/209539 (72%)]\tAll Loss: 1.4495\tTriple Loss(1): 0.0269\tClassification Loss: 1.3957\r\n",
      "Train Epoch: 35 [152320/209539 (73%)]\tAll Loss: 1.1198\tTriple Loss(1): 0.0985\tClassification Loss: 0.9228\r\n",
      "Train Epoch: 35 [152960/209539 (73%)]\tAll Loss: 1.0420\tTriple Loss(1): 0.0321\tClassification Loss: 0.9778\r\n",
      "Train Epoch: 35 [153600/209539 (73%)]\tAll Loss: 1.3102\tTriple Loss(1): 0.0451\tClassification Loss: 1.2200\r\n",
      "Train Epoch: 35 [154240/209539 (74%)]\tAll Loss: 1.2052\tTriple Loss(1): 0.0037\tClassification Loss: 1.1978\r\n",
      "Train Epoch: 35 [154880/209539 (74%)]\tAll Loss: 1.3669\tTriple Loss(1): 0.0156\tClassification Loss: 1.3356\r\n",
      "Train Epoch: 35 [155520/209539 (74%)]\tAll Loss: 1.1738\tTriple Loss(1): 0.0039\tClassification Loss: 1.1660\r\n",
      "Train Epoch: 35 [156160/209539 (75%)]\tAll Loss: 1.9487\tTriple Loss(0): 0.3060\tClassification Loss: 1.3367\r\n",
      "Train Epoch: 35 [156800/209539 (75%)]\tAll Loss: 1.5109\tTriple Loss(1): 0.0299\tClassification Loss: 1.4511\r\n",
      "Train Epoch: 35 [157440/209539 (75%)]\tAll Loss: 1.2885\tTriple Loss(1): 0.0140\tClassification Loss: 1.2605\r\n",
      "Train Epoch: 35 [158080/209539 (75%)]\tAll Loss: 1.5946\tTriple Loss(0): 0.2742\tClassification Loss: 1.0463\r\n",
      "Train Epoch: 35 [158720/209539 (76%)]\tAll Loss: 1.4004\tTriple Loss(0): 0.2076\tClassification Loss: 0.9851\r\n",
      "Train Epoch: 35 [159360/209539 (76%)]\tAll Loss: 1.0222\tTriple Loss(1): 0.1015\tClassification Loss: 0.8193\r\n",
      "Train Epoch: 35 [160000/209539 (76%)]\tAll Loss: 1.2764\tTriple Loss(1): 0.0217\tClassification Loss: 1.2329\r\n",
      "Train Epoch: 35 [160640/209539 (77%)]\tAll Loss: 1.0456\tTriple Loss(1): 0.0208\tClassification Loss: 1.0041\r\n",
      "Train Epoch: 35 [161280/209539 (77%)]\tAll Loss: 0.9139\tTriple Loss(1): 0.0000\tClassification Loss: 0.9139\r\n",
      "Train Epoch: 35 [161920/209539 (77%)]\tAll Loss: 1.5830\tTriple Loss(0): 0.2165\tClassification Loss: 1.1500\r\n",
      "Train Epoch: 35 [162560/209539 (78%)]\tAll Loss: 1.1181\tTriple Loss(1): 0.0301\tClassification Loss: 1.0579\r\n",
      "Train Epoch: 35 [163200/209539 (78%)]\tAll Loss: 1.0698\tTriple Loss(1): 0.0837\tClassification Loss: 0.9024\r\n",
      "Train Epoch: 35 [163840/209539 (78%)]\tAll Loss: 1.8685\tTriple Loss(0): 0.3175\tClassification Loss: 1.2335\r\n",
      "Train Epoch: 35 [164480/209539 (78%)]\tAll Loss: 1.1762\tTriple Loss(1): 0.0199\tClassification Loss: 1.1364\r\n",
      "Train Epoch: 35 [165120/209539 (79%)]\tAll Loss: 0.9303\tTriple Loss(1): 0.0000\tClassification Loss: 0.9303\r\n",
      "Train Epoch: 35 [165760/209539 (79%)]\tAll Loss: 1.4725\tTriple Loss(1): 0.0348\tClassification Loss: 1.4029\r\n",
      "Train Epoch: 35 [166400/209539 (79%)]\tAll Loss: 1.3701\tTriple Loss(1): 0.0338\tClassification Loss: 1.3024\r\n",
      "Train Epoch: 35 [167040/209539 (80%)]\tAll Loss: 2.0108\tTriple Loss(0): 0.3040\tClassification Loss: 1.4027\r\n",
      "Train Epoch: 35 [167680/209539 (80%)]\tAll Loss: 1.2948\tTriple Loss(1): 0.0116\tClassification Loss: 1.2715\r\n",
      "Train Epoch: 35 [168320/209539 (80%)]\tAll Loss: 1.2538\tTriple Loss(1): 0.0536\tClassification Loss: 1.1466\r\n",
      "Train Epoch: 35 [168960/209539 (81%)]\tAll Loss: 0.9757\tTriple Loss(1): 0.0168\tClassification Loss: 0.9420\r\n",
      "Train Epoch: 35 [169600/209539 (81%)]\tAll Loss: 1.3790\tTriple Loss(0): 0.1795\tClassification Loss: 1.0201\r\n",
      "Train Epoch: 35 [170240/209539 (81%)]\tAll Loss: 1.1635\tTriple Loss(1): 0.0813\tClassification Loss: 1.0010\r\n",
      "Train Epoch: 35 [170880/209539 (82%)]\tAll Loss: 1.3179\tTriple Loss(1): 0.0548\tClassification Loss: 1.2082\r\n",
      "Train Epoch: 35 [171520/209539 (82%)]\tAll Loss: 1.2879\tTriple Loss(0): 0.0805\tClassification Loss: 1.1269\r\n",
      "Train Epoch: 35 [172160/209539 (82%)]\tAll Loss: 1.2512\tTriple Loss(1): 0.0345\tClassification Loss: 1.1821\r\n",
      "Train Epoch: 35 [172800/209539 (82%)]\tAll Loss: 1.3280\tTriple Loss(1): 0.0580\tClassification Loss: 1.2120\r\n",
      "Train Epoch: 35 [173440/209539 (83%)]\tAll Loss: 1.6006\tTriple Loss(1): 0.0217\tClassification Loss: 1.5572\r\n",
      "Train Epoch: 35 [174080/209539 (83%)]\tAll Loss: 1.8143\tTriple Loss(0): 0.3484\tClassification Loss: 1.1175\r\n",
      "Train Epoch: 35 [174720/209539 (83%)]\tAll Loss: 1.1550\tTriple Loss(1): 0.0298\tClassification Loss: 1.0953\r\n",
      "Train Epoch: 35 [175360/209539 (84%)]\tAll Loss: 1.2991\tTriple Loss(1): 0.0135\tClassification Loss: 1.2720\r\n",
      "Train Epoch: 35 [176000/209539 (84%)]\tAll Loss: 1.2173\tTriple Loss(1): 0.0207\tClassification Loss: 1.1759\r\n",
      "Train Epoch: 35 [176640/209539 (84%)]\tAll Loss: 0.9082\tTriple Loss(1): 0.0299\tClassification Loss: 0.8484\r\n",
      "Train Epoch: 35 [177280/209539 (85%)]\tAll Loss: 2.2971\tTriple Loss(0): 0.4365\tClassification Loss: 1.4241\r\n",
      "Train Epoch: 35 [177920/209539 (85%)]\tAll Loss: 1.4470\tTriple Loss(1): 0.0285\tClassification Loss: 1.3900\r\n",
      "Train Epoch: 35 [178560/209539 (85%)]\tAll Loss: 1.1444\tTriple Loss(1): 0.0000\tClassification Loss: 1.1444\r\n",
      "Train Epoch: 35 [179200/209539 (86%)]\tAll Loss: 2.2378\tTriple Loss(0): 0.4427\tClassification Loss: 1.3525\r\n",
      "Train Epoch: 35 [179840/209539 (86%)]\tAll Loss: 1.3068\tTriple Loss(1): 0.0191\tClassification Loss: 1.2685\r\n",
      "Train Epoch: 35 [180480/209539 (86%)]\tAll Loss: 1.2605\tTriple Loss(1): 0.0075\tClassification Loss: 1.2455\r\n",
      "Train Epoch: 35 [181120/209539 (86%)]\tAll Loss: 1.5245\tTriple Loss(1): 0.0175\tClassification Loss: 1.4894\r\n",
      "Train Epoch: 35 [181760/209539 (87%)]\tAll Loss: 1.6234\tTriple Loss(0): 0.3333\tClassification Loss: 0.9569\r\n",
      "Train Epoch: 35 [182400/209539 (87%)]\tAll Loss: 1.6086\tTriple Loss(1): 0.0152\tClassification Loss: 1.5782\r\n",
      "Train Epoch: 35 [183040/209539 (87%)]\tAll Loss: 1.2381\tTriple Loss(1): 0.0182\tClassification Loss: 1.2017\r\n",
      "Train Epoch: 35 [183680/209539 (88%)]\tAll Loss: 1.2580\tTriple Loss(1): 0.0614\tClassification Loss: 1.1353\r\n",
      "Train Epoch: 35 [184320/209539 (88%)]\tAll Loss: 1.1731\tTriple Loss(1): 0.0331\tClassification Loss: 1.1068\r\n",
      "Train Epoch: 35 [184960/209539 (88%)]\tAll Loss: 1.1525\tTriple Loss(1): 0.0368\tClassification Loss: 1.0788\r\n",
      "Train Epoch: 35 [185600/209539 (89%)]\tAll Loss: 1.2631\tTriple Loss(1): 0.0441\tClassification Loss: 1.1749\r\n",
      "Train Epoch: 35 [186240/209539 (89%)]\tAll Loss: 1.2332\tTriple Loss(1): 0.0140\tClassification Loss: 1.2051\r\n",
      "Train Epoch: 35 [186880/209539 (89%)]\tAll Loss: 1.3383\tTriple Loss(1): 0.0575\tClassification Loss: 1.2233\r\n",
      "Train Epoch: 35 [187520/209539 (89%)]\tAll Loss: 2.2341\tTriple Loss(0): 0.4625\tClassification Loss: 1.3092\r\n",
      "Train Epoch: 35 [188160/209539 (90%)]\tAll Loss: 1.0503\tTriple Loss(1): 0.0096\tClassification Loss: 1.0311\r\n",
      "Train Epoch: 35 [188800/209539 (90%)]\tAll Loss: 1.3876\tTriple Loss(0): 0.1691\tClassification Loss: 1.0494\r\n",
      "Train Epoch: 35 [189440/209539 (90%)]\tAll Loss: 1.2680\tTriple Loss(1): 0.0443\tClassification Loss: 1.1794\r\n",
      "Train Epoch: 35 [190080/209539 (91%)]\tAll Loss: 1.0612\tTriple Loss(1): 0.0129\tClassification Loss: 1.0354\r\n",
      "Train Epoch: 35 [190720/209539 (91%)]\tAll Loss: 1.3527\tTriple Loss(1): 0.0443\tClassification Loss: 1.2641\r\n",
      "Train Epoch: 35 [191360/209539 (91%)]\tAll Loss: 0.9066\tTriple Loss(1): 0.0000\tClassification Loss: 0.9066\r\n",
      "Train Epoch: 35 [192000/209539 (92%)]\tAll Loss: 1.5776\tTriple Loss(1): 0.0224\tClassification Loss: 1.5328\r\n",
      "Train Epoch: 35 [192640/209539 (92%)]\tAll Loss: 1.1299\tTriple Loss(1): 0.0614\tClassification Loss: 1.0070\r\n",
      "Train Epoch: 35 [193280/209539 (92%)]\tAll Loss: 1.0006\tTriple Loss(1): 0.0000\tClassification Loss: 1.0006\r\n",
      "Train Epoch: 35 [193920/209539 (93%)]\tAll Loss: 1.3581\tTriple Loss(0): 0.1904\tClassification Loss: 0.9773\r\n",
      "Train Epoch: 35 [194560/209539 (93%)]\tAll Loss: 1.1190\tTriple Loss(1): 0.0629\tClassification Loss: 0.9933\r\n",
      "Train Epoch: 35 [195200/209539 (93%)]\tAll Loss: 1.1871\tTriple Loss(1): 0.0089\tClassification Loss: 1.1692\r\n",
      "Train Epoch: 35 [195840/209539 (93%)]\tAll Loss: 1.6856\tTriple Loss(0): 0.4359\tClassification Loss: 0.8138\r\n",
      "Train Epoch: 35 [196480/209539 (94%)]\tAll Loss: 1.4592\tTriple Loss(1): 0.0000\tClassification Loss: 1.4592\r\n",
      "Train Epoch: 35 [197120/209539 (94%)]\tAll Loss: 1.1002\tTriple Loss(1): 0.0121\tClassification Loss: 1.0760\r\n",
      "Train Epoch: 35 [197760/209539 (94%)]\tAll Loss: 1.2312\tTriple Loss(1): 0.0441\tClassification Loss: 1.1430\r\n",
      "Train Epoch: 35 [198400/209539 (95%)]\tAll Loss: 1.1365\tTriple Loss(1): 0.0098\tClassification Loss: 1.1169\r\n",
      "Train Epoch: 35 [199040/209539 (95%)]\tAll Loss: 1.3504\tTriple Loss(1): 0.0350\tClassification Loss: 1.2805\r\n",
      "Train Epoch: 35 [199680/209539 (95%)]\tAll Loss: 1.2444\tTriple Loss(1): 0.0000\tClassification Loss: 1.2444\r\n",
      "Train Epoch: 35 [200320/209539 (96%)]\tAll Loss: 1.2061\tTriple Loss(1): 0.0000\tClassification Loss: 1.2061\r\n",
      "Train Epoch: 35 [200960/209539 (96%)]\tAll Loss: 1.5073\tTriple Loss(0): 0.2616\tClassification Loss: 0.9840\r\n",
      "Train Epoch: 35 [201600/209539 (96%)]\tAll Loss: 0.9849\tTriple Loss(1): 0.0000\tClassification Loss: 0.9849\r\n",
      "Train Epoch: 35 [202240/209539 (97%)]\tAll Loss: 1.1306\tTriple Loss(1): 0.0122\tClassification Loss: 1.1063\r\n",
      "Train Epoch: 35 [202880/209539 (97%)]\tAll Loss: 0.7804\tTriple Loss(1): 0.0063\tClassification Loss: 0.7677\r\n",
      "Train Epoch: 35 [203520/209539 (97%)]\tAll Loss: 1.1425\tTriple Loss(1): 0.0268\tClassification Loss: 1.0890\r\n",
      "Train Epoch: 35 [204160/209539 (97%)]\tAll Loss: 1.4603\tTriple Loss(1): 0.0187\tClassification Loss: 1.4228\r\n",
      "Train Epoch: 35 [204800/209539 (98%)]\tAll Loss: 1.5766\tTriple Loss(1): 0.0696\tClassification Loss: 1.4374\r\n",
      "Train Epoch: 35 [205440/209539 (98%)]\tAll Loss: 1.0766\tTriple Loss(1): 0.0603\tClassification Loss: 0.9561\r\n",
      "Train Epoch: 35 [206080/209539 (98%)]\tAll Loss: 1.1823\tTriple Loss(1): 0.0134\tClassification Loss: 1.1556\r\n",
      "Train Epoch: 35 [206720/209539 (99%)]\tAll Loss: 0.9777\tTriple Loss(1): 0.0000\tClassification Loss: 0.9777\r\n",
      "Train Epoch: 35 [207360/209539 (99%)]\tAll Loss: 1.0380\tTriple Loss(1): 0.0297\tClassification Loss: 0.9785\r\n",
      "Train Epoch: 35 [208000/209539 (99%)]\tAll Loss: 1.4795\tTriple Loss(0): 0.1876\tClassification Loss: 1.1043\r\n",
      "Train Epoch: 35 [208640/209539 (100%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0002\tClassification Loss: 1.2104\r\n",
      "Train Epoch: 35 [209280/209539 (100%)]\tAll Loss: 1.8740\tTriple Loss(0): 0.3762\tClassification Loss: 1.1215\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/35_epochs\r\n",
      "Train Epoch: 36 [0/209539 (0%)]\tAll Loss: 1.4877\tTriple Loss(1): 0.0524\tClassification Loss: 1.3829\r\n",
      "\r\n",
      "Test set: Average loss: 1.0696\r\n",
      "Top 1 Accuracy: 55096/80128 (69%)\r\n",
      "Top 3 Accuracy: 70285/80128 (88%)\r\n",
      "Top 5 Accuracy: 74774/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 36 [640/209539 (0%)]\tAll Loss: 2.2192\tTriple Loss(0): 0.3836\tClassification Loss: 1.4519\r\n",
      "Train Epoch: 36 [1280/209539 (1%)]\tAll Loss: 1.4667\tTriple Loss(0): 0.2456\tClassification Loss: 0.9755\r\n",
      "Train Epoch: 36 [1920/209539 (1%)]\tAll Loss: 1.3367\tTriple Loss(1): 0.0290\tClassification Loss: 1.2787\r\n",
      "Train Epoch: 36 [2560/209539 (1%)]\tAll Loss: 1.5214\tTriple Loss(1): 0.0308\tClassification Loss: 1.4598\r\n",
      "Train Epoch: 36 [3200/209539 (2%)]\tAll Loss: 1.7280\tTriple Loss(0): 0.2132\tClassification Loss: 1.3015\r\n",
      "Train Epoch: 36 [3840/209539 (2%)]\tAll Loss: 1.0881\tTriple Loss(1): 0.0360\tClassification Loss: 1.0161\r\n",
      "Train Epoch: 36 [4480/209539 (2%)]\tAll Loss: 1.3685\tTriple Loss(1): 0.0003\tClassification Loss: 1.3678\r\n",
      "Train Epoch: 36 [5120/209539 (2%)]\tAll Loss: 1.3025\tTriple Loss(1): 0.0255\tClassification Loss: 1.2515\r\n",
      "Train Epoch: 36 [5760/209539 (3%)]\tAll Loss: 1.2716\tTriple Loss(1): 0.0693\tClassification Loss: 1.1329\r\n",
      "Train Epoch: 36 [6400/209539 (3%)]\tAll Loss: 0.9331\tTriple Loss(1): 0.0334\tClassification Loss: 0.8664\r\n",
      "Train Epoch: 36 [7040/209539 (3%)]\tAll Loss: 1.4456\tTriple Loss(0): 0.2503\tClassification Loss: 0.9451\r\n",
      "Train Epoch: 36 [7680/209539 (4%)]\tAll Loss: 1.0485\tTriple Loss(1): 0.0674\tClassification Loss: 0.9137\r\n",
      "Train Epoch: 36 [8320/209539 (4%)]\tAll Loss: 1.1643\tTriple Loss(1): 0.0000\tClassification Loss: 1.1643\r\n",
      "Train Epoch: 36 [8960/209539 (4%)]\tAll Loss: 1.0459\tTriple Loss(1): 0.0097\tClassification Loss: 1.0265\r\n",
      "Train Epoch: 36 [9600/209539 (5%)]\tAll Loss: 1.4670\tTriple Loss(1): 0.0578\tClassification Loss: 1.3514\r\n",
      "Train Epoch: 36 [10240/209539 (5%)]\tAll Loss: 0.9786\tTriple Loss(1): 0.0000\tClassification Loss: 0.9786\r\n",
      "Train Epoch: 36 [10880/209539 (5%)]\tAll Loss: 1.2330\tTriple Loss(1): 0.0205\tClassification Loss: 1.1919\r\n",
      "Train Epoch: 36 [11520/209539 (5%)]\tAll Loss: 1.3826\tTriple Loss(1): 0.0000\tClassification Loss: 1.3826\r\n",
      "Train Epoch: 36 [12160/209539 (6%)]\tAll Loss: 1.0758\tTriple Loss(1): 0.0294\tClassification Loss: 1.0171\r\n",
      "Train Epoch: 36 [12800/209539 (6%)]\tAll Loss: 1.2560\tTriple Loss(0): 0.2124\tClassification Loss: 0.8313\r\n",
      "Train Epoch: 36 [13440/209539 (6%)]\tAll Loss: 1.1889\tTriple Loss(1): 0.0318\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 36 [14080/209539 (7%)]\tAll Loss: 1.3111\tTriple Loss(1): 0.0412\tClassification Loss: 1.2288\r\n",
      "Train Epoch: 36 [14720/209539 (7%)]\tAll Loss: 1.3309\tTriple Loss(1): 0.0219\tClassification Loss: 1.2872\r\n",
      "Train Epoch: 36 [15360/209539 (7%)]\tAll Loss: 1.4598\tTriple Loss(0): 0.1721\tClassification Loss: 1.1155\r\n",
      "Train Epoch: 36 [16000/209539 (8%)]\tAll Loss: 1.9791\tTriple Loss(0): 0.3069\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 36 [16640/209539 (8%)]\tAll Loss: 1.6448\tTriple Loss(1): 0.0000\tClassification Loss: 1.6448\r\n",
      "Train Epoch: 36 [17280/209539 (8%)]\tAll Loss: 1.2620\tTriple Loss(1): 0.0036\tClassification Loss: 1.2549\r\n",
      "Train Epoch: 36 [17920/209539 (9%)]\tAll Loss: 1.1858\tTriple Loss(1): 0.0000\tClassification Loss: 1.1858\r\n",
      "Train Epoch: 36 [18560/209539 (9%)]\tAll Loss: 1.5135\tTriple Loss(0): 0.2244\tClassification Loss: 1.0648\r\n",
      "Train Epoch: 36 [19200/209539 (9%)]\tAll Loss: 1.5352\tTriple Loss(0): 0.2525\tClassification Loss: 1.0302\r\n",
      "Train Epoch: 36 [19840/209539 (9%)]\tAll Loss: 1.0550\tTriple Loss(1): 0.0302\tClassification Loss: 0.9947\r\n",
      "Train Epoch: 36 [20480/209539 (10%)]\tAll Loss: 1.0657\tTriple Loss(1): 0.0268\tClassification Loss: 1.0121\r\n",
      "Train Epoch: 36 [21120/209539 (10%)]\tAll Loss: 1.7342\tTriple Loss(0): 0.2087\tClassification Loss: 1.3168\r\n",
      "Train Epoch: 36 [21760/209539 (10%)]\tAll Loss: 1.2574\tTriple Loss(1): 0.0513\tClassification Loss: 1.1549\r\n",
      "Train Epoch: 36 [22400/209539 (11%)]\tAll Loss: 0.8835\tTriple Loss(1): 0.0219\tClassification Loss: 0.8397\r\n",
      "Train Epoch: 36 [23040/209539 (11%)]\tAll Loss: 1.1828\tTriple Loss(1): 0.0052\tClassification Loss: 1.1723\r\n",
      "Train Epoch: 36 [23680/209539 (11%)]\tAll Loss: 1.0234\tTriple Loss(1): 0.0000\tClassification Loss: 1.0234\r\n",
      "Train Epoch: 36 [24320/209539 (12%)]\tAll Loss: 1.8795\tTriple Loss(0): 0.2978\tClassification Loss: 1.2839\r\n",
      "Train Epoch: 36 [24960/209539 (12%)]\tAll Loss: 1.2927\tTriple Loss(0): 0.1612\tClassification Loss: 0.9702\r\n",
      "Train Epoch: 36 [25600/209539 (12%)]\tAll Loss: 1.0682\tTriple Loss(1): 0.0340\tClassification Loss: 1.0001\r\n",
      "Train Epoch: 36 [26240/209539 (13%)]\tAll Loss: 1.1133\tTriple Loss(1): 0.0563\tClassification Loss: 1.0008\r\n",
      "Train Epoch: 36 [26880/209539 (13%)]\tAll Loss: 1.8759\tTriple Loss(0): 0.3229\tClassification Loss: 1.2300\r\n",
      "Train Epoch: 36 [27520/209539 (13%)]\tAll Loss: 1.7180\tTriple Loss(0): 0.2894\tClassification Loss: 1.1392\r\n",
      "Train Epoch: 36 [28160/209539 (13%)]\tAll Loss: 1.3858\tTriple Loss(1): 0.0686\tClassification Loss: 1.2486\r\n",
      "Train Epoch: 36 [28800/209539 (14%)]\tAll Loss: 1.5331\tTriple Loss(1): 0.0576\tClassification Loss: 1.4179\r\n",
      "Train Epoch: 36 [29440/209539 (14%)]\tAll Loss: 1.2429\tTriple Loss(1): 0.0383\tClassification Loss: 1.1663\r\n",
      "Train Epoch: 36 [30080/209539 (14%)]\tAll Loss: 1.2442\tTriple Loss(1): 0.0263\tClassification Loss: 1.1916\r\n",
      "Train Epoch: 36 [30720/209539 (15%)]\tAll Loss: 1.1185\tTriple Loss(1): 0.0450\tClassification Loss: 1.0284\r\n",
      "Train Epoch: 36 [31360/209539 (15%)]\tAll Loss: 2.0036\tTriple Loss(0): 0.4916\tClassification Loss: 1.0204\r\n",
      "Train Epoch: 36 [32000/209539 (15%)]\tAll Loss: 1.1961\tTriple Loss(1): 0.0079\tClassification Loss: 1.1803\r\n",
      "Train Epoch: 36 [32640/209539 (16%)]\tAll Loss: 1.7854\tTriple Loss(0): 0.2788\tClassification Loss: 1.2279\r\n",
      "Train Epoch: 36 [33280/209539 (16%)]\tAll Loss: 1.2267\tTriple Loss(1): 0.0387\tClassification Loss: 1.1492\r\n",
      "Train Epoch: 36 [33920/209539 (16%)]\tAll Loss: 1.1673\tTriple Loss(1): 0.0000\tClassification Loss: 1.1673\r\n",
      "Train Epoch: 36 [34560/209539 (16%)]\tAll Loss: 1.2879\tTriple Loss(1): 0.0070\tClassification Loss: 1.2739\r\n",
      "Train Epoch: 36 [35200/209539 (17%)]\tAll Loss: 1.0997\tTriple Loss(1): 0.0000\tClassification Loss: 1.0997\r\n",
      "Train Epoch: 36 [35840/209539 (17%)]\tAll Loss: 1.2052\tTriple Loss(1): 0.0808\tClassification Loss: 1.0436\r\n",
      "Train Epoch: 36 [36480/209539 (17%)]\tAll Loss: 1.0507\tTriple Loss(1): 0.0456\tClassification Loss: 0.9595\r\n",
      "Train Epoch: 36 [37120/209539 (18%)]\tAll Loss: 1.4168\tTriple Loss(1): 0.0369\tClassification Loss: 1.3430\r\n",
      "Train Epoch: 36 [37760/209539 (18%)]\tAll Loss: 1.1570\tTriple Loss(1): 0.0000\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 36 [38400/209539 (18%)]\tAll Loss: 1.2853\tTriple Loss(1): 0.0868\tClassification Loss: 1.1118\r\n",
      "Train Epoch: 36 [39040/209539 (19%)]\tAll Loss: 0.7433\tTriple Loss(1): 0.0141\tClassification Loss: 0.7150\r\n",
      "Train Epoch: 36 [39680/209539 (19%)]\tAll Loss: 1.0199\tTriple Loss(1): 0.0147\tClassification Loss: 0.9905\r\n",
      "Train Epoch: 36 [40320/209539 (19%)]\tAll Loss: 1.2438\tTriple Loss(1): 0.0044\tClassification Loss: 1.2349\r\n",
      "Train Epoch: 36 [40960/209539 (20%)]\tAll Loss: 1.3249\tTriple Loss(1): 0.0046\tClassification Loss: 1.3156\r\n",
      "Train Epoch: 36 [41600/209539 (20%)]\tAll Loss: 1.1008\tTriple Loss(1): 0.0000\tClassification Loss: 1.1008\r\n",
      "Train Epoch: 36 [42240/209539 (20%)]\tAll Loss: 1.4757\tTriple Loss(0): 0.2126\tClassification Loss: 1.0506\r\n",
      "Train Epoch: 36 [42880/209539 (20%)]\tAll Loss: 0.8687\tTriple Loss(1): 0.0366\tClassification Loss: 0.7955\r\n",
      "Train Epoch: 36 [43520/209539 (21%)]\tAll Loss: 1.3185\tTriple Loss(1): 0.0000\tClassification Loss: 1.3185\r\n",
      "Train Epoch: 36 [44160/209539 (21%)]\tAll Loss: 1.5489\tTriple Loss(1): 0.0643\tClassification Loss: 1.4204\r\n",
      "Train Epoch: 36 [44800/209539 (21%)]\tAll Loss: 1.5761\tTriple Loss(1): 0.0197\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 36 [45440/209539 (22%)]\tAll Loss: 1.6584\tTriple Loss(1): 0.0721\tClassification Loss: 1.5142\r\n",
      "Train Epoch: 36 [46080/209539 (22%)]\tAll Loss: 1.9412\tTriple Loss(0): 0.4589\tClassification Loss: 1.0234\r\n",
      "Train Epoch: 36 [46720/209539 (22%)]\tAll Loss: 1.5374\tTriple Loss(1): 0.0183\tClassification Loss: 1.5008\r\n",
      "Train Epoch: 36 [47360/209539 (23%)]\tAll Loss: 0.9647\tTriple Loss(1): 0.0265\tClassification Loss: 0.9117\r\n",
      "Train Epoch: 36 [48000/209539 (23%)]\tAll Loss: 1.3437\tTriple Loss(1): 0.0678\tClassification Loss: 1.2080\r\n",
      "Train Epoch: 36 [48640/209539 (23%)]\tAll Loss: 1.4760\tTriple Loss(1): 0.0061\tClassification Loss: 1.4638\r\n",
      "Train Epoch: 36 [49280/209539 (24%)]\tAll Loss: 1.1712\tTriple Loss(1): 0.0117\tClassification Loss: 1.1479\r\n",
      "Train Epoch: 36 [49920/209539 (24%)]\tAll Loss: 1.3078\tTriple Loss(1): 0.0058\tClassification Loss: 1.2962\r\n",
      "Train Epoch: 36 [50560/209539 (24%)]\tAll Loss: 1.2388\tTriple Loss(1): 0.0323\tClassification Loss: 1.1742\r\n",
      "Train Epoch: 36 [51200/209539 (24%)]\tAll Loss: 1.4176\tTriple Loss(1): 0.0307\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 36 [51840/209539 (25%)]\tAll Loss: 1.1066\tTriple Loss(1): 0.0000\tClassification Loss: 1.1066\r\n",
      "Train Epoch: 36 [52480/209539 (25%)]\tAll Loss: 1.1575\tTriple Loss(1): 0.0214\tClassification Loss: 1.1148\r\n",
      "Train Epoch: 36 [53120/209539 (25%)]\tAll Loss: 1.3266\tTriple Loss(1): 0.0235\tClassification Loss: 1.2795\r\n",
      "Train Epoch: 36 [53760/209539 (26%)]\tAll Loss: 2.0962\tTriple Loss(0): 0.3066\tClassification Loss: 1.4829\r\n",
      "Train Epoch: 36 [54400/209539 (26%)]\tAll Loss: 1.6493\tTriple Loss(1): 0.0366\tClassification Loss: 1.5760\r\n",
      "Train Epoch: 36 [55040/209539 (26%)]\tAll Loss: 1.5286\tTriple Loss(0): 0.2804\tClassification Loss: 0.9678\r\n",
      "Train Epoch: 36 [55680/209539 (27%)]\tAll Loss: 1.7952\tTriple Loss(0): 0.2186\tClassification Loss: 1.3581\r\n",
      "Train Epoch: 36 [56320/209539 (27%)]\tAll Loss: 0.7535\tTriple Loss(1): 0.0000\tClassification Loss: 0.7535\r\n",
      "Train Epoch: 36 [56960/209539 (27%)]\tAll Loss: 1.0455\tTriple Loss(1): 0.0496\tClassification Loss: 0.9462\r\n",
      "Train Epoch: 36 [57600/209539 (27%)]\tAll Loss: 0.9946\tTriple Loss(1): 0.0000\tClassification Loss: 0.9946\r\n",
      "Train Epoch: 36 [58240/209539 (28%)]\tAll Loss: 1.1218\tTriple Loss(1): 0.0351\tClassification Loss: 1.0516\r\n",
      "Train Epoch: 36 [58880/209539 (28%)]\tAll Loss: 1.4405\tTriple Loss(1): 0.0405\tClassification Loss: 1.3594\r\n",
      "Train Epoch: 36 [59520/209539 (28%)]\tAll Loss: 1.1587\tTriple Loss(1): 0.0468\tClassification Loss: 1.0652\r\n",
      "Train Epoch: 36 [60160/209539 (29%)]\tAll Loss: 1.2989\tTriple Loss(1): 0.0238\tClassification Loss: 1.2513\r\n",
      "Train Epoch: 36 [60800/209539 (29%)]\tAll Loss: 1.3802\tTriple Loss(1): 0.0278\tClassification Loss: 1.3247\r\n",
      "Train Epoch: 36 [61440/209539 (29%)]\tAll Loss: 1.2496\tTriple Loss(1): 0.0408\tClassification Loss: 1.1680\r\n",
      "Train Epoch: 36 [62080/209539 (30%)]\tAll Loss: 1.4571\tTriple Loss(0): 0.1231\tClassification Loss: 1.2110\r\n",
      "Train Epoch: 36 [62720/209539 (30%)]\tAll Loss: 1.1007\tTriple Loss(1): 0.0000\tClassification Loss: 1.1007\r\n",
      "Train Epoch: 36 [63360/209539 (30%)]\tAll Loss: 1.3892\tTriple Loss(1): 0.0161\tClassification Loss: 1.3571\r\n",
      "Train Epoch: 36 [64000/209539 (31%)]\tAll Loss: 1.4698\tTriple Loss(1): 0.0120\tClassification Loss: 1.4457\r\n",
      "Train Epoch: 36 [64640/209539 (31%)]\tAll Loss: 1.5660\tTriple Loss(0): 0.1093\tClassification Loss: 1.3473\r\n",
      "Train Epoch: 36 [65280/209539 (31%)]\tAll Loss: 1.3012\tTriple Loss(1): 0.0000\tClassification Loss: 1.3012\r\n",
      "Train Epoch: 36 [65920/209539 (31%)]\tAll Loss: 1.3938\tTriple Loss(1): 0.0180\tClassification Loss: 1.3578\r\n",
      "Train Epoch: 36 [66560/209539 (32%)]\tAll Loss: 1.0690\tTriple Loss(1): 0.0171\tClassification Loss: 1.0349\r\n",
      "Train Epoch: 36 [67200/209539 (32%)]\tAll Loss: 1.0649\tTriple Loss(1): 0.0000\tClassification Loss: 1.0649\r\n",
      "Train Epoch: 36 [67840/209539 (32%)]\tAll Loss: 1.6554\tTriple Loss(0): 0.2772\tClassification Loss: 1.1010\r\n",
      "Train Epoch: 36 [68480/209539 (33%)]\tAll Loss: 1.2472\tTriple Loss(1): 0.0128\tClassification Loss: 1.2217\r\n",
      "Train Epoch: 36 [69120/209539 (33%)]\tAll Loss: 1.3149\tTriple Loss(1): 0.0203\tClassification Loss: 1.2743\r\n",
      "Train Epoch: 36 [69760/209539 (33%)]\tAll Loss: 1.6793\tTriple Loss(0): 0.2899\tClassification Loss: 1.0995\r\n",
      "Train Epoch: 36 [70400/209539 (34%)]\tAll Loss: 1.1800\tTriple Loss(1): 0.0166\tClassification Loss: 1.1469\r\n",
      "Train Epoch: 36 [71040/209539 (34%)]\tAll Loss: 1.4750\tTriple Loss(1): 0.0028\tClassification Loss: 1.4694\r\n",
      "Train Epoch: 36 [71680/209539 (34%)]\tAll Loss: 1.5770\tTriple Loss(1): 0.0402\tClassification Loss: 1.4965\r\n",
      "Train Epoch: 36 [72320/209539 (35%)]\tAll Loss: 1.6653\tTriple Loss(0): 0.2714\tClassification Loss: 1.1225\r\n",
      "Train Epoch: 36 [72960/209539 (35%)]\tAll Loss: 1.2793\tTriple Loss(1): 0.0099\tClassification Loss: 1.2595\r\n",
      "Train Epoch: 36 [73600/209539 (35%)]\tAll Loss: 1.4008\tTriple Loss(1): 0.0793\tClassification Loss: 1.2422\r\n",
      "Train Epoch: 36 [74240/209539 (35%)]\tAll Loss: 1.3971\tTriple Loss(1): 0.0218\tClassification Loss: 1.3535\r\n",
      "Train Epoch: 36 [74880/209539 (36%)]\tAll Loss: 2.0316\tTriple Loss(0): 0.2199\tClassification Loss: 1.5917\r\n",
      "Train Epoch: 36 [75520/209539 (36%)]\tAll Loss: 1.0410\tTriple Loss(1): 0.0141\tClassification Loss: 1.0128\r\n",
      "Train Epoch: 36 [76160/209539 (36%)]\tAll Loss: 1.0881\tTriple Loss(1): 0.0172\tClassification Loss: 1.0537\r\n",
      "Train Epoch: 36 [76800/209539 (37%)]\tAll Loss: 0.9978\tTriple Loss(1): 0.0154\tClassification Loss: 0.9670\r\n",
      "Train Epoch: 36 [77440/209539 (37%)]\tAll Loss: 1.2802\tTriple Loss(1): 0.0000\tClassification Loss: 1.2802\r\n",
      "Train Epoch: 36 [78080/209539 (37%)]\tAll Loss: 1.2356\tTriple Loss(1): 0.0335\tClassification Loss: 1.1685\r\n",
      "Train Epoch: 36 [78720/209539 (38%)]\tAll Loss: 1.3587\tTriple Loss(1): 0.0146\tClassification Loss: 1.3295\r\n",
      "Train Epoch: 36 [79360/209539 (38%)]\tAll Loss: 1.1778\tTriple Loss(1): 0.0558\tClassification Loss: 1.0661\r\n",
      "Train Epoch: 36 [80000/209539 (38%)]\tAll Loss: 1.0312\tTriple Loss(1): 0.0000\tClassification Loss: 1.0312\r\n",
      "Train Epoch: 36 [80640/209539 (38%)]\tAll Loss: 1.9158\tTriple Loss(0): 0.2925\tClassification Loss: 1.3308\r\n",
      "Train Epoch: 36 [81280/209539 (39%)]\tAll Loss: 1.3273\tTriple Loss(1): 0.0147\tClassification Loss: 1.2980\r\n",
      "Train Epoch: 36 [81920/209539 (39%)]\tAll Loss: 1.1785\tTriple Loss(1): 0.0023\tClassification Loss: 1.1738\r\n",
      "Train Epoch: 36 [82560/209539 (39%)]\tAll Loss: 1.8250\tTriple Loss(0): 0.2356\tClassification Loss: 1.3539\r\n",
      "Train Epoch: 36 [83200/209539 (40%)]\tAll Loss: 1.3111\tTriple Loss(1): 0.0456\tClassification Loss: 1.2198\r\n",
      "Train Epoch: 36 [83840/209539 (40%)]\tAll Loss: 1.1687\tTriple Loss(1): 0.0000\tClassification Loss: 1.1687\r\n",
      "Train Epoch: 36 [84480/209539 (40%)]\tAll Loss: 1.0110\tTriple Loss(1): 0.0280\tClassification Loss: 0.9550\r\n",
      "Train Epoch: 36 [85120/209539 (41%)]\tAll Loss: 1.5931\tTriple Loss(1): 0.0767\tClassification Loss: 1.4398\r\n",
      "Train Epoch: 36 [85760/209539 (41%)]\tAll Loss: 1.7640\tTriple Loss(0): 0.2287\tClassification Loss: 1.3065\r\n",
      "Train Epoch: 36 [86400/209539 (41%)]\tAll Loss: 1.3176\tTriple Loss(1): 0.0516\tClassification Loss: 1.2144\r\n",
      "Train Epoch: 36 [87040/209539 (42%)]\tAll Loss: 1.7935\tTriple Loss(0): 0.3550\tClassification Loss: 1.0836\r\n",
      "Train Epoch: 36 [87680/209539 (42%)]\tAll Loss: 1.2209\tTriple Loss(1): 0.0543\tClassification Loss: 1.1124\r\n",
      "Train Epoch: 36 [88320/209539 (42%)]\tAll Loss: 1.2056\tTriple Loss(1): 0.0555\tClassification Loss: 1.0945\r\n",
      "Train Epoch: 36 [88960/209539 (42%)]\tAll Loss: 1.4446\tTriple Loss(1): 0.0644\tClassification Loss: 1.3158\r\n",
      "Train Epoch: 36 [89600/209539 (43%)]\tAll Loss: 1.2906\tTriple Loss(1): 0.0343\tClassification Loss: 1.2221\r\n",
      "Train Epoch: 36 [90240/209539 (43%)]\tAll Loss: 1.1204\tTriple Loss(1): 0.0201\tClassification Loss: 1.0801\r\n",
      "Train Epoch: 36 [90880/209539 (43%)]\tAll Loss: 1.5528\tTriple Loss(1): 0.0213\tClassification Loss: 1.5103\r\n",
      "Train Epoch: 36 [91520/209539 (44%)]\tAll Loss: 1.0386\tTriple Loss(1): 0.0040\tClassification Loss: 1.0307\r\n",
      "Train Epoch: 36 [92160/209539 (44%)]\tAll Loss: 1.1378\tTriple Loss(1): 0.0253\tClassification Loss: 1.0872\r\n",
      "Train Epoch: 36 [92800/209539 (44%)]\tAll Loss: 1.2217\tTriple Loss(1): 0.1002\tClassification Loss: 1.0212\r\n",
      "Train Epoch: 36 [93440/209539 (45%)]\tAll Loss: 1.3338\tTriple Loss(1): 0.0102\tClassification Loss: 1.3133\r\n",
      "Train Epoch: 36 [94080/209539 (45%)]\tAll Loss: 1.1745\tTriple Loss(1): 0.0219\tClassification Loss: 1.1307\r\n",
      "Train Epoch: 36 [94720/209539 (45%)]\tAll Loss: 1.8976\tTriple Loss(0): 0.3642\tClassification Loss: 1.1692\r\n",
      "Train Epoch: 36 [95360/209539 (46%)]\tAll Loss: 1.2706\tTriple Loss(1): 0.0131\tClassification Loss: 1.2445\r\n",
      "Train Epoch: 36 [96000/209539 (46%)]\tAll Loss: 1.3535\tTriple Loss(1): 0.0000\tClassification Loss: 1.3535\r\n",
      "Train Epoch: 36 [96640/209539 (46%)]\tAll Loss: 2.0233\tTriple Loss(0): 0.2855\tClassification Loss: 1.4524\r\n",
      "Train Epoch: 36 [97280/209539 (46%)]\tAll Loss: 1.0965\tTriple Loss(1): 0.0331\tClassification Loss: 1.0304\r\n",
      "Train Epoch: 36 [97920/209539 (47%)]\tAll Loss: 1.4339\tTriple Loss(1): 0.0333\tClassification Loss: 1.3674\r\n",
      "Train Epoch: 36 [98560/209539 (47%)]\tAll Loss: 1.3695\tTriple Loss(1): 0.0144\tClassification Loss: 1.3407\r\n",
      "Train Epoch: 36 [99200/209539 (47%)]\tAll Loss: 1.0257\tTriple Loss(1): 0.0156\tClassification Loss: 0.9945\r\n",
      "Train Epoch: 36 [99840/209539 (48%)]\tAll Loss: 1.1999\tTriple Loss(1): 0.0511\tClassification Loss: 1.0977\r\n",
      "Train Epoch: 36 [100480/209539 (48%)]\tAll Loss: 1.1681\tTriple Loss(1): 0.0118\tClassification Loss: 1.1444\r\n",
      "Train Epoch: 36 [101120/209539 (48%)]\tAll Loss: 1.2724\tTriple Loss(1): 0.0211\tClassification Loss: 1.2301\r\n",
      "Train Epoch: 36 [101760/209539 (49%)]\tAll Loss: 1.1370\tTriple Loss(1): 0.0450\tClassification Loss: 1.0470\r\n",
      "Train Epoch: 36 [102400/209539 (49%)]\tAll Loss: 0.9459\tTriple Loss(1): 0.0000\tClassification Loss: 0.9459\r\n",
      "Train Epoch: 36 [103040/209539 (49%)]\tAll Loss: 1.2021\tTriple Loss(1): 0.0306\tClassification Loss: 1.1410\r\n",
      "Train Epoch: 36 [103680/209539 (49%)]\tAll Loss: 1.1324\tTriple Loss(1): 0.0037\tClassification Loss: 1.1250\r\n",
      "Train Epoch: 36 [104320/209539 (50%)]\tAll Loss: 1.5297\tTriple Loss(0): 0.2902\tClassification Loss: 0.9492\r\n",
      "Train Epoch: 36 [104960/209539 (50%)]\tAll Loss: 1.0790\tTriple Loss(1): 0.0000\tClassification Loss: 1.0790\r\n",
      "Train Epoch: 36 [105600/209539 (50%)]\tAll Loss: 1.1942\tTriple Loss(1): 0.0144\tClassification Loss: 1.1654\r\n",
      "Train Epoch: 36 [106240/209539 (51%)]\tAll Loss: 1.5781\tTriple Loss(0): 0.2725\tClassification Loss: 1.0331\r\n",
      "Train Epoch: 36 [106880/209539 (51%)]\tAll Loss: 1.2457\tTriple Loss(1): 0.0083\tClassification Loss: 1.2291\r\n",
      "Train Epoch: 36 [107520/209539 (51%)]\tAll Loss: 1.0230\tTriple Loss(1): 0.0059\tClassification Loss: 1.0112\r\n",
      "Train Epoch: 36 [108160/209539 (52%)]\tAll Loss: 1.0535\tTriple Loss(1): 0.0040\tClassification Loss: 1.0454\r\n",
      "Train Epoch: 36 [108800/209539 (52%)]\tAll Loss: 1.7264\tTriple Loss(0): 0.2870\tClassification Loss: 1.1524\r\n",
      "Train Epoch: 36 [109440/209539 (52%)]\tAll Loss: 1.5860\tTriple Loss(1): 0.0622\tClassification Loss: 1.4617\r\n",
      "Train Epoch: 36 [110080/209539 (53%)]\tAll Loss: 1.3622\tTriple Loss(1): 0.0220\tClassification Loss: 1.3183\r\n",
      "Train Epoch: 36 [110720/209539 (53%)]\tAll Loss: 1.3845\tTriple Loss(1): 0.0350\tClassification Loss: 1.3145\r\n",
      "Train Epoch: 36 [111360/209539 (53%)]\tAll Loss: 1.8332\tTriple Loss(0): 0.3858\tClassification Loss: 1.0616\r\n",
      "Train Epoch: 36 [112000/209539 (53%)]\tAll Loss: 1.2104\tTriple Loss(1): 0.0727\tClassification Loss: 1.0651\r\n",
      "Train Epoch: 36 [112640/209539 (54%)]\tAll Loss: 1.2544\tTriple Loss(1): 0.0687\tClassification Loss: 1.1169\r\n",
      "Train Epoch: 36 [113280/209539 (54%)]\tAll Loss: 0.8721\tTriple Loss(1): 0.0124\tClassification Loss: 0.8472\r\n",
      "Train Epoch: 36 [113920/209539 (54%)]\tAll Loss: 1.2605\tTriple Loss(1): 0.0000\tClassification Loss: 1.2605\r\n",
      "Train Epoch: 36 [114560/209539 (55%)]\tAll Loss: 1.2883\tTriple Loss(1): 0.0406\tClassification Loss: 1.2070\r\n",
      "Train Epoch: 36 [115200/209539 (55%)]\tAll Loss: 2.0032\tTriple Loss(0): 0.3289\tClassification Loss: 1.3455\r\n",
      "Train Epoch: 36 [115840/209539 (55%)]\tAll Loss: 1.2176\tTriple Loss(1): 0.0191\tClassification Loss: 1.1794\r\n",
      "Train Epoch: 36 [116480/209539 (56%)]\tAll Loss: 1.7172\tTriple Loss(0): 0.2398\tClassification Loss: 1.2375\r\n",
      "Train Epoch: 36 [117120/209539 (56%)]\tAll Loss: 1.1794\tTriple Loss(1): 0.0240\tClassification Loss: 1.1314\r\n",
      "Train Epoch: 36 [117760/209539 (56%)]\tAll Loss: 1.2160\tTriple Loss(1): 0.0106\tClassification Loss: 1.1948\r\n",
      "Train Epoch: 36 [118400/209539 (57%)]\tAll Loss: 0.9224\tTriple Loss(1): 0.0025\tClassification Loss: 0.9173\r\n",
      "Train Epoch: 36 [119040/209539 (57%)]\tAll Loss: 1.6965\tTriple Loss(0): 0.2219\tClassification Loss: 1.2528\r\n",
      "Train Epoch: 36 [119680/209539 (57%)]\tAll Loss: 1.2912\tTriple Loss(1): 0.0013\tClassification Loss: 1.2886\r\n",
      "Train Epoch: 36 [120320/209539 (57%)]\tAll Loss: 1.3453\tTriple Loss(1): 0.0076\tClassification Loss: 1.3302\r\n",
      "Train Epoch: 36 [120960/209539 (58%)]\tAll Loss: 1.0700\tTriple Loss(1): 0.0041\tClassification Loss: 1.0618\r\n",
      "Train Epoch: 36 [121600/209539 (58%)]\tAll Loss: 1.3451\tTriple Loss(0): 0.1822\tClassification Loss: 0.9807\r\n",
      "Train Epoch: 36 [122240/209539 (58%)]\tAll Loss: 1.0696\tTriple Loss(1): 0.0236\tClassification Loss: 1.0224\r\n",
      "Train Epoch: 36 [122880/209539 (59%)]\tAll Loss: 0.9904\tTriple Loss(1): 0.0091\tClassification Loss: 0.9722\r\n",
      "Train Epoch: 36 [123520/209539 (59%)]\tAll Loss: 2.1455\tTriple Loss(0): 0.4635\tClassification Loss: 1.2184\r\n",
      "Train Epoch: 36 [124160/209539 (59%)]\tAll Loss: 1.5590\tTriple Loss(0): 0.2031\tClassification Loss: 1.1528\r\n",
      "Train Epoch: 36 [124800/209539 (60%)]\tAll Loss: 1.1160\tTriple Loss(1): 0.0239\tClassification Loss: 1.0682\r\n",
      "Train Epoch: 36 [125440/209539 (60%)]\tAll Loss: 1.6428\tTriple Loss(0): 0.2583\tClassification Loss: 1.1262\r\n",
      "Train Epoch: 36 [126080/209539 (60%)]\tAll Loss: 1.4082\tTriple Loss(1): 0.0406\tClassification Loss: 1.3271\r\n",
      "Train Epoch: 36 [126720/209539 (60%)]\tAll Loss: 1.3712\tTriple Loss(1): 0.0116\tClassification Loss: 1.3479\r\n",
      "Train Epoch: 36 [127360/209539 (61%)]\tAll Loss: 1.6198\tTriple Loss(1): 0.0416\tClassification Loss: 1.5367\r\n",
      "Train Epoch: 36 [128000/209539 (61%)]\tAll Loss: 1.6856\tTriple Loss(1): 0.0887\tClassification Loss: 1.5082\r\n",
      "Train Epoch: 36 [128640/209539 (61%)]\tAll Loss: 0.9161\tTriple Loss(1): 0.0275\tClassification Loss: 0.8611\r\n",
      "Train Epoch: 36 [129280/209539 (62%)]\tAll Loss: 2.1115\tTriple Loss(0): 0.4620\tClassification Loss: 1.1876\r\n",
      "Train Epoch: 36 [129920/209539 (62%)]\tAll Loss: 1.1175\tTriple Loss(1): 0.0465\tClassification Loss: 1.0245\r\n",
      "Train Epoch: 36 [130560/209539 (62%)]\tAll Loss: 0.9013\tTriple Loss(1): 0.0104\tClassification Loss: 0.8805\r\n",
      "Train Epoch: 36 [131200/209539 (63%)]\tAll Loss: 0.9608\tTriple Loss(1): 0.0134\tClassification Loss: 0.9341\r\n",
      "Train Epoch: 36 [131840/209539 (63%)]\tAll Loss: 1.0308\tTriple Loss(1): 0.0244\tClassification Loss: 0.9820\r\n",
      "Train Epoch: 36 [132480/209539 (63%)]\tAll Loss: 1.1523\tTriple Loss(1): 0.0564\tClassification Loss: 1.0394\r\n",
      "Train Epoch: 36 [133120/209539 (64%)]\tAll Loss: 1.2059\tTriple Loss(1): 0.0671\tClassification Loss: 1.0716\r\n",
      "Train Epoch: 36 [133760/209539 (64%)]\tAll Loss: 1.4932\tTriple Loss(0): 0.1526\tClassification Loss: 1.1880\r\n",
      "Train Epoch: 36 [134400/209539 (64%)]\tAll Loss: 1.1627\tTriple Loss(1): 0.0327\tClassification Loss: 1.0973\r\n",
      "Train Epoch: 36 [135040/209539 (64%)]\tAll Loss: 1.1201\tTriple Loss(1): 0.0000\tClassification Loss: 1.1201\r\n",
      "Train Epoch: 36 [135680/209539 (65%)]\tAll Loss: 2.0144\tTriple Loss(0): 0.2796\tClassification Loss: 1.4551\r\n",
      "Train Epoch: 36 [136320/209539 (65%)]\tAll Loss: 1.4397\tTriple Loss(1): 0.0569\tClassification Loss: 1.3260\r\n",
      "Train Epoch: 36 [136960/209539 (65%)]\tAll Loss: 1.6498\tTriple Loss(0): 0.1886\tClassification Loss: 1.2726\r\n",
      "Train Epoch: 36 [137600/209539 (66%)]\tAll Loss: 1.1416\tTriple Loss(1): 0.0000\tClassification Loss: 1.1416\r\n",
      "Train Epoch: 36 [138240/209539 (66%)]\tAll Loss: 1.4211\tTriple Loss(1): 0.0443\tClassification Loss: 1.3324\r\n",
      "Train Epoch: 36 [138880/209539 (66%)]\tAll Loss: 1.4039\tTriple Loss(1): 0.0389\tClassification Loss: 1.3260\r\n",
      "Train Epoch: 36 [139520/209539 (67%)]\tAll Loss: 1.1210\tTriple Loss(1): 0.0262\tClassification Loss: 1.0686\r\n",
      "Train Epoch: 36 [140160/209539 (67%)]\tAll Loss: 1.3092\tTriple Loss(1): 0.0403\tClassification Loss: 1.2286\r\n",
      "Train Epoch: 36 [140800/209539 (67%)]\tAll Loss: 1.4610\tTriple Loss(1): 0.0297\tClassification Loss: 1.4016\r\n",
      "Train Epoch: 36 [141440/209539 (68%)]\tAll Loss: 1.3864\tTriple Loss(1): 0.0484\tClassification Loss: 1.2896\r\n",
      "Train Epoch: 36 [142080/209539 (68%)]\tAll Loss: 1.4289\tTriple Loss(1): 0.0833\tClassification Loss: 1.2622\r\n",
      "Train Epoch: 36 [142720/209539 (68%)]\tAll Loss: 1.7802\tTriple Loss(1): 0.0575\tClassification Loss: 1.6652\r\n",
      "Train Epoch: 36 [143360/209539 (68%)]\tAll Loss: 0.8800\tTriple Loss(1): 0.0015\tClassification Loss: 0.8770\r\n",
      "Train Epoch: 36 [144000/209539 (69%)]\tAll Loss: 1.2356\tTriple Loss(1): 0.0037\tClassification Loss: 1.2282\r\n",
      "Train Epoch: 36 [144640/209539 (69%)]\tAll Loss: 1.5370\tTriple Loss(0): 0.2527\tClassification Loss: 1.0316\r\n",
      "Train Epoch: 36 [145280/209539 (69%)]\tAll Loss: 1.1844\tTriple Loss(1): 0.0000\tClassification Loss: 1.1844\r\n",
      "Train Epoch: 36 [145920/209539 (70%)]\tAll Loss: 0.9792\tTriple Loss(1): 0.0109\tClassification Loss: 0.9575\r\n",
      "Train Epoch: 36 [146560/209539 (70%)]\tAll Loss: 1.1067\tTriple Loss(1): 0.0127\tClassification Loss: 1.0813\r\n",
      "Train Epoch: 36 [147200/209539 (70%)]\tAll Loss: 1.2441\tTriple Loss(1): 0.0511\tClassification Loss: 1.1418\r\n",
      "Train Epoch: 36 [147840/209539 (71%)]\tAll Loss: 1.2189\tTriple Loss(1): 0.0107\tClassification Loss: 1.1975\r\n",
      "Train Epoch: 36 [148480/209539 (71%)]\tAll Loss: 0.9655\tTriple Loss(1): 0.0578\tClassification Loss: 0.8499\r\n",
      "Train Epoch: 36 [149120/209539 (71%)]\tAll Loss: 1.4169\tTriple Loss(1): 0.0416\tClassification Loss: 1.3336\r\n",
      "Train Epoch: 36 [149760/209539 (71%)]\tAll Loss: 1.1029\tTriple Loss(1): 0.0178\tClassification Loss: 1.0673\r\n",
      "Train Epoch: 36 [150400/209539 (72%)]\tAll Loss: 1.2821\tTriple Loss(1): 0.0287\tClassification Loss: 1.2248\r\n",
      "Train Epoch: 36 [151040/209539 (72%)]\tAll Loss: 0.9922\tTriple Loss(1): 0.0058\tClassification Loss: 0.9806\r\n",
      "Train Epoch: 36 [151680/209539 (72%)]\tAll Loss: 1.4035\tTriple Loss(1): 0.0067\tClassification Loss: 1.3900\r\n",
      "Train Epoch: 36 [152320/209539 (73%)]\tAll Loss: 1.2046\tTriple Loss(0): 0.0768\tClassification Loss: 1.0511\r\n",
      "Train Epoch: 36 [152960/209539 (73%)]\tAll Loss: 1.0757\tTriple Loss(1): 0.0326\tClassification Loss: 1.0105\r\n",
      "Train Epoch: 36 [153600/209539 (73%)]\tAll Loss: 1.1787\tTriple Loss(1): 0.0000\tClassification Loss: 1.1787\r\n",
      "Train Epoch: 36 [154240/209539 (74%)]\tAll Loss: 1.2347\tTriple Loss(1): 0.0099\tClassification Loss: 1.2149\r\n",
      "Train Epoch: 36 [154880/209539 (74%)]\tAll Loss: 1.3436\tTriple Loss(1): 0.0000\tClassification Loss: 1.3436\r\n",
      "Train Epoch: 36 [155520/209539 (74%)]\tAll Loss: 1.0844\tTriple Loss(1): 0.0197\tClassification Loss: 1.0450\r\n",
      "Train Epoch: 36 [156160/209539 (75%)]\tAll Loss: 1.9113\tTriple Loss(0): 0.2995\tClassification Loss: 1.3122\r\n",
      "Train Epoch: 36 [156800/209539 (75%)]\tAll Loss: 1.4486\tTriple Loss(1): 0.0052\tClassification Loss: 1.4382\r\n",
      "Train Epoch: 36 [157440/209539 (75%)]\tAll Loss: 1.4429\tTriple Loss(1): 0.0000\tClassification Loss: 1.4429\r\n",
      "Train Epoch: 36 [158080/209539 (75%)]\tAll Loss: 1.6039\tTriple Loss(1): 0.1078\tClassification Loss: 1.3882\r\n",
      "Train Epoch: 36 [158720/209539 (76%)]\tAll Loss: 1.7019\tTriple Loss(0): 0.3261\tClassification Loss: 1.0497\r\n",
      "Train Epoch: 36 [159360/209539 (76%)]\tAll Loss: 0.9778\tTriple Loss(1): 0.0341\tClassification Loss: 0.9097\r\n",
      "Train Epoch: 36 [160000/209539 (76%)]\tAll Loss: 1.5187\tTriple Loss(1): 0.0554\tClassification Loss: 1.4080\r\n",
      "Train Epoch: 36 [160640/209539 (77%)]\tAll Loss: 1.6294\tTriple Loss(0): 0.2984\tClassification Loss: 1.0325\r\n",
      "Train Epoch: 36 [161280/209539 (77%)]\tAll Loss: 1.1580\tTriple Loss(1): 0.0157\tClassification Loss: 1.1266\r\n",
      "Train Epoch: 36 [161920/209539 (77%)]\tAll Loss: 1.0746\tTriple Loss(1): 0.0000\tClassification Loss: 1.0746\r\n",
      "Train Epoch: 36 [162560/209539 (78%)]\tAll Loss: 1.1827\tTriple Loss(1): 0.0714\tClassification Loss: 1.0399\r\n",
      "Train Epoch: 36 [163200/209539 (78%)]\tAll Loss: 1.1046\tTriple Loss(1): 0.0194\tClassification Loss: 1.0657\r\n",
      "Train Epoch: 36 [163840/209539 (78%)]\tAll Loss: 1.2447\tTriple Loss(1): 0.0979\tClassification Loss: 1.0489\r\n",
      "Train Epoch: 36 [164480/209539 (78%)]\tAll Loss: 1.1383\tTriple Loss(1): 0.0705\tClassification Loss: 0.9972\r\n",
      "Train Epoch: 36 [165120/209539 (79%)]\tAll Loss: 1.0765\tTriple Loss(1): 0.0546\tClassification Loss: 0.9673\r\n",
      "Train Epoch: 36 [165760/209539 (79%)]\tAll Loss: 1.8348\tTriple Loss(0): 0.2902\tClassification Loss: 1.2544\r\n",
      "Train Epoch: 36 [166400/209539 (79%)]\tAll Loss: 1.3976\tTriple Loss(1): 0.0363\tClassification Loss: 1.3251\r\n",
      "Train Epoch: 36 [167040/209539 (80%)]\tAll Loss: 2.0068\tTriple Loss(0): 0.2641\tClassification Loss: 1.4786\r\n",
      "Train Epoch: 36 [167680/209539 (80%)]\tAll Loss: 1.1965\tTriple Loss(1): 0.0000\tClassification Loss: 1.1964\r\n",
      "Train Epoch: 36 [168320/209539 (80%)]\tAll Loss: 1.2665\tTriple Loss(1): 0.0479\tClassification Loss: 1.1707\r\n",
      "Train Epoch: 36 [168960/209539 (81%)]\tAll Loss: 1.1571\tTriple Loss(1): 0.0407\tClassification Loss: 1.0757\r\n",
      "Train Epoch: 36 [169600/209539 (81%)]\tAll Loss: 1.3316\tTriple Loss(1): 0.1028\tClassification Loss: 1.1261\r\n",
      "Train Epoch: 36 [170240/209539 (81%)]\tAll Loss: 1.1629\tTriple Loss(1): 0.0000\tClassification Loss: 1.1629\r\n",
      "Train Epoch: 36 [170880/209539 (82%)]\tAll Loss: 1.3372\tTriple Loss(1): 0.0246\tClassification Loss: 1.2880\r\n",
      "Train Epoch: 36 [171520/209539 (82%)]\tAll Loss: 1.6692\tTriple Loss(0): 0.3226\tClassification Loss: 1.0239\r\n",
      "Train Epoch: 36 [172160/209539 (82%)]\tAll Loss: 1.4606\tTriple Loss(1): 0.0603\tClassification Loss: 1.3401\r\n",
      "Train Epoch: 36 [172800/209539 (82%)]\tAll Loss: 1.2266\tTriple Loss(1): 0.0299\tClassification Loss: 1.1669\r\n",
      "Train Epoch: 36 [173440/209539 (83%)]\tAll Loss: 1.3257\tTriple Loss(1): 0.0208\tClassification Loss: 1.2841\r\n",
      "Train Epoch: 36 [174080/209539 (83%)]\tAll Loss: 1.1892\tTriple Loss(1): 0.0215\tClassification Loss: 1.1461\r\n",
      "Train Epoch: 36 [174720/209539 (83%)]\tAll Loss: 1.3581\tTriple Loss(1): 0.0971\tClassification Loss: 1.1639\r\n",
      "Train Epoch: 36 [175360/209539 (84%)]\tAll Loss: 1.8653\tTriple Loss(0): 0.3208\tClassification Loss: 1.2238\r\n",
      "Train Epoch: 36 [176000/209539 (84%)]\tAll Loss: 1.2264\tTriple Loss(1): 0.0000\tClassification Loss: 1.2264\r\n",
      "Train Epoch: 36 [176640/209539 (84%)]\tAll Loss: 1.1224\tTriple Loss(0): 0.1604\tClassification Loss: 0.8017\r\n",
      "Train Epoch: 36 [177280/209539 (85%)]\tAll Loss: 1.3892\tTriple Loss(1): 0.0000\tClassification Loss: 1.3892\r\n",
      "Train Epoch: 36 [177920/209539 (85%)]\tAll Loss: 1.7444\tTriple Loss(0): 0.2969\tClassification Loss: 1.1507\r\n",
      "Train Epoch: 36 [178560/209539 (85%)]\tAll Loss: 1.0740\tTriple Loss(1): 0.0003\tClassification Loss: 1.0734\r\n",
      "Train Epoch: 36 [179200/209539 (86%)]\tAll Loss: 1.4291\tTriple Loss(1): 0.0488\tClassification Loss: 1.3314\r\n",
      "Train Epoch: 36 [179840/209539 (86%)]\tAll Loss: 1.3700\tTriple Loss(1): 0.0243\tClassification Loss: 1.3214\r\n",
      "Train Epoch: 36 [180480/209539 (86%)]\tAll Loss: 1.1840\tTriple Loss(1): 0.0000\tClassification Loss: 1.1840\r\n",
      "Train Epoch: 36 [181120/209539 (86%)]\tAll Loss: 1.4714\tTriple Loss(1): 0.0387\tClassification Loss: 1.3939\r\n",
      "Train Epoch: 36 [181760/209539 (87%)]\tAll Loss: 1.1688\tTriple Loss(1): 0.0414\tClassification Loss: 1.0861\r\n",
      "Train Epoch: 36 [182400/209539 (87%)]\tAll Loss: 1.4403\tTriple Loss(1): 0.0029\tClassification Loss: 1.4345\r\n",
      "Train Epoch: 36 [183040/209539 (87%)]\tAll Loss: 1.2495\tTriple Loss(1): 0.0090\tClassification Loss: 1.2315\r\n",
      "Train Epoch: 36 [183680/209539 (88%)]\tAll Loss: 1.6728\tTriple Loss(0): 0.3446\tClassification Loss: 0.9835\r\n",
      "Train Epoch: 36 [184320/209539 (88%)]\tAll Loss: 1.2048\tTriple Loss(1): 0.0191\tClassification Loss: 1.1666\r\n",
      "Train Epoch: 36 [184960/209539 (88%)]\tAll Loss: 1.0676\tTriple Loss(1): 0.0311\tClassification Loss: 1.0054\r\n",
      "Train Epoch: 36 [185600/209539 (89%)]\tAll Loss: 1.4168\tTriple Loss(0): 0.1281\tClassification Loss: 1.1606\r\n",
      "Train Epoch: 36 [186240/209539 (89%)]\tAll Loss: 1.3491\tTriple Loss(1): 0.0000\tClassification Loss: 1.3491\r\n",
      "Train Epoch: 36 [186880/209539 (89%)]\tAll Loss: 1.1982\tTriple Loss(1): 0.0000\tClassification Loss: 1.1982\r\n",
      "Train Epoch: 36 [187520/209539 (89%)]\tAll Loss: 1.6372\tTriple Loss(0): 0.1565\tClassification Loss: 1.3242\r\n",
      "Train Epoch: 36 [188160/209539 (90%)]\tAll Loss: 0.9757\tTriple Loss(1): 0.0054\tClassification Loss: 0.9650\r\n",
      "Train Epoch: 36 [188800/209539 (90%)]\tAll Loss: 1.1974\tTriple Loss(1): 0.0517\tClassification Loss: 1.0940\r\n",
      "Train Epoch: 36 [189440/209539 (90%)]\tAll Loss: 1.0093\tTriple Loss(1): 0.0148\tClassification Loss: 0.9797\r\n",
      "Train Epoch: 36 [190080/209539 (91%)]\tAll Loss: 1.1669\tTriple Loss(1): 0.0000\tClassification Loss: 1.1669\r\n",
      "Train Epoch: 36 [190720/209539 (91%)]\tAll Loss: 1.2440\tTriple Loss(1): 0.0199\tClassification Loss: 1.2042\r\n",
      "Train Epoch: 36 [191360/209539 (91%)]\tAll Loss: 0.8963\tTriple Loss(1): 0.0000\tClassification Loss: 0.8963\r\n",
      "Train Epoch: 36 [192000/209539 (92%)]\tAll Loss: 2.0062\tTriple Loss(0): 0.2801\tClassification Loss: 1.4460\r\n",
      "Train Epoch: 36 [192640/209539 (92%)]\tAll Loss: 0.9539\tTriple Loss(1): 0.0000\tClassification Loss: 0.9539\r\n",
      "Train Epoch: 36 [193280/209539 (92%)]\tAll Loss: 1.2800\tTriple Loss(1): 0.0567\tClassification Loss: 1.1666\r\n",
      "Train Epoch: 36 [193920/209539 (93%)]\tAll Loss: 1.4099\tTriple Loss(0): 0.1634\tClassification Loss: 1.0832\r\n",
      "Train Epoch: 36 [194560/209539 (93%)]\tAll Loss: 1.1470\tTriple Loss(1): 0.0000\tClassification Loss: 1.1470\r\n",
      "Train Epoch: 36 [195200/209539 (93%)]\tAll Loss: 1.3233\tTriple Loss(1): 0.0165\tClassification Loss: 1.2903\r\n",
      "Train Epoch: 36 [195840/209539 (93%)]\tAll Loss: 0.7990\tTriple Loss(1): 0.0292\tClassification Loss: 0.7405\r\n",
      "Train Epoch: 36 [196480/209539 (94%)]\tAll Loss: 1.9220\tTriple Loss(0): 0.2689\tClassification Loss: 1.3843\r\n",
      "Train Epoch: 36 [197120/209539 (94%)]\tAll Loss: 2.0711\tTriple Loss(0): 0.3251\tClassification Loss: 1.4209\r\n",
      "Train Epoch: 36 [197760/209539 (94%)]\tAll Loss: 1.2214\tTriple Loss(1): 0.0118\tClassification Loss: 1.1978\r\n",
      "Train Epoch: 36 [198400/209539 (95%)]\tAll Loss: 1.8245\tTriple Loss(0): 0.3570\tClassification Loss: 1.1104\r\n",
      "Train Epoch: 36 [199040/209539 (95%)]\tAll Loss: 1.2818\tTriple Loss(1): 0.0000\tClassification Loss: 1.2818\r\n",
      "Train Epoch: 36 [199680/209539 (95%)]\tAll Loss: 1.2547\tTriple Loss(1): 0.0471\tClassification Loss: 1.1605\r\n",
      "Train Epoch: 36 [200320/209539 (96%)]\tAll Loss: 1.3376\tTriple Loss(1): 0.0304\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 36 [200960/209539 (96%)]\tAll Loss: 0.9991\tTriple Loss(1): 0.0171\tClassification Loss: 0.9650\r\n",
      "Train Epoch: 36 [201600/209539 (96%)]\tAll Loss: 1.0553\tTriple Loss(1): 0.0266\tClassification Loss: 1.0022\r\n",
      "Train Epoch: 36 [202240/209539 (97%)]\tAll Loss: 1.3510\tTriple Loss(1): 0.0175\tClassification Loss: 1.3160\r\n",
      "Train Epoch: 36 [202880/209539 (97%)]\tAll Loss: 1.4420\tTriple Loss(0): 0.2354\tClassification Loss: 0.9712\r\n",
      "Train Epoch: 36 [203520/209539 (97%)]\tAll Loss: 1.2560\tTriple Loss(1): 0.0000\tClassification Loss: 1.2560\r\n",
      "Train Epoch: 36 [204160/209539 (97%)]\tAll Loss: 1.7551\tTriple Loss(1): 0.0323\tClassification Loss: 1.6904\r\n",
      "Train Epoch: 36 [204800/209539 (98%)]\tAll Loss: 1.6091\tTriple Loss(1): 0.0629\tClassification Loss: 1.4833\r\n",
      "Train Epoch: 36 [205440/209539 (98%)]\tAll Loss: 1.7476\tTriple Loss(0): 0.4475\tClassification Loss: 0.8526\r\n",
      "Train Epoch: 36 [206080/209539 (98%)]\tAll Loss: 1.2994\tTriple Loss(1): 0.0027\tClassification Loss: 1.2940\r\n",
      "Train Epoch: 36 [206720/209539 (99%)]\tAll Loss: 0.9423\tTriple Loss(1): 0.0000\tClassification Loss: 0.9423\r\n",
      "Train Epoch: 36 [207360/209539 (99%)]\tAll Loss: 1.0498\tTriple Loss(1): 0.0043\tClassification Loss: 1.0413\r\n",
      "Train Epoch: 36 [208000/209539 (99%)]\tAll Loss: 1.0811\tTriple Loss(1): 0.0145\tClassification Loss: 1.0520\r\n",
      "Train Epoch: 36 [208640/209539 (100%)]\tAll Loss: 1.2767\tTriple Loss(1): 0.0053\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 36 [209280/209539 (100%)]\tAll Loss: 2.0872\tTriple Loss(0): 0.4422\tClassification Loss: 1.2029\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/36_epochs\r\n",
      "Train Epoch: 37 [0/209539 (0%)]\tAll Loss: 1.7710\tTriple Loss(1): 0.1550\tClassification Loss: 1.4610\r\n",
      "\r\n",
      "Test set: Average loss: 1.0665\r\n",
      "Top 1 Accuracy: 55271/80128 (69%)\r\n",
      "Top 3 Accuracy: 70450/80128 (88%)\r\n",
      "Top 5 Accuracy: 74883/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 37 [640/209539 (0%)]\tAll Loss: 1.8733\tTriple Loss(0): 0.3182\tClassification Loss: 1.2368\r\n",
      "Train Epoch: 37 [1280/209539 (1%)]\tAll Loss: 1.4690\tTriple Loss(1): 0.1549\tClassification Loss: 1.1593\r\n",
      "Train Epoch: 37 [1920/209539 (1%)]\tAll Loss: 1.2488\tTriple Loss(1): 0.0733\tClassification Loss: 1.1022\r\n",
      "Train Epoch: 37 [2560/209539 (1%)]\tAll Loss: 1.4115\tTriple Loss(1): 0.0031\tClassification Loss: 1.4053\r\n",
      "Train Epoch: 37 [3200/209539 (2%)]\tAll Loss: 1.3712\tTriple Loss(1): 0.0000\tClassification Loss: 1.3712\r\n",
      "Train Epoch: 37 [3840/209539 (2%)]\tAll Loss: 1.0183\tTriple Loss(1): 0.0110\tClassification Loss: 0.9963\r\n",
      "Train Epoch: 37 [4480/209539 (2%)]\tAll Loss: 1.3059\tTriple Loss(1): 0.0004\tClassification Loss: 1.3050\r\n",
      "Train Epoch: 37 [5120/209539 (2%)]\tAll Loss: 1.2558\tTriple Loss(1): 0.0368\tClassification Loss: 1.1822\r\n",
      "Train Epoch: 37 [5760/209539 (3%)]\tAll Loss: 1.6117\tTriple Loss(0): 0.2184\tClassification Loss: 1.1750\r\n",
      "Train Epoch: 37 [6400/209539 (3%)]\tAll Loss: 0.9578\tTriple Loss(1): 0.0000\tClassification Loss: 0.9578\r\n",
      "Train Epoch: 37 [7040/209539 (3%)]\tAll Loss: 1.1589\tTriple Loss(1): 0.0692\tClassification Loss: 1.0206\r\n",
      "Train Epoch: 37 [7680/209539 (4%)]\tAll Loss: 1.1081\tTriple Loss(1): 0.0787\tClassification Loss: 0.9507\r\n",
      "Train Epoch: 37 [8320/209539 (4%)]\tAll Loss: 1.1059\tTriple Loss(1): 0.0860\tClassification Loss: 0.9339\r\n",
      "Train Epoch: 37 [8960/209539 (4%)]\tAll Loss: 1.1897\tTriple Loss(1): 0.0144\tClassification Loss: 1.1608\r\n",
      "Train Epoch: 37 [9600/209539 (5%)]\tAll Loss: 1.4809\tTriple Loss(1): 0.0209\tClassification Loss: 1.4390\r\n",
      "Train Epoch: 37 [10240/209539 (5%)]\tAll Loss: 1.1749\tTriple Loss(1): 0.0389\tClassification Loss: 1.0971\r\n",
      "Train Epoch: 37 [10880/209539 (5%)]\tAll Loss: 1.1521\tTriple Loss(1): 0.0000\tClassification Loss: 1.1521\r\n",
      "Train Epoch: 37 [11520/209539 (5%)]\tAll Loss: 1.2084\tTriple Loss(1): 0.0058\tClassification Loss: 1.1968\r\n",
      "Train Epoch: 37 [12160/209539 (6%)]\tAll Loss: 0.9802\tTriple Loss(1): 0.0156\tClassification Loss: 0.9489\r\n",
      "Train Epoch: 37 [12800/209539 (6%)]\tAll Loss: 0.9442\tTriple Loss(1): 0.0153\tClassification Loss: 0.9137\r\n",
      "Train Epoch: 37 [13440/209539 (6%)]\tAll Loss: 0.9833\tTriple Loss(1): 0.0114\tClassification Loss: 0.9605\r\n",
      "Train Epoch: 37 [14080/209539 (7%)]\tAll Loss: 1.3817\tTriple Loss(1): 0.0135\tClassification Loss: 1.3546\r\n",
      "Train Epoch: 37 [14720/209539 (7%)]\tAll Loss: 1.2917\tTriple Loss(1): 0.0131\tClassification Loss: 1.2655\r\n",
      "Train Epoch: 37 [15360/209539 (7%)]\tAll Loss: 1.1146\tTriple Loss(1): 0.0087\tClassification Loss: 1.0973\r\n",
      "Train Epoch: 37 [16000/209539 (8%)]\tAll Loss: 1.7226\tTriple Loss(0): 0.1413\tClassification Loss: 1.4400\r\n",
      "Train Epoch: 37 [16640/209539 (8%)]\tAll Loss: 1.5142\tTriple Loss(1): 0.0150\tClassification Loss: 1.4842\r\n",
      "Train Epoch: 37 [17280/209539 (8%)]\tAll Loss: 1.1533\tTriple Loss(1): 0.0173\tClassification Loss: 1.1187\r\n",
      "Train Epoch: 37 [17920/209539 (9%)]\tAll Loss: 1.2182\tTriple Loss(1): 0.0139\tClassification Loss: 1.1904\r\n",
      "Train Epoch: 37 [18560/209539 (9%)]\tAll Loss: 1.1361\tTriple Loss(1): 0.0115\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 37 [19200/209539 (9%)]\tAll Loss: 1.1809\tTriple Loss(1): 0.0090\tClassification Loss: 1.1629\r\n",
      "Train Epoch: 37 [19840/209539 (9%)]\tAll Loss: 1.7002\tTriple Loss(0): 0.2587\tClassification Loss: 1.1828\r\n",
      "Train Epoch: 37 [20480/209539 (10%)]\tAll Loss: 0.9893\tTriple Loss(1): 0.0000\tClassification Loss: 0.9893\r\n",
      "Train Epoch: 37 [21120/209539 (10%)]\tAll Loss: 1.3601\tTriple Loss(1): 0.0009\tClassification Loss: 1.3582\r\n",
      "Train Epoch: 37 [21760/209539 (10%)]\tAll Loss: 1.4035\tTriple Loss(0): 0.1792\tClassification Loss: 1.0452\r\n",
      "Train Epoch: 37 [22400/209539 (11%)]\tAll Loss: 0.9572\tTriple Loss(1): 0.0035\tClassification Loss: 0.9503\r\n",
      "Train Epoch: 37 [23040/209539 (11%)]\tAll Loss: 1.4277\tTriple Loss(0): 0.1326\tClassification Loss: 1.1625\r\n",
      "Train Epoch: 37 [23680/209539 (11%)]\tAll Loss: 0.9174\tTriple Loss(1): 0.0373\tClassification Loss: 0.8428\r\n",
      "Train Epoch: 37 [24320/209539 (12%)]\tAll Loss: 1.9410\tTriple Loss(0): 0.3059\tClassification Loss: 1.3292\r\n",
      "Train Epoch: 37 [24960/209539 (12%)]\tAll Loss: 1.1658\tTriple Loss(1): 0.0000\tClassification Loss: 1.1658\r\n",
      "Train Epoch: 37 [25600/209539 (12%)]\tAll Loss: 1.1791\tTriple Loss(1): 0.0197\tClassification Loss: 1.1397\r\n",
      "Train Epoch: 37 [26240/209539 (13%)]\tAll Loss: 1.1777\tTriple Loss(1): 0.0629\tClassification Loss: 1.0518\r\n",
      "Train Epoch: 37 [26880/209539 (13%)]\tAll Loss: 1.3716\tTriple Loss(1): 0.0355\tClassification Loss: 1.3005\r\n",
      "Train Epoch: 37 [27520/209539 (13%)]\tAll Loss: 1.1070\tTriple Loss(1): 0.0175\tClassification Loss: 1.0720\r\n",
      "Train Epoch: 37 [28160/209539 (13%)]\tAll Loss: 1.2296\tTriple Loss(1): 0.0127\tClassification Loss: 1.2041\r\n",
      "Train Epoch: 37 [28800/209539 (14%)]\tAll Loss: 1.5043\tTriple Loss(1): 0.0000\tClassification Loss: 1.5043\r\n",
      "Train Epoch: 37 [29440/209539 (14%)]\tAll Loss: 1.4793\tTriple Loss(1): 0.0023\tClassification Loss: 1.4747\r\n",
      "Train Epoch: 37 [30080/209539 (14%)]\tAll Loss: 1.0883\tTriple Loss(1): 0.0195\tClassification Loss: 1.0494\r\n",
      "Train Epoch: 37 [30720/209539 (15%)]\tAll Loss: 0.9784\tTriple Loss(1): 0.0324\tClassification Loss: 0.9136\r\n",
      "Train Epoch: 37 [31360/209539 (15%)]\tAll Loss: 1.1661\tTriple Loss(1): 0.0365\tClassification Loss: 1.0932\r\n",
      "Train Epoch: 37 [32000/209539 (15%)]\tAll Loss: 1.3687\tTriple Loss(1): 0.0758\tClassification Loss: 1.2171\r\n",
      "Train Epoch: 37 [32640/209539 (16%)]\tAll Loss: 1.4982\tTriple Loss(0): 0.1856\tClassification Loss: 1.1269\r\n",
      "Train Epoch: 37 [33280/209539 (16%)]\tAll Loss: 1.6367\tTriple Loss(0): 0.2295\tClassification Loss: 1.1777\r\n",
      "Train Epoch: 37 [33920/209539 (16%)]\tAll Loss: 1.1094\tTriple Loss(1): 0.0000\tClassification Loss: 1.1094\r\n",
      "Train Epoch: 37 [34560/209539 (16%)]\tAll Loss: 1.1755\tTriple Loss(1): 0.0000\tClassification Loss: 1.1755\r\n",
      "Train Epoch: 37 [35200/209539 (17%)]\tAll Loss: 1.1450\tTriple Loss(1): 0.0732\tClassification Loss: 0.9986\r\n",
      "Train Epoch: 37 [35840/209539 (17%)]\tAll Loss: 0.9336\tTriple Loss(1): 0.0386\tClassification Loss: 0.8564\r\n",
      "Train Epoch: 37 [36480/209539 (17%)]\tAll Loss: 1.1286\tTriple Loss(1): 0.0517\tClassification Loss: 1.0253\r\n",
      "Train Epoch: 37 [37120/209539 (18%)]\tAll Loss: 1.3788\tTriple Loss(1): 0.0009\tClassification Loss: 1.3770\r\n",
      "Train Epoch: 37 [37760/209539 (18%)]\tAll Loss: 1.3842\tTriple Loss(1): 0.0064\tClassification Loss: 1.3714\r\n",
      "Train Epoch: 37 [38400/209539 (18%)]\tAll Loss: 1.2353\tTriple Loss(1): 0.0065\tClassification Loss: 1.2224\r\n",
      "Train Epoch: 37 [39040/209539 (19%)]\tAll Loss: 0.8731\tTriple Loss(1): 0.0220\tClassification Loss: 0.8291\r\n",
      "Train Epoch: 37 [39680/209539 (19%)]\tAll Loss: 0.9907\tTriple Loss(1): 0.0000\tClassification Loss: 0.9907\r\n",
      "Train Epoch: 37 [40320/209539 (19%)]\tAll Loss: 1.2576\tTriple Loss(1): 0.0000\tClassification Loss: 1.2576\r\n",
      "Train Epoch: 37 [40960/209539 (20%)]\tAll Loss: 1.1666\tTriple Loss(1): 0.0443\tClassification Loss: 1.0779\r\n",
      "Train Epoch: 37 [41600/209539 (20%)]\tAll Loss: 1.2197\tTriple Loss(1): 0.0151\tClassification Loss: 1.1895\r\n",
      "Train Epoch: 37 [42240/209539 (20%)]\tAll Loss: 1.1229\tTriple Loss(1): 0.0116\tClassification Loss: 1.0998\r\n",
      "Train Epoch: 37 [42880/209539 (20%)]\tAll Loss: 0.9660\tTriple Loss(1): 0.0000\tClassification Loss: 0.9660\r\n",
      "Train Epoch: 37 [43520/209539 (21%)]\tAll Loss: 1.1691\tTriple Loss(1): 0.0046\tClassification Loss: 1.1599\r\n",
      "Train Epoch: 37 [44160/209539 (21%)]\tAll Loss: 2.0486\tTriple Loss(0): 0.3706\tClassification Loss: 1.3075\r\n",
      "Train Epoch: 37 [44800/209539 (21%)]\tAll Loss: 1.2281\tTriple Loss(1): 0.0015\tClassification Loss: 1.2252\r\n",
      "Train Epoch: 37 [45440/209539 (22%)]\tAll Loss: 1.6539\tTriple Loss(1): 0.0626\tClassification Loss: 1.5288\r\n",
      "Train Epoch: 37 [46080/209539 (22%)]\tAll Loss: 1.1252\tTriple Loss(1): 0.0067\tClassification Loss: 1.1117\r\n",
      "Train Epoch: 37 [46720/209539 (22%)]\tAll Loss: 1.5656\tTriple Loss(0): 0.1523\tClassification Loss: 1.2609\r\n",
      "Train Epoch: 37 [47360/209539 (23%)]\tAll Loss: 0.8836\tTriple Loss(1): 0.0000\tClassification Loss: 0.8836\r\n",
      "Train Epoch: 37 [48000/209539 (23%)]\tAll Loss: 1.0501\tTriple Loss(1): 0.0000\tClassification Loss: 1.0501\r\n",
      "Train Epoch: 37 [48640/209539 (23%)]\tAll Loss: 1.7302\tTriple Loss(1): 0.0418\tClassification Loss: 1.6467\r\n",
      "Train Epoch: 37 [49280/209539 (24%)]\tAll Loss: 1.1646\tTriple Loss(1): 0.0055\tClassification Loss: 1.1535\r\n",
      "Train Epoch: 37 [49920/209539 (24%)]\tAll Loss: 1.1819\tTriple Loss(1): 0.0000\tClassification Loss: 1.1819\r\n",
      "Train Epoch: 37 [50560/209539 (24%)]\tAll Loss: 1.2394\tTriple Loss(1): 0.0000\tClassification Loss: 1.2394\r\n",
      "Train Epoch: 37 [51200/209539 (24%)]\tAll Loss: 1.2620\tTriple Loss(1): 0.0575\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 37 [51840/209539 (25%)]\tAll Loss: 1.1983\tTriple Loss(0): 0.1540\tClassification Loss: 0.8902\r\n",
      "Train Epoch: 37 [52480/209539 (25%)]\tAll Loss: 1.0980\tTriple Loss(1): 0.0000\tClassification Loss: 1.0980\r\n",
      "Train Epoch: 37 [53120/209539 (25%)]\tAll Loss: 1.1078\tTriple Loss(1): 0.0522\tClassification Loss: 1.0034\r\n",
      "Train Epoch: 37 [53760/209539 (26%)]\tAll Loss: 1.3415\tTriple Loss(1): 0.0430\tClassification Loss: 1.2554\r\n",
      "Train Epoch: 37 [54400/209539 (26%)]\tAll Loss: 1.5896\tTriple Loss(1): 0.0149\tClassification Loss: 1.5599\r\n",
      "Train Epoch: 37 [55040/209539 (26%)]\tAll Loss: 1.0568\tTriple Loss(1): 0.0564\tClassification Loss: 0.9439\r\n",
      "Train Epoch: 37 [55680/209539 (27%)]\tAll Loss: 1.5172\tTriple Loss(1): 0.0587\tClassification Loss: 1.3997\r\n",
      "Train Epoch: 37 [56320/209539 (27%)]\tAll Loss: 0.8873\tTriple Loss(1): 0.0560\tClassification Loss: 0.7753\r\n",
      "Train Epoch: 37 [56960/209539 (27%)]\tAll Loss: 0.9894\tTriple Loss(1): 0.0289\tClassification Loss: 0.9316\r\n",
      "Train Epoch: 37 [57600/209539 (27%)]\tAll Loss: 1.9475\tTriple Loss(0): 0.4471\tClassification Loss: 1.0533\r\n",
      "Train Epoch: 37 [58240/209539 (28%)]\tAll Loss: 1.0143\tTriple Loss(1): 0.0113\tClassification Loss: 0.9917\r\n",
      "Train Epoch: 37 [58880/209539 (28%)]\tAll Loss: 1.4858\tTriple Loss(1): 0.0490\tClassification Loss: 1.3877\r\n",
      "Train Epoch: 37 [59520/209539 (28%)]\tAll Loss: 1.0687\tTriple Loss(1): 0.0824\tClassification Loss: 0.9038\r\n",
      "Train Epoch: 37 [60160/209539 (29%)]\tAll Loss: 1.7456\tTriple Loss(0): 0.1470\tClassification Loss: 1.4516\r\n",
      "Train Epoch: 37 [60800/209539 (29%)]\tAll Loss: 1.3040\tTriple Loss(1): 0.0000\tClassification Loss: 1.3040\r\n",
      "Train Epoch: 37 [61440/209539 (29%)]\tAll Loss: 1.7486\tTriple Loss(0): 0.2410\tClassification Loss: 1.2666\r\n",
      "Train Epoch: 37 [62080/209539 (30%)]\tAll Loss: 1.3689\tTriple Loss(1): 0.0165\tClassification Loss: 1.3359\r\n",
      "Train Epoch: 37 [62720/209539 (30%)]\tAll Loss: 1.0822\tTriple Loss(1): 0.0085\tClassification Loss: 1.0651\r\n",
      "Train Epoch: 37 [63360/209539 (30%)]\tAll Loss: 1.3347\tTriple Loss(1): 0.0357\tClassification Loss: 1.2633\r\n",
      "Train Epoch: 37 [64000/209539 (31%)]\tAll Loss: 1.2757\tTriple Loss(1): 0.0350\tClassification Loss: 1.2057\r\n",
      "Train Epoch: 37 [64640/209539 (31%)]\tAll Loss: 1.5624\tTriple Loss(1): 0.0263\tClassification Loss: 1.5098\r\n",
      "Train Epoch: 37 [65280/209539 (31%)]\tAll Loss: 1.2824\tTriple Loss(1): 0.0000\tClassification Loss: 1.2824\r\n",
      "Train Epoch: 37 [65920/209539 (31%)]\tAll Loss: 1.5208\tTriple Loss(1): 0.0439\tClassification Loss: 1.4329\r\n",
      "Train Epoch: 37 [66560/209539 (32%)]\tAll Loss: 1.2352\tTriple Loss(1): 0.0445\tClassification Loss: 1.1462\r\n",
      "Train Epoch: 37 [67200/209539 (32%)]\tAll Loss: 1.4439\tTriple Loss(0): 0.2002\tClassification Loss: 1.0435\r\n",
      "Train Epoch: 37 [67840/209539 (32%)]\tAll Loss: 1.0222\tTriple Loss(1): 0.0000\tClassification Loss: 1.0222\r\n",
      "Train Epoch: 37 [68480/209539 (33%)]\tAll Loss: 1.4468\tTriple Loss(1): 0.0118\tClassification Loss: 1.4233\r\n",
      "Train Epoch: 37 [69120/209539 (33%)]\tAll Loss: 1.5258\tTriple Loss(0): 0.1455\tClassification Loss: 1.2348\r\n",
      "Train Epoch: 37 [69760/209539 (33%)]\tAll Loss: 1.5589\tTriple Loss(0): 0.3888\tClassification Loss: 0.7813\r\n",
      "Train Epoch: 37 [70400/209539 (34%)]\tAll Loss: 1.1374\tTriple Loss(1): 0.0156\tClassification Loss: 1.1062\r\n",
      "Train Epoch: 37 [71040/209539 (34%)]\tAll Loss: 1.4025\tTriple Loss(1): 0.0009\tClassification Loss: 1.4007\r\n",
      "Train Epoch: 37 [71680/209539 (34%)]\tAll Loss: 1.4215\tTriple Loss(1): 0.0193\tClassification Loss: 1.3829\r\n",
      "Train Epoch: 37 [72320/209539 (35%)]\tAll Loss: 1.1778\tTriple Loss(1): 0.0361\tClassification Loss: 1.1056\r\n",
      "Train Epoch: 37 [72960/209539 (35%)]\tAll Loss: 1.1678\tTriple Loss(1): 0.0139\tClassification Loss: 1.1399\r\n",
      "Train Epoch: 37 [73600/209539 (35%)]\tAll Loss: 1.1471\tTriple Loss(1): 0.0000\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 37 [74240/209539 (35%)]\tAll Loss: 1.5133\tTriple Loss(1): 0.0258\tClassification Loss: 1.4617\r\n",
      "Train Epoch: 37 [74880/209539 (36%)]\tAll Loss: 1.9116\tTriple Loss(0): 0.1458\tClassification Loss: 1.6201\r\n",
      "Train Epoch: 37 [75520/209539 (36%)]\tAll Loss: 1.3517\tTriple Loss(0): 0.2249\tClassification Loss: 0.9019\r\n",
      "Train Epoch: 37 [76160/209539 (36%)]\tAll Loss: 1.0356\tTriple Loss(1): 0.0000\tClassification Loss: 1.0356\r\n",
      "Train Epoch: 37 [76800/209539 (37%)]\tAll Loss: 1.1059\tTriple Loss(1): 0.0076\tClassification Loss: 1.0907\r\n",
      "Train Epoch: 37 [77440/209539 (37%)]\tAll Loss: 1.4101\tTriple Loss(1): 0.0269\tClassification Loss: 1.3563\r\n",
      "Train Epoch: 37 [78080/209539 (37%)]\tAll Loss: 1.1701\tTriple Loss(1): 0.0375\tClassification Loss: 1.0952\r\n",
      "Train Epoch: 37 [78720/209539 (38%)]\tAll Loss: 1.5802\tTriple Loss(1): 0.0356\tClassification Loss: 1.5090\r\n",
      "Train Epoch: 37 [79360/209539 (38%)]\tAll Loss: 1.3171\tTriple Loss(1): 0.0232\tClassification Loss: 1.2707\r\n",
      "Train Epoch: 37 [80000/209539 (38%)]\tAll Loss: 1.3783\tTriple Loss(0): 0.1708\tClassification Loss: 1.0366\r\n",
      "Train Epoch: 37 [80640/209539 (38%)]\tAll Loss: 1.0702\tTriple Loss(1): 0.0081\tClassification Loss: 1.0540\r\n",
      "Train Epoch: 37 [81280/209539 (39%)]\tAll Loss: 1.3278\tTriple Loss(1): 0.0000\tClassification Loss: 1.3278\r\n",
      "Train Epoch: 37 [81920/209539 (39%)]\tAll Loss: 1.2246\tTriple Loss(1): 0.0898\tClassification Loss: 1.0449\r\n",
      "Train Epoch: 37 [82560/209539 (39%)]\tAll Loss: 1.3821\tTriple Loss(1): 0.0360\tClassification Loss: 1.3100\r\n",
      "Train Epoch: 37 [83200/209539 (40%)]\tAll Loss: 1.4127\tTriple Loss(1): 0.0019\tClassification Loss: 1.4089\r\n",
      "Train Epoch: 37 [83840/209539 (40%)]\tAll Loss: 1.2086\tTriple Loss(1): 0.0308\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 37 [84480/209539 (40%)]\tAll Loss: 1.0846\tTriple Loss(1): 0.0004\tClassification Loss: 1.0837\r\n",
      "Train Epoch: 37 [85120/209539 (41%)]\tAll Loss: 1.5839\tTriple Loss(1): 0.0117\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 37 [85760/209539 (41%)]\tAll Loss: 1.3026\tTriple Loss(1): 0.0555\tClassification Loss: 1.1916\r\n",
      "Train Epoch: 37 [86400/209539 (41%)]\tAll Loss: 1.2134\tTriple Loss(1): 0.0118\tClassification Loss: 1.1899\r\n",
      "Train Epoch: 37 [87040/209539 (42%)]\tAll Loss: 1.0395\tTriple Loss(1): 0.0000\tClassification Loss: 1.0395\r\n",
      "Train Epoch: 37 [87680/209539 (42%)]\tAll Loss: 1.1213\tTriple Loss(1): 0.0518\tClassification Loss: 1.0177\r\n",
      "Train Epoch: 37 [88320/209539 (42%)]\tAll Loss: 1.4047\tTriple Loss(1): 0.0592\tClassification Loss: 1.2863\r\n",
      "Train Epoch: 37 [88960/209539 (42%)]\tAll Loss: 1.6856\tTriple Loss(0): 0.2964\tClassification Loss: 1.0928\r\n",
      "Train Epoch: 37 [89600/209539 (43%)]\tAll Loss: 1.7593\tTriple Loss(0): 0.2154\tClassification Loss: 1.3284\r\n",
      "Train Epoch: 37 [90240/209539 (43%)]\tAll Loss: 1.1869\tTriple Loss(1): 0.0520\tClassification Loss: 1.0829\r\n",
      "Train Epoch: 37 [90880/209539 (43%)]\tAll Loss: 1.6173\tTriple Loss(1): 0.0038\tClassification Loss: 1.6098\r\n",
      "Train Epoch: 37 [91520/209539 (44%)]\tAll Loss: 1.2387\tTriple Loss(1): 0.0348\tClassification Loss: 1.1691\r\n",
      "Train Epoch: 37 [92160/209539 (44%)]\tAll Loss: 1.0056\tTriple Loss(1): 0.0000\tClassification Loss: 1.0056\r\n",
      "Train Epoch: 37 [92800/209539 (44%)]\tAll Loss: 1.0142\tTriple Loss(1): 0.0000\tClassification Loss: 1.0142\r\n",
      "Train Epoch: 37 [93440/209539 (45%)]\tAll Loss: 1.1672\tTriple Loss(1): 0.0000\tClassification Loss: 1.1672\r\n",
      "Train Epoch: 37 [94080/209539 (45%)]\tAll Loss: 1.2992\tTriple Loss(1): 0.0321\tClassification Loss: 1.2350\r\n",
      "Train Epoch: 37 [94720/209539 (45%)]\tAll Loss: 1.3651\tTriple Loss(1): 0.0180\tClassification Loss: 1.3291\r\n",
      "Train Epoch: 37 [95360/209539 (46%)]\tAll Loss: 1.1778\tTriple Loss(1): 0.0000\tClassification Loss: 1.1778\r\n",
      "Train Epoch: 37 [96000/209539 (46%)]\tAll Loss: 1.4406\tTriple Loss(1): 0.0280\tClassification Loss: 1.3845\r\n",
      "Train Epoch: 37 [96640/209539 (46%)]\tAll Loss: 1.6087\tTriple Loss(1): 0.1008\tClassification Loss: 1.4071\r\n",
      "Train Epoch: 37 [97280/209539 (46%)]\tAll Loss: 1.1984\tTriple Loss(1): 0.0396\tClassification Loss: 1.1191\r\n",
      "Train Epoch: 37 [97920/209539 (47%)]\tAll Loss: 1.7283\tTriple Loss(0): 0.3228\tClassification Loss: 1.0827\r\n",
      "Train Epoch: 37 [98560/209539 (47%)]\tAll Loss: 1.5243\tTriple Loss(1): 0.0000\tClassification Loss: 1.5243\r\n",
      "Train Epoch: 37 [99200/209539 (47%)]\tAll Loss: 1.2242\tTriple Loss(0): 0.1346\tClassification Loss: 0.9550\r\n",
      "Train Epoch: 37 [99840/209539 (48%)]\tAll Loss: 1.3433\tTriple Loss(1): 0.0386\tClassification Loss: 1.2662\r\n",
      "Train Epoch: 37 [100480/209539 (48%)]\tAll Loss: 1.1707\tTriple Loss(1): 0.0100\tClassification Loss: 1.1506\r\n",
      "Train Epoch: 37 [101120/209539 (48%)]\tAll Loss: 1.2594\tTriple Loss(1): 0.0584\tClassification Loss: 1.1425\r\n",
      "Train Epoch: 37 [101760/209539 (49%)]\tAll Loss: 1.1992\tTriple Loss(1): 0.0794\tClassification Loss: 1.0403\r\n",
      "Train Epoch: 37 [102400/209539 (49%)]\tAll Loss: 1.6795\tTriple Loss(0): 0.2837\tClassification Loss: 1.1120\r\n",
      "Train Epoch: 37 [103040/209539 (49%)]\tAll Loss: 1.6477\tTriple Loss(0): 0.2867\tClassification Loss: 1.0744\r\n",
      "Train Epoch: 37 [103680/209539 (49%)]\tAll Loss: 1.1513\tTriple Loss(1): 0.0000\tClassification Loss: 1.1513\r\n",
      "Train Epoch: 37 [104320/209539 (50%)]\tAll Loss: 1.0747\tTriple Loss(1): 0.0430\tClassification Loss: 0.9887\r\n",
      "Train Epoch: 37 [104960/209539 (50%)]\tAll Loss: 1.3319\tTriple Loss(1): 0.0781\tClassification Loss: 1.1757\r\n",
      "Train Epoch: 37 [105600/209539 (50%)]\tAll Loss: 1.3121\tTriple Loss(1): 0.0534\tClassification Loss: 1.2054\r\n",
      "Train Epoch: 37 [106240/209539 (51%)]\tAll Loss: 1.2032\tTriple Loss(1): 0.0283\tClassification Loss: 1.1465\r\n",
      "Train Epoch: 37 [106880/209539 (51%)]\tAll Loss: 1.8051\tTriple Loss(0): 0.3784\tClassification Loss: 1.0482\r\n",
      "Train Epoch: 37 [107520/209539 (51%)]\tAll Loss: 1.2095\tTriple Loss(1): 0.0036\tClassification Loss: 1.2023\r\n",
      "Train Epoch: 37 [108160/209539 (52%)]\tAll Loss: 1.1328\tTriple Loss(1): 0.0000\tClassification Loss: 1.1328\r\n",
      "Train Epoch: 37 [108800/209539 (52%)]\tAll Loss: 0.9496\tTriple Loss(1): 0.0091\tClassification Loss: 0.9313\r\n",
      "Train Epoch: 37 [109440/209539 (52%)]\tAll Loss: 1.3993\tTriple Loss(1): 0.0389\tClassification Loss: 1.3215\r\n",
      "Train Epoch: 37 [110080/209539 (53%)]\tAll Loss: 1.4624\tTriple Loss(1): 0.0373\tClassification Loss: 1.3879\r\n",
      "Train Epoch: 37 [110720/209539 (53%)]\tAll Loss: 1.8357\tTriple Loss(0): 0.2489\tClassification Loss: 1.3379\r\n",
      "Train Epoch: 37 [111360/209539 (53%)]\tAll Loss: 0.9117\tTriple Loss(1): 0.0012\tClassification Loss: 0.9093\r\n",
      "Train Epoch: 37 [112000/209539 (53%)]\tAll Loss: 1.0317\tTriple Loss(1): 0.0079\tClassification Loss: 1.0158\r\n",
      "Train Epoch: 37 [112640/209539 (54%)]\tAll Loss: 1.6506\tTriple Loss(0): 0.1719\tClassification Loss: 1.3069\r\n",
      "Train Epoch: 37 [113280/209539 (54%)]\tAll Loss: 0.9131\tTriple Loss(1): 0.0146\tClassification Loss: 0.8839\r\n",
      "Train Epoch: 37 [113920/209539 (54%)]\tAll Loss: 1.1479\tTriple Loss(1): 0.0243\tClassification Loss: 1.0993\r\n",
      "Train Epoch: 37 [114560/209539 (55%)]\tAll Loss: 1.4697\tTriple Loss(1): 0.0721\tClassification Loss: 1.3254\r\n",
      "Train Epoch: 37 [115200/209539 (55%)]\tAll Loss: 1.6117\tTriple Loss(1): 0.0470\tClassification Loss: 1.5176\r\n",
      "Train Epoch: 37 [115840/209539 (55%)]\tAll Loss: 1.2996\tTriple Loss(1): 0.0493\tClassification Loss: 1.2010\r\n",
      "Train Epoch: 37 [116480/209539 (56%)]\tAll Loss: 1.1987\tTriple Loss(1): 0.0042\tClassification Loss: 1.1902\r\n",
      "Train Epoch: 37 [117120/209539 (56%)]\tAll Loss: 1.2542\tTriple Loss(1): 0.0804\tClassification Loss: 1.0933\r\n",
      "Train Epoch: 37 [117760/209539 (56%)]\tAll Loss: 2.2094\tTriple Loss(0): 0.4943\tClassification Loss: 1.2208\r\n",
      "Train Epoch: 37 [118400/209539 (57%)]\tAll Loss: 1.5463\tTriple Loss(0): 0.3394\tClassification Loss: 0.8674\r\n",
      "Train Epoch: 37 [119040/209539 (57%)]\tAll Loss: 1.6470\tTriple Loss(0): 0.1322\tClassification Loss: 1.3826\r\n",
      "Train Epoch: 37 [119680/209539 (57%)]\tAll Loss: 1.1509\tTriple Loss(1): 0.0000\tClassification Loss: 1.1509\r\n",
      "Train Epoch: 37 [120320/209539 (57%)]\tAll Loss: 1.5053\tTriple Loss(0): 0.1986\tClassification Loss: 1.1082\r\n",
      "Train Epoch: 37 [120960/209539 (58%)]\tAll Loss: 1.1042\tTriple Loss(1): 0.0053\tClassification Loss: 1.0935\r\n",
      "Train Epoch: 37 [121600/209539 (58%)]\tAll Loss: 1.2009\tTriple Loss(1): 0.0203\tClassification Loss: 1.1603\r\n",
      "Train Epoch: 37 [122240/209539 (58%)]\tAll Loss: 0.9434\tTriple Loss(1): 0.0321\tClassification Loss: 0.8791\r\n",
      "Train Epoch: 37 [122880/209539 (59%)]\tAll Loss: 1.3629\tTriple Loss(1): 0.0631\tClassification Loss: 1.2367\r\n",
      "Train Epoch: 37 [123520/209539 (59%)]\tAll Loss: 1.1947\tTriple Loss(1): 0.0119\tClassification Loss: 1.1708\r\n",
      "Train Epoch: 37 [124160/209539 (59%)]\tAll Loss: 1.0898\tTriple Loss(1): 0.0240\tClassification Loss: 1.0418\r\n",
      "Train Epoch: 37 [124800/209539 (60%)]\tAll Loss: 1.4735\tTriple Loss(0): 0.2550\tClassification Loss: 0.9635\r\n",
      "Train Epoch: 37 [125440/209539 (60%)]\tAll Loss: 1.2748\tTriple Loss(1): 0.0011\tClassification Loss: 1.2726\r\n",
      "Train Epoch: 37 [126080/209539 (60%)]\tAll Loss: 1.3496\tTriple Loss(1): 0.0333\tClassification Loss: 1.2830\r\n",
      "Train Epoch: 37 [126720/209539 (60%)]\tAll Loss: 1.6318\tTriple Loss(0): 0.1819\tClassification Loss: 1.2680\r\n",
      "Train Epoch: 37 [127360/209539 (61%)]\tAll Loss: 1.3709\tTriple Loss(1): 0.0190\tClassification Loss: 1.3328\r\n",
      "Train Epoch: 37 [128000/209539 (61%)]\tAll Loss: 2.1381\tTriple Loss(0): 0.3741\tClassification Loss: 1.3899\r\n",
      "Train Epoch: 37 [128640/209539 (61%)]\tAll Loss: 1.4042\tTriple Loss(0): 0.2057\tClassification Loss: 0.9928\r\n",
      "Train Epoch: 37 [129280/209539 (62%)]\tAll Loss: 1.3140\tTriple Loss(1): 0.0105\tClassification Loss: 1.2929\r\n",
      "Train Epoch: 37 [129920/209539 (62%)]\tAll Loss: 1.1546\tTriple Loss(0): 0.0768\tClassification Loss: 1.0009\r\n",
      "Train Epoch: 37 [130560/209539 (62%)]\tAll Loss: 0.9858\tTriple Loss(1): 0.0210\tClassification Loss: 0.9438\r\n",
      "Train Epoch: 37 [131200/209539 (63%)]\tAll Loss: 1.0358\tTriple Loss(1): 0.0618\tClassification Loss: 0.9121\r\n",
      "Train Epoch: 37 [131840/209539 (63%)]\tAll Loss: 1.1869\tTriple Loss(1): 0.0104\tClassification Loss: 1.1661\r\n",
      "Train Epoch: 37 [132480/209539 (63%)]\tAll Loss: 1.0417\tTriple Loss(1): 0.0038\tClassification Loss: 1.0340\r\n",
      "Train Epoch: 37 [133120/209539 (64%)]\tAll Loss: 1.5389\tTriple Loss(0): 0.1977\tClassification Loss: 1.1434\r\n",
      "Train Epoch: 37 [133760/209539 (64%)]\tAll Loss: 1.0935\tTriple Loss(1): 0.0249\tClassification Loss: 1.0438\r\n",
      "Train Epoch: 37 [134400/209539 (64%)]\tAll Loss: 0.9323\tTriple Loss(1): 0.0286\tClassification Loss: 0.8751\r\n",
      "Train Epoch: 37 [135040/209539 (64%)]\tAll Loss: 1.1545\tTriple Loss(1): 0.0278\tClassification Loss: 1.0989\r\n",
      "Train Epoch: 37 [135680/209539 (65%)]\tAll Loss: 1.5328\tTriple Loss(1): 0.0350\tClassification Loss: 1.4629\r\n",
      "Train Epoch: 37 [136320/209539 (65%)]\tAll Loss: 1.9588\tTriple Loss(0): 0.4098\tClassification Loss: 1.1391\r\n",
      "Train Epoch: 37 [136960/209539 (65%)]\tAll Loss: 1.1733\tTriple Loss(1): 0.0091\tClassification Loss: 1.1551\r\n",
      "Train Epoch: 37 [137600/209539 (66%)]\tAll Loss: 1.1533\tTriple Loss(1): 0.0284\tClassification Loss: 1.0966\r\n",
      "Train Epoch: 37 [138240/209539 (66%)]\tAll Loss: 1.7451\tTriple Loss(0): 0.2200\tClassification Loss: 1.3051\r\n",
      "Train Epoch: 37 [138880/209539 (66%)]\tAll Loss: 1.3387\tTriple Loss(1): 0.0838\tClassification Loss: 1.1712\r\n",
      "Train Epoch: 37 [139520/209539 (67%)]\tAll Loss: 1.0581\tTriple Loss(1): 0.0440\tClassification Loss: 0.9701\r\n",
      "Train Epoch: 37 [140160/209539 (67%)]\tAll Loss: 1.3273\tTriple Loss(1): 0.0662\tClassification Loss: 1.1949\r\n",
      "Train Epoch: 37 [140800/209539 (67%)]\tAll Loss: 1.8101\tTriple Loss(0): 0.3405\tClassification Loss: 1.1291\r\n",
      "Train Epoch: 37 [141440/209539 (68%)]\tAll Loss: 1.1570\tTriple Loss(1): 0.0000\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 37 [142080/209539 (68%)]\tAll Loss: 1.2470\tTriple Loss(1): 0.0000\tClassification Loss: 1.2470\r\n",
      "Train Epoch: 37 [142720/209539 (68%)]\tAll Loss: 1.6606\tTriple Loss(1): 0.0000\tClassification Loss: 1.6606\r\n",
      "Train Epoch: 37 [143360/209539 (68%)]\tAll Loss: 0.9291\tTriple Loss(1): 0.0060\tClassification Loss: 0.9170\r\n",
      "Train Epoch: 37 [144000/209539 (69%)]\tAll Loss: 1.2736\tTriple Loss(1): 0.0506\tClassification Loss: 1.1724\r\n",
      "Train Epoch: 37 [144640/209539 (69%)]\tAll Loss: 1.0591\tTriple Loss(1): 0.0398\tClassification Loss: 0.9795\r\n",
      "Train Epoch: 37 [145280/209539 (69%)]\tAll Loss: 1.7112\tTriple Loss(0): 0.2596\tClassification Loss: 1.1921\r\n",
      "Train Epoch: 37 [145920/209539 (70%)]\tAll Loss: 1.1107\tTriple Loss(1): 0.0201\tClassification Loss: 1.0705\r\n",
      "Train Epoch: 37 [146560/209539 (70%)]\tAll Loss: 0.9975\tTriple Loss(1): 0.0110\tClassification Loss: 0.9755\r\n",
      "Train Epoch: 37 [147200/209539 (70%)]\tAll Loss: 0.9840\tTriple Loss(1): 0.0000\tClassification Loss: 0.9840\r\n",
      "Train Epoch: 37 [147840/209539 (71%)]\tAll Loss: 1.1073\tTriple Loss(1): 0.0084\tClassification Loss: 1.0906\r\n",
      "Train Epoch: 37 [148480/209539 (71%)]\tAll Loss: 0.8924\tTriple Loss(1): 0.0114\tClassification Loss: 0.8697\r\n",
      "Train Epoch: 37 [149120/209539 (71%)]\tAll Loss: 1.4188\tTriple Loss(1): 0.0339\tClassification Loss: 1.3510\r\n",
      "Train Epoch: 37 [149760/209539 (71%)]\tAll Loss: 1.8833\tTriple Loss(0): 0.4002\tClassification Loss: 1.0829\r\n",
      "Train Epoch: 37 [150400/209539 (72%)]\tAll Loss: 1.5334\tTriple Loss(1): 0.0491\tClassification Loss: 1.4353\r\n",
      "Train Epoch: 37 [151040/209539 (72%)]\tAll Loss: 1.0167\tTriple Loss(1): 0.0142\tClassification Loss: 0.9883\r\n",
      "Train Epoch: 37 [151680/209539 (72%)]\tAll Loss: 1.4797\tTriple Loss(1): 0.0539\tClassification Loss: 1.3719\r\n",
      "Train Epoch: 37 [152320/209539 (73%)]\tAll Loss: 1.0484\tTriple Loss(1): 0.0142\tClassification Loss: 1.0199\r\n",
      "Train Epoch: 37 [152960/209539 (73%)]\tAll Loss: 1.0531\tTriple Loss(1): 0.0212\tClassification Loss: 1.0108\r\n",
      "Train Epoch: 37 [153600/209539 (73%)]\tAll Loss: 1.2151\tTriple Loss(1): 0.0000\tClassification Loss: 1.2151\r\n",
      "Train Epoch: 37 [154240/209539 (74%)]\tAll Loss: 1.3662\tTriple Loss(0): 0.1962\tClassification Loss: 0.9737\r\n",
      "Train Epoch: 37 [154880/209539 (74%)]\tAll Loss: 1.2270\tTriple Loss(1): 0.0000\tClassification Loss: 1.2270\r\n",
      "Train Epoch: 37 [155520/209539 (74%)]\tAll Loss: 1.1499\tTriple Loss(1): 0.0000\tClassification Loss: 1.1499\r\n",
      "Train Epoch: 37 [156160/209539 (75%)]\tAll Loss: 1.4860\tTriple Loss(1): 0.0821\tClassification Loss: 1.3218\r\n",
      "Train Epoch: 37 [156800/209539 (75%)]\tAll Loss: 2.1833\tTriple Loss(0): 0.2348\tClassification Loss: 1.7136\r\n",
      "Train Epoch: 37 [157440/209539 (75%)]\tAll Loss: 1.3458\tTriple Loss(1): 0.0481\tClassification Loss: 1.2497\r\n",
      "Train Epoch: 37 [158080/209539 (75%)]\tAll Loss: 1.1768\tTriple Loss(1): 0.0484\tClassification Loss: 1.0800\r\n",
      "Train Epoch: 37 [158720/209539 (76%)]\tAll Loss: 0.9237\tTriple Loss(1): 0.0000\tClassification Loss: 0.9237\r\n",
      "Train Epoch: 37 [159360/209539 (76%)]\tAll Loss: 0.8233\tTriple Loss(1): 0.0231\tClassification Loss: 0.7771\r\n",
      "Train Epoch: 37 [160000/209539 (76%)]\tAll Loss: 1.4800\tTriple Loss(1): 0.0689\tClassification Loss: 1.3422\r\n",
      "Train Epoch: 37 [160640/209539 (77%)]\tAll Loss: 0.9870\tTriple Loss(1): 0.0379\tClassification Loss: 0.9113\r\n",
      "Train Epoch: 37 [161280/209539 (77%)]\tAll Loss: 1.7464\tTriple Loss(0): 0.3540\tClassification Loss: 1.0384\r\n",
      "Train Epoch: 37 [161920/209539 (77%)]\tAll Loss: 1.3772\tTriple Loss(1): 0.0475\tClassification Loss: 1.2823\r\n",
      "Train Epoch: 37 [162560/209539 (78%)]\tAll Loss: 1.2054\tTriple Loss(1): 0.0626\tClassification Loss: 1.0803\r\n",
      "Train Epoch: 37 [163200/209539 (78%)]\tAll Loss: 0.9208\tTriple Loss(1): 0.0170\tClassification Loss: 0.8868\r\n",
      "Train Epoch: 37 [163840/209539 (78%)]\tAll Loss: 1.3719\tTriple Loss(1): 0.1137\tClassification Loss: 1.1445\r\n",
      "Train Epoch: 37 [164480/209539 (78%)]\tAll Loss: 0.9446\tTriple Loss(1): 0.0167\tClassification Loss: 0.9113\r\n",
      "Train Epoch: 37 [165120/209539 (79%)]\tAll Loss: 1.1690\tTriple Loss(1): 0.0752\tClassification Loss: 1.0185\r\n",
      "Train Epoch: 37 [165760/209539 (79%)]\tAll Loss: 1.2567\tTriple Loss(1): 0.0178\tClassification Loss: 1.2210\r\n",
      "Train Epoch: 37 [166400/209539 (79%)]\tAll Loss: 1.3893\tTriple Loss(1): 0.0457\tClassification Loss: 1.2978\r\n",
      "Train Epoch: 37 [167040/209539 (80%)]\tAll Loss: 1.9628\tTriple Loss(0): 0.3233\tClassification Loss: 1.3162\r\n",
      "Train Epoch: 37 [167680/209539 (80%)]\tAll Loss: 1.3171\tTriple Loss(1): 0.0095\tClassification Loss: 1.2981\r\n",
      "Train Epoch: 37 [168320/209539 (80%)]\tAll Loss: 1.1859\tTriple Loss(1): 0.0240\tClassification Loss: 1.1379\r\n",
      "Train Epoch: 37 [168960/209539 (81%)]\tAll Loss: 1.1868\tTriple Loss(1): 0.0831\tClassification Loss: 1.0205\r\n",
      "Train Epoch: 37 [169600/209539 (81%)]\tAll Loss: 1.1068\tTriple Loss(1): 0.0000\tClassification Loss: 1.1068\r\n",
      "Train Epoch: 37 [170240/209539 (81%)]\tAll Loss: 1.1355\tTriple Loss(1): 0.0223\tClassification Loss: 1.0909\r\n",
      "Train Epoch: 37 [170880/209539 (82%)]\tAll Loss: 1.1999\tTriple Loss(1): 0.0082\tClassification Loss: 1.1835\r\n",
      "Train Epoch: 37 [171520/209539 (82%)]\tAll Loss: 1.0507\tTriple Loss(1): 0.0000\tClassification Loss: 1.0507\r\n",
      "Train Epoch: 37 [172160/209539 (82%)]\tAll Loss: 1.2755\tTriple Loss(1): 0.0225\tClassification Loss: 1.2304\r\n",
      "Train Epoch: 37 [172800/209539 (82%)]\tAll Loss: 1.4177\tTriple Loss(0): 0.1587\tClassification Loss: 1.1003\r\n",
      "Train Epoch: 37 [173440/209539 (83%)]\tAll Loss: 2.3847\tTriple Loss(0): 0.4200\tClassification Loss: 1.5447\r\n",
      "Train Epoch: 37 [174080/209539 (83%)]\tAll Loss: 1.2617\tTriple Loss(1): 0.0242\tClassification Loss: 1.2132\r\n",
      "Train Epoch: 37 [174720/209539 (83%)]\tAll Loss: 1.5663\tTriple Loss(0): 0.2297\tClassification Loss: 1.1069\r\n",
      "Train Epoch: 37 [175360/209539 (84%)]\tAll Loss: 1.1559\tTriple Loss(1): 0.0000\tClassification Loss: 1.1559\r\n",
      "Train Epoch: 37 [176000/209539 (84%)]\tAll Loss: 1.3209\tTriple Loss(1): 0.0668\tClassification Loss: 1.1873\r\n",
      "Train Epoch: 37 [176640/209539 (84%)]\tAll Loss: 0.8979\tTriple Loss(1): 0.0589\tClassification Loss: 0.7801\r\n",
      "Train Epoch: 37 [177280/209539 (85%)]\tAll Loss: 1.3883\tTriple Loss(1): 0.0000\tClassification Loss: 1.3883\r\n",
      "Train Epoch: 37 [177920/209539 (85%)]\tAll Loss: 1.6047\tTriple Loss(1): 0.0336\tClassification Loss: 1.5375\r\n",
      "Train Epoch: 37 [178560/209539 (85%)]\tAll Loss: 1.4703\tTriple Loss(0): 0.1921\tClassification Loss: 1.0861\r\n",
      "Train Epoch: 37 [179200/209539 (86%)]\tAll Loss: 1.4537\tTriple Loss(1): 0.0253\tClassification Loss: 1.4030\r\n",
      "Train Epoch: 37 [179840/209539 (86%)]\tAll Loss: 1.1196\tTriple Loss(1): 0.0023\tClassification Loss: 1.1150\r\n",
      "Train Epoch: 37 [180480/209539 (86%)]\tAll Loss: 1.4188\tTriple Loss(1): 0.0449\tClassification Loss: 1.3290\r\n",
      "Train Epoch: 37 [181120/209539 (86%)]\tAll Loss: 1.2408\tTriple Loss(1): 0.0086\tClassification Loss: 1.2236\r\n",
      "Train Epoch: 37 [181760/209539 (87%)]\tAll Loss: 1.1495\tTriple Loss(1): 0.0199\tClassification Loss: 1.1097\r\n",
      "Train Epoch: 37 [182400/209539 (87%)]\tAll Loss: 1.3377\tTriple Loss(1): 0.0031\tClassification Loss: 1.3315\r\n",
      "Train Epoch: 37 [183040/209539 (87%)]\tAll Loss: 1.2712\tTriple Loss(1): 0.0169\tClassification Loss: 1.2375\r\n",
      "Train Epoch: 37 [183680/209539 (88%)]\tAll Loss: 0.8998\tTriple Loss(1): 0.0074\tClassification Loss: 0.8850\r\n",
      "Train Epoch: 37 [184320/209539 (88%)]\tAll Loss: 1.0314\tTriple Loss(1): 0.0208\tClassification Loss: 0.9897\r\n",
      "Train Epoch: 37 [184960/209539 (88%)]\tAll Loss: 0.8527\tTriple Loss(1): 0.0000\tClassification Loss: 0.8527\r\n",
      "Train Epoch: 37 [185600/209539 (89%)]\tAll Loss: 1.0659\tTriple Loss(1): 0.0046\tClassification Loss: 1.0566\r\n",
      "Train Epoch: 37 [186240/209539 (89%)]\tAll Loss: 1.3776\tTriple Loss(1): 0.0478\tClassification Loss: 1.2821\r\n",
      "Train Epoch: 37 [186880/209539 (89%)]\tAll Loss: 1.3928\tTriple Loss(1): 0.0138\tClassification Loss: 1.3653\r\n",
      "Train Epoch: 37 [187520/209539 (89%)]\tAll Loss: 1.5127\tTriple Loss(1): 0.0803\tClassification Loss: 1.3521\r\n",
      "Train Epoch: 37 [188160/209539 (90%)]\tAll Loss: 1.0197\tTriple Loss(1): 0.0000\tClassification Loss: 1.0197\r\n",
      "Train Epoch: 37 [188800/209539 (90%)]\tAll Loss: 1.4142\tTriple Loss(0): 0.2417\tClassification Loss: 0.9309\r\n",
      "Train Epoch: 37 [189440/209539 (90%)]\tAll Loss: 1.7749\tTriple Loss(0): 0.2778\tClassification Loss: 1.2192\r\n",
      "Train Epoch: 37 [190080/209539 (91%)]\tAll Loss: 1.0687\tTriple Loss(1): 0.0000\tClassification Loss: 1.0687\r\n",
      "Train Epoch: 37 [190720/209539 (91%)]\tAll Loss: 0.9786\tTriple Loss(1): 0.0000\tClassification Loss: 0.9786\r\n",
      "Train Epoch: 37 [191360/209539 (91%)]\tAll Loss: 1.3298\tTriple Loss(0): 0.1208\tClassification Loss: 1.0883\r\n",
      "Train Epoch: 37 [192000/209539 (92%)]\tAll Loss: 1.4860\tTriple Loss(1): 0.0149\tClassification Loss: 1.4562\r\n",
      "Train Epoch: 37 [192640/209539 (92%)]\tAll Loss: 1.0248\tTriple Loss(1): 0.0162\tClassification Loss: 0.9924\r\n",
      "Train Epoch: 37 [193280/209539 (92%)]\tAll Loss: 0.9496\tTriple Loss(1): 0.0134\tClassification Loss: 0.9227\r\n",
      "Train Epoch: 37 [193920/209539 (93%)]\tAll Loss: 1.7099\tTriple Loss(0): 0.4028\tClassification Loss: 0.9044\r\n",
      "Train Epoch: 37 [194560/209539 (93%)]\tAll Loss: 1.1720\tTriple Loss(1): 0.0543\tClassification Loss: 1.0633\r\n",
      "Train Epoch: 37 [195200/209539 (93%)]\tAll Loss: 1.2677\tTriple Loss(1): 0.0242\tClassification Loss: 1.2192\r\n",
      "Train Epoch: 37 [195840/209539 (93%)]\tAll Loss: 0.9641\tTriple Loss(1): 0.0000\tClassification Loss: 0.9641\r\n",
      "Train Epoch: 37 [196480/209539 (94%)]\tAll Loss: 1.2491\tTriple Loss(1): 0.0000\tClassification Loss: 1.2491\r\n",
      "Train Epoch: 37 [197120/209539 (94%)]\tAll Loss: 1.7982\tTriple Loss(0): 0.2652\tClassification Loss: 1.2677\r\n",
      "Train Epoch: 37 [197760/209539 (94%)]\tAll Loss: 1.3167\tTriple Loss(1): 0.0220\tClassification Loss: 1.2726\r\n",
      "Train Epoch: 37 [198400/209539 (95%)]\tAll Loss: 1.1430\tTriple Loss(1): 0.0040\tClassification Loss: 1.1350\r\n",
      "Train Epoch: 37 [199040/209539 (95%)]\tAll Loss: 1.6740\tTriple Loss(0): 0.2251\tClassification Loss: 1.2238\r\n",
      "Train Epoch: 37 [199680/209539 (95%)]\tAll Loss: 1.3072\tTriple Loss(1): 0.0229\tClassification Loss: 1.2613\r\n",
      "Train Epoch: 37 [200320/209539 (96%)]\tAll Loss: 1.4044\tTriple Loss(1): 0.0342\tClassification Loss: 1.3360\r\n",
      "Train Epoch: 37 [200960/209539 (96%)]\tAll Loss: 0.8170\tTriple Loss(1): 0.0000\tClassification Loss: 0.8170\r\n",
      "Train Epoch: 37 [201600/209539 (96%)]\tAll Loss: 1.2028\tTriple Loss(0): 0.1033\tClassification Loss: 0.9961\r\n",
      "Train Epoch: 37 [202240/209539 (97%)]\tAll Loss: 1.0593\tTriple Loss(1): 0.0063\tClassification Loss: 1.0466\r\n",
      "Train Epoch: 37 [202880/209539 (97%)]\tAll Loss: 0.7222\tTriple Loss(1): 0.0000\tClassification Loss: 0.7222\r\n",
      "Train Epoch: 37 [203520/209539 (97%)]\tAll Loss: 1.8214\tTriple Loss(0): 0.2682\tClassification Loss: 1.2850\r\n",
      "Train Epoch: 37 [204160/209539 (97%)]\tAll Loss: 1.8810\tTriple Loss(1): 0.0777\tClassification Loss: 1.7256\r\n",
      "Train Epoch: 37 [204800/209539 (98%)]\tAll Loss: 1.5182\tTriple Loss(1): 0.0120\tClassification Loss: 1.4943\r\n",
      "Train Epoch: 37 [205440/209539 (98%)]\tAll Loss: 0.9524\tTriple Loss(1): 0.0448\tClassification Loss: 0.8628\r\n",
      "Train Epoch: 37 [206080/209539 (98%)]\tAll Loss: 1.1066\tTriple Loss(1): 0.0215\tClassification Loss: 1.0635\r\n",
      "Train Epoch: 37 [206720/209539 (99%)]\tAll Loss: 1.4375\tTriple Loss(0): 0.2723\tClassification Loss: 0.8929\r\n",
      "Train Epoch: 37 [207360/209539 (99%)]\tAll Loss: 0.9843\tTriple Loss(1): 0.0000\tClassification Loss: 0.9843\r\n",
      "Train Epoch: 37 [208000/209539 (99%)]\tAll Loss: 1.2332\tTriple Loss(1): 0.0419\tClassification Loss: 1.1494\r\n",
      "Train Epoch: 37 [208640/209539 (100%)]\tAll Loss: 1.1769\tTriple Loss(1): 0.0346\tClassification Loss: 1.1077\r\n",
      "Train Epoch: 37 [209280/209539 (100%)]\tAll Loss: 1.4821\tTriple Loss(1): 0.0000\tClassification Loss: 1.4821\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/37_epochs\r\n",
      "Train Epoch: 38 [0/209539 (0%)]\tAll Loss: 1.9764\tTriple Loss(0): 0.3238\tClassification Loss: 1.3289\r\n",
      "\r\n",
      "Test set: Average loss: 1.0716\r\n",
      "Top 1 Accuracy: 55034/80128 (69%)\r\n",
      "Top 3 Accuracy: 70256/80128 (88%)\r\n",
      "Top 5 Accuracy: 74780/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 38 [640/209539 (0%)]\tAll Loss: 1.9043\tTriple Loss(0): 0.3536\tClassification Loss: 1.1971\r\n",
      "Train Epoch: 38 [1280/209539 (1%)]\tAll Loss: 1.1798\tTriple Loss(1): 0.0157\tClassification Loss: 1.1483\r\n",
      "Train Epoch: 38 [1920/209539 (1%)]\tAll Loss: 1.3790\tTriple Loss(1): 0.0144\tClassification Loss: 1.3502\r\n",
      "Train Epoch: 38 [2560/209539 (1%)]\tAll Loss: 1.4754\tTriple Loss(1): 0.0182\tClassification Loss: 1.4390\r\n",
      "Train Epoch: 38 [3200/209539 (2%)]\tAll Loss: 1.3381\tTriple Loss(1): 0.0458\tClassification Loss: 1.2465\r\n",
      "Train Epoch: 38 [3840/209539 (2%)]\tAll Loss: 1.1031\tTriple Loss(1): 0.0075\tClassification Loss: 1.0881\r\n",
      "Train Epoch: 38 [4480/209539 (2%)]\tAll Loss: 1.4601\tTriple Loss(1): 0.0442\tClassification Loss: 1.3717\r\n",
      "Train Epoch: 38 [5120/209539 (2%)]\tAll Loss: 1.2928\tTriple Loss(1): 0.0134\tClassification Loss: 1.2661\r\n",
      "Train Epoch: 38 [5760/209539 (3%)]\tAll Loss: 1.3169\tTriple Loss(1): 0.0027\tClassification Loss: 1.3116\r\n",
      "Train Epoch: 38 [6400/209539 (3%)]\tAll Loss: 0.9433\tTriple Loss(1): 0.0097\tClassification Loss: 0.9239\r\n",
      "Train Epoch: 38 [7040/209539 (3%)]\tAll Loss: 1.0097\tTriple Loss(1): 0.0197\tClassification Loss: 0.9702\r\n",
      "Train Epoch: 38 [7680/209539 (4%)]\tAll Loss: 1.4112\tTriple Loss(0): 0.2741\tClassification Loss: 0.8629\r\n",
      "Train Epoch: 38 [8320/209539 (4%)]\tAll Loss: 1.0244\tTriple Loss(1): 0.0425\tClassification Loss: 0.9394\r\n",
      "Train Epoch: 38 [8960/209539 (4%)]\tAll Loss: 1.1514\tTriple Loss(1): 0.0186\tClassification Loss: 1.1143\r\n",
      "Train Epoch: 38 [9600/209539 (5%)]\tAll Loss: 1.5314\tTriple Loss(1): 0.0620\tClassification Loss: 1.4073\r\n",
      "Train Epoch: 38 [10240/209539 (5%)]\tAll Loss: 1.6012\tTriple Loss(0): 0.2078\tClassification Loss: 1.1856\r\n",
      "Train Epoch: 38 [10880/209539 (5%)]\tAll Loss: 1.1658\tTriple Loss(1): 0.0173\tClassification Loss: 1.1313\r\n",
      "Train Epoch: 38 [11520/209539 (5%)]\tAll Loss: 1.1769\tTriple Loss(1): 0.0241\tClassification Loss: 1.1286\r\n",
      "Train Epoch: 38 [12160/209539 (6%)]\tAll Loss: 1.5249\tTriple Loss(0): 0.2406\tClassification Loss: 1.0437\r\n",
      "Train Epoch: 38 [12800/209539 (6%)]\tAll Loss: 0.8251\tTriple Loss(1): 0.0000\tClassification Loss: 0.8251\r\n",
      "Train Epoch: 38 [13440/209539 (6%)]\tAll Loss: 1.1561\tTriple Loss(1): 0.0483\tClassification Loss: 1.0595\r\n",
      "Train Epoch: 38 [14080/209539 (7%)]\tAll Loss: 1.2176\tTriple Loss(1): 0.0038\tClassification Loss: 1.2100\r\n",
      "Train Epoch: 38 [14720/209539 (7%)]\tAll Loss: 1.2652\tTriple Loss(1): 0.0305\tClassification Loss: 1.2043\r\n",
      "Train Epoch: 38 [15360/209539 (7%)]\tAll Loss: 1.4719\tTriple Loss(0): 0.2289\tClassification Loss: 1.0141\r\n",
      "Train Epoch: 38 [16000/209539 (8%)]\tAll Loss: 2.2513\tTriple Loss(0): 0.4682\tClassification Loss: 1.3149\r\n",
      "Train Epoch: 38 [16640/209539 (8%)]\tAll Loss: 1.5309\tTriple Loss(1): 0.0113\tClassification Loss: 1.5083\r\n",
      "Train Epoch: 38 [17280/209539 (8%)]\tAll Loss: 1.2747\tTriple Loss(1): 0.0587\tClassification Loss: 1.1572\r\n",
      "Train Epoch: 38 [17920/209539 (9%)]\tAll Loss: 1.0651\tTriple Loss(1): 0.0082\tClassification Loss: 1.0487\r\n",
      "Train Epoch: 38 [18560/209539 (9%)]\tAll Loss: 1.1275\tTriple Loss(1): 0.0000\tClassification Loss: 1.1275\r\n",
      "Train Epoch: 38 [19200/209539 (9%)]\tAll Loss: 1.0461\tTriple Loss(1): 0.0067\tClassification Loss: 1.0327\r\n",
      "Train Epoch: 38 [19840/209539 (9%)]\tAll Loss: 1.6598\tTriple Loss(0): 0.2494\tClassification Loss: 1.1609\r\n",
      "Train Epoch: 38 [20480/209539 (10%)]\tAll Loss: 1.1368\tTriple Loss(1): 0.0488\tClassification Loss: 1.0393\r\n",
      "Train Epoch: 38 [21120/209539 (10%)]\tAll Loss: 1.4929\tTriple Loss(1): 0.0000\tClassification Loss: 1.4929\r\n",
      "Train Epoch: 38 [21760/209539 (10%)]\tAll Loss: 1.0640\tTriple Loss(1): 0.0133\tClassification Loss: 1.0374\r\n",
      "Train Epoch: 38 [22400/209539 (11%)]\tAll Loss: 1.0964\tTriple Loss(1): 0.0097\tClassification Loss: 1.0770\r\n",
      "Train Epoch: 38 [23040/209539 (11%)]\tAll Loss: 1.5666\tTriple Loss(0): 0.1126\tClassification Loss: 1.3414\r\n",
      "Train Epoch: 38 [23680/209539 (11%)]\tAll Loss: 1.0006\tTriple Loss(1): 0.0040\tClassification Loss: 0.9926\r\n",
      "Train Epoch: 38 [24320/209539 (12%)]\tAll Loss: 1.4056\tTriple Loss(1): 0.0248\tClassification Loss: 1.3559\r\n",
      "Train Epoch: 38 [24960/209539 (12%)]\tAll Loss: 0.9216\tTriple Loss(1): 0.0038\tClassification Loss: 0.9140\r\n",
      "Train Epoch: 38 [25600/209539 (12%)]\tAll Loss: 1.7145\tTriple Loss(0): 0.2770\tClassification Loss: 1.1605\r\n",
      "Train Epoch: 38 [26240/209539 (13%)]\tAll Loss: 0.9812\tTriple Loss(1): 0.0372\tClassification Loss: 0.9068\r\n",
      "Train Epoch: 38 [26880/209539 (13%)]\tAll Loss: 1.2982\tTriple Loss(1): 0.0000\tClassification Loss: 1.2982\r\n",
      "Train Epoch: 38 [27520/209539 (13%)]\tAll Loss: 1.1421\tTriple Loss(1): 0.0028\tClassification Loss: 1.1366\r\n",
      "Train Epoch: 38 [28160/209539 (13%)]\tAll Loss: 0.9854\tTriple Loss(1): 0.0026\tClassification Loss: 0.9803\r\n",
      "Train Epoch: 38 [28800/209539 (14%)]\tAll Loss: 1.4152\tTriple Loss(1): 0.0230\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 38 [29440/209539 (14%)]\tAll Loss: 1.3091\tTriple Loss(1): 0.0167\tClassification Loss: 1.2757\r\n",
      "Train Epoch: 38 [30080/209539 (14%)]\tAll Loss: 0.9674\tTriple Loss(1): 0.0279\tClassification Loss: 0.9116\r\n",
      "Train Epoch: 38 [30720/209539 (15%)]\tAll Loss: 1.0224\tTriple Loss(1): 0.0000\tClassification Loss: 1.0224\r\n",
      "Train Epoch: 38 [31360/209539 (15%)]\tAll Loss: 1.6462\tTriple Loss(0): 0.3289\tClassification Loss: 0.9883\r\n",
      "Train Epoch: 38 [32000/209539 (15%)]\tAll Loss: 1.3299\tTriple Loss(1): 0.0064\tClassification Loss: 1.3171\r\n",
      "Train Epoch: 38 [32640/209539 (16%)]\tAll Loss: 1.3460\tTriple Loss(1): 0.0364\tClassification Loss: 1.2732\r\n",
      "Train Epoch: 38 [33280/209539 (16%)]\tAll Loss: 1.2515\tTriple Loss(1): 0.0393\tClassification Loss: 1.1729\r\n",
      "Train Epoch: 38 [33920/209539 (16%)]\tAll Loss: 1.1557\tTriple Loss(1): 0.0008\tClassification Loss: 1.1540\r\n",
      "Train Epoch: 38 [34560/209539 (16%)]\tAll Loss: 1.0814\tTriple Loss(1): 0.0000\tClassification Loss: 1.0814\r\n",
      "Train Epoch: 38 [35200/209539 (17%)]\tAll Loss: 0.9809\tTriple Loss(1): 0.0000\tClassification Loss: 0.9809\r\n",
      "Train Epoch: 38 [35840/209539 (17%)]\tAll Loss: 1.0454\tTriple Loss(1): 0.0216\tClassification Loss: 1.0022\r\n",
      "Train Epoch: 38 [36480/209539 (17%)]\tAll Loss: 0.9940\tTriple Loss(1): 0.0033\tClassification Loss: 0.9875\r\n",
      "Train Epoch: 38 [37120/209539 (18%)]\tAll Loss: 2.0464\tTriple Loss(0): 0.3032\tClassification Loss: 1.4399\r\n",
      "Train Epoch: 38 [37760/209539 (18%)]\tAll Loss: 1.3555\tTriple Loss(1): 0.0134\tClassification Loss: 1.3287\r\n",
      "Train Epoch: 38 [38400/209539 (18%)]\tAll Loss: 2.0200\tTriple Loss(0): 0.4018\tClassification Loss: 1.2164\r\n",
      "Train Epoch: 38 [39040/209539 (19%)]\tAll Loss: 0.8954\tTriple Loss(1): 0.0149\tClassification Loss: 0.8656\r\n",
      "Train Epoch: 38 [39680/209539 (19%)]\tAll Loss: 1.1424\tTriple Loss(1): 0.0314\tClassification Loss: 1.0797\r\n",
      "Train Epoch: 38 [40320/209539 (19%)]\tAll Loss: 1.2851\tTriple Loss(1): 0.0233\tClassification Loss: 1.2385\r\n",
      "Train Epoch: 38 [40960/209539 (20%)]\tAll Loss: 1.2712\tTriple Loss(1): 0.0000\tClassification Loss: 1.2712\r\n",
      "Train Epoch: 38 [41600/209539 (20%)]\tAll Loss: 1.1818\tTriple Loss(1): 0.0232\tClassification Loss: 1.1355\r\n",
      "Train Epoch: 38 [42240/209539 (20%)]\tAll Loss: 1.1112\tTriple Loss(1): 0.0094\tClassification Loss: 1.0925\r\n",
      "Train Epoch: 38 [42880/209539 (20%)]\tAll Loss: 0.8746\tTriple Loss(1): 0.0027\tClassification Loss: 0.8693\r\n",
      "Train Epoch: 38 [43520/209539 (21%)]\tAll Loss: 1.2486\tTriple Loss(1): 0.0283\tClassification Loss: 1.1919\r\n",
      "Train Epoch: 38 [44160/209539 (21%)]\tAll Loss: 1.4410\tTriple Loss(1): 0.0007\tClassification Loss: 1.4397\r\n",
      "Train Epoch: 38 [44800/209539 (21%)]\tAll Loss: 1.5403\tTriple Loss(1): 0.0034\tClassification Loss: 1.5334\r\n",
      "Train Epoch: 38 [45440/209539 (22%)]\tAll Loss: 1.6892\tTriple Loss(1): 0.0062\tClassification Loss: 1.6769\r\n",
      "Train Epoch: 38 [46080/209539 (22%)]\tAll Loss: 1.1701\tTriple Loss(1): 0.0000\tClassification Loss: 1.1701\r\n",
      "Train Epoch: 38 [46720/209539 (22%)]\tAll Loss: 1.3868\tTriple Loss(1): 0.0000\tClassification Loss: 1.3868\r\n",
      "Train Epoch: 38 [47360/209539 (23%)]\tAll Loss: 1.2958\tTriple Loss(0): 0.2575\tClassification Loss: 0.7808\r\n",
      "Train Epoch: 38 [48000/209539 (23%)]\tAll Loss: 1.4672\tTriple Loss(0): 0.2362\tClassification Loss: 0.9947\r\n",
      "Train Epoch: 38 [48640/209539 (23%)]\tAll Loss: 1.9782\tTriple Loss(0): 0.2824\tClassification Loss: 1.4134\r\n",
      "Train Epoch: 38 [49280/209539 (24%)]\tAll Loss: 1.1147\tTriple Loss(1): 0.0120\tClassification Loss: 1.0906\r\n",
      "Train Epoch: 38 [49920/209539 (24%)]\tAll Loss: 1.2747\tTriple Loss(1): 0.0053\tClassification Loss: 1.2641\r\n",
      "Train Epoch: 38 [50560/209539 (24%)]\tAll Loss: 1.4979\tTriple Loss(1): 0.0000\tClassification Loss: 1.4979\r\n",
      "Train Epoch: 38 [51200/209539 (24%)]\tAll Loss: 1.4654\tTriple Loss(1): 0.0088\tClassification Loss: 1.4477\r\n",
      "Train Epoch: 38 [51840/209539 (25%)]\tAll Loss: 1.1649\tTriple Loss(1): 0.0095\tClassification Loss: 1.1459\r\n",
      "Train Epoch: 38 [52480/209539 (25%)]\tAll Loss: 1.1072\tTriple Loss(1): 0.0121\tClassification Loss: 1.0829\r\n",
      "Train Epoch: 38 [53120/209539 (25%)]\tAll Loss: 1.2997\tTriple Loss(1): 0.0409\tClassification Loss: 1.2179\r\n",
      "Train Epoch: 38 [53760/209539 (26%)]\tAll Loss: 1.4147\tTriple Loss(1): 0.0667\tClassification Loss: 1.2814\r\n",
      "Train Epoch: 38 [54400/209539 (26%)]\tAll Loss: 1.9169\tTriple Loss(1): 0.0298\tClassification Loss: 1.8573\r\n",
      "Train Epoch: 38 [55040/209539 (26%)]\tAll Loss: 0.8996\tTriple Loss(1): 0.0123\tClassification Loss: 0.8749\r\n",
      "Train Epoch: 38 [55680/209539 (27%)]\tAll Loss: 1.4328\tTriple Loss(1): 0.0313\tClassification Loss: 1.3702\r\n",
      "Train Epoch: 38 [56320/209539 (27%)]\tAll Loss: 0.8890\tTriple Loss(1): 0.0258\tClassification Loss: 0.8373\r\n",
      "Train Epoch: 38 [56960/209539 (27%)]\tAll Loss: 1.9570\tTriple Loss(0): 0.3706\tClassification Loss: 1.2158\r\n",
      "Train Epoch: 38 [57600/209539 (27%)]\tAll Loss: 1.1641\tTriple Loss(0): 0.0977\tClassification Loss: 0.9687\r\n",
      "Train Epoch: 38 [58240/209539 (28%)]\tAll Loss: 1.2142\tTriple Loss(1): 0.0953\tClassification Loss: 1.0236\r\n",
      "Train Epoch: 38 [58880/209539 (28%)]\tAll Loss: 1.1792\tTriple Loss(1): 0.0054\tClassification Loss: 1.1684\r\n",
      "Train Epoch: 38 [59520/209539 (28%)]\tAll Loss: 1.4386\tTriple Loss(0): 0.2391\tClassification Loss: 0.9603\r\n",
      "Train Epoch: 38 [60160/209539 (29%)]\tAll Loss: 1.7116\tTriple Loss(0): 0.2542\tClassification Loss: 1.2031\r\n",
      "Train Epoch: 38 [60800/209539 (29%)]\tAll Loss: 1.4261\tTriple Loss(1): 0.0465\tClassification Loss: 1.3330\r\n",
      "Train Epoch: 38 [61440/209539 (29%)]\tAll Loss: 1.2704\tTriple Loss(1): 0.0106\tClassification Loss: 1.2492\r\n",
      "Train Epoch: 38 [62080/209539 (30%)]\tAll Loss: 2.0063\tTriple Loss(0): 0.2482\tClassification Loss: 1.5100\r\n",
      "Train Epoch: 38 [62720/209539 (30%)]\tAll Loss: 1.1568\tTriple Loss(1): 0.0308\tClassification Loss: 1.0953\r\n",
      "Train Epoch: 38 [63360/209539 (30%)]\tAll Loss: 1.2634\tTriple Loss(1): 0.0157\tClassification Loss: 1.2319\r\n",
      "Train Epoch: 38 [64000/209539 (31%)]\tAll Loss: 1.6837\tTriple Loss(0): 0.2331\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 38 [64640/209539 (31%)]\tAll Loss: 1.6058\tTriple Loss(1): 0.0487\tClassification Loss: 1.5084\r\n",
      "Train Epoch: 38 [65280/209539 (31%)]\tAll Loss: 1.4016\tTriple Loss(1): 0.0046\tClassification Loss: 1.3924\r\n",
      "Train Epoch: 38 [65920/209539 (31%)]\tAll Loss: 1.6013\tTriple Loss(1): 0.0359\tClassification Loss: 1.5294\r\n",
      "Train Epoch: 38 [66560/209539 (32%)]\tAll Loss: 0.9604\tTriple Loss(1): 0.0065\tClassification Loss: 0.9474\r\n",
      "Train Epoch: 38 [67200/209539 (32%)]\tAll Loss: 1.1380\tTriple Loss(1): 0.0059\tClassification Loss: 1.1262\r\n",
      "Train Epoch: 38 [67840/209539 (32%)]\tAll Loss: 1.5226\tTriple Loss(0): 0.2083\tClassification Loss: 1.1061\r\n",
      "Train Epoch: 38 [68480/209539 (33%)]\tAll Loss: 1.4658\tTriple Loss(1): 0.0556\tClassification Loss: 1.3546\r\n",
      "Train Epoch: 38 [69120/209539 (33%)]\tAll Loss: 1.2418\tTriple Loss(1): 0.0163\tClassification Loss: 1.2093\r\n",
      "Train Epoch: 38 [69760/209539 (33%)]\tAll Loss: 1.1138\tTriple Loss(1): 0.0613\tClassification Loss: 0.9913\r\n",
      "Train Epoch: 38 [70400/209539 (34%)]\tAll Loss: 1.0262\tTriple Loss(1): 0.0107\tClassification Loss: 1.0048\r\n",
      "Train Epoch: 38 [71040/209539 (34%)]\tAll Loss: 1.6093\tTriple Loss(1): 0.0651\tClassification Loss: 1.4791\r\n",
      "Train Epoch: 38 [71680/209539 (34%)]\tAll Loss: 1.6473\tTriple Loss(1): 0.0056\tClassification Loss: 1.6361\r\n",
      "Train Epoch: 38 [72320/209539 (35%)]\tAll Loss: 1.1721\tTriple Loss(1): 0.0513\tClassification Loss: 1.0695\r\n",
      "Train Epoch: 38 [72960/209539 (35%)]\tAll Loss: 1.2977\tTriple Loss(1): 0.0371\tClassification Loss: 1.2234\r\n",
      "Train Epoch: 38 [73600/209539 (35%)]\tAll Loss: 1.4500\tTriple Loss(1): 0.0000\tClassification Loss: 1.4500\r\n",
      "Train Epoch: 38 [74240/209539 (35%)]\tAll Loss: 1.3861\tTriple Loss(1): 0.0000\tClassification Loss: 1.3861\r\n",
      "Train Epoch: 38 [74880/209539 (36%)]\tAll Loss: 2.2524\tTriple Loss(0): 0.3460\tClassification Loss: 1.5604\r\n",
      "Train Epoch: 38 [75520/209539 (36%)]\tAll Loss: 0.7967\tTriple Loss(1): 0.0086\tClassification Loss: 0.7795\r\n",
      "Train Epoch: 38 [76160/209539 (36%)]\tAll Loss: 1.3921\tTriple Loss(0): 0.1771\tClassification Loss: 1.0379\r\n",
      "Train Epoch: 38 [76800/209539 (37%)]\tAll Loss: 1.1206\tTriple Loss(1): 0.0697\tClassification Loss: 0.9813\r\n",
      "Train Epoch: 38 [77440/209539 (37%)]\tAll Loss: 1.3516\tTriple Loss(1): 0.0000\tClassification Loss: 1.3516\r\n",
      "Train Epoch: 38 [78080/209539 (37%)]\tAll Loss: 1.1948\tTriple Loss(1): 0.0091\tClassification Loss: 1.1766\r\n",
      "Train Epoch: 38 [78720/209539 (38%)]\tAll Loss: 1.3516\tTriple Loss(1): 0.0066\tClassification Loss: 1.3383\r\n",
      "Train Epoch: 38 [79360/209539 (38%)]\tAll Loss: 1.2367\tTriple Loss(1): 0.0498\tClassification Loss: 1.1371\r\n",
      "Train Epoch: 38 [80000/209539 (38%)]\tAll Loss: 1.1999\tTriple Loss(1): 0.0508\tClassification Loss: 1.0983\r\n",
      "Train Epoch: 38 [80640/209539 (38%)]\tAll Loss: 1.3679\tTriple Loss(1): 0.0228\tClassification Loss: 1.3224\r\n",
      "Train Epoch: 38 [81280/209539 (39%)]\tAll Loss: 1.3573\tTriple Loss(1): 0.0671\tClassification Loss: 1.2232\r\n",
      "Train Epoch: 38 [81920/209539 (39%)]\tAll Loss: 1.1270\tTriple Loss(1): 0.0263\tClassification Loss: 1.0745\r\n",
      "Train Epoch: 38 [82560/209539 (39%)]\tAll Loss: 1.1890\tTriple Loss(1): 0.0042\tClassification Loss: 1.1806\r\n",
      "Train Epoch: 38 [83200/209539 (40%)]\tAll Loss: 1.4472\tTriple Loss(1): 0.1341\tClassification Loss: 1.1790\r\n",
      "Train Epoch: 38 [83840/209539 (40%)]\tAll Loss: 1.3256\tTriple Loss(1): 0.0232\tClassification Loss: 1.2792\r\n",
      "Train Epoch: 38 [84480/209539 (40%)]\tAll Loss: 1.1251\tTriple Loss(1): 0.0167\tClassification Loss: 1.0917\r\n",
      "Train Epoch: 38 [85120/209539 (41%)]\tAll Loss: 1.5510\tTriple Loss(1): 0.0104\tClassification Loss: 1.5301\r\n",
      "Train Epoch: 38 [85760/209539 (41%)]\tAll Loss: 1.2277\tTriple Loss(1): 0.0351\tClassification Loss: 1.1574\r\n",
      "Train Epoch: 38 [86400/209539 (41%)]\tAll Loss: 1.1349\tTriple Loss(1): 0.0017\tClassification Loss: 1.1315\r\n",
      "Train Epoch: 38 [87040/209539 (42%)]\tAll Loss: 1.1151\tTriple Loss(1): 0.0397\tClassification Loss: 1.0357\r\n",
      "Train Epoch: 38 [87680/209539 (42%)]\tAll Loss: 1.1097\tTriple Loss(1): 0.0391\tClassification Loss: 1.0315\r\n",
      "Train Epoch: 38 [88320/209539 (42%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0506\tClassification Loss: 1.1096\r\n",
      "Train Epoch: 38 [88960/209539 (42%)]\tAll Loss: 1.2544\tTriple Loss(1): 0.0192\tClassification Loss: 1.2160\r\n",
      "Train Epoch: 38 [89600/209539 (43%)]\tAll Loss: 1.1532\tTriple Loss(1): 0.0000\tClassification Loss: 1.1532\r\n",
      "Train Epoch: 38 [90240/209539 (43%)]\tAll Loss: 1.1746\tTriple Loss(1): 0.0411\tClassification Loss: 1.0924\r\n",
      "Train Epoch: 38 [90880/209539 (43%)]\tAll Loss: 1.8506\tTriple Loss(0): 0.2720\tClassification Loss: 1.3066\r\n",
      "Train Epoch: 38 [91520/209539 (44%)]\tAll Loss: 1.6968\tTriple Loss(0): 0.2299\tClassification Loss: 1.2370\r\n",
      "Train Epoch: 38 [92160/209539 (44%)]\tAll Loss: 1.0906\tTriple Loss(1): 0.0233\tClassification Loss: 1.0439\r\n",
      "Train Epoch: 38 [92800/209539 (44%)]\tAll Loss: 1.0345\tTriple Loss(1): 0.0294\tClassification Loss: 0.9758\r\n",
      "Train Epoch: 38 [93440/209539 (45%)]\tAll Loss: 1.2237\tTriple Loss(1): 0.0020\tClassification Loss: 1.2197\r\n",
      "Train Epoch: 38 [94080/209539 (45%)]\tAll Loss: 1.0792\tTriple Loss(1): 0.0182\tClassification Loss: 1.0428\r\n",
      "Train Epoch: 38 [94720/209539 (45%)]\tAll Loss: 1.3333\tTriple Loss(1): 0.0363\tClassification Loss: 1.2608\r\n",
      "Train Epoch: 38 [95360/209539 (46%)]\tAll Loss: 1.4323\tTriple Loss(1): 0.0127\tClassification Loss: 1.4068\r\n",
      "Train Epoch: 38 [96000/209539 (46%)]\tAll Loss: 1.2227\tTriple Loss(1): 0.0130\tClassification Loss: 1.1968\r\n",
      "Train Epoch: 38 [96640/209539 (46%)]\tAll Loss: 1.3581\tTriple Loss(1): 0.0036\tClassification Loss: 1.3509\r\n",
      "Train Epoch: 38 [97280/209539 (46%)]\tAll Loss: 1.0512\tTriple Loss(1): 0.0059\tClassification Loss: 1.0393\r\n",
      "Train Epoch: 38 [97920/209539 (47%)]\tAll Loss: 1.4462\tTriple Loss(1): 0.0000\tClassification Loss: 1.4462\r\n",
      "Train Epoch: 38 [98560/209539 (47%)]\tAll Loss: 1.7099\tTriple Loss(1): 0.0216\tClassification Loss: 1.6668\r\n",
      "Train Epoch: 38 [99200/209539 (47%)]\tAll Loss: 1.0526\tTriple Loss(1): 0.0000\tClassification Loss: 1.0526\r\n",
      "Train Epoch: 38 [99840/209539 (48%)]\tAll Loss: 1.3809\tTriple Loss(0): 0.1187\tClassification Loss: 1.1436\r\n",
      "Train Epoch: 38 [100480/209539 (48%)]\tAll Loss: 1.0500\tTriple Loss(1): 0.0050\tClassification Loss: 1.0400\r\n",
      "Train Epoch: 38 [101120/209539 (48%)]\tAll Loss: 1.2868\tTriple Loss(1): 0.0000\tClassification Loss: 1.2868\r\n",
      "Train Epoch: 38 [101760/209539 (49%)]\tAll Loss: 1.2089\tTriple Loss(1): 0.0225\tClassification Loss: 1.1638\r\n",
      "Train Epoch: 38 [102400/209539 (49%)]\tAll Loss: 1.0735\tTriple Loss(1): 0.0212\tClassification Loss: 1.0311\r\n",
      "Train Epoch: 38 [103040/209539 (49%)]\tAll Loss: 1.7913\tTriple Loss(0): 0.3898\tClassification Loss: 1.0116\r\n",
      "Train Epoch: 38 [103680/209539 (49%)]\tAll Loss: 1.2618\tTriple Loss(1): 0.0160\tClassification Loss: 1.2298\r\n",
      "Train Epoch: 38 [104320/209539 (50%)]\tAll Loss: 0.9497\tTriple Loss(1): 0.0000\tClassification Loss: 0.9497\r\n",
      "Train Epoch: 38 [104960/209539 (50%)]\tAll Loss: 1.1779\tTriple Loss(1): 0.0268\tClassification Loss: 1.1242\r\n",
      "Train Epoch: 38 [105600/209539 (50%)]\tAll Loss: 1.0877\tTriple Loss(1): 0.0184\tClassification Loss: 1.0509\r\n",
      "Train Epoch: 38 [106240/209539 (51%)]\tAll Loss: 1.5909\tTriple Loss(0): 0.3274\tClassification Loss: 0.9361\r\n",
      "Train Epoch: 38 [106880/209539 (51%)]\tAll Loss: 1.3794\tTriple Loss(0): 0.2157\tClassification Loss: 0.9479\r\n",
      "Train Epoch: 38 [107520/209539 (51%)]\tAll Loss: 1.2447\tTriple Loss(1): 0.0494\tClassification Loss: 1.1459\r\n",
      "Train Epoch: 38 [108160/209539 (52%)]\tAll Loss: 1.5718\tTriple Loss(0): 0.1890\tClassification Loss: 1.1938\r\n",
      "Train Epoch: 38 [108800/209539 (52%)]\tAll Loss: 1.3116\tTriple Loss(1): 0.0117\tClassification Loss: 1.2882\r\n",
      "Train Epoch: 38 [109440/209539 (52%)]\tAll Loss: 1.4221\tTriple Loss(1): 0.0295\tClassification Loss: 1.3630\r\n",
      "Train Epoch: 38 [110080/209539 (53%)]\tAll Loss: 1.4214\tTriple Loss(1): 0.0255\tClassification Loss: 1.3704\r\n",
      "Train Epoch: 38 [110720/209539 (53%)]\tAll Loss: 1.2255\tTriple Loss(1): 0.0000\tClassification Loss: 1.2255\r\n",
      "Train Epoch: 38 [111360/209539 (53%)]\tAll Loss: 0.8817\tTriple Loss(1): 0.0000\tClassification Loss: 0.8817\r\n",
      "Train Epoch: 38 [112000/209539 (53%)]\tAll Loss: 1.0304\tTriple Loss(1): 0.0102\tClassification Loss: 1.0101\r\n",
      "Train Epoch: 38 [112640/209539 (54%)]\tAll Loss: 1.0790\tTriple Loss(1): 0.0000\tClassification Loss: 1.0790\r\n",
      "Train Epoch: 38 [113280/209539 (54%)]\tAll Loss: 0.9272\tTriple Loss(1): 0.0102\tClassification Loss: 0.9069\r\n",
      "Train Epoch: 38 [113920/209539 (54%)]\tAll Loss: 1.7009\tTriple Loss(0): 0.2016\tClassification Loss: 1.2977\r\n",
      "Train Epoch: 38 [114560/209539 (55%)]\tAll Loss: 1.3358\tTriple Loss(1): 0.0000\tClassification Loss: 1.3358\r\n",
      "Train Epoch: 38 [115200/209539 (55%)]\tAll Loss: 1.8064\tTriple Loss(0): 0.2132\tClassification Loss: 1.3800\r\n",
      "Train Epoch: 38 [115840/209539 (55%)]\tAll Loss: 1.2511\tTriple Loss(1): 0.0000\tClassification Loss: 1.2511\r\n",
      "Train Epoch: 38 [116480/209539 (56%)]\tAll Loss: 1.2575\tTriple Loss(1): 0.0076\tClassification Loss: 1.2424\r\n",
      "Train Epoch: 38 [117120/209539 (56%)]\tAll Loss: 1.3402\tTriple Loss(1): 0.0452\tClassification Loss: 1.2498\r\n",
      "Train Epoch: 38 [117760/209539 (56%)]\tAll Loss: 1.3339\tTriple Loss(1): 0.0209\tClassification Loss: 1.2920\r\n",
      "Train Epoch: 38 [118400/209539 (57%)]\tAll Loss: 1.0196\tTriple Loss(1): 0.0986\tClassification Loss: 0.8224\r\n",
      "Train Epoch: 38 [119040/209539 (57%)]\tAll Loss: 1.5148\tTriple Loss(1): 0.0204\tClassification Loss: 1.4740\r\n",
      "Train Epoch: 38 [119680/209539 (57%)]\tAll Loss: 1.1797\tTriple Loss(1): 0.0349\tClassification Loss: 1.1098\r\n",
      "Train Epoch: 38 [120320/209539 (57%)]\tAll Loss: 1.1121\tTriple Loss(1): 0.0003\tClassification Loss: 1.1115\r\n",
      "Train Epoch: 38 [120960/209539 (58%)]\tAll Loss: 1.2124\tTriple Loss(1): 0.0488\tClassification Loss: 1.1148\r\n",
      "Train Epoch: 38 [121600/209539 (58%)]\tAll Loss: 1.2599\tTriple Loss(1): 0.0641\tClassification Loss: 1.1317\r\n",
      "Train Epoch: 38 [122240/209539 (58%)]\tAll Loss: 1.2256\tTriple Loss(1): 0.0547\tClassification Loss: 1.1161\r\n",
      "Train Epoch: 38 [122880/209539 (59%)]\tAll Loss: 1.0481\tTriple Loss(1): 0.0070\tClassification Loss: 1.0341\r\n",
      "Train Epoch: 38 [123520/209539 (59%)]\tAll Loss: 1.2085\tTriple Loss(1): 0.0399\tClassification Loss: 1.1288\r\n",
      "Train Epoch: 38 [124160/209539 (59%)]\tAll Loss: 1.1581\tTriple Loss(1): 0.0196\tClassification Loss: 1.1189\r\n",
      "Train Epoch: 38 [124800/209539 (60%)]\tAll Loss: 1.1425\tTriple Loss(1): 0.0601\tClassification Loss: 1.0222\r\n",
      "Train Epoch: 38 [125440/209539 (60%)]\tAll Loss: 1.2751\tTriple Loss(1): 0.0145\tClassification Loss: 1.2460\r\n",
      "Train Epoch: 38 [126080/209539 (60%)]\tAll Loss: 1.3789\tTriple Loss(1): 0.0173\tClassification Loss: 1.3444\r\n",
      "Train Epoch: 38 [126720/209539 (60%)]\tAll Loss: 1.3862\tTriple Loss(1): 0.0271\tClassification Loss: 1.3320\r\n",
      "Train Epoch: 38 [127360/209539 (61%)]\tAll Loss: 1.2221\tTriple Loss(1): 0.0474\tClassification Loss: 1.1273\r\n",
      "Train Epoch: 38 [128000/209539 (61%)]\tAll Loss: 1.6562\tTriple Loss(0): 0.1457\tClassification Loss: 1.3647\r\n",
      "Train Epoch: 38 [128640/209539 (61%)]\tAll Loss: 0.9774\tTriple Loss(1): 0.0470\tClassification Loss: 0.8834\r\n",
      "Train Epoch: 38 [129280/209539 (62%)]\tAll Loss: 1.2410\tTriple Loss(1): 0.0829\tClassification Loss: 1.0752\r\n",
      "Train Epoch: 38 [129920/209539 (62%)]\tAll Loss: 1.3266\tTriple Loss(1): 0.0000\tClassification Loss: 1.3266\r\n",
      "Train Epoch: 38 [130560/209539 (62%)]\tAll Loss: 0.9618\tTriple Loss(1): 0.0025\tClassification Loss: 0.9568\r\n",
      "Train Epoch: 38 [131200/209539 (63%)]\tAll Loss: 0.9710\tTriple Loss(1): 0.0010\tClassification Loss: 0.9691\r\n",
      "Train Epoch: 38 [131840/209539 (63%)]\tAll Loss: 1.0921\tTriple Loss(1): 0.0000\tClassification Loss: 1.0921\r\n",
      "Train Epoch: 38 [132480/209539 (63%)]\tAll Loss: 1.0705\tTriple Loss(1): 0.0800\tClassification Loss: 0.9105\r\n",
      "Train Epoch: 38 [133120/209539 (64%)]\tAll Loss: 1.1106\tTriple Loss(1): 0.0402\tClassification Loss: 1.0301\r\n",
      "Train Epoch: 38 [133760/209539 (64%)]\tAll Loss: 1.2899\tTriple Loss(1): 0.0159\tClassification Loss: 1.2582\r\n",
      "Train Epoch: 38 [134400/209539 (64%)]\tAll Loss: 1.5273\tTriple Loss(0): 0.2365\tClassification Loss: 1.0543\r\n",
      "Train Epoch: 38 [135040/209539 (64%)]\tAll Loss: 1.3542\tTriple Loss(1): 0.0298\tClassification Loss: 1.2946\r\n",
      "Train Epoch: 38 [135680/209539 (65%)]\tAll Loss: 1.5118\tTriple Loss(1): 0.0370\tClassification Loss: 1.4379\r\n",
      "Train Epoch: 38 [136320/209539 (65%)]\tAll Loss: 0.9758\tTriple Loss(1): 0.0113\tClassification Loss: 0.9532\r\n",
      "Train Epoch: 38 [136960/209539 (65%)]\tAll Loss: 1.2281\tTriple Loss(1): 0.0584\tClassification Loss: 1.1113\r\n",
      "Train Epoch: 38 [137600/209539 (66%)]\tAll Loss: 1.0790\tTriple Loss(1): 0.0262\tClassification Loss: 1.0265\r\n",
      "Train Epoch: 38 [138240/209539 (66%)]\tAll Loss: 1.6023\tTriple Loss(0): 0.1797\tClassification Loss: 1.2429\r\n",
      "Train Epoch: 38 [138880/209539 (66%)]\tAll Loss: 1.2360\tTriple Loss(1): 0.0141\tClassification Loss: 1.2078\r\n",
      "Train Epoch: 38 [139520/209539 (67%)]\tAll Loss: 1.2331\tTriple Loss(1): 0.0074\tClassification Loss: 1.2184\r\n",
      "Train Epoch: 38 [140160/209539 (67%)]\tAll Loss: 1.1432\tTriple Loss(1): 0.0000\tClassification Loss: 1.1432\r\n",
      "Train Epoch: 38 [140800/209539 (67%)]\tAll Loss: 1.3273\tTriple Loss(1): 0.0000\tClassification Loss: 1.3273\r\n",
      "Train Epoch: 38 [141440/209539 (68%)]\tAll Loss: 1.2910\tTriple Loss(1): 0.0000\tClassification Loss: 1.2910\r\n",
      "Train Epoch: 38 [142080/209539 (68%)]\tAll Loss: 1.3942\tTriple Loss(1): 0.0699\tClassification Loss: 1.2545\r\n",
      "Train Epoch: 38 [142720/209539 (68%)]\tAll Loss: 1.7279\tTriple Loss(1): 0.0000\tClassification Loss: 1.7279\r\n",
      "Train Epoch: 38 [143360/209539 (68%)]\tAll Loss: 1.1283\tTriple Loss(1): 0.0146\tClassification Loss: 1.0990\r\n",
      "Train Epoch: 38 [144000/209539 (69%)]\tAll Loss: 1.2245\tTriple Loss(1): 0.0244\tClassification Loss: 1.1757\r\n",
      "Train Epoch: 38 [144640/209539 (69%)]\tAll Loss: 1.1645\tTriple Loss(1): 0.0437\tClassification Loss: 1.0772\r\n",
      "Train Epoch: 38 [145280/209539 (69%)]\tAll Loss: 1.3387\tTriple Loss(1): 0.0000\tClassification Loss: 1.3387\r\n",
      "Train Epoch: 38 [145920/209539 (70%)]\tAll Loss: 1.1538\tTriple Loss(1): 0.0045\tClassification Loss: 1.1447\r\n",
      "Train Epoch: 38 [146560/209539 (70%)]\tAll Loss: 1.0223\tTriple Loss(1): 0.0087\tClassification Loss: 1.0049\r\n",
      "Train Epoch: 38 [147200/209539 (70%)]\tAll Loss: 1.2812\tTriple Loss(1): 0.0212\tClassification Loss: 1.2388\r\n",
      "Train Epoch: 38 [147840/209539 (71%)]\tAll Loss: 1.4351\tTriple Loss(0): 0.2207\tClassification Loss: 0.9937\r\n",
      "Train Epoch: 38 [148480/209539 (71%)]\tAll Loss: 1.1512\tTriple Loss(1): 0.0448\tClassification Loss: 1.0617\r\n",
      "Train Epoch: 38 [149120/209539 (71%)]\tAll Loss: 1.1094\tTriple Loss(1): 0.0192\tClassification Loss: 1.0709\r\n",
      "Train Epoch: 38 [149760/209539 (71%)]\tAll Loss: 1.2787\tTriple Loss(0): 0.2212\tClassification Loss: 0.8364\r\n",
      "Train Epoch: 38 [150400/209539 (72%)]\tAll Loss: 1.8306\tTriple Loss(0): 0.2480\tClassification Loss: 1.3347\r\n",
      "Train Epoch: 38 [151040/209539 (72%)]\tAll Loss: 1.1309\tTriple Loss(1): 0.0857\tClassification Loss: 0.9594\r\n",
      "Train Epoch: 38 [151680/209539 (72%)]\tAll Loss: 1.4249\tTriple Loss(1): 0.0342\tClassification Loss: 1.3565\r\n",
      "Train Epoch: 38 [152320/209539 (73%)]\tAll Loss: 1.6536\tTriple Loss(0): 0.2429\tClassification Loss: 1.1678\r\n",
      "Train Epoch: 38 [152960/209539 (73%)]\tAll Loss: 1.1163\tTriple Loss(1): 0.0324\tClassification Loss: 1.0515\r\n",
      "Train Epoch: 38 [153600/209539 (73%)]\tAll Loss: 1.2668\tTriple Loss(1): 0.0433\tClassification Loss: 1.1803\r\n",
      "Train Epoch: 38 [154240/209539 (74%)]\tAll Loss: 1.3350\tTriple Loss(1): 0.0541\tClassification Loss: 1.2268\r\n",
      "Train Epoch: 38 [154880/209539 (74%)]\tAll Loss: 1.1593\tTriple Loss(1): 0.0000\tClassification Loss: 1.1593\r\n",
      "Train Epoch: 38 [155520/209539 (74%)]\tAll Loss: 1.2176\tTriple Loss(1): 0.0000\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 38 [156160/209539 (75%)]\tAll Loss: 1.4144\tTriple Loss(0): 0.1214\tClassification Loss: 1.1715\r\n",
      "Train Epoch: 38 [156800/209539 (75%)]\tAll Loss: 1.5267\tTriple Loss(1): 0.0000\tClassification Loss: 1.5267\r\n",
      "Train Epoch: 38 [157440/209539 (75%)]\tAll Loss: 1.3279\tTriple Loss(1): 0.0000\tClassification Loss: 1.3279\r\n",
      "Train Epoch: 38 [158080/209539 (75%)]\tAll Loss: 1.2053\tTriple Loss(1): 0.0065\tClassification Loss: 1.1923\r\n",
      "Train Epoch: 38 [158720/209539 (76%)]\tAll Loss: 1.2133\tTriple Loss(1): 0.0134\tClassification Loss: 1.1865\r\n",
      "Train Epoch: 38 [159360/209539 (76%)]\tAll Loss: 0.9843\tTriple Loss(1): 0.0342\tClassification Loss: 0.9158\r\n",
      "Train Epoch: 38 [160000/209539 (76%)]\tAll Loss: 1.4117\tTriple Loss(1): 0.0170\tClassification Loss: 1.3777\r\n",
      "Train Epoch: 38 [160640/209539 (77%)]\tAll Loss: 1.0061\tTriple Loss(1): 0.0026\tClassification Loss: 1.0009\r\n",
      "Train Epoch: 38 [161280/209539 (77%)]\tAll Loss: 1.0907\tTriple Loss(1): 0.0215\tClassification Loss: 1.0476\r\n",
      "Train Epoch: 38 [161920/209539 (77%)]\tAll Loss: 1.1656\tTriple Loss(1): 0.0000\tClassification Loss: 1.1656\r\n",
      "Train Epoch: 38 [162560/209539 (78%)]\tAll Loss: 1.3278\tTriple Loss(1): 0.0672\tClassification Loss: 1.1933\r\n",
      "Train Epoch: 38 [163200/209539 (78%)]\tAll Loss: 1.2192\tTriple Loss(0): 0.1729\tClassification Loss: 0.8735\r\n",
      "Train Epoch: 38 [163840/209539 (78%)]\tAll Loss: 1.1637\tTriple Loss(1): 0.0253\tClassification Loss: 1.1130\r\n",
      "Train Epoch: 38 [164480/209539 (78%)]\tAll Loss: 0.9685\tTriple Loss(1): 0.0139\tClassification Loss: 0.9407\r\n",
      "Train Epoch: 38 [165120/209539 (79%)]\tAll Loss: 0.9789\tTriple Loss(1): 0.0341\tClassification Loss: 0.9108\r\n",
      "Train Epoch: 38 [165760/209539 (79%)]\tAll Loss: 1.2418\tTriple Loss(1): 0.0000\tClassification Loss: 1.2418\r\n",
      "Train Epoch: 38 [166400/209539 (79%)]\tAll Loss: 1.3108\tTriple Loss(0): 0.1335\tClassification Loss: 1.0438\r\n",
      "Train Epoch: 38 [167040/209539 (80%)]\tAll Loss: 1.5860\tTriple Loss(1): 0.0397\tClassification Loss: 1.5065\r\n",
      "Train Epoch: 38 [167680/209539 (80%)]\tAll Loss: 1.3525\tTriple Loss(1): 0.0454\tClassification Loss: 1.2616\r\n",
      "Train Epoch: 38 [168320/209539 (80%)]\tAll Loss: 0.9147\tTriple Loss(1): 0.0000\tClassification Loss: 0.9147\r\n",
      "Train Epoch: 38 [168960/209539 (81%)]\tAll Loss: 1.2541\tTriple Loss(0): 0.0940\tClassification Loss: 1.0661\r\n",
      "Train Epoch: 38 [169600/209539 (81%)]\tAll Loss: 1.2132\tTriple Loss(1): 0.0096\tClassification Loss: 1.1940\r\n",
      "Train Epoch: 38 [170240/209539 (81%)]\tAll Loss: 1.2197\tTriple Loss(1): 0.0036\tClassification Loss: 1.2125\r\n",
      "Train Epoch: 38 [170880/209539 (82%)]\tAll Loss: 1.2598\tTriple Loss(1): 0.0785\tClassification Loss: 1.1028\r\n",
      "Train Epoch: 38 [171520/209539 (82%)]\tAll Loss: 1.5868\tTriple Loss(0): 0.2289\tClassification Loss: 1.1289\r\n",
      "Train Epoch: 38 [172160/209539 (82%)]\tAll Loss: 1.2950\tTriple Loss(1): 0.0000\tClassification Loss: 1.2950\r\n",
      "Train Epoch: 38 [172800/209539 (82%)]\tAll Loss: 1.8093\tTriple Loss(0): 0.3159\tClassification Loss: 1.1776\r\n",
      "Train Epoch: 38 [173440/209539 (83%)]\tAll Loss: 1.4371\tTriple Loss(1): 0.0546\tClassification Loss: 1.3280\r\n",
      "Train Epoch: 38 [174080/209539 (83%)]\tAll Loss: 1.1060\tTriple Loss(1): 0.0538\tClassification Loss: 0.9985\r\n",
      "Train Epoch: 38 [174720/209539 (83%)]\tAll Loss: 1.0863\tTriple Loss(1): 0.0000\tClassification Loss: 1.0863\r\n",
      "Train Epoch: 38 [175360/209539 (84%)]\tAll Loss: 1.3200\tTriple Loss(1): 0.0720\tClassification Loss: 1.1759\r\n",
      "Train Epoch: 38 [176000/209539 (84%)]\tAll Loss: 1.1168\tTriple Loss(1): 0.0125\tClassification Loss: 1.0918\r\n",
      "Train Epoch: 38 [176640/209539 (84%)]\tAll Loss: 0.7706\tTriple Loss(1): 0.0243\tClassification Loss: 0.7221\r\n",
      "Train Epoch: 38 [177280/209539 (85%)]\tAll Loss: 1.5056\tTriple Loss(1): 0.0001\tClassification Loss: 1.5054\r\n",
      "Train Epoch: 38 [177920/209539 (85%)]\tAll Loss: 1.5957\tTriple Loss(0): 0.1694\tClassification Loss: 1.2570\r\n",
      "Train Epoch: 38 [178560/209539 (85%)]\tAll Loss: 0.9902\tTriple Loss(1): 0.0000\tClassification Loss: 0.9902\r\n",
      "Train Epoch: 38 [179200/209539 (86%)]\tAll Loss: 1.4708\tTriple Loss(1): 0.0226\tClassification Loss: 1.4256\r\n",
      "Train Epoch: 38 [179840/209539 (86%)]\tAll Loss: 1.3019\tTriple Loss(1): 0.0102\tClassification Loss: 1.2816\r\n",
      "Train Epoch: 38 [180480/209539 (86%)]\tAll Loss: 1.2120\tTriple Loss(1): 0.0003\tClassification Loss: 1.2113\r\n",
      "Train Epoch: 38 [181120/209539 (86%)]\tAll Loss: 1.3290\tTriple Loss(1): 0.0000\tClassification Loss: 1.3290\r\n",
      "Train Epoch: 38 [181760/209539 (87%)]\tAll Loss: 1.6024\tTriple Loss(0): 0.2840\tClassification Loss: 1.0343\r\n",
      "Train Epoch: 38 [182400/209539 (87%)]\tAll Loss: 1.3990\tTriple Loss(1): 0.0288\tClassification Loss: 1.3414\r\n",
      "Train Epoch: 38 [183040/209539 (87%)]\tAll Loss: 1.3359\tTriple Loss(1): 0.0603\tClassification Loss: 1.2152\r\n",
      "Train Epoch: 38 [183680/209539 (88%)]\tAll Loss: 1.1887\tTriple Loss(0): 0.1808\tClassification Loss: 0.8272\r\n",
      "Train Epoch: 38 [184320/209539 (88%)]\tAll Loss: 1.1318\tTriple Loss(1): 0.0003\tClassification Loss: 1.1312\r\n",
      "Train Epoch: 38 [184960/209539 (88%)]\tAll Loss: 1.0809\tTriple Loss(1): 0.0727\tClassification Loss: 0.9355\r\n",
      "Train Epoch: 38 [185600/209539 (89%)]\tAll Loss: 1.2551\tTriple Loss(1): 0.0132\tClassification Loss: 1.2286\r\n",
      "Train Epoch: 38 [186240/209539 (89%)]\tAll Loss: 1.9040\tTriple Loss(0): 0.2790\tClassification Loss: 1.3460\r\n",
      "Train Epoch: 38 [186880/209539 (89%)]\tAll Loss: 1.1419\tTriple Loss(1): 0.0412\tClassification Loss: 1.0595\r\n",
      "Train Epoch: 38 [187520/209539 (89%)]\tAll Loss: 1.2465\tTriple Loss(1): 0.0062\tClassification Loss: 1.2341\r\n",
      "Train Epoch: 38 [188160/209539 (90%)]\tAll Loss: 1.1847\tTriple Loss(0): 0.1897\tClassification Loss: 0.8054\r\n",
      "Train Epoch: 38 [188800/209539 (90%)]\tAll Loss: 1.7721\tTriple Loss(0): 0.3265\tClassification Loss: 1.1191\r\n",
      "Train Epoch: 38 [189440/209539 (90%)]\tAll Loss: 1.1350\tTriple Loss(1): 0.0450\tClassification Loss: 1.0449\r\n",
      "Train Epoch: 38 [190080/209539 (91%)]\tAll Loss: 1.1589\tTriple Loss(1): 0.0419\tClassification Loss: 1.0752\r\n",
      "Train Epoch: 38 [190720/209539 (91%)]\tAll Loss: 1.0945\tTriple Loss(1): 0.0049\tClassification Loss: 1.0848\r\n",
      "Train Epoch: 38 [191360/209539 (91%)]\tAll Loss: 1.1249\tTriple Loss(0): 0.1580\tClassification Loss: 0.8089\r\n",
      "Train Epoch: 38 [192000/209539 (92%)]\tAll Loss: 1.5771\tTriple Loss(1): 0.0000\tClassification Loss: 1.5771\r\n",
      "Train Epoch: 38 [192640/209539 (92%)]\tAll Loss: 1.2099\tTriple Loss(1): 0.0017\tClassification Loss: 1.2064\r\n",
      "Train Epoch: 38 [193280/209539 (92%)]\tAll Loss: 1.1541\tTriple Loss(1): 0.0035\tClassification Loss: 1.1471\r\n",
      "Train Epoch: 38 [193920/209539 (93%)]\tAll Loss: 1.0600\tTriple Loss(1): 0.0106\tClassification Loss: 1.0388\r\n",
      "Train Epoch: 38 [194560/209539 (93%)]\tAll Loss: 1.0807\tTriple Loss(1): 0.0022\tClassification Loss: 1.0764\r\n",
      "Train Epoch: 38 [195200/209539 (93%)]\tAll Loss: 1.3018\tTriple Loss(1): 0.0684\tClassification Loss: 1.1650\r\n",
      "Train Epoch: 38 [195840/209539 (93%)]\tAll Loss: 1.6317\tTriple Loss(0): 0.3437\tClassification Loss: 0.9442\r\n",
      "Train Epoch: 38 [196480/209539 (94%)]\tAll Loss: 1.3518\tTriple Loss(1): 0.0097\tClassification Loss: 1.3323\r\n",
      "Train Epoch: 38 [197120/209539 (94%)]\tAll Loss: 1.1812\tTriple Loss(1): 0.0281\tClassification Loss: 1.1250\r\n",
      "Train Epoch: 38 [197760/209539 (94%)]\tAll Loss: 1.1284\tTriple Loss(1): 0.0000\tClassification Loss: 1.1284\r\n",
      "Train Epoch: 38 [198400/209539 (95%)]\tAll Loss: 1.1391\tTriple Loss(1): 0.0053\tClassification Loss: 1.1285\r\n",
      "Train Epoch: 38 [199040/209539 (95%)]\tAll Loss: 1.2756\tTriple Loss(1): 0.0097\tClassification Loss: 1.2563\r\n",
      "Train Epoch: 38 [199680/209539 (95%)]\tAll Loss: 1.2188\tTriple Loss(1): 0.0378\tClassification Loss: 1.1431\r\n",
      "Train Epoch: 38 [200320/209539 (96%)]\tAll Loss: 1.2670\tTriple Loss(1): 0.0579\tClassification Loss: 1.1512\r\n",
      "Train Epoch: 38 [200960/209539 (96%)]\tAll Loss: 0.9886\tTriple Loss(1): 0.0048\tClassification Loss: 0.9790\r\n",
      "Train Epoch: 38 [201600/209539 (96%)]\tAll Loss: 1.2628\tTriple Loss(1): 0.1078\tClassification Loss: 1.0473\r\n",
      "Train Epoch: 38 [202240/209539 (97%)]\tAll Loss: 1.6399\tTriple Loss(0): 0.3573\tClassification Loss: 0.9253\r\n",
      "Train Epoch: 38 [202880/209539 (97%)]\tAll Loss: 0.8222\tTriple Loss(1): 0.0000\tClassification Loss: 0.8222\r\n",
      "Train Epoch: 38 [203520/209539 (97%)]\tAll Loss: 1.2201\tTriple Loss(1): 0.0200\tClassification Loss: 1.1800\r\n",
      "Train Epoch: 38 [204160/209539 (97%)]\tAll Loss: 1.4714\tTriple Loss(1): 0.0266\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 38 [204800/209539 (98%)]\tAll Loss: 1.3248\tTriple Loss(1): 0.0000\tClassification Loss: 1.3248\r\n",
      "Train Epoch: 38 [205440/209539 (98%)]\tAll Loss: 0.9328\tTriple Loss(1): 0.0060\tClassification Loss: 0.9209\r\n",
      "Train Epoch: 38 [206080/209539 (98%)]\tAll Loss: 1.0237\tTriple Loss(1): 0.0274\tClassification Loss: 0.9689\r\n",
      "Train Epoch: 38 [206720/209539 (99%)]\tAll Loss: 1.6672\tTriple Loss(0): 0.2981\tClassification Loss: 1.0710\r\n",
      "Train Epoch: 38 [207360/209539 (99%)]\tAll Loss: 1.1505\tTriple Loss(1): 0.0169\tClassification Loss: 1.1167\r\n",
      "Train Epoch: 38 [208000/209539 (99%)]\tAll Loss: 1.0408\tTriple Loss(1): 0.0315\tClassification Loss: 0.9778\r\n",
      "Train Epoch: 38 [208640/209539 (100%)]\tAll Loss: 1.2173\tTriple Loss(1): 0.0000\tClassification Loss: 1.2173\r\n",
      "Train Epoch: 38 [209280/209539 (100%)]\tAll Loss: 1.7046\tTriple Loss(0): 0.2470\tClassification Loss: 1.2106\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/38_epochs\r\n",
      "Train Epoch: 39 [0/209539 (0%)]\tAll Loss: 1.7270\tTriple Loss(1): 0.1816\tClassification Loss: 1.3637\r\n",
      "\r\n",
      "Test set: Average loss: 1.0748\r\n",
      "Top 1 Accuracy: 54966/80128 (69%)\r\n",
      "Top 3 Accuracy: 70196/80128 (88%)\r\n",
      "Top 5 Accuracy: 74781/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 39 [640/209539 (0%)]\tAll Loss: 1.2768\tTriple Loss(1): 0.0000\tClassification Loss: 1.2768\r\n",
      "Train Epoch: 39 [1280/209539 (1%)]\tAll Loss: 1.2533\tTriple Loss(1): 0.0514\tClassification Loss: 1.1505\r\n",
      "Train Epoch: 39 [1920/209539 (1%)]\tAll Loss: 0.9893\tTriple Loss(1): 0.0113\tClassification Loss: 0.9666\r\n",
      "Train Epoch: 39 [2560/209539 (1%)]\tAll Loss: 1.6796\tTriple Loss(1): 0.0335\tClassification Loss: 1.6125\r\n",
      "Train Epoch: 39 [3200/209539 (2%)]\tAll Loss: 1.3416\tTriple Loss(1): 0.0316\tClassification Loss: 1.2784\r\n",
      "Train Epoch: 39 [3840/209539 (2%)]\tAll Loss: 1.1611\tTriple Loss(1): 0.0937\tClassification Loss: 0.9737\r\n",
      "Train Epoch: 39 [4480/209539 (2%)]\tAll Loss: 1.2250\tTriple Loss(1): 0.0253\tClassification Loss: 1.1744\r\n",
      "Train Epoch: 39 [5120/209539 (2%)]\tAll Loss: 1.1328\tTriple Loss(1): 0.0448\tClassification Loss: 1.0432\r\n",
      "Train Epoch: 39 [5760/209539 (3%)]\tAll Loss: 1.2823\tTriple Loss(1): 0.0259\tClassification Loss: 1.2306\r\n",
      "Train Epoch: 39 [6400/209539 (3%)]\tAll Loss: 1.4137\tTriple Loss(0): 0.2744\tClassification Loss: 0.8650\r\n",
      "Train Epoch: 39 [7040/209539 (3%)]\tAll Loss: 1.2262\tTriple Loss(1): 0.0061\tClassification Loss: 1.2140\r\n",
      "Train Epoch: 39 [7680/209539 (4%)]\tAll Loss: 0.8109\tTriple Loss(1): 0.0189\tClassification Loss: 0.7731\r\n",
      "Train Epoch: 39 [8320/209539 (4%)]\tAll Loss: 1.3412\tTriple Loss(1): 0.1110\tClassification Loss: 1.1192\r\n",
      "Train Epoch: 39 [8960/209539 (4%)]\tAll Loss: 1.6564\tTriple Loss(0): 0.2655\tClassification Loss: 1.1253\r\n",
      "Train Epoch: 39 [9600/209539 (5%)]\tAll Loss: 1.2603\tTriple Loss(1): 0.0030\tClassification Loss: 1.2542\r\n",
      "Train Epoch: 39 [10240/209539 (5%)]\tAll Loss: 1.1954\tTriple Loss(1): 0.0015\tClassification Loss: 1.1923\r\n",
      "Train Epoch: 39 [10880/209539 (5%)]\tAll Loss: 1.1868\tTriple Loss(1): 0.0000\tClassification Loss: 1.1868\r\n",
      "Train Epoch: 39 [11520/209539 (5%)]\tAll Loss: 1.1275\tTriple Loss(1): 0.0046\tClassification Loss: 1.1182\r\n",
      "Train Epoch: 39 [12160/209539 (6%)]\tAll Loss: 1.5538\tTriple Loss(0): 0.2723\tClassification Loss: 1.0091\r\n",
      "Train Epoch: 39 [12800/209539 (6%)]\tAll Loss: 0.8294\tTriple Loss(1): 0.0240\tClassification Loss: 0.7815\r\n",
      "Train Epoch: 39 [13440/209539 (6%)]\tAll Loss: 1.5158\tTriple Loss(0): 0.2178\tClassification Loss: 1.0802\r\n",
      "Train Epoch: 39 [14080/209539 (7%)]\tAll Loss: 1.3333\tTriple Loss(1): 0.0789\tClassification Loss: 1.1755\r\n",
      "Train Epoch: 39 [14720/209539 (7%)]\tAll Loss: 1.2628\tTriple Loss(1): 0.0061\tClassification Loss: 1.2505\r\n",
      "Train Epoch: 39 [15360/209539 (7%)]\tAll Loss: 1.1497\tTriple Loss(1): 0.0000\tClassification Loss: 1.1497\r\n",
      "Train Epoch: 39 [16000/209539 (8%)]\tAll Loss: 1.5570\tTriple Loss(1): 0.0382\tClassification Loss: 1.4806\r\n",
      "Train Epoch: 39 [16640/209539 (8%)]\tAll Loss: 1.6038\tTriple Loss(1): 0.0454\tClassification Loss: 1.5130\r\n",
      "Train Epoch: 39 [17280/209539 (8%)]\tAll Loss: 1.2244\tTriple Loss(1): 0.0385\tClassification Loss: 1.1474\r\n",
      "Train Epoch: 39 [17920/209539 (9%)]\tAll Loss: 1.1788\tTriple Loss(1): 0.0000\tClassification Loss: 1.1788\r\n",
      "Train Epoch: 39 [18560/209539 (9%)]\tAll Loss: 1.4916\tTriple Loss(0): 0.1889\tClassification Loss: 1.1138\r\n",
      "Train Epoch: 39 [19200/209539 (9%)]\tAll Loss: 1.1042\tTriple Loss(1): 0.0502\tClassification Loss: 1.0037\r\n",
      "Train Epoch: 39 [19840/209539 (9%)]\tAll Loss: 1.2084\tTriple Loss(1): 0.0363\tClassification Loss: 1.1357\r\n",
      "Train Epoch: 39 [20480/209539 (10%)]\tAll Loss: 1.4828\tTriple Loss(0): 0.2480\tClassification Loss: 0.9868\r\n",
      "Train Epoch: 39 [21120/209539 (10%)]\tAll Loss: 1.5531\tTriple Loss(1): 0.0802\tClassification Loss: 1.3927\r\n",
      "Train Epoch: 39 [21760/209539 (10%)]\tAll Loss: 1.1659\tTriple Loss(1): 0.0095\tClassification Loss: 1.1469\r\n",
      "Train Epoch: 39 [22400/209539 (11%)]\tAll Loss: 1.0317\tTriple Loss(1): 0.0243\tClassification Loss: 0.9831\r\n",
      "Train Epoch: 39 [23040/209539 (11%)]\tAll Loss: 1.7144\tTriple Loss(0): 0.3026\tClassification Loss: 1.1092\r\n",
      "Train Epoch: 39 [23680/209539 (11%)]\tAll Loss: 0.8960\tTriple Loss(1): 0.0115\tClassification Loss: 0.8730\r\n",
      "Train Epoch: 39 [24320/209539 (12%)]\tAll Loss: 1.2531\tTriple Loss(1): 0.0349\tClassification Loss: 1.1832\r\n",
      "Train Epoch: 39 [24960/209539 (12%)]\tAll Loss: 1.0880\tTriple Loss(1): 0.0491\tClassification Loss: 0.9898\r\n",
      "Train Epoch: 39 [25600/209539 (12%)]\tAll Loss: 1.2039\tTriple Loss(1): 0.0105\tClassification Loss: 1.1829\r\n",
      "Train Epoch: 39 [26240/209539 (13%)]\tAll Loss: 1.1197\tTriple Loss(1): 0.0292\tClassification Loss: 1.0613\r\n",
      "Train Epoch: 39 [26880/209539 (13%)]\tAll Loss: 1.8195\tTriple Loss(0): 0.3160\tClassification Loss: 1.1876\r\n",
      "Train Epoch: 39 [27520/209539 (13%)]\tAll Loss: 1.0552\tTriple Loss(1): 0.0000\tClassification Loss: 1.0552\r\n",
      "Train Epoch: 39 [28160/209539 (13%)]\tAll Loss: 1.2726\tTriple Loss(1): 0.0270\tClassification Loss: 1.2185\r\n",
      "Train Epoch: 39 [28800/209539 (14%)]\tAll Loss: 1.5126\tTriple Loss(1): 0.0462\tClassification Loss: 1.4202\r\n",
      "Train Epoch: 39 [29440/209539 (14%)]\tAll Loss: 1.1659\tTriple Loss(1): 0.0086\tClassification Loss: 1.1486\r\n",
      "Train Epoch: 39 [30080/209539 (14%)]\tAll Loss: 1.0693\tTriple Loss(1): 0.0275\tClassification Loss: 1.0144\r\n",
      "Train Epoch: 39 [30720/209539 (15%)]\tAll Loss: 1.1765\tTriple Loss(1): 0.0000\tClassification Loss: 1.1765\r\n",
      "Train Epoch: 39 [31360/209539 (15%)]\tAll Loss: 1.0587\tTriple Loss(1): 0.0003\tClassification Loss: 1.0581\r\n",
      "Train Epoch: 39 [32000/209539 (15%)]\tAll Loss: 1.3173\tTriple Loss(1): 0.0204\tClassification Loss: 1.2765\r\n",
      "Train Epoch: 39 [32640/209539 (16%)]\tAll Loss: 1.3549\tTriple Loss(1): 0.0005\tClassification Loss: 1.3538\r\n",
      "Train Epoch: 39 [33280/209539 (16%)]\tAll Loss: 1.1216\tTriple Loss(1): 0.0034\tClassification Loss: 1.1147\r\n",
      "Train Epoch: 39 [33920/209539 (16%)]\tAll Loss: 1.1860\tTriple Loss(1): 0.0000\tClassification Loss: 1.1860\r\n",
      "Train Epoch: 39 [34560/209539 (16%)]\tAll Loss: 1.4436\tTriple Loss(0): 0.1640\tClassification Loss: 1.1156\r\n",
      "Train Epoch: 39 [35200/209539 (17%)]\tAll Loss: 1.9978\tTriple Loss(0): 0.3983\tClassification Loss: 1.2013\r\n",
      "Train Epoch: 39 [35840/209539 (17%)]\tAll Loss: 1.0215\tTriple Loss(1): 0.0457\tClassification Loss: 0.9301\r\n",
      "Train Epoch: 39 [36480/209539 (17%)]\tAll Loss: 0.9348\tTriple Loss(1): 0.0163\tClassification Loss: 0.9022\r\n",
      "Train Epoch: 39 [37120/209539 (18%)]\tAll Loss: 1.3846\tTriple Loss(1): 0.0000\tClassification Loss: 1.3846\r\n",
      "Train Epoch: 39 [37760/209539 (18%)]\tAll Loss: 1.3363\tTriple Loss(1): 0.0063\tClassification Loss: 1.3238\r\n",
      "Train Epoch: 39 [38400/209539 (18%)]\tAll Loss: 1.2074\tTriple Loss(1): 0.0031\tClassification Loss: 1.2013\r\n",
      "Train Epoch: 39 [39040/209539 (19%)]\tAll Loss: 0.7609\tTriple Loss(1): 0.0019\tClassification Loss: 0.7570\r\n",
      "Train Epoch: 39 [39680/209539 (19%)]\tAll Loss: 1.2055\tTriple Loss(1): 0.0122\tClassification Loss: 1.1811\r\n",
      "Train Epoch: 39 [40320/209539 (19%)]\tAll Loss: 1.1499\tTriple Loss(1): 0.0463\tClassification Loss: 1.0574\r\n",
      "Train Epoch: 39 [40960/209539 (20%)]\tAll Loss: 1.2045\tTriple Loss(1): 0.0233\tClassification Loss: 1.1578\r\n",
      "Train Epoch: 39 [41600/209539 (20%)]\tAll Loss: 0.8617\tTriple Loss(1): 0.0002\tClassification Loss: 0.8613\r\n",
      "Train Epoch: 39 [42240/209539 (20%)]\tAll Loss: 1.1962\tTriple Loss(1): 0.0269\tClassification Loss: 1.1423\r\n",
      "Train Epoch: 39 [42880/209539 (20%)]\tAll Loss: 0.9555\tTriple Loss(1): 0.0380\tClassification Loss: 0.8795\r\n",
      "Train Epoch: 39 [43520/209539 (21%)]\tAll Loss: 1.3505\tTriple Loss(1): 0.0158\tClassification Loss: 1.3188\r\n",
      "Train Epoch: 39 [44160/209539 (21%)]\tAll Loss: 1.4180\tTriple Loss(1): 0.0060\tClassification Loss: 1.4060\r\n",
      "Train Epoch: 39 [44800/209539 (21%)]\tAll Loss: 1.4725\tTriple Loss(1): 0.0000\tClassification Loss: 1.4725\r\n",
      "Train Epoch: 39 [45440/209539 (22%)]\tAll Loss: 1.5594\tTriple Loss(1): 0.0137\tClassification Loss: 1.5320\r\n",
      "Train Epoch: 39 [46080/209539 (22%)]\tAll Loss: 1.6906\tTriple Loss(0): 0.2870\tClassification Loss: 1.1166\r\n",
      "Train Epoch: 39 [46720/209539 (22%)]\tAll Loss: 1.5106\tTriple Loss(1): 0.0302\tClassification Loss: 1.4502\r\n",
      "Train Epoch: 39 [47360/209539 (23%)]\tAll Loss: 1.4827\tTriple Loss(0): 0.3334\tClassification Loss: 0.8159\r\n",
      "Train Epoch: 39 [48000/209539 (23%)]\tAll Loss: 1.0841\tTriple Loss(1): 0.0110\tClassification Loss: 1.0622\r\n",
      "Train Epoch: 39 [48640/209539 (23%)]\tAll Loss: 1.8459\tTriple Loss(0): 0.1754\tClassification Loss: 1.4951\r\n",
      "Train Epoch: 39 [49280/209539 (24%)]\tAll Loss: 1.4351\tTriple Loss(0): 0.1641\tClassification Loss: 1.1070\r\n",
      "Train Epoch: 39 [49920/209539 (24%)]\tAll Loss: 1.7361\tTriple Loss(0): 0.2396\tClassification Loss: 1.2570\r\n",
      "Train Epoch: 39 [50560/209539 (24%)]\tAll Loss: 1.1610\tTriple Loss(1): 0.0132\tClassification Loss: 1.1346\r\n",
      "Train Epoch: 39 [51200/209539 (24%)]\tAll Loss: 1.3775\tTriple Loss(1): 0.0248\tClassification Loss: 1.3279\r\n",
      "Train Epoch: 39 [51840/209539 (25%)]\tAll Loss: 1.4104\tTriple Loss(0): 0.1264\tClassification Loss: 1.1575\r\n",
      "Train Epoch: 39 [52480/209539 (25%)]\tAll Loss: 1.1466\tTriple Loss(1): 0.0421\tClassification Loss: 1.0625\r\n",
      "Train Epoch: 39 [53120/209539 (25%)]\tAll Loss: 1.3080\tTriple Loss(1): 0.0531\tClassification Loss: 1.2018\r\n",
      "Train Epoch: 39 [53760/209539 (26%)]\tAll Loss: 1.3882\tTriple Loss(1): 0.0163\tClassification Loss: 1.3555\r\n",
      "Train Epoch: 39 [54400/209539 (26%)]\tAll Loss: 1.6920\tTriple Loss(1): 0.0055\tClassification Loss: 1.6811\r\n",
      "Train Epoch: 39 [55040/209539 (26%)]\tAll Loss: 1.0386\tTriple Loss(1): 0.0300\tClassification Loss: 0.9785\r\n",
      "Train Epoch: 39 [55680/209539 (27%)]\tAll Loss: 1.5667\tTriple Loss(1): 0.0599\tClassification Loss: 1.4470\r\n",
      "Train Epoch: 39 [56320/209539 (27%)]\tAll Loss: 0.8361\tTriple Loss(1): 0.0018\tClassification Loss: 0.8325\r\n",
      "Train Epoch: 39 [56960/209539 (27%)]\tAll Loss: 1.2005\tTriple Loss(1): 0.0319\tClassification Loss: 1.1367\r\n",
      "Train Epoch: 39 [57600/209539 (27%)]\tAll Loss: 0.9836\tTriple Loss(1): 0.0236\tClassification Loss: 0.9365\r\n",
      "Train Epoch: 39 [58240/209539 (28%)]\tAll Loss: 1.4323\tTriple Loss(0): 0.2100\tClassification Loss: 1.0122\r\n",
      "Train Epoch: 39 [58880/209539 (28%)]\tAll Loss: 1.2720\tTriple Loss(1): 0.0016\tClassification Loss: 1.2689\r\n",
      "Train Epoch: 39 [59520/209539 (28%)]\tAll Loss: 1.2472\tTriple Loss(1): 0.0113\tClassification Loss: 1.2246\r\n",
      "Train Epoch: 39 [60160/209539 (29%)]\tAll Loss: 1.9494\tTriple Loss(0): 0.3507\tClassification Loss: 1.2480\r\n",
      "Train Epoch: 39 [60800/209539 (29%)]\tAll Loss: 1.3793\tTriple Loss(1): 0.0610\tClassification Loss: 1.2573\r\n",
      "Train Epoch: 39 [61440/209539 (29%)]\tAll Loss: 1.1008\tTriple Loss(1): 0.0007\tClassification Loss: 1.0994\r\n",
      "Train Epoch: 39 [62080/209539 (30%)]\tAll Loss: 1.2942\tTriple Loss(1): 0.0476\tClassification Loss: 1.1990\r\n",
      "Train Epoch: 39 [62720/209539 (30%)]\tAll Loss: 1.2699\tTriple Loss(1): 0.0623\tClassification Loss: 1.1452\r\n",
      "Train Epoch: 39 [63360/209539 (30%)]\tAll Loss: 1.2693\tTriple Loss(1): 0.0429\tClassification Loss: 1.1835\r\n",
      "Train Epoch: 39 [64000/209539 (31%)]\tAll Loss: 1.5801\tTriple Loss(1): 0.0770\tClassification Loss: 1.4260\r\n",
      "Train Epoch: 39 [64640/209539 (31%)]\tAll Loss: 1.7154\tTriple Loss(1): 0.0158\tClassification Loss: 1.6838\r\n",
      "Train Epoch: 39 [65280/209539 (31%)]\tAll Loss: 1.3036\tTriple Loss(1): 0.0064\tClassification Loss: 1.2909\r\n",
      "Train Epoch: 39 [65920/209539 (31%)]\tAll Loss: 1.5315\tTriple Loss(1): 0.0272\tClassification Loss: 1.4771\r\n",
      "Train Epoch: 39 [66560/209539 (32%)]\tAll Loss: 1.0688\tTriple Loss(1): 0.0006\tClassification Loss: 1.0677\r\n",
      "Train Epoch: 39 [67200/209539 (32%)]\tAll Loss: 1.0696\tTriple Loss(1): 0.0182\tClassification Loss: 1.0333\r\n",
      "Train Epoch: 39 [67840/209539 (32%)]\tAll Loss: 1.8700\tTriple Loss(0): 0.3518\tClassification Loss: 1.1664\r\n",
      "Train Epoch: 39 [68480/209539 (33%)]\tAll Loss: 1.3808\tTriple Loss(1): 0.0610\tClassification Loss: 1.2589\r\n",
      "Train Epoch: 39 [69120/209539 (33%)]\tAll Loss: 1.3537\tTriple Loss(1): 0.0027\tClassification Loss: 1.3483\r\n",
      "Train Epoch: 39 [69760/209539 (33%)]\tAll Loss: 1.1843\tTriple Loss(1): 0.0726\tClassification Loss: 1.0391\r\n",
      "Train Epoch: 39 [70400/209539 (34%)]\tAll Loss: 1.1614\tTriple Loss(1): 0.0635\tClassification Loss: 1.0343\r\n",
      "Train Epoch: 39 [71040/209539 (34%)]\tAll Loss: 1.3723\tTriple Loss(1): 0.0098\tClassification Loss: 1.3527\r\n",
      "Train Epoch: 39 [71680/209539 (34%)]\tAll Loss: 1.6127\tTriple Loss(1): 0.0170\tClassification Loss: 1.5787\r\n",
      "Train Epoch: 39 [72320/209539 (35%)]\tAll Loss: 0.9494\tTriple Loss(1): 0.0000\tClassification Loss: 0.9494\r\n",
      "Train Epoch: 39 [72960/209539 (35%)]\tAll Loss: 1.3330\tTriple Loss(1): 0.0358\tClassification Loss: 1.2614\r\n",
      "Train Epoch: 39 [73600/209539 (35%)]\tAll Loss: 1.4929\tTriple Loss(1): 0.0408\tClassification Loss: 1.4113\r\n",
      "Train Epoch: 39 [74240/209539 (35%)]\tAll Loss: 1.8364\tTriple Loss(0): 0.1844\tClassification Loss: 1.4676\r\n",
      "Train Epoch: 39 [74880/209539 (36%)]\tAll Loss: 1.9633\tTriple Loss(0): 0.2120\tClassification Loss: 1.5393\r\n",
      "Train Epoch: 39 [75520/209539 (36%)]\tAll Loss: 1.0145\tTriple Loss(1): 0.0027\tClassification Loss: 1.0092\r\n",
      "Train Epoch: 39 [76160/209539 (36%)]\tAll Loss: 0.9226\tTriple Loss(1): 0.0249\tClassification Loss: 0.8727\r\n",
      "Train Epoch: 39 [76800/209539 (37%)]\tAll Loss: 1.4971\tTriple Loss(0): 0.1878\tClassification Loss: 1.1214\r\n",
      "Train Epoch: 39 [77440/209539 (37%)]\tAll Loss: 1.2130\tTriple Loss(1): 0.0000\tClassification Loss: 1.2130\r\n",
      "Train Epoch: 39 [78080/209539 (37%)]\tAll Loss: 1.6251\tTriple Loss(0): 0.1753\tClassification Loss: 1.2745\r\n",
      "Train Epoch: 39 [78720/209539 (38%)]\tAll Loss: 1.2529\tTriple Loss(1): 0.0030\tClassification Loss: 1.2468\r\n",
      "Train Epoch: 39 [79360/209539 (38%)]\tAll Loss: 1.2043\tTriple Loss(1): 0.0561\tClassification Loss: 1.0920\r\n",
      "Train Epoch: 39 [80000/209539 (38%)]\tAll Loss: 1.0730\tTriple Loss(1): 0.0000\tClassification Loss: 1.0730\r\n",
      "Train Epoch: 39 [80640/209539 (38%)]\tAll Loss: 1.2569\tTriple Loss(1): 0.0179\tClassification Loss: 1.2211\r\n",
      "Train Epoch: 39 [81280/209539 (39%)]\tAll Loss: 1.3817\tTriple Loss(1): 0.0454\tClassification Loss: 1.2908\r\n",
      "Train Epoch: 39 [81920/209539 (39%)]\tAll Loss: 1.2333\tTriple Loss(1): 0.0075\tClassification Loss: 1.2183\r\n",
      "Train Epoch: 39 [82560/209539 (39%)]\tAll Loss: 1.4184\tTriple Loss(0): 0.1530\tClassification Loss: 1.1125\r\n",
      "Train Epoch: 39 [83200/209539 (40%)]\tAll Loss: 1.1752\tTriple Loss(1): 0.0000\tClassification Loss: 1.1752\r\n",
      "Train Epoch: 39 [83840/209539 (40%)]\tAll Loss: 1.6984\tTriple Loss(0): 0.3003\tClassification Loss: 1.0979\r\n",
      "Train Epoch: 39 [84480/209539 (40%)]\tAll Loss: 1.2443\tTriple Loss(1): 0.0614\tClassification Loss: 1.1215\r\n",
      "Train Epoch: 39 [85120/209539 (41%)]\tAll Loss: 2.4621\tTriple Loss(0): 0.3598\tClassification Loss: 1.7425\r\n",
      "Train Epoch: 39 [85760/209539 (41%)]\tAll Loss: 1.2608\tTriple Loss(1): 0.0253\tClassification Loss: 1.2101\r\n",
      "Train Epoch: 39 [86400/209539 (41%)]\tAll Loss: 1.1820\tTriple Loss(1): 0.0692\tClassification Loss: 1.0436\r\n",
      "Train Epoch: 39 [87040/209539 (42%)]\tAll Loss: 1.2898\tTriple Loss(1): 0.0485\tClassification Loss: 1.1929\r\n",
      "Train Epoch: 39 [87680/209539 (42%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0011\tClassification Loss: 1.2086\r\n",
      "Train Epoch: 39 [88320/209539 (42%)]\tAll Loss: 1.1918\tTriple Loss(1): 0.0193\tClassification Loss: 1.1532\r\n",
      "Train Epoch: 39 [88960/209539 (42%)]\tAll Loss: 1.2527\tTriple Loss(1): 0.0006\tClassification Loss: 1.2515\r\n",
      "Train Epoch: 39 [89600/209539 (43%)]\tAll Loss: 1.3000\tTriple Loss(1): 0.0000\tClassification Loss: 1.3000\r\n",
      "Train Epoch: 39 [90240/209539 (43%)]\tAll Loss: 1.0858\tTriple Loss(1): 0.0101\tClassification Loss: 1.0655\r\n",
      "Train Epoch: 39 [90880/209539 (43%)]\tAll Loss: 2.2023\tTriple Loss(0): 0.3615\tClassification Loss: 1.4794\r\n",
      "Train Epoch: 39 [91520/209539 (44%)]\tAll Loss: 1.2347\tTriple Loss(1): 0.0289\tClassification Loss: 1.1769\r\n",
      "Train Epoch: 39 [92160/209539 (44%)]\tAll Loss: 1.0892\tTriple Loss(1): 0.0683\tClassification Loss: 0.9526\r\n",
      "Train Epoch: 39 [92800/209539 (44%)]\tAll Loss: 1.1295\tTriple Loss(1): 0.0654\tClassification Loss: 0.9988\r\n",
      "Train Epoch: 39 [93440/209539 (45%)]\tAll Loss: 1.6408\tTriple Loss(0): 0.3053\tClassification Loss: 1.0301\r\n",
      "Train Epoch: 39 [94080/209539 (45%)]\tAll Loss: 1.2040\tTriple Loss(1): 0.0673\tClassification Loss: 1.0694\r\n",
      "Train Epoch: 39 [94720/209539 (45%)]\tAll Loss: 1.3337\tTriple Loss(1): 0.0618\tClassification Loss: 1.2100\r\n",
      "Train Epoch: 39 [95360/209539 (46%)]\tAll Loss: 1.3348\tTriple Loss(1): 0.0573\tClassification Loss: 1.2202\r\n",
      "Train Epoch: 39 [96000/209539 (46%)]\tAll Loss: 1.2225\tTriple Loss(1): 0.0171\tClassification Loss: 1.1882\r\n",
      "Train Epoch: 39 [96640/209539 (46%)]\tAll Loss: 1.4857\tTriple Loss(1): 0.0000\tClassification Loss: 1.4857\r\n",
      "Train Epoch: 39 [97280/209539 (46%)]\tAll Loss: 1.2587\tTriple Loss(1): 0.0329\tClassification Loss: 1.1928\r\n",
      "Train Epoch: 39 [97920/209539 (47%)]\tAll Loss: 1.4627\tTriple Loss(1): 0.0815\tClassification Loss: 1.2996\r\n",
      "Train Epoch: 39 [98560/209539 (47%)]\tAll Loss: 1.5364\tTriple Loss(1): 0.0492\tClassification Loss: 1.4381\r\n",
      "Train Epoch: 39 [99200/209539 (47%)]\tAll Loss: 1.0180\tTriple Loss(1): 0.0056\tClassification Loss: 1.0067\r\n",
      "Train Epoch: 39 [99840/209539 (48%)]\tAll Loss: 1.0678\tTriple Loss(1): 0.0000\tClassification Loss: 1.0678\r\n",
      "Train Epoch: 39 [100480/209539 (48%)]\tAll Loss: 1.2446\tTriple Loss(1): 0.0325\tClassification Loss: 1.1796\r\n",
      "Train Epoch: 39 [101120/209539 (48%)]\tAll Loss: 1.0990\tTriple Loss(1): 0.0215\tClassification Loss: 1.0561\r\n",
      "Train Epoch: 39 [101760/209539 (49%)]\tAll Loss: 0.9999\tTriple Loss(1): 0.0000\tClassification Loss: 0.9999\r\n",
      "Train Epoch: 39 [102400/209539 (49%)]\tAll Loss: 1.0430\tTriple Loss(1): 0.0112\tClassification Loss: 1.0206\r\n",
      "Train Epoch: 39 [103040/209539 (49%)]\tAll Loss: 1.4128\tTriple Loss(0): 0.1508\tClassification Loss: 1.1111\r\n",
      "Train Epoch: 39 [103680/209539 (49%)]\tAll Loss: 1.7142\tTriple Loss(0): 0.2333\tClassification Loss: 1.2477\r\n",
      "Train Epoch: 39 [104320/209539 (50%)]\tAll Loss: 0.9142\tTriple Loss(1): 0.0075\tClassification Loss: 0.8992\r\n",
      "Train Epoch: 39 [104960/209539 (50%)]\tAll Loss: 1.8724\tTriple Loss(0): 0.3065\tClassification Loss: 1.2595\r\n",
      "Train Epoch: 39 [105600/209539 (50%)]\tAll Loss: 1.1412\tTriple Loss(1): 0.0000\tClassification Loss: 1.1412\r\n",
      "Train Epoch: 39 [106240/209539 (51%)]\tAll Loss: 1.1359\tTriple Loss(1): 0.0000\tClassification Loss: 1.1359\r\n",
      "Train Epoch: 39 [106880/209539 (51%)]\tAll Loss: 1.1498\tTriple Loss(1): 0.0180\tClassification Loss: 1.1138\r\n",
      "Train Epoch: 39 [107520/209539 (51%)]\tAll Loss: 1.1869\tTriple Loss(1): 0.0000\tClassification Loss: 1.1869\r\n",
      "Train Epoch: 39 [108160/209539 (52%)]\tAll Loss: 1.1686\tTriple Loss(1): 0.0000\tClassification Loss: 1.1686\r\n",
      "Train Epoch: 39 [108800/209539 (52%)]\tAll Loss: 1.0954\tTriple Loss(1): 0.0544\tClassification Loss: 0.9866\r\n",
      "Train Epoch: 39 [109440/209539 (52%)]\tAll Loss: 1.5732\tTriple Loss(1): 0.0137\tClassification Loss: 1.5458\r\n",
      "Train Epoch: 39 [110080/209539 (53%)]\tAll Loss: 1.3899\tTriple Loss(1): 0.0301\tClassification Loss: 1.3296\r\n",
      "Train Epoch: 39 [110720/209539 (53%)]\tAll Loss: 1.7819\tTriple Loss(0): 0.2407\tClassification Loss: 1.3004\r\n",
      "Train Epoch: 39 [111360/209539 (53%)]\tAll Loss: 1.2669\tTriple Loss(0): 0.1879\tClassification Loss: 0.8911\r\n",
      "Train Epoch: 39 [112000/209539 (53%)]\tAll Loss: 1.7065\tTriple Loss(0): 0.3061\tClassification Loss: 1.0944\r\n",
      "Train Epoch: 39 [112640/209539 (54%)]\tAll Loss: 0.9654\tTriple Loss(1): 0.0115\tClassification Loss: 0.9423\r\n",
      "Train Epoch: 39 [113280/209539 (54%)]\tAll Loss: 0.9700\tTriple Loss(1): 0.0000\tClassification Loss: 0.9700\r\n",
      "Train Epoch: 39 [113920/209539 (54%)]\tAll Loss: 1.7561\tTriple Loss(0): 0.2283\tClassification Loss: 1.2994\r\n",
      "Train Epoch: 39 [114560/209539 (55%)]\tAll Loss: 1.2187\tTriple Loss(1): 0.0000\tClassification Loss: 1.2187\r\n",
      "Train Epoch: 39 [115200/209539 (55%)]\tAll Loss: 1.3422\tTriple Loss(1): 0.0000\tClassification Loss: 1.3422\r\n",
      "Train Epoch: 39 [115840/209539 (55%)]\tAll Loss: 1.3530\tTriple Loss(1): 0.0109\tClassification Loss: 1.3312\r\n",
      "Train Epoch: 39 [116480/209539 (56%)]\tAll Loss: 1.1736\tTriple Loss(1): 0.0081\tClassification Loss: 1.1573\r\n",
      "Train Epoch: 39 [117120/209539 (56%)]\tAll Loss: 1.1888\tTriple Loss(1): 0.0000\tClassification Loss: 1.1888\r\n",
      "Train Epoch: 39 [117760/209539 (56%)]\tAll Loss: 1.2857\tTriple Loss(1): 0.0010\tClassification Loss: 1.2837\r\n",
      "Train Epoch: 39 [118400/209539 (57%)]\tAll Loss: 0.9361\tTriple Loss(1): 0.0022\tClassification Loss: 0.9318\r\n",
      "Train Epoch: 39 [119040/209539 (57%)]\tAll Loss: 1.5269\tTriple Loss(1): 0.0500\tClassification Loss: 1.4269\r\n",
      "Train Epoch: 39 [119680/209539 (57%)]\tAll Loss: 1.2650\tTriple Loss(1): 0.0890\tClassification Loss: 1.0869\r\n",
      "Train Epoch: 39 [120320/209539 (57%)]\tAll Loss: 1.3720\tTriple Loss(1): 0.0824\tClassification Loss: 1.2071\r\n",
      "Train Epoch: 39 [120960/209539 (58%)]\tAll Loss: 1.0400\tTriple Loss(1): 0.0000\tClassification Loss: 1.0399\r\n",
      "Train Epoch: 39 [121600/209539 (58%)]\tAll Loss: 1.0532\tTriple Loss(1): 0.0000\tClassification Loss: 1.0532\r\n",
      "Train Epoch: 39 [122240/209539 (58%)]\tAll Loss: 1.0264\tTriple Loss(1): 0.0418\tClassification Loss: 0.9428\r\n",
      "Train Epoch: 39 [122880/209539 (59%)]\tAll Loss: 1.1123\tTriple Loss(1): 0.0000\tClassification Loss: 1.1123\r\n",
      "Train Epoch: 39 [123520/209539 (59%)]\tAll Loss: 1.7628\tTriple Loss(0): 0.2670\tClassification Loss: 1.2288\r\n",
      "Train Epoch: 39 [124160/209539 (59%)]\tAll Loss: 1.1519\tTriple Loss(1): 0.0234\tClassification Loss: 1.1050\r\n",
      "Train Epoch: 39 [124800/209539 (60%)]\tAll Loss: 1.4010\tTriple Loss(0): 0.2618\tClassification Loss: 0.8774\r\n",
      "Train Epoch: 39 [125440/209539 (60%)]\tAll Loss: 1.6102\tTriple Loss(0): 0.2219\tClassification Loss: 1.1663\r\n",
      "Train Epoch: 39 [126080/209539 (60%)]\tAll Loss: 1.7145\tTriple Loss(0): 0.2449\tClassification Loss: 1.2247\r\n",
      "Train Epoch: 39 [126720/209539 (60%)]\tAll Loss: 1.2886\tTriple Loss(1): 0.0251\tClassification Loss: 1.2384\r\n",
      "Train Epoch: 39 [127360/209539 (61%)]\tAll Loss: 1.4135\tTriple Loss(1): 0.0753\tClassification Loss: 1.2629\r\n",
      "Train Epoch: 39 [128000/209539 (61%)]\tAll Loss: 1.8666\tTriple Loss(0): 0.2526\tClassification Loss: 1.3614\r\n",
      "Train Epoch: 39 [128640/209539 (61%)]\tAll Loss: 0.8384\tTriple Loss(1): 0.0030\tClassification Loss: 0.8323\r\n",
      "Train Epoch: 39 [129280/209539 (62%)]\tAll Loss: 1.2046\tTriple Loss(1): 0.0370\tClassification Loss: 1.1305\r\n",
      "Train Epoch: 39 [129920/209539 (62%)]\tAll Loss: 1.0317\tTriple Loss(1): 0.0000\tClassification Loss: 1.0317\r\n",
      "Train Epoch: 39 [130560/209539 (62%)]\tAll Loss: 1.0050\tTriple Loss(1): 0.0270\tClassification Loss: 0.9510\r\n",
      "Train Epoch: 39 [131200/209539 (63%)]\tAll Loss: 0.9743\tTriple Loss(1): 0.0084\tClassification Loss: 0.9575\r\n",
      "Train Epoch: 39 [131840/209539 (63%)]\tAll Loss: 1.1717\tTriple Loss(1): 0.0483\tClassification Loss: 1.0752\r\n",
      "Train Epoch: 39 [132480/209539 (63%)]\tAll Loss: 1.0191\tTriple Loss(1): 0.0318\tClassification Loss: 0.9554\r\n",
      "Train Epoch: 39 [133120/209539 (64%)]\tAll Loss: 0.8930\tTriple Loss(1): 0.0000\tClassification Loss: 0.8930\r\n",
      "Train Epoch: 39 [133760/209539 (64%)]\tAll Loss: 1.2267\tTriple Loss(1): 0.0069\tClassification Loss: 1.2129\r\n",
      "Train Epoch: 39 [134400/209539 (64%)]\tAll Loss: 1.5138\tTriple Loss(0): 0.3216\tClassification Loss: 0.8705\r\n",
      "Train Epoch: 39 [135040/209539 (64%)]\tAll Loss: 1.3630\tTriple Loss(1): 0.0310\tClassification Loss: 1.3011\r\n",
      "Train Epoch: 39 [135680/209539 (65%)]\tAll Loss: 1.4235\tTriple Loss(1): 0.0271\tClassification Loss: 1.3693\r\n",
      "Train Epoch: 39 [136320/209539 (65%)]\tAll Loss: 1.1011\tTriple Loss(1): 0.0053\tClassification Loss: 1.0905\r\n",
      "Train Epoch: 39 [136960/209539 (65%)]\tAll Loss: 1.6636\tTriple Loss(0): 0.2668\tClassification Loss: 1.1300\r\n",
      "Train Epoch: 39 [137600/209539 (66%)]\tAll Loss: 1.6223\tTriple Loss(0): 0.2728\tClassification Loss: 1.0766\r\n",
      "Train Epoch: 39 [138240/209539 (66%)]\tAll Loss: 1.4299\tTriple Loss(1): 0.0120\tClassification Loss: 1.4058\r\n",
      "Train Epoch: 39 [138880/209539 (66%)]\tAll Loss: 1.2170\tTriple Loss(1): 0.0316\tClassification Loss: 1.1537\r\n",
      "Train Epoch: 39 [139520/209539 (67%)]\tAll Loss: 1.0453\tTriple Loss(1): 0.0447\tClassification Loss: 0.9560\r\n",
      "Train Epoch: 39 [140160/209539 (67%)]\tAll Loss: 1.2047\tTriple Loss(1): 0.0059\tClassification Loss: 1.1930\r\n",
      "Train Epoch: 39 [140800/209539 (67%)]\tAll Loss: 1.7177\tTriple Loss(0): 0.2526\tClassification Loss: 1.2125\r\n",
      "Train Epoch: 39 [141440/209539 (68%)]\tAll Loss: 1.5304\tTriple Loss(0): 0.1945\tClassification Loss: 1.1414\r\n",
      "Train Epoch: 39 [142080/209539 (68%)]\tAll Loss: 1.2779\tTriple Loss(1): 0.0438\tClassification Loss: 1.1902\r\n",
      "Train Epoch: 39 [142720/209539 (68%)]\tAll Loss: 1.4423\tTriple Loss(1): 0.0000\tClassification Loss: 1.4423\r\n",
      "Train Epoch: 39 [143360/209539 (68%)]\tAll Loss: 0.9137\tTriple Loss(1): 0.0000\tClassification Loss: 0.9137\r\n",
      "Train Epoch: 39 [144000/209539 (69%)]\tAll Loss: 1.2307\tTriple Loss(1): 0.0493\tClassification Loss: 1.1322\r\n",
      "Train Epoch: 39 [144640/209539 (69%)]\tAll Loss: 1.0637\tTriple Loss(1): 0.0112\tClassification Loss: 1.0413\r\n",
      "Train Epoch: 39 [145280/209539 (69%)]\tAll Loss: 1.2251\tTriple Loss(1): 0.0280\tClassification Loss: 1.1691\r\n",
      "Train Epoch: 39 [145920/209539 (70%)]\tAll Loss: 1.2660\tTriple Loss(1): 0.0452\tClassification Loss: 1.1756\r\n",
      "Train Epoch: 39 [146560/209539 (70%)]\tAll Loss: 1.0153\tTriple Loss(1): 0.0731\tClassification Loss: 0.8690\r\n",
      "Train Epoch: 39 [147200/209539 (70%)]\tAll Loss: 1.0998\tTriple Loss(1): 0.0099\tClassification Loss: 1.0800\r\n",
      "Train Epoch: 39 [147840/209539 (71%)]\tAll Loss: 0.9490\tTriple Loss(1): 0.0546\tClassification Loss: 0.8398\r\n",
      "Train Epoch: 39 [148480/209539 (71%)]\tAll Loss: 0.9192\tTriple Loss(1): 0.0000\tClassification Loss: 0.9192\r\n",
      "Train Epoch: 39 [149120/209539 (71%)]\tAll Loss: 1.1802\tTriple Loss(1): 0.0152\tClassification Loss: 1.1498\r\n",
      "Train Epoch: 39 [149760/209539 (71%)]\tAll Loss: 1.0360\tTriple Loss(1): 0.0330\tClassification Loss: 0.9700\r\n",
      "Train Epoch: 39 [150400/209539 (72%)]\tAll Loss: 1.2999\tTriple Loss(1): 0.0154\tClassification Loss: 1.2691\r\n",
      "Train Epoch: 39 [151040/209539 (72%)]\tAll Loss: 1.0701\tTriple Loss(1): 0.0570\tClassification Loss: 0.9561\r\n",
      "Train Epoch: 39 [151680/209539 (72%)]\tAll Loss: 1.3459\tTriple Loss(1): 0.0000\tClassification Loss: 1.3459\r\n",
      "Train Epoch: 39 [152320/209539 (73%)]\tAll Loss: 1.6762\tTriple Loss(0): 0.3343\tClassification Loss: 1.0076\r\n",
      "Train Epoch: 39 [152960/209539 (73%)]\tAll Loss: 1.5385\tTriple Loss(0): 0.2741\tClassification Loss: 0.9904\r\n",
      "Train Epoch: 39 [153600/209539 (73%)]\tAll Loss: 1.2947\tTriple Loss(1): 0.0767\tClassification Loss: 1.1414\r\n",
      "Train Epoch: 39 [154240/209539 (74%)]\tAll Loss: 0.9217\tTriple Loss(1): 0.0000\tClassification Loss: 0.9217\r\n",
      "Train Epoch: 39 [154880/209539 (74%)]\tAll Loss: 1.1216\tTriple Loss(1): 0.0117\tClassification Loss: 1.0982\r\n",
      "Train Epoch: 39 [155520/209539 (74%)]\tAll Loss: 1.7175\tTriple Loss(0): 0.3214\tClassification Loss: 1.0746\r\n",
      "Train Epoch: 39 [156160/209539 (75%)]\tAll Loss: 2.0590\tTriple Loss(0): 0.4687\tClassification Loss: 1.1216\r\n",
      "Train Epoch: 39 [156800/209539 (75%)]\tAll Loss: 1.4324\tTriple Loss(1): 0.0155\tClassification Loss: 1.4015\r\n",
      "Train Epoch: 39 [157440/209539 (75%)]\tAll Loss: 1.8759\tTriple Loss(0): 0.2484\tClassification Loss: 1.3791\r\n",
      "Train Epoch: 39 [158080/209539 (75%)]\tAll Loss: 1.5829\tTriple Loss(0): 0.2674\tClassification Loss: 1.0480\r\n",
      "Train Epoch: 39 [158720/209539 (76%)]\tAll Loss: 0.9797\tTriple Loss(1): 0.0008\tClassification Loss: 0.9781\r\n",
      "Train Epoch: 39 [159360/209539 (76%)]\tAll Loss: 1.0444\tTriple Loss(1): 0.0260\tClassification Loss: 0.9924\r\n",
      "Train Epoch: 39 [160000/209539 (76%)]\tAll Loss: 1.8686\tTriple Loss(0): 0.2381\tClassification Loss: 1.3924\r\n",
      "Train Epoch: 39 [160640/209539 (77%)]\tAll Loss: 1.0171\tTriple Loss(1): 0.0000\tClassification Loss: 1.0171\r\n",
      "Train Epoch: 39 [161280/209539 (77%)]\tAll Loss: 1.1294\tTriple Loss(1): 0.0702\tClassification Loss: 0.9889\r\n",
      "Train Epoch: 39 [161920/209539 (77%)]\tAll Loss: 1.7279\tTriple Loss(0): 0.3196\tClassification Loss: 1.0887\r\n",
      "Train Epoch: 39 [162560/209539 (78%)]\tAll Loss: 1.6579\tTriple Loss(0): 0.2967\tClassification Loss: 1.0645\r\n",
      "Train Epoch: 39 [163200/209539 (78%)]\tAll Loss: 1.1147\tTriple Loss(1): 0.0465\tClassification Loss: 1.0218\r\n",
      "Train Epoch: 39 [163840/209539 (78%)]\tAll Loss: 1.2386\tTriple Loss(0): 0.1021\tClassification Loss: 1.0344\r\n",
      "Train Epoch: 39 [164480/209539 (78%)]\tAll Loss: 1.1418\tTriple Loss(1): 0.0000\tClassification Loss: 1.1418\r\n",
      "Train Epoch: 39 [165120/209539 (79%)]\tAll Loss: 0.9164\tTriple Loss(1): 0.0061\tClassification Loss: 0.9042\r\n",
      "Train Epoch: 39 [165760/209539 (79%)]\tAll Loss: 1.7995\tTriple Loss(0): 0.2908\tClassification Loss: 1.2180\r\n",
      "Train Epoch: 39 [166400/209539 (79%)]\tAll Loss: 1.2027\tTriple Loss(1): 0.0147\tClassification Loss: 1.1733\r\n",
      "Train Epoch: 39 [167040/209539 (80%)]\tAll Loss: 1.3711\tTriple Loss(1): 0.0065\tClassification Loss: 1.3581\r\n",
      "Train Epoch: 39 [167680/209539 (80%)]\tAll Loss: 1.3776\tTriple Loss(1): 0.0358\tClassification Loss: 1.3061\r\n",
      "Train Epoch: 39 [168320/209539 (80%)]\tAll Loss: 1.2196\tTriple Loss(1): 0.0392\tClassification Loss: 1.1411\r\n",
      "Train Epoch: 39 [168960/209539 (81%)]\tAll Loss: 0.9598\tTriple Loss(1): 0.0000\tClassification Loss: 0.9598\r\n",
      "Train Epoch: 39 [169600/209539 (81%)]\tAll Loss: 1.0026\tTriple Loss(1): 0.0130\tClassification Loss: 0.9766\r\n",
      "Train Epoch: 39 [170240/209539 (81%)]\tAll Loss: 1.1540\tTriple Loss(1): 0.0122\tClassification Loss: 1.1297\r\n",
      "Train Epoch: 39 [170880/209539 (82%)]\tAll Loss: 1.2304\tTriple Loss(1): 0.0010\tClassification Loss: 1.2284\r\n",
      "Train Epoch: 39 [171520/209539 (82%)]\tAll Loss: 1.0108\tTriple Loss(1): 0.0404\tClassification Loss: 0.9300\r\n",
      "Train Epoch: 39 [172160/209539 (82%)]\tAll Loss: 1.7652\tTriple Loss(0): 0.2492\tClassification Loss: 1.2668\r\n",
      "Train Epoch: 39 [172800/209539 (82%)]\tAll Loss: 1.3690\tTriple Loss(1): 0.0584\tClassification Loss: 1.2523\r\n",
      "Train Epoch: 39 [173440/209539 (83%)]\tAll Loss: 1.4201\tTriple Loss(1): 0.0602\tClassification Loss: 1.2997\r\n",
      "Train Epoch: 39 [174080/209539 (83%)]\tAll Loss: 0.9055\tTriple Loss(1): 0.0000\tClassification Loss: 0.9055\r\n",
      "Train Epoch: 39 [174720/209539 (83%)]\tAll Loss: 1.0531\tTriple Loss(1): 0.0000\tClassification Loss: 1.0531\r\n",
      "Train Epoch: 39 [175360/209539 (84%)]\tAll Loss: 1.2207\tTriple Loss(1): 0.0162\tClassification Loss: 1.1883\r\n",
      "Train Epoch: 39 [176000/209539 (84%)]\tAll Loss: 1.1263\tTriple Loss(1): 0.0182\tClassification Loss: 1.0899\r\n",
      "Train Epoch: 39 [176640/209539 (84%)]\tAll Loss: 0.8430\tTriple Loss(1): 0.0000\tClassification Loss: 0.8430\r\n",
      "Train Epoch: 39 [177280/209539 (85%)]\tAll Loss: 1.5756\tTriple Loss(1): 0.0582\tClassification Loss: 1.4593\r\n",
      "Train Epoch: 39 [177920/209539 (85%)]\tAll Loss: 1.3364\tTriple Loss(1): 0.0633\tClassification Loss: 1.2098\r\n",
      "Train Epoch: 39 [178560/209539 (85%)]\tAll Loss: 1.1738\tTriple Loss(1): 0.0273\tClassification Loss: 1.1192\r\n",
      "Train Epoch: 39 [179200/209539 (86%)]\tAll Loss: 1.3937\tTriple Loss(1): 0.0632\tClassification Loss: 1.2673\r\n",
      "Train Epoch: 39 [179840/209539 (86%)]\tAll Loss: 1.2831\tTriple Loss(1): 0.0726\tClassification Loss: 1.1378\r\n",
      "Train Epoch: 39 [180480/209539 (86%)]\tAll Loss: 1.1331\tTriple Loss(1): 0.0003\tClassification Loss: 1.1326\r\n",
      "Train Epoch: 39 [181120/209539 (86%)]\tAll Loss: 1.2898\tTriple Loss(1): 0.0105\tClassification Loss: 1.2687\r\n",
      "Train Epoch: 39 [181760/209539 (87%)]\tAll Loss: 1.6686\tTriple Loss(0): 0.3020\tClassification Loss: 1.0645\r\n",
      "Train Epoch: 39 [182400/209539 (87%)]\tAll Loss: 2.1306\tTriple Loss(0): 0.3409\tClassification Loss: 1.4488\r\n",
      "Train Epoch: 39 [183040/209539 (87%)]\tAll Loss: 1.2415\tTriple Loss(1): 0.0123\tClassification Loss: 1.2169\r\n",
      "Train Epoch: 39 [183680/209539 (88%)]\tAll Loss: 1.0193\tTriple Loss(1): 0.0003\tClassification Loss: 1.0187\r\n",
      "Train Epoch: 39 [184320/209539 (88%)]\tAll Loss: 1.0569\tTriple Loss(1): 0.0216\tClassification Loss: 1.0137\r\n",
      "Train Epoch: 39 [184960/209539 (88%)]\tAll Loss: 1.2127\tTriple Loss(1): 0.0096\tClassification Loss: 1.1934\r\n",
      "Train Epoch: 39 [185600/209539 (89%)]\tAll Loss: 1.1281\tTriple Loss(1): 0.0101\tClassification Loss: 1.1078\r\n",
      "Train Epoch: 39 [186240/209539 (89%)]\tAll Loss: 1.4172\tTriple Loss(1): 0.0925\tClassification Loss: 1.2322\r\n",
      "Train Epoch: 39 [186880/209539 (89%)]\tAll Loss: 1.6904\tTriple Loss(0): 0.2645\tClassification Loss: 1.1614\r\n",
      "Train Epoch: 39 [187520/209539 (89%)]\tAll Loss: 1.3629\tTriple Loss(1): 0.0397\tClassification Loss: 1.2835\r\n",
      "Train Epoch: 39 [188160/209539 (90%)]\tAll Loss: 1.3318\tTriple Loss(1): 0.1020\tClassification Loss: 1.1279\r\n",
      "Train Epoch: 39 [188800/209539 (90%)]\tAll Loss: 1.2107\tTriple Loss(1): 0.0000\tClassification Loss: 1.2107\r\n",
      "Train Epoch: 39 [189440/209539 (90%)]\tAll Loss: 1.0044\tTriple Loss(1): 0.0000\tClassification Loss: 1.0044\r\n",
      "Train Epoch: 39 [190080/209539 (91%)]\tAll Loss: 1.1172\tTriple Loss(1): 0.0433\tClassification Loss: 1.0307\r\n",
      "Train Epoch: 39 [190720/209539 (91%)]\tAll Loss: 1.6060\tTriple Loss(0): 0.1992\tClassification Loss: 1.2076\r\n",
      "Train Epoch: 39 [191360/209539 (91%)]\tAll Loss: 0.9446\tTriple Loss(1): 0.0315\tClassification Loss: 0.8816\r\n",
      "Train Epoch: 39 [192000/209539 (92%)]\tAll Loss: 1.3379\tTriple Loss(1): 0.0000\tClassification Loss: 1.3379\r\n",
      "Train Epoch: 39 [192640/209539 (92%)]\tAll Loss: 1.1678\tTriple Loss(1): 0.0000\tClassification Loss: 1.1678\r\n",
      "Train Epoch: 39 [193280/209539 (92%)]\tAll Loss: 1.0789\tTriple Loss(1): 0.0058\tClassification Loss: 1.0673\r\n",
      "Train Epoch: 39 [193920/209539 (93%)]\tAll Loss: 1.2059\tTriple Loss(1): 0.0180\tClassification Loss: 1.1700\r\n",
      "Train Epoch: 39 [194560/209539 (93%)]\tAll Loss: 1.2378\tTriple Loss(1): 0.0818\tClassification Loss: 1.0741\r\n",
      "Train Epoch: 39 [195200/209539 (93%)]\tAll Loss: 1.2944\tTriple Loss(1): 0.0174\tClassification Loss: 1.2596\r\n",
      "Train Epoch: 39 [195840/209539 (93%)]\tAll Loss: 0.7658\tTriple Loss(1): 0.0210\tClassification Loss: 0.7238\r\n",
      "Train Epoch: 39 [196480/209539 (94%)]\tAll Loss: 1.3139\tTriple Loss(1): 0.0015\tClassification Loss: 1.3109\r\n",
      "Train Epoch: 39 [197120/209539 (94%)]\tAll Loss: 1.6528\tTriple Loss(0): 0.2771\tClassification Loss: 1.0985\r\n",
      "Train Epoch: 39 [197760/209539 (94%)]\tAll Loss: 1.2351\tTriple Loss(1): 0.0240\tClassification Loss: 1.1871\r\n",
      "Train Epoch: 39 [198400/209539 (95%)]\tAll Loss: 1.0976\tTriple Loss(1): 0.0000\tClassification Loss: 1.0976\r\n",
      "Train Epoch: 39 [199040/209539 (95%)]\tAll Loss: 2.0130\tTriple Loss(0): 0.3370\tClassification Loss: 1.3391\r\n",
      "Train Epoch: 39 [199680/209539 (95%)]\tAll Loss: 1.2208\tTriple Loss(1): 0.0265\tClassification Loss: 1.1678\r\n",
      "Train Epoch: 39 [200320/209539 (96%)]\tAll Loss: 2.1011\tTriple Loss(0): 0.4467\tClassification Loss: 1.2076\r\n",
      "Train Epoch: 39 [200960/209539 (96%)]\tAll Loss: 1.1156\tTriple Loss(1): 0.0039\tClassification Loss: 1.1077\r\n",
      "Train Epoch: 39 [201600/209539 (96%)]\tAll Loss: 1.2116\tTriple Loss(0): 0.1352\tClassification Loss: 0.9412\r\n",
      "Train Epoch: 39 [202240/209539 (97%)]\tAll Loss: 1.4260\tTriple Loss(0): 0.1925\tClassification Loss: 1.0410\r\n",
      "Train Epoch: 39 [202880/209539 (97%)]\tAll Loss: 0.7946\tTriple Loss(1): 0.0082\tClassification Loss: 0.7782\r\n",
      "Train Epoch: 39 [203520/209539 (97%)]\tAll Loss: 1.1938\tTriple Loss(1): 0.0000\tClassification Loss: 1.1938\r\n",
      "Train Epoch: 39 [204160/209539 (97%)]\tAll Loss: 1.4351\tTriple Loss(1): 0.0052\tClassification Loss: 1.4247\r\n",
      "Train Epoch: 39 [204800/209539 (98%)]\tAll Loss: 1.5471\tTriple Loss(1): 0.0000\tClassification Loss: 1.5471\r\n",
      "Train Epoch: 39 [205440/209539 (98%)]\tAll Loss: 0.8533\tTriple Loss(1): 0.0076\tClassification Loss: 0.8380\r\n",
      "Train Epoch: 39 [206080/209539 (98%)]\tAll Loss: 1.2897\tTriple Loss(1): 0.0246\tClassification Loss: 1.2405\r\n",
      "Train Epoch: 39 [206720/209539 (99%)]\tAll Loss: 0.9830\tTriple Loss(1): 0.0151\tClassification Loss: 0.9529\r\n",
      "Train Epoch: 39 [207360/209539 (99%)]\tAll Loss: 0.8471\tTriple Loss(1): 0.0000\tClassification Loss: 0.8471\r\n",
      "Train Epoch: 39 [208000/209539 (99%)]\tAll Loss: 1.0799\tTriple Loss(1): 0.0045\tClassification Loss: 1.0708\r\n",
      "Train Epoch: 39 [208640/209539 (100%)]\tAll Loss: 1.2430\tTriple Loss(1): 0.0000\tClassification Loss: 1.2430\r\n",
      "Train Epoch: 39 [209280/209539 (100%)]\tAll Loss: 1.6875\tTriple Loss(0): 0.1420\tClassification Loss: 1.4036\r\n",
      "Model saved to /home/ma02526/ResNet/base/models/with_eastern=False/lr=0.001/39_epochs\r\n",
      "Train Epoch: 40 [0/209539 (0%)]\tAll Loss: 1.5332\tTriple Loss(1): 0.1202\tClassification Loss: 1.2927\r\n",
      "\r\n",
      "Test set: Average loss: 1.0789\r\n",
      "Top 1 Accuracy: 54848/80128 (68%)\r\n",
      "Top 3 Accuracy: 70232/80128 (88%)\r\n",
      "Top 5 Accuracy: 74743/80128 (93%)\r\n",
      " \r\n",
      "Train Epoch: 40 [640/209539 (0%)]\tAll Loss: 1.2477\tTriple Loss(1): 0.0471\tClassification Loss: 1.1535\r\n",
      "Train Epoch: 40 [1280/209539 (1%)]\tAll Loss: 1.0165\tTriple Loss(1): 0.0094\tClassification Loss: 0.9977\r\n",
      "Train Epoch: 40 [1920/209539 (1%)]\tAll Loss: 1.1865\tTriple Loss(1): 0.0011\tClassification Loss: 1.1843\r\n",
      "Train Epoch: 40 [2560/209539 (1%)]\tAll Loss: 1.4357\tTriple Loss(1): 0.0000\tClassification Loss: 1.4357\r\n",
      "Train Epoch: 40 [3200/209539 (2%)]\tAll Loss: 1.2006\tTriple Loss(1): 0.0164\tClassification Loss: 1.1677\r\n",
      "Train Epoch: 40 [3840/209539 (2%)]\tAll Loss: 1.1874\tTriple Loss(1): 0.0418\tClassification Loss: 1.1039\r\n",
      "Train Epoch: 40 [4480/209539 (2%)]\tAll Loss: 1.3000\tTriple Loss(1): 0.0005\tClassification Loss: 1.2990\r\n",
      "Train Epoch: 40 [5120/209539 (2%)]\tAll Loss: 1.1697\tTriple Loss(1): 0.0525\tClassification Loss: 1.0648\r\n",
      "Train Epoch: 40 [5760/209539 (3%)]\tAll Loss: 1.3294\tTriple Loss(1): 0.0000\tClassification Loss: 1.3294\r\n",
      "Train Epoch: 40 [6400/209539 (3%)]\tAll Loss: 0.9401\tTriple Loss(1): 0.0023\tClassification Loss: 0.9354\r\n",
      "Train Epoch: 40 [7040/209539 (3%)]\tAll Loss: 1.7997\tTriple Loss(0): 0.3268\tClassification Loss: 1.1462\r\n",
      "Train Epoch: 40 [7680/209539 (4%)]\tAll Loss: 0.9429\tTriple Loss(1): 0.0445\tClassification Loss: 0.8539\r\n",
      "Train Epoch: 40 [8320/209539 (4%)]\tAll Loss: 1.1088\tTriple Loss(1): 0.0264\tClassification Loss: 1.0560\r\n",
      "Train Epoch: 40 [8960/209539 (4%)]\tAll Loss: 1.1272\tTriple Loss(1): 0.0045\tClassification Loss: 1.1183\r\n",
      "Train Epoch: 40 [9600/209539 (5%)]\tAll Loss: 1.6056\tTriple Loss(1): 0.0769\tClassification Loss: 1.4517\r\n",
      "Train Epoch: 40 [10240/209539 (5%)]\tAll Loss: 1.5693\tTriple Loss(0): 0.3129\tClassification Loss: 0.9435\r\n",
      "Train Epoch: 40 [10880/209539 (5%)]\tAll Loss: 1.2660\tTriple Loss(1): 0.0545\tClassification Loss: 1.1570\r\n",
      "Train Epoch: 40 [11520/209539 (5%)]\tAll Loss: 1.7211\tTriple Loss(0): 0.2135\tClassification Loss: 1.2942\r\n",
      "Train Epoch: 40 [12160/209539 (6%)]\tAll Loss: 1.0010\tTriple Loss(1): 0.0243\tClassification Loss: 0.9524\r\n",
      "Train Epoch: 40 [12800/209539 (6%)]\tAll Loss: 0.8830\tTriple Loss(1): 0.0189\tClassification Loss: 0.8453\r\n",
      "Train Epoch: 40 [13440/209539 (6%)]\tAll Loss: 1.0478\tTriple Loss(1): 0.0253\tClassification Loss: 0.9973\r\n",
      "Train Epoch: 40 [14080/209539 (7%)]\tAll Loss: 1.3472\tTriple Loss(1): 0.0351\tClassification Loss: 1.2769\r\n",
      "Train Epoch: 40 [14720/209539 (7%)]\tAll Loss: 1.2176\tTriple Loss(1): 0.0000\tClassification Loss: 1.2176\r\n",
      "Train Epoch: 40 [15360/209539 (7%)]\tAll Loss: 1.0595\tTriple Loss(1): 0.0118\tClassification Loss: 1.0360\r\n",
      "Train Epoch: 40 [16000/209539 (8%)]\tAll Loss: 1.4347\tTriple Loss(1): 0.0083\tClassification Loss: 1.4182\r\n",
      "Train Epoch: 40 [16640/209539 (8%)]\tAll Loss: 1.5533\tTriple Loss(1): 0.0178\tClassification Loss: 1.5177\r\n",
      "Train Epoch: 40 [17280/209539 (8%)]\tAll Loss: 1.3347\tTriple Loss(1): 0.0242\tClassification Loss: 1.2863\r\n",
      "Train Epoch: 40 [17920/209539 (9%)]\tAll Loss: 1.4588\tTriple Loss(0): 0.0901\tClassification Loss: 1.2785\r\n",
      "Train Epoch: 40 [18560/209539 (9%)]\tAll Loss: 1.1346\tTriple Loss(1): 0.0000\tClassification Loss: 1.1346\r\n",
      "Train Epoch: 40 [19200/209539 (9%)]\tAll Loss: 1.1009\tTriple Loss(1): 0.0059\tClassification Loss: 1.0891\r\n",
      "Train Epoch: 40 [19840/209539 (9%)]\tAll Loss: 1.7845\tTriple Loss(0): 0.3017\tClassification Loss: 1.1811\r\n",
      "Train Epoch: 40 [20480/209539 (10%)]\tAll Loss: 1.0513\tTriple Loss(1): 0.0314\tClassification Loss: 0.9884\r\n",
      "Train Epoch: 40 [21120/209539 (10%)]\tAll Loss: 1.3418\tTriple Loss(1): 0.0079\tClassification Loss: 1.3261\r\n",
      "Train Epoch: 40 [21760/209539 (10%)]\tAll Loss: 1.0808\tTriple Loss(1): 0.0517\tClassification Loss: 0.9775\r\n",
      "Train Epoch: 40 [22400/209539 (11%)]\tAll Loss: 0.9821\tTriple Loss(1): 0.0158\tClassification Loss: 0.9506\r\n",
      "Train Epoch: 40 [23040/209539 (11%)]\tAll Loss: 1.4096\tTriple Loss(0): 0.1883\tClassification Loss: 1.0331\r\n",
      "Train Epoch: 40 [23680/209539 (11%)]\tAll Loss: 0.9565\tTriple Loss(1): 0.0371\tClassification Loss: 0.8824\r\n",
      "Train Epoch: 40 [24320/209539 (12%)]\tAll Loss: 1.4345\tTriple Loss(1): 0.0332\tClassification Loss: 1.3681\r\n",
      "Train Epoch: 40 [24960/209539 (12%)]\tAll Loss: 1.1327\tTriple Loss(1): 0.0000\tClassification Loss: 1.1327\r\n",
      "Train Epoch: 40 [25600/209539 (12%)]\tAll Loss: 1.0832\tTriple Loss(1): 0.0119\tClassification Loss: 1.0594\r\n",
      "Train Epoch: 40 [26240/209539 (13%)]\tAll Loss: 1.4410\tTriple Loss(0): 0.2603\tClassification Loss: 0.9204\r\n",
      "Train Epoch: 40 [26880/209539 (13%)]\tAll Loss: 1.1853\tTriple Loss(1): 0.0000\tClassification Loss: 1.1853\r\n",
      "Train Epoch: 40 [27520/209539 (13%)]\tAll Loss: 1.7650\tTriple Loss(0): 0.3467\tClassification Loss: 1.0716\r\n",
      "Train Epoch: 40 [28160/209539 (13%)]\tAll Loss: 1.0047\tTriple Loss(1): 0.0004\tClassification Loss: 1.0039\r\n",
      "Train Epoch: 40 [28800/209539 (14%)]\tAll Loss: 1.5090\tTriple Loss(1): 0.0000\tClassification Loss: 1.5090\r\n",
      "Train Epoch: 40 [29440/209539 (14%)]\tAll Loss: 1.2070\tTriple Loss(1): 0.0101\tClassification Loss: 1.1868\r\n",
      "Train Epoch: 40 [30080/209539 (14%)]\tAll Loss: 1.0797\tTriple Loss(1): 0.0000\tClassification Loss: 1.0797\r\n",
      "Train Epoch: 40 [30720/209539 (15%)]\tAll Loss: 1.0872\tTriple Loss(1): 0.0422\tClassification Loss: 1.0027\r\n",
      "Train Epoch: 40 [31360/209539 (15%)]\tAll Loss: 1.2299\tTriple Loss(1): 0.0665\tClassification Loss: 1.0969\r\n",
      "Train Epoch: 40 [32000/209539 (15%)]\tAll Loss: 1.5443\tTriple Loss(0): 0.1897\tClassification Loss: 1.1649\r\n",
      "Train Epoch: 40 [32640/209539 (16%)]\tAll Loss: 1.2966\tTriple Loss(1): 0.0808\tClassification Loss: 1.1350\r\n",
      "Train Epoch: 40 [33280/209539 (16%)]\tAll Loss: 1.4803\tTriple Loss(0): 0.1583\tClassification Loss: 1.1636\r\n",
      "Train Epoch: 40 [33920/209539 (16%)]\tAll Loss: 1.1602\tTriple Loss(1): 0.0164\tClassification Loss: 1.1274\r\n",
      "Train Epoch: 40 [34560/209539 (16%)]\tAll Loss: 1.4170\tTriple Loss(0): 0.2209\tClassification Loss: 0.9752\r\n",
      "Train Epoch: 40 [35200/209539 (17%)]\tAll Loss: 1.0154\tTriple Loss(1): 0.0025\tClassification Loss: 1.0104\r\n",
      "Train Epoch: 40 [35840/209539 (17%)]\tAll Loss: 1.2070\tTriple Loss(1): 0.0310\tClassification Loss: 1.1450\r\n",
      "Train Epoch: 40 [36480/209539 (17%)]\tAll Loss: 0.8140\tTriple Loss(1): 0.0124\tClassification Loss: 0.7893\r\n",
      "Train Epoch: 40 [37120/209539 (18%)]\tAll Loss: 1.4665\tTriple Loss(1): 0.0386\tClassification Loss: 1.3893\r\n",
      "Train Epoch: 40 [37760/209539 (18%)]\tAll Loss: 1.0781\tTriple Loss(1): 0.0000\tClassification Loss: 1.0781\r\n",
      "Train Epoch: 40 [38400/209539 (18%)]\tAll Loss: 1.8017\tTriple Loss(0): 0.2169\tClassification Loss: 1.3679\r\n"
     ]
    }
   ],
   "source": [
    "# with_eastern. Freeze=False. lr=0.01\n",
    "! python train.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\r\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\r\n",
      "Loading model freeze=False/lr=0.001/29_epochs\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\r\n",
      "  warnings.warn(msg, category=FutureWarning)\r\n",
      "/home/ma02526/anaconda3/envs/deep-fashion-retrieval/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\r\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\r\n",
      "len(triplet_in_shop_loader): 63\r\n",
      "Loading model freeze=False/lr=0.001/29_epochs\r\n",
      "Train Epoch: 30 [0/209539 (0%)]\tAll Loss: 2.7167\tTriple Loss(1): 0.3659\tClassification Loss: 1.9850\r\n",
      "train.py:222: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\r\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"train.py\", line 330, in <module>\r\n",
      "    train(epoch)\r\n",
      "  File \"train.py\", line 196, in train\r\n",
      "    test(step_no, full=True)\r\n",
      "  File \"train.py\", line 221, in test\r\n",
      "    data, target = data.cuda(GPU_ID), target.cuda(GPU_ID)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "! python train.py\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to /home/ma02526/ResNet/base/Button-Down/2ce22b7dda277475b4d64fb7065d7dec3ec44583.jpg from /home/ma02526/ResNet/base/scrapped/furor/2ce22b7dda277475b4d64fb7065d7dec3ec44583.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/4ea3e8eeb318c87ce236e2649f6ae6f298119c38.jpg from /home/ma02526/ResNet/base/scrapped/furor/4ea3e8eeb318c87ce236e2649f6ae6f298119c38.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/05c6fcb9733e1e507b3a020db963ac7bbaad99c9.jpg from /home/ma02526/ResNet/base/scrapped/furor/05c6fcb9733e1e507b3a020db963ac7bbaad99c9.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/6a2cb464153d8c371c6d9f1237c65844c00fb91c.jpg from /home/ma02526/ResNet/base/scrapped/furor/6a2cb464153d8c371c6d9f1237c65844c00fb91c.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/6b709976fc59cbae2a46efc5d38cb1e5f49d6ac1.jpg from /home/ma02526/ResNet/base/scrapped/furor/6b709976fc59cbae2a46efc5d38cb1e5f49d6ac1.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/7e614948ea71fc411fabe6251407bca8537d4cb2.jpg from /home/ma02526/ResNet/base/scrapped/furor/7e614948ea71fc411fabe6251407bca8537d4cb2.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/08eb67440fe2e1607a1cf4aa44ade94dfb7293b7.jpg from /home/ma02526/ResNet/base/scrapped/furor/08eb67440fe2e1607a1cf4aa44ade94dfb7293b7.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/9c042435b6aadc00d8058ff9f67ec8f73e316d64.jpg from /home/ma02526/ResNet/base/scrapped/furor/9c042435b6aadc00d8058ff9f67ec8f73e316d64.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/9d4add5db812dd105218f18d34d495216f01e0a7.jpg from /home/ma02526/ResNet/base/scrapped/furor/9d4add5db812dd105218f18d34d495216f01e0a7.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/17b1e22a9e655a37b65c94d07cba9b7ec3ca1cea.jpg from /home/ma02526/ResNet/base/scrapped/furor/17b1e22a9e655a37b65c94d07cba9b7ec3ca1cea.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/52d4a41cb602ef654531c7962cd41b3f4db77bda.jpg from /home/ma02526/ResNet/base/scrapped/furor/52d4a41cb602ef654531c7962cd41b3f4db77bda.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/324a838358d533bf39fd48b639a045611110292f.jpg from /home/ma02526/ResNet/base/scrapped/furor/324a838358d533bf39fd48b639a045611110292f.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/381c000ec1d1c35b653d7c41fe697042c075b2e2.jpg from /home/ma02526/ResNet/base/scrapped/furor/381c000ec1d1c35b653d7c41fe697042c075b2e2.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/447e2787123d8c7fc1ec2f1f2e1bfd5758861d45.jpg from /home/ma02526/ResNet/base/scrapped/furor/447e2787123d8c7fc1ec2f1f2e1bfd5758861d45.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/546bf8d0c5f587816f9b866f435b753564e07f82.jpg from /home/ma02526/ResNet/base/scrapped/furor/546bf8d0c5f587816f9b866f435b753564e07f82.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/785ec85fb36ece7ec24d570438b6593980f4eb69.jpg from /home/ma02526/ResNet/base/scrapped/furor/785ec85fb36ece7ec24d570438b6593980f4eb69.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/858a7285c73553818e3eabd09a3fd23f2ebe4c0e.jpg from /home/ma02526/ResNet/base/scrapped/furor/858a7285c73553818e3eabd09a3fd23f2ebe4c0e.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/952d3c80e9eb651e8a575bd1b62e14f94a65d810.jpg from /home/ma02526/ResNet/base/scrapped/furor/952d3c80e9eb651e8a575bd1b62e14f94a65d810.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/5188cbddbc5b792ed3ee4067b90c7a0f6f682010.jpg from /home/ma02526/ResNet/base/scrapped/furor/5188cbddbc5b792ed3ee4067b90c7a0f6f682010.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/6275cc9c9625295184c5297845cd894c20e5c82a.jpg from /home/ma02526/ResNet/base/scrapped/furor/6275cc9c9625295184c5297845cd894c20e5c82a.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/9293f81e7066cba7484100e839515b0059795712.jpg from /home/ma02526/ResNet/base/scrapped/furor/9293f81e7066cba7484100e839515b0059795712.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/37929decf3f7e4e2fd1c77c03c00d95a93c57ba3.jpg from /home/ma02526/ResNet/base/scrapped/furor/37929decf3f7e4e2fd1c77c03c00d95a93c57ba3.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/69501b4f3babdc33cac9d6572c2a6a7ef7ca9d76.jpg from /home/ma02526/ResNet/base/scrapped/furor/69501b4f3babdc33cac9d6572c2a6a7ef7ca9d76.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/919276e2e47b78fa4b5001c6793de747cfc86ca9.jpg from /home/ma02526/ResNet/base/scrapped/furor/919276e2e47b78fa4b5001c6793de747cfc86ca9.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/370454332d01a5590ed062b459bdfb8a79365fe0.jpg from /home/ma02526/ResNet/base/scrapped/furor/370454332d01a5590ed062b459bdfb8a79365fe0.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/a71b704fbbb3361b9e8e22f068b2a2e312fe8277.jpg from /home/ma02526/ResNet/base/scrapped/furor/a71b704fbbb3361b9e8e22f068b2a2e312fe8277.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/a253af18ff88fa7ff57ce362669718c58253a553.jpg from /home/ma02526/ResNet/base/scrapped/furor/a253af18ff88fa7ff57ce362669718c58253a553.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/a744f19a0507234b8091979e2f7093f0008b58d5.jpg from /home/ma02526/ResNet/base/scrapped/furor/a744f19a0507234b8091979e2f7093f0008b58d5.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/abe3cb42a807f5173a7f19af97526ff7f55e8adc.jpg from /home/ma02526/ResNet/base/scrapped/furor/abe3cb42a807f5173a7f19af97526ff7f55e8adc.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/ad9f1c67eb486afd88101ae414629165fecec7e5.jpg from /home/ma02526/ResNet/base/scrapped/furor/ad9f1c67eb486afd88101ae414629165fecec7e5.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/b5bdbc9b9aaf2684cc2b8c0744fb4587ef19ab9c.jpg from /home/ma02526/ResNet/base/scrapped/furor/b5bdbc9b9aaf2684cc2b8c0744fb4587ef19ab9c.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/b8c4a9f1fa3b3f680d1ab7a2684ad2fdc74d8d6b.jpg from /home/ma02526/ResNet/base/scrapped/furor/b8c4a9f1fa3b3f680d1ab7a2684ad2fdc74d8d6b.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/bad684434f098bd7422e336cb3273879a1ee33a9.jpg from /home/ma02526/ResNet/base/scrapped/furor/bad684434f098bd7422e336cb3273879a1ee33a9.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/bf073a943f20944af973158da88ace7727e22e27.jpg from /home/ma02526/ResNet/base/scrapped/furor/bf073a943f20944af973158da88ace7727e22e27.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/bf5206b35a29802d44a8a7e09b61226da69862ef.jpg from /home/ma02526/ResNet/base/scrapped/furor/bf5206b35a29802d44a8a7e09b61226da69862ef.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/c07bd861a4bd26d2c19e32779a126db92d932179.jpg from /home/ma02526/ResNet/base/scrapped/furor/c07bd861a4bd26d2c19e32779a126db92d932179.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/c31ad3e7c3fb827f313c3a882791a806f90c9924.jpg from /home/ma02526/ResNet/base/scrapped/furor/c31ad3e7c3fb827f313c3a882791a806f90c9924.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/e43913d8198600315ecf880d715c21f6d96147da.jpg from /home/ma02526/ResNet/base/scrapped/furor/e43913d8198600315ecf880d715c21f6d96147da.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/eb809ca432edac18ded7a0bdfa0ece3d1403345f.jpg from /home/ma02526/ResNet/base/scrapped/furor/eb809ca432edac18ded7a0bdfa0ece3d1403345f.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/ec26d9fb411a1e85d7d772f86879a241b9af3bab.jpg from /home/ma02526/ResNet/base/scrapped/furor/ec26d9fb411a1e85d7d772f86879a241b9af3bab.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/f6bb631f717d65100625c6a976d2c2947ac3ef99.jpg from /home/ma02526/ResNet/base/scrapped/furor/f6bb631f717d65100625c6a976d2c2947ac3ef99.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/f043c9f7bc4cc6b8c1776a2d3288ab686428a96a.jpg from /home/ma02526/ResNet/base/scrapped/furor/f043c9f7bc4cc6b8c1776a2d3288ab686428a96a.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/ffd1148852b11ea9c62b6248cea4528d8ca24af4.jpg from /home/ma02526/ResNet/base/scrapped/furor/ffd1148852b11ea9c62b6248cea4528d8ca24af4.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/2dbb63e5c2924af330c585b26804b0ae0d60c390.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/2dbb63e5c2924af330c585b26804b0ae0d60c390.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/3bd459531131678c65d73ebe3b0777f3fd707476.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/3bd459531131678c65d73ebe3b0777f3fd707476.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/3ed1345c4c974601ebe1f35791bacbc5281f4923.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/3ed1345c4c974601ebe1f35791bacbc5281f4923.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/6c49283254d01a9d4b28db3fe58c4701b9a7c224.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/6c49283254d01a9d4b28db3fe58c4701b9a7c224.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/6e525675d3c2baee7435b6828daea0d51b4a202a.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/6e525675d3c2baee7435b6828daea0d51b4a202a.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/9c14f51e18538f496f6be9cf1d0a1bc0d417c12c.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/9c14f51e18538f496f6be9cf1d0a1bc0d417c12c.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/31a55026f359805537df1ed84de6ee04a1eb2680.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/31a55026f359805537df1ed84de6ee04a1eb2680.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/37aa47c37cd40a3cf3c62791bd9bcfd9fd4612e2.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/37aa47c37cd40a3cf3c62791bd9bcfd9fd4612e2.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/57c6b73d1f42304ff8c6f72aedc45281e9076c61.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/57c6b73d1f42304ff8c6f72aedc45281e9076c61.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/74cf2fe8702f5615c39f92f5d73ac9dfd28b5822.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/74cf2fe8702f5615c39f92f5d73ac9dfd28b5822.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/88a7d17948cc3ff2f5de811e4ec4e3e657157a5f.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/88a7d17948cc3ff2f5de811e4ec4e3e657157a5f.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/90f411c18e546495eb9aa29f1eaf4cc14e5af468.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/90f411c18e546495eb9aa29f1eaf4cc14e5af468.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/94b426d452f553482e33a5a63586d2b85725a5e4.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/94b426d452f553482e33a5a63586d2b85725a5e4.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/8245fb85d684dd012e16eba6106eae2dccfb47ca.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/8245fb85d684dd012e16eba6106eae2dccfb47ca.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/238221cfcf0d2e5dd90bed7f413bb0d1627c1f28.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/238221cfcf0d2e5dd90bed7f413bb0d1627c1f28.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/923755b90c5641258e84b584d258ebfa8664ef08.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/923755b90c5641258e84b584d258ebfa8664ef08.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/4738532d8ab99fdafba70f40906264d1f0975253.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/4738532d8ab99fdafba70f40906264d1f0975253.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/8676554562afda34889dd428d1557040db0eb7c9.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/8676554562afda34889dd428d1557040db0eb7c9.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/afb9ad606aca0a863f042ad4a0a5e46907847df9.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/afb9ad606aca0a863f042ad4a0a5e46907847df9.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/b639891dd79f11dbc5834bd2658a9b7d4c12f3b7.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/b639891dd79f11dbc5834bd2658a9b7d4c12f3b7.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/bea3d7219fe691ac816dc983497466cc9c8e0299.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/bea3d7219fe691ac816dc983497466cc9c8e0299.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/c0ece2fc2a51962bdcaf4310c8e25e150e6d936c.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/c0ece2fc2a51962bdcaf4310c8e25e150e6d936c.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/cf8dafdcedf70f477927ccd1d4566cf7a8fa544f.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/cf8dafdcedf70f477927ccd1d4566cf7a8fa544f.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/d2f158ff302d2010ed273797eabd85a5ba4a6bb0.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/d2f158ff302d2010ed273797eabd85a5ba4a6bb0.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/df7335827b63abedc94b26da7afdf4806552ede6.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/df7335827b63abedc94b26da7afdf4806552ede6.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/e1030ce2e3039d26739249ba2cb1c0a1708ae323.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/e1030ce2e3039d26739249ba2cb1c0a1708ae323.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/e01625b41838fbd710435cb772f8fe07a512eb03.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/e01625b41838fbd710435cb772f8fe07a512eb03.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/f028a009f20ff582bb7505c05bb4ec3bac35b313.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/f028a009f20ff582bb7505c05bb4ec3bac35b313.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/f98a2d7f6ed39ff1d252118cf5cf0efc9c2a53a8.jpg from /home/ma02526/ResNet/base/scrapped/zellbury/f98a2d7f6ed39ff1d252118cf5cf0efc9c2a53a8.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/1b3bc4766f31be2f0b1e09ae43626001ffedcd34.jpg from /home/ma02526/ResNet/base/scrapped/elo/1b3bc4766f31be2f0b1e09ae43626001ffedcd34.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/06a7cace70f8b9c5a8097212742238efad7550a6.jpg from /home/ma02526/ResNet/base/scrapped/elo/06a7cace70f8b9c5a8097212742238efad7550a6.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/37a14fa03825ffac851cd6fdfbe4ac98f740b0b1.jpg from /home/ma02526/ResNet/base/scrapped/elo/37a14fa03825ffac851cd6fdfbe4ac98f740b0b1.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/99b0fd70c87aa860585d308342b55d08129c5b02.jpg from /home/ma02526/ResNet/base/scrapped/elo/99b0fd70c87aa860585d308342b55d08129c5b02.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/207cd38335916b4f006332342901e45f3441ac29.jpg from /home/ma02526/ResNet/base/scrapped/elo/207cd38335916b4f006332342901e45f3441ac29.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/5549fe1bc3fdeeb7feae539ea09b2cfa22f9f9ba.jpg from /home/ma02526/ResNet/base/scrapped/elo/5549fe1bc3fdeeb7feae539ea09b2cfa22f9f9ba.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/18331c7fc71d3b5e6202f0104184099624335c91.jpg from /home/ma02526/ResNet/base/scrapped/elo/18331c7fc71d3b5e6202f0104184099624335c91.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/a6b6bc7c71ed0c8c4ef1882fb4251a173f33e2b7.jpg from /home/ma02526/ResNet/base/scrapped/elo/a6b6bc7c71ed0c8c4ef1882fb4251a173f33e2b7.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/b65ea113d3b31be2b4380cce04a288556a2ea1d0.jpg from /home/ma02526/ResNet/base/scrapped/elo/b65ea113d3b31be2b4380cce04a288556a2ea1d0.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/c4b2308dc03dfdc8044c42975f19a66b3ee0dc44.jpg from /home/ma02526/ResNet/base/scrapped/elo/c4b2308dc03dfdc8044c42975f19a66b3ee0dc44.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/c9ac6076c03b9d34136cf2ed5854e98037f64956.jpg from /home/ma02526/ResNet/base/scrapped/elo/c9ac6076c03b9d34136cf2ed5854e98037f64956.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/d0ade2dfa287d756e270f082d10994e367b4a156.jpg from /home/ma02526/ResNet/base/scrapped/elo/d0ade2dfa287d756e270f082d10994e367b4a156.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/d5c7b71a36c10784174edc363b2fdeaefd81ac74.jpg from /home/ma02526/ResNet/base/scrapped/elo/d5c7b71a36c10784174edc363b2fdeaefd81ac74.jpg\n",
      "File copied to /home/ma02526/ResNet/base/Button-Down/d617b11e08332c6750a65b337e187aecd065fa3a.jpg from /home/ma02526/ResNet/base/scrapped/elo/d617b11e08332c6750a65b337e187aecd065fa3a.jpg\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "target_dir = r\"/home/ma02526/ResNet/base/Button-Down\"\n",
    "filename = r\"/home/ma02526/ResNet/to_copy.txt\"\n",
    "with open(filename) as file:\n",
    "    for line in file.readlines():\n",
    "        pth = os.path.join(DATASET_BASE, line.strip())\n",
    "        if not os.path.exists(pth):\n",
    "            print(f'{pth} doesnt exist')\n",
    "        else:\n",
    "            tgt_path = os.path.join(target_dir, pth.split('/')[-1])\n",
    "            # print(tgt_path)\n",
    "            shutil.copyfile(pth, tgt_path)\n",
    "            print(f'File copied to {tgt_path} from {pth}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# from config import *\n",
    "# import os\n",
    "#\n",
    "# def read_lines(path):\n",
    "#     with open(path) as fin:\n",
    "#         lines = fin.readlines()[1:]\n",
    "#         # lines = list(filter(lambda x: len(x) > 0, lines))\n",
    "#         lines = (line.rstrip() for line in lines)  # All lines including the blank ones\n",
    "#         lines = (line for line in lines if line)  # Non-blank lines\n",
    "#         pairs = list(map(lambda x: x.strip().split(), lines))\n",
    "#     return pairs\n",
    "#\n",
    "# list_btd = r\"/home/ma02526/ResNet/base/scrapped/list_bbox_scrapped_btd.txt\"\n",
    "# list_bbox = os.path.join(DATASET_BASE, r'scrapped', 'list_bbox_scrapped.txt')\n",
    "#\n",
    "# pairs = read_lines(list_bbox)\n",
    "# btd_set = read_lines(list_btd)\n",
    "# btd_set = [x[0] for x in btd_set]\n",
    "# print(btd_set)\n",
    "#\n",
    "# result = []\n",
    "# for pair in pairs:\n",
    "#     if pair[0] in btd_set:\n",
    "#         result.append(pair)\n",
    "#\n",
    "# # print(btd_set)\n",
    "# # print([pair[0] for pair in pairs])\n",
    "#\n",
    "# result = [\" \".join(line) for line in result]\n",
    "#\n",
    "# out_file = r\"btd_bbox.txt\"\n",
    "# with open(out_file, 'w') as fin:\n",
    "#     fin.write(\"\\n\".join(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from config import *\n",
    "from plotcm import plot_confusion_matrix\n",
    "\n",
    "import importlib\n",
    "dfretrieval = importlib.import_module(\"deep-fashion-retrieval/train\")\n",
    "from dfretrieval import get_conf_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# names = ['Anorak', 'Blazer', 'Bomber', 'Button-Down', 'Cardigan',\n",
    "#               'Flannel', 'Henley', 'Hoodie', 'Jacket', 'Jersey', 'Parka',\n",
    "#               'Peacoat', 'Sweater', 'Tank', 'Tee', 'Top', 'Turtleneck',\n",
    "#               'Chinos','Jeans', 'Joggers', 'Shorts', 'Sweatpants',\n",
    "#               'Sweatshorts', 'Trunks', 'Coat', 'Robe']\n",
    "\n",
    "all_categories = ['Anorak', 'Blazer','Blouse', 'Bomber', 'Button-Down', 'Cardigan',\n",
    "              'Flannel', 'Henley', 'Halter', 'Hoodie', 'Jacket', 'Jersey', 'Parka',\n",
    "              'Peacoat', 'Poncho ', 'Sweater', 'Tank', 'Tee', 'Top', 'Turtleneck', 'Capris',\n",
    "              'Chinos','Culottes', 'Cutoffs', 'Gauchos', 'Jeans', 'Jeggings', 'Jodhpurs', 'Joggers', 'Leggings',\n",
    "                  'Sarong', 'Shorts', 'Skirt', 'Sweatpants',\n",
    "              'Sweatshorts', 'Trunks', 'Caftan', 'Eastern', 'Coat', 'Coverup', 'Dress', 'Jumpsuit', 'Kaftan', 'Kimono',\n",
    "                  'Nightdress', 'Onesie', 'Robe', 'Romper', 'Shirtdress', 'Sundress']\n",
    "\n",
    "cm = get_conf_matrix()\n",
    "print(cm.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HLtMI6lYCGip",
    "colab_type": "code",
    "outputId": "4a176329-4b70-43c8-d737-a215fc2d3bbb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1586706743938,
     "user_tz": -300,
     "elapsed": 4738,
     "user": {
      "displayName": "Muhammad Ali",
      "photoUrl": "",
      "userId": "15673831022739340207"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plot_confusion_matrix(cm, all_categories)\n",
    "plt.savefig('with_eastern_conf_matrix.png')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[   0    0    0    0    1    0    0    1   35    0    0    0    0    0\n",
      "     1    0    0    0    0    0    0    0    0    0    1    0]\n",
      " [   0  590    0    0  294    1    0    0  691    0    0    0   19   37\n",
      "   317   15    0    0   26    6   79    0    0    0   12    0]\n",
      " [   0    1    0    0    3    0    0    0   64    0    0    0    0    0\n",
      "     8    0    0    0    1    0    2    0    0    0    1    0]\n",
      " [   0    0    0    3    5    2    0    0   21    0    0    0    2    4\n",
      "    39    1    0    0    2    0    6    0    0    0    2    0]\n",
      " [   0   73    0    0 1756    0    0    5  521    0    0    0  261   76\n",
      "   772   50    0    0   31   18  128    1    0    0   13    0]\n",
      " [   0    0    0    2    8    5    0    1   10    0    0    0    3    1\n",
      "    58    3    0    0    0    2    7    0    0    0    0    0]\n",
      " [   0    5    0    0    9    0    0    8   15    0    0    0    9    1\n",
      "   138    1    0    0    2    4    3    0    0    0    0    0]\n",
      " [   0    2    0    0   89    0    0  188  211    0    0    0   72   24\n",
      "   506    6    0    0    6    9   25    0    0    0    0    0]\n",
      " [   0  147    0    0  322    0    0   19 1792    0    3    0   21   35\n",
      "   391   29    0    0   37   16   73    0    0    0   34    0]\n",
      " [   0    1    0    0    2    0    0    1   14    0    0    0    0   20\n",
      "   174    0    0    0    0    2    0    0    0    0    0    0]\n",
      " [   0    2    0    0   11    0    0    0  136    0    6    0    2    0\n",
      "     5    0    0    0    1    0    3    0    0    0   19    0]\n",
      " [   0    4    0    0    3    0    0    0   19    0    0    0    0    0\n",
      "     2    0    0    0    0    0    1    0    0    0    5    0]\n",
      " [   0   19    0    0  671    0    0   15  196    0    0    0 1140   81\n",
      "  1308   42    0    0   33    9   89    0    0    0    3    0]\n",
      " [   0    8    0    0   46    0    0    1   36    0    0    0   18 2499\n",
      "  1295   40    0    0   20   14  246    0    0    0    2    0]\n",
      " [   0   22    0    3  160    0    0   17  222    0    0    0  111  665\n",
      "  8501   64    0    0   75   37  353    0    0    0    4    0]\n",
      " [   0   11    0    0  144    0    0    1  102    1    0    0   73  553\n",
      "  1492  124    0    0   21   37  246    1    0    0    2    0]\n",
      " [   0    0    0    0    7    0    0    1    1    0    0    0   14    1\n",
      "    17    2    0    0    0    0    4    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    2    0    0    0    0    1\n",
      "    11    0    0   19   29   65   24    0    0    0    1    0]\n",
      " [   0    1    0    0   22    0    0    0   61    0    0    0   10   13\n",
      "   208    2    0    2 1368  204   54    5    0    0    0    0]\n",
      " [   0    5    0    0   23    0    0    0   40    0    0    0    4   14\n",
      "   170    6    0    5  184  617   79    9    0    0    0    0]\n",
      " [   0    9    0    0   49    0    0    0   94    0    0    0   22  295\n",
      "   772   21    0    1   77   86 4037    4    0    2    2    0]\n",
      " [   0    3    0    0   13    0    0    1   19    0    0    0    2    6\n",
      "   130    0    0    8  106  419   62   55    0    0    0    0]\n",
      " [   0    0    0    0    8    0    0    0    4    0    0    0    4    2\n",
      "    61    0    0    0    6   69  163    6    2    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
      "     5    0    0    0    0    0   87    0    0    6    0    0]\n",
      " [   0   28    0    0   90    0    0    0  251    0    1    0    2    2\n",
      "    30    2    0    0    2    1    8    0    0    0  164    0]\n",
      " [   0    1    0    1   10    0    0    0    5    0    0    0    0    5\n",
      "    12    5    0    0    0    0    3    0    0    0    1    0]]\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": [],
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "16Q2YK_uS-jI",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# os.path.exists('//content/drive/My Drive/Deep Fashion Retrieval/base/img/Leaf_Print-Sleeve_Tee/img_00000031.jpg')\n"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'May30_00-44-53'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  }
 ]
}